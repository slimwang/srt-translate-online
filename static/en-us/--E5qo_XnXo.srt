1
00:00:00,470 --> 00:00:01,960
Now that you have trained
your first model.

2
00:00:01,960 --> 00:00:04,790
There is something very
important I want to discuss.

3
00:00:04,790 --> 00:00:08,109
You might have seen in the assignment
that we had a training set

4
00:00:08,109 --> 00:00:11,510
as well as a validation set and
a test set.

5
00:00:11,510 --> 00:00:13,280
What is that all about?

6
00:00:13,280 --> 00:00:17,090
Don't skip that part, it has to do
with measuring how well you're doing,

7
00:00:17,090 --> 00:00:19,450
without accidentally shooting
yourself in the foot.

8
00:00:19,450 --> 00:00:22,280
And it is a lot more subtle
than you might initially think.

9
00:00:22,280 --> 00:00:26,330
It's also very important because as
we'll discover later, once you know how

10
00:00:26,330 --> 00:00:30,780
to measure your performance on a problem
you've already solved half of it.

11
00:00:30,780 --> 00:00:33,540
Let me explain why measuring
performance is subtle.

12
00:00:33,540 --> 00:00:35,860
Let's go back to our
classification task.

13
00:00:35,860 --> 00:00:38,430
You've got a whole lot
of images with labels.

14
00:00:38,430 --> 00:00:42,540
You could say, okay, I'm going to run
my classifier on those images and

15
00:00:42,540 --> 00:00:44,150
see how many I got right.

16
00:00:44,150 --> 00:00:47,790
That's my error measure, and then you
go out and use your classifier on new

17
00:00:47,790 --> 00:00:51,080
images, images that you've
never seen in the past.

18
00:00:51,080 --> 00:00:53,240
And you measure how many you get right,
and

19
00:00:53,240 --> 00:00:56,910
your performance gets worse,
the classifier doesn't do as well.

20
00:00:56,910 --> 00:00:58,330
So what happened?

21
00:00:58,330 --> 00:01:02,601
Well imagine I construct a classifier
that simply compares the new image to

22
00:01:02,601 --> 00:01:06,598
any of the other images that I've
already seen in my training set, and

23
00:01:06,598 --> 00:01:08,520
just return the label.

24
00:01:08,520 --> 00:01:11,480
By the measure we defined earlier,
it's a great classifier.

25
00:01:11,480 --> 00:01:14,660
It would get 100% accuracy
on the training sets.

26
00:01:14,660 --> 00:01:19,820
But as soon as it sees a new image
it's lost, it has no idea what to do.

27
00:01:19,820 --> 00:01:21,570
It's not a great classifier.

28
00:01:21,570 --> 00:01:25,425
The problem is that your classifier
has memorized the training set and

29
00:01:25,425 --> 00:01:28,070
it fails to generalize to new examples.

30
00:01:28,070 --> 00:01:30,260
It's not just a theoretical problem.

31
00:01:30,260 --> 00:01:33,380
Every classifier that you will
build will tend to try and

32
00:01:33,380 --> 00:01:35,230
memorize the training sets.

33
00:01:35,230 --> 00:01:37,720
And it will usually do that very,
very well.

34
00:01:37,720 --> 00:01:42,260
Your job, though, is to help it
generalize to new data instead.

35
00:01:42,260 --> 00:01:45,550
So, how do we measure
the generalization instead of measuring

36
00:01:45,550 --> 00:01:48,570
how well the classifier
memorized the data?

37
00:01:48,570 --> 00:01:52,610
The simplest way is to take a small
subset of the training set.

38
00:01:52,610 --> 00:01:56,070
Not use it in training and
measure the error on that test data.

39
00:01:57,170 --> 00:01:59,970
Problem solved,
now your classifier cannot cheat

40
00:01:59,970 --> 00:02:03,670
because it never sees the test data so
it can't memorize it.

41
00:02:03,670 --> 00:02:07,050
But there is still a problem because
training a classifier is usually

42
00:02:07,050 --> 00:02:08,930
a process of trial and error.

43
00:02:08,930 --> 00:02:12,170
You try a classifier,
you measure its performance, and

44
00:02:12,170 --> 00:02:14,840
then you try another one,
and you measure again.

45
00:02:14,840 --> 00:02:16,630
And another, and another.

46
00:02:16,630 --> 00:02:20,610
You tweak the model, you explore
the parameters, you measure, and

47
00:02:20,610 --> 00:02:24,780
finally, you have what you think
is the prefect classifier.

48
00:02:24,780 --> 00:02:28,480
And then after all this care you've
taken to separate your test data from

49
00:02:28,480 --> 00:02:32,600
your training data and only measuring
your performance on the test data,

50
00:02:32,600 --> 00:02:35,520
now you deploy your system in
a real production environment.

51
00:02:35,520 --> 00:02:36,910
And you get more data.

52
00:02:36,910 --> 00:02:39,500
And you score your performance
on that new data, and

53
00:02:39,500 --> 00:02:41,990
it doesn't do nearly as well.

54
00:02:41,990 --> 00:02:43,960
What can possibly have happened?

55
00:02:43,960 --> 00:02:47,580
What happened is that your
classifier has seen your test data

56
00:02:47,580 --> 00:02:49,890
indirectly through your own eyes.

57
00:02:49,890 --> 00:02:53,990
Every time you made a decision about
which classifier to use, which parameter

58
00:02:53,990 --> 00:02:59,050
to tune, you actually gave information
to your classifier about the test set.

59
00:02:59,050 --> 00:03:01,590
Just a tiny bit, but it adds up.

60
00:03:01,590 --> 00:03:03,540
So over time, as you run many and

61
00:03:03,540 --> 00:03:07,620
many experiments, your test data
bleeds into your training data.

62
00:03:07,620 --> 00:03:09,140
So what can you do?

63
00:03:09,140 --> 00:03:11,100
There are many ways to deal with this.

64
00:03:11,100 --> 00:03:12,590
I'll give you the simplest one.

65
00:03:12,590 --> 00:03:16,320
Take another chunk of your training
sets and hide it under a rock.

66
00:03:16,320 --> 00:03:19,330
Never look at it until you
have made your final decision.

67
00:03:19,330 --> 00:03:23,440
You can use your validation set
to measure your actual error and

68
00:03:23,440 --> 00:03:26,330
maybe the validation set will
bleed into the training set.

69
00:03:26,330 --> 00:03:29,850
But that's okay because you're
always have this test set

70
00:03:29,850 --> 00:03:32,560
that you can rely on to actually
measure your real performance
