1
00:00:00,180 --> 00:00:02,640
Here is the implementation of
the queuing lock or the so

2
00:00:02,640 --> 00:00:04,100
called Anderson lock.

3
00:00:04,100 --> 00:00:08,230
The lock is an array and the array
elements have values has lock or

4
00:00:08,230 --> 00:00:09,220
must wait.

5
00:00:09,220 --> 00:00:12,910
Initially the first element of
the array has the value has lock and

6
00:00:12,910 --> 00:00:15,510
all the other elements
have the value must wait.

7
00:00:15,510 --> 00:00:19,400
Also part of the lock structure
is the global variable queuelast.

8
00:00:19,400 --> 00:00:21,990
To obtain a ticket
into this queueing lock

9
00:00:21,990 --> 00:00:26,870
a process must first perform a read and
increment atomic operation.

10
00:00:26,870 --> 00:00:29,060
That will return the myplace so

11
00:00:29,060 --> 00:00:32,380
the index into that particular
process into the queue.

12
00:00:32,380 --> 00:00:36,000
The process will then continue
to spin on the corresponding

13
00:00:36,000 --> 00:00:40,410
element of the array as long
as its value is must-wait.

14
00:00:40,410 --> 00:00:43,180
For the very first thread
that arrives at this lock,

15
00:00:43,180 --> 00:00:46,360
the value of flags of zero will
be has-lock, so that one will

16
00:00:46,360 --> 00:00:50,590
successfully acquire the lock and
proceed in the critical section.

17
00:00:50,590 --> 00:00:55,450
Every subsequent thread, as long as the
first thread is in the critical section,

18
00:00:55,450 --> 00:00:58,440
will come and
will get tickets that will point to some

19
00:00:58,440 --> 00:01:02,000
elements of the flags array that
are from zero to p minus one.

20
00:01:02,000 --> 00:01:06,380
And they will not be able to proceed,
their value will be must-wait.

21
00:01:06,380 --> 00:01:10,780
When the owner of the lock is done with
the critical section, it will reset its

22
00:01:10,780 --> 00:01:15,300
element in the array, you must wait
in order to get this build ready for

23
00:01:15,300 --> 00:01:19,610
the next threads and next processes
that try to acquire this lock.

24
00:01:19,610 --> 00:01:24,340
Notice that we're using some modular
math in order to wrap around this index

25
00:01:24,340 --> 00:01:27,450
read in increment and
just continue increasing the value

26
00:01:27,450 --> 00:01:30,610
queuelast where as our
array's of limited size.

27
00:01:30,610 --> 00:01:35,270
Releasing the lock means that we
need to change the value of the next

28
00:01:35,270 --> 00:01:36,900
element in the array.

29
00:01:36,900 --> 00:01:42,440
So myplace+1, we will change its value
from must-wait, it will become has-lock.

30
00:01:42,440 --> 00:01:44,650
That means that that thread,
that process,

31
00:01:44,650 --> 00:01:47,990
will now come out of
the spin loop that it's in.

32
00:01:47,990 --> 00:01:52,200
Notice that the atomic in this spin
lock implementation involves a read and

33
00:01:52,200 --> 00:01:55,520
increment on a variable, queuelast.

34
00:01:55,520 --> 00:01:59,610
All of this spinning disc implementation
happens on completely different

35
00:01:59,610 --> 00:02:00,250
variables.

36
00:02:00,250 --> 00:02:02,550
So, the elements of the flags array.

37
00:02:02,550 --> 00:02:06,390
For this reason, issuing the atomic
operation, read an increment.

38
00:02:06,390 --> 00:02:10,600
Any kind of invalidation
traffic is not going to concern

39
00:02:10,600 --> 00:02:14,180
any of the spinning on
the elements of the flags array.

40
00:02:14,180 --> 00:02:17,430
These are two different memory
locations, two different variables.

41
00:02:17,430 --> 00:02:21,850
From a latency perspective, this lock is
not very efficient, because it performs

42
00:02:21,850 --> 00:02:25,250
a more complex atomic operation,
the read and increment.

43
00:02:25,250 --> 00:02:30,365
This read and increment operation takes
more cycles than an atomic test and set.

44
00:02:30,365 --> 00:02:33,525
In addition, it needs to perform
this modular shift in order to

45
00:02:33,525 --> 00:02:35,685
find the right index into the array.

46
00:02:35,685 --> 00:02:39,345
All of that needs to happen before
it can determine whether or

47
00:02:39,345 --> 00:02:42,745
not it has to spin or
be in the critical section.

48
00:02:42,745 --> 00:02:44,555
So the latency is not good.

49
00:02:44,555 --> 00:02:48,960
The delays really good however
when a lock is freed, the next

50
00:02:48,960 --> 00:02:54,690
processor to run is directly signaled
by changing the value of its flag.

51
00:02:54,690 --> 00:02:57,960
Since we're spinning on different
locations, we can afford to spin all

52
00:02:57,960 --> 00:03:03,810
the time, and therefore we can notice
that the value has changed immediately.

53
00:03:03,810 --> 00:03:05,390
From a contention perspective,

54
00:03:05,390 --> 00:03:09,950
this lock is much better than any of the
other alternatives that we discussed.

55
00:03:09,950 --> 00:03:13,440
Since the atomic is only
executed once up front and

56
00:03:13,440 --> 00:03:15,440
it's not part of the spinning code.

57
00:03:15,440 --> 00:03:17,970
Plus, the atomic instructions and
the spinning

58
00:03:17,970 --> 00:03:22,230
are done on different variables, so
the invalidations that are triggered by

59
00:03:22,230 --> 00:03:27,010
the atomic will not affect the processor
stability to spin on local caches.

60
00:03:27,010 --> 00:03:29,220
However, in order for
us to achieve this,

61
00:03:29,220 --> 00:03:33,380
we have to make sure that first we
have a cache coherent architecture.

62
00:03:33,380 --> 00:03:35,840
If we don't have cache
coherent architecture,

63
00:03:35,840 --> 00:03:40,395
this spinning has to happen on
potentially remote memory refer.

64
00:03:40,395 --> 00:03:43,335
Second we also have to make
sure that every element

65
00:03:43,335 --> 00:03:45,365
is in a separate cache line.

66
00:03:45,365 --> 00:03:49,055
Otherwise when we change the value
of one element of the array,

67
00:03:49,055 --> 00:03:51,665
we will invalidate
the entire cache line so

68
00:03:51,665 --> 00:03:55,335
we will invalidate potentially
the caches of the other

69
00:03:55,335 --> 00:03:59,325
elements in the array of the processors
that are spinning on other elements.

70
00:03:59,325 --> 00:04:01,960
And that's clearly not
something we want to achieve.

71
00:04:01,960 --> 00:04:06,410
To summarize the benefits of this lock,
come from the fact that it addresses

72
00:04:06,410 --> 00:04:11,100
the key problem that we mentioned with
the other spin lock implementations.

73
00:04:11,100 --> 00:04:15,210
In that everyone saw hat the log
was free at the same time and

74
00:04:15,210 --> 00:04:18,180
everyone tried to acquire
the lock at the same time.

75
00:04:18,180 --> 00:04:20,470
The queue in lock solves that problem.

76
00:04:20,470 --> 00:04:25,450
By having a separate, essentially
a private, lock in this array of locks,

77
00:04:25,450 --> 00:04:28,830
only one thread at a time sees
that the lock is free, and

78
00:04:28,830 --> 00:04:31,610
only one thread at a time
attempts to acquire the lock.
