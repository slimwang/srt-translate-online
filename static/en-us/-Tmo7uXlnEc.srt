1
00:00:00,100 --> 00:00:03,790
The model of noise we have, and we're going to refer to it as a generative model

2
00:00:03,790 --> 00:00:08,109
because I have a model of how the noise is actually put within the system.

3
00:00:08,109 --> 00:00:11,440
Essentially we have a particular model that we believe in.

4
00:00:11,440 --> 00:00:13,110
Computer vision, you have to have faith.

5
00:00:13,110 --> 00:00:17,450
The model that we believe is, there's actually some real line underneath.

6
00:00:17,450 --> 00:00:21,550
And that each point has been perturbed off that line.

7
00:00:21,550 --> 00:00:23,740
Here it's written as epsilon.

8
00:00:23,740 --> 00:00:28,130
By some Gaussian noise that has moved you off that line.

9
00:00:28,130 --> 00:00:32,360
And the probability of a Gaussian goes down by a function of what?

10
00:00:32,360 --> 00:00:33,280
X squared.

11
00:00:33,280 --> 00:00:37,840
So if you wanted to find the most likely solution, you would want to

12
00:00:37,840 --> 00:00:42,670
find the points that make the data that you're observing most likely.

13
00:00:42,670 --> 00:00:46,970
So those would be the ones where the sum of the square distances are the least

14
00:00:46,970 --> 00:00:48,460
because, and things,

15
00:00:48,460 --> 00:00:51,540
just thinking about the Gaussian, won't go into too much detail.

16
00:00:51,540 --> 00:00:54,960
If you assume all those points were independently done, so the probability of

17
00:00:54,960 --> 00:00:57,950
getting all those points is just the product of those probabilities.

18
00:00:57,950 --> 00:01:00,510
The product of those probabilities is a product of all those Gaussians.

19
00:01:00,510 --> 00:01:03,510
Those Gaussians go e to the minus x squared.

20
00:01:03,510 --> 00:01:07,090
When you multiply all those e to the minus x squared, right?

21
00:01:07,090 --> 00:01:11,910
You just sum up all of those x squareds and so whatever makes that sum of

22
00:01:11,910 --> 00:01:16,788
squared difference the smallest, right, will be the highest probability.

23
00:01:16,788 --> 00:01:17,364
And so,

24
00:01:17,364 --> 00:01:21,760
there's a whole bunch of assumptions going on underneath here in terms of

25
00:01:21,760 --> 00:01:24,680
the probability distributions and being independent and all that kind of stuff.

26
00:01:24,680 --> 00:01:29,990
But basically this idea of minimizing the perpendicular least squares comes from

27
00:01:29,990 --> 00:01:35,400
this idea that closer points are more likely, right?

28
00:01:35,400 --> 00:01:38,810
It's more likely a point will land closer to a line then

29
00:01:38,810 --> 00:01:43,460
further away from that line, if the point actually came from the line.

30
00:01:43,460 --> 00:01:43,990
Got that?

31
00:01:43,990 --> 00:01:47,530
That's going to be important to remember for, for our next lesson.

32
00:01:47,530 --> 00:01:48,190
All right.

33
00:01:48,190 --> 00:01:51,400
So that's written here in this equation.

34
00:01:51,400 --> 00:01:56,490
Basically we're saying the point x y that we actually measure is just some

35
00:01:57,840 --> 00:02:01,520
real point on the line u v, okay?

36
00:02:01,520 --> 00:02:06,066
That has been perturbed in the direction perpendicular to that line, a b,

37
00:02:06,066 --> 00:02:09,590
that's the line that we were solving for.

38
00:02:09,590 --> 00:02:13,522
And that perturbation has been some amount of noise that came from a Gaussian

39
00:02:13,522 --> 00:02:18,220
with a zero mean and some standard deviation sigma or variance sigma squared.

40
00:02:18,220 --> 00:02:20,765
So, for the line fitting or in this case,

41
00:02:20,765 --> 00:02:24,420
a model fit in general, this is our underlying generative model.

42
00:02:24,420 --> 00:02:26,800
That it is a Gaussian perturbation.
