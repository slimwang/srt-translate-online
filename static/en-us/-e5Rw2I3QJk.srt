1
00:00:01,150 --> 00:00:03,469
We'll now talk about TCP congestion

2
00:00:03,469 --> 00:00:07,560
control in the context of modern datacenters.

3
00:00:07,560 --> 00:00:10,610
And we'll talk about a particular TCP

4
00:00:10,610 --> 00:00:13,600
throughput collapse problem called the TCP incast

5
00:00:13,600 --> 00:00:21,100
problem. A typical data center consists of a set of server racks. Each holding

6
00:00:21,100 --> 00:00:26,880
a large number of servers, the switches that connect those racks of servers, and

7
00:00:26,880 --> 00:00:30,690
the connecting links that connect those switches

8
00:00:30,690 --> 00:00:32,920
to other parts of the topology. So the

9
00:00:32,920 --> 00:00:36,900
network architecture is typically made up of some

10
00:00:36,900 --> 00:00:40,990
sort of tree. And, switching elements that progressively

11
00:00:40,990 --> 00:00:43,130
are more specialized and expensive as we move

12
00:00:43,130 --> 00:00:45,940
up the network hierarchy. Some of the characteristics

13
00:00:45,940 --> 00:00:52,180
of a data center network include a high fan in, there is a very high amount

14
00:00:52,180 --> 00:00:58,270
of fan in between the leaves of the tree and the top of the root. Workloads

15
00:00:58,270 --> 00:01:01,600
are high bandwidth and low latency, and many

16
00:01:01,600 --> 00:01:04,640
clients issue requests in parallel, each with a

17
00:01:04,640 --> 00:01:08,430
relatively small amount of data per request. The

18
00:01:08,430 --> 00:01:10,890
other constraint that we face, is that the

19
00:01:10,890 --> 00:01:13,690
buffers in these switches can be quite small.

20
00:01:13,690 --> 00:01:17,420
So when we combine the requirements of high

21
00:01:17,420 --> 00:01:20,800
bandwidth and low latency for the applications,

22
00:01:20,800 --> 00:01:24,010
the presence of many parallel requests coming

23
00:01:24,010 --> 00:01:26,350
from these servers. And the fact that

24
00:01:26,350 --> 00:01:30,160
the switches have relatively small buffers, we

25
00:01:30,160 --> 00:01:31,580
can see that potentially there will be

26
00:01:31,580 --> 00:01:35,180
a problem. The throughput collapse that results

27
00:01:35,180 --> 00:01:39,180
from this phenomenon is called the TCP

28
00:01:39,180 --> 00:01:42,910
Incast problem. Incast is a drastic reduction

29
00:01:42,910 --> 00:01:45,850
in application throughput that results when

30
00:01:45,850 --> 00:01:49,910
servers using TCP. All simultaneously request

31
00:01:49,910 --> 00:01:52,820
data, leading to a gross underutilization

32
00:01:52,820 --> 00:01:56,390
of network capacity in many-to-one communication networks

33
00:01:56,390 --> 00:02:01,570
like a datacenter. The filling up of the buffers here at the switches

34
00:02:01,570 --> 00:02:05,680
result in bursty retransmissions that overfill

35
00:02:05,680 --> 00:02:08,930
the switch buffers. And these bursting retransmissions

36
00:02:08,930 --> 00:02:12,600
are cause by TCP timeouts. The TCP

37
00:02:12,600 --> 00:02:15,570
timeouts can lasts hundreds of milliseconds. But the

38
00:02:15,570 --> 00:02:17,660
roundtrip time in a data center network, is

39
00:02:17,660 --> 00:02:20,840
typically less than a millisecond. Often just hundreds

40
00:02:20,840 --> 00:02:23,790
of microseconds. Because the roundtrip times are so

41
00:02:23,790 --> 00:02:27,030
much less than TCP timeouts. The centers will

42
00:02:27,030 --> 00:02:30,580
have to wait for the TCP timeout. Before

43
00:02:30,580 --> 00:02:34,180
they retransmit. An application throughput can be reduced

44
00:02:34,180 --> 00:02:37,730
by as much as 90% as a result of link idle time.
