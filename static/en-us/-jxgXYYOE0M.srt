1
00:00:00,025 --> 00:00:03,512
So let's recap some of the important lessons from this unit.

2
00:00:03,512 --> 00:00:05,873
If you've made it this far, good for you.

3
00:00:05,873 --> 00:00:07,637
There's a lot of material in this unit,

4
00:00:07,637 --> 00:00:10,080
advanced material that's pushing the frontiers of field,

5
00:00:10,080 --> 00:00:12,625
but that's why you took this class, right?

6
00:00:12,625 --> 00:00:15,238
So here's some of the lessons we learned along the way.

7
00:00:15,238 --> 00:00:16,985
Dense N-body.

8
00:00:16,985 --> 00:00:18,860
We learn 2 main lessons here.

9
00:00:18,860 --> 00:00:22,132
One is the importance of minimizing global memory bandwidth,

10
00:00:22,132 --> 00:00:26,463
and the second, reducing parallelism by increasing the amount of work per thread

11
00:00:26,463 --> 00:00:31,101
might reduce your overall communication costs and hence, your overall run time.

12
00:00:31,101 --> 00:00:33,940
Sparse Matrix Vector Multiply.

13
00:00:33,940 --> 00:00:36,973
The right data structure can make a big difference,

14
00:00:36,973 --> 00:00:39,376
and the keys to high performance with our implementation

15
00:00:39,376 --> 00:00:42,512
was reducing load imbalance between threads,

16
00:00:42,512 --> 00:00:44,180
keeping all of our threads busy

17
00:00:44,180 --> 00:00:47,948
and optimizing to use the most efficient communication possible.

18
00:00:47,948 --> 00:00:50,186
Breadth-first Reversal Traversal.

19
00:00:50,186 --> 00:00:54,891
Choosing the most efficient parallel algorithm is perhaps the most important thing you can do.

20
00:00:54,891 --> 00:00:58,995
For large problems, a superior problem in terms of asymptotic word complexity

21
00:00:58,995 --> 00:01:02,675
will nearly always be even an optimized, more expensive algorithm.

22
00:01:02,675 --> 00:01:05,194
And the real challenge in our optimized algorithm

23
00:01:05,194 --> 00:01:08,836
was handling the irregular expansion and compaction at each step.

24
00:01:08,836 --> 00:01:12,936
Scan is the natural primitive to handle these operations.

25
00:01:12,936 --> 00:01:14,872
List ranking.

26
00:01:14,872 --> 00:01:18,509
This is a good example of a problem that is not inherently parallel.

27
00:01:18,509 --> 00:01:23,923
To solve it well on a GPU, we use the important technique of trading more work for fewer steps.

28
00:01:23,923 --> 00:01:27,449
And once again, we see the power of scan to address a problem

29
00:01:27,449 --> 00:01:31,333
that at first glance seems difficult to parallelize.

30
00:01:31,333 --> 00:01:33,223
And the hash table.

31
00:01:33,223 --> 00:01:36,001
The key insight in the hash table implementation

32
00:01:36,001 --> 00:01:38,395
was recognizing that a serial data structure

33
00:01:38,395 --> 00:01:40,530
was the wrong fit for this problem.

34
00:01:40,530 --> 00:01:42,766
Instead, we used cuckoo hashing,

35
00:01:42,766 --> 00:01:47,577
which was much more parallel friendly.
