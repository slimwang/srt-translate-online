1
00:00:00,270 --> 00:00:00,880
Hey, Charles.

2
00:00:00,880 --> 00:00:01,930
>> Oh, hi Michael.

3
00:00:01,930 --> 00:00:03,570
>> It's funny running into to you here.

4
00:00:03,570 --> 00:00:05,910
>> It is. It's always funny running in to you over the interwebs.

5
00:00:05,910 --> 00:00:08,770
>> So, today, I have the pleasure

6
00:00:08,770 --> 00:00:10,590
of telling you about computation learning theory.

7
00:00:10,590 --> 00:00:12,000
>> Hmm, it's my favorite kind of theory.

8
00:00:12,000 --> 00:00:15,650
>> [LAUGH] Well, sure. Now how do you like theory in general?

9
00:00:15,650 --> 00:00:17,320
>> I am happy that there are peolpe that do theory.

10
00:00:17,320 --> 00:00:19,315
>> There we go. Alright. And now you are about to be

11
00:00:19,315 --> 00:00:21,570
one of those people, at least for the next you know, hour.

12
00:00:21,570 --> 00:00:22,510
>> No, no. I am not doing theory.

13
00:00:22,510 --> 00:00:24,110
You're doing theory. I'm listening to you do theory.

14
00:00:24,110 --> 00:00:25,430
>> No. I am coaching

15
00:00:25,430 --> 00:00:27,430
you through it. Don't you understand? It's about,

16
00:00:27,430 --> 00:00:30,410
it's about the learning process. Learning about learning.

17
00:00:30,410 --> 00:00:31,110
>> Fine.

18
00:00:32,280 --> 00:00:34,850
>> So let's start out with a quiz.

19
00:00:34,850 --> 00:00:36,670
No wait. So let me let me at least set

20
00:00:36,670 --> 00:00:39,440
the stage as to why, what you know, what

21
00:00:39,440 --> 00:00:40,910
it is that we're talking about today and how it's

22
00:00:40,910 --> 00:00:42,960
different from what we've talked about, in the previous

23
00:00:42,960 --> 00:00:44,770
days. So mostly what we've been talking about up to

24
00:00:44,770 --> 00:00:48,170
this point, is, algorithms for doing learning. Alright we talked

25
00:00:48,170 --> 00:00:50,630
about what, what learning, the machine learning field was like.

26
00:00:50,630 --> 00:00:51,980
And we talked about a number of

27
00:00:51,980 --> 00:00:56,210
specific algorithms for building classifiers, and building regression.

28
00:00:56,210 --> 00:01:00,010
And there's a problem with that. Specifically,

29
00:01:00,010 --> 00:01:01,990
I have this, this feeling that it's very,

30
00:01:01,990 --> 00:01:04,120
very necessary to always make sure you

31
00:01:04,120 --> 00:01:06,080
know what problem you're solving, before you start

32
00:01:06,080 --> 00:01:08,610
proposing algorithms for solving it. And we haven't

33
00:01:08,610 --> 00:01:10,830
really nailed down what exactly that problem is.

34
00:01:10,830 --> 00:01:11,510
>> Hm.

35
00:01:11,510 --> 00:01:13,240
>> And that makes things hard. It makes it hard to

36
00:01:13,240 --> 00:01:15,780
know, for example, whether or not one algorithm's better than another.

37
00:01:16,920 --> 00:01:19,600
>> One algorithm's better than another, if it works better.

38
00:01:19,600 --> 00:01:23,380
>> If it works better? Yeah, well, you know, that's not wrong. But

39
00:01:23,380 --> 00:01:24,430
I think it still is not a

40
00:01:24,430 --> 00:01:27,100
very helpful definition for designing better algorithms.

41
00:01:27,100 --> 00:01:27,822
>> Okay.

42
00:01:27,822 --> 00:01:29,130
>> So so we're going to talk

43
00:01:29,130 --> 00:01:31,090
about this computational learning theory. And I want

44
00:01:31,090 --> 00:01:33,164
to say that it's not about particular

45
00:01:33,164 --> 00:01:36,080
algorithms, it's about some other stuff. But I

46
00:01:36,080 --> 00:01:39,150
thought it would be helpful if we started out by saying. well, what is it

47
00:01:39,150 --> 00:01:42,150
that we talked about so far? So one of the things we talked about so far

48
00:01:42,150 --> 00:01:46,525
is algorithms for doing for learning classifiers. So, I

49
00:01:46,525 --> 00:01:48,350
wanted to draw a picture of that, and then I

50
00:01:48,350 --> 00:01:50,560
realized maybe that would be actually a useful quiz.

51
00:01:50,560 --> 00:01:55,680
So, if you can image each of three boxes is

52
00:01:55,680 --> 00:01:58,000
the output of a classifier in a two dimensional

53
00:01:58,000 --> 00:02:00,433
space. So it's a two dimensional input space. We've ran

54
00:02:00,433 --> 00:02:02,780
a leaner, and when it spits out a classifier,

55
00:02:02,780 --> 00:02:06,100
this is how it classifies the the regions of the

56
00:02:06,100 --> 00:02:09,410
space. So I, I use blue lines to separate the

57
00:02:10,538 --> 00:02:13,480
square into regions. The two dimensional space into regions, and

58
00:02:13,480 --> 00:02:15,250
then I labeled each region with a minus or a

59
00:02:15,250 --> 00:02:17,670
plus, so you can tell what the classifier was doing.

60
00:02:17,670 --> 00:02:23,070
>> Okay. So I'm wondering if you could fill in underneath each one,

61
00:02:23,070 --> 00:02:27,850
what learning algorithm was probably used to, to find this classifier?

62
00:02:29,030 --> 00:02:29,380
>> maybe.

63
00:02:29,380 --> 00:02:31,170
>> You want to give it a shot? It's not, it's not

64
00:02:31,170 --> 00:02:32,700
that important to this lecture, but I thought it

65
00:02:32,700 --> 00:02:34,610
might be helpful in just kind of setting the stage.

66
00:02:34,610 --> 00:02:36,180
>> Okay. I'll give it a try.
