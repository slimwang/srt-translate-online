1
00:00:00,560 --> 00:00:05,050
Given that we have chosen a Training and a test set, it is important to

2
00:00:05,050 --> 00:00:11,030
think of how the classifiers perform for a given size of the Training set.

3
00:00:11,030 --> 00:00:14,670
Learning curves are a plot of the model performance,

4
00:00:14,670 --> 00:00:18,640
usually denoted by the risk, the cost or

5
00:00:18,640 --> 00:00:23,360
the score versus the size of the Training set and the test set.

6
00:00:23,360 --> 00:00:28,810
In the case of classifiers we can always use the score or one minus score

7
00:00:28,810 --> 00:00:34,440
versus the Training Set and the Test Set size to draw our learning curves.

8
00:00:34,440 --> 00:00:38,420
The learning curves are useful in a further iteration back to

9
00:00:38,420 --> 00:00:43,990
determining how much data we might need to optimally train the model.

10
00:00:43,990 --> 00:00:49,620
Now let's go to I pod phone notebook and make some learning curve plots.

11
00:00:49,620 --> 00:00:56,750
Up to now, we have trained our models using the X_train and Y_train data sets.

12
00:00:56,750 --> 00:01:00,290
Now let us look at how to draw learning curves.

13
00:01:00,290 --> 00:01:06,300
In this piece of the code, which is taken from the Scikit learns libraries, we

14
00:01:06,300 --> 00:01:10,670
are going to draw some learning curves for the three models that we have built.

15
00:01:11,690 --> 00:01:15,680
Let's go ahead and run this piece of code to see what happens.

16
00:01:16,850 --> 00:01:22,190
The piece of code here defines how we are going to plot the learning curve.

17
00:01:22,190 --> 00:01:26,570
Notice if you use the side kit learn libraries directly,

18
00:01:26,570 --> 00:01:30,790
you will be plotting the score versus the Training examples.

19
00:01:32,370 --> 00:01:35,560
In our case, we are plotting one minus score.

20
00:01:36,860 --> 00:01:41,810
This is so that the graph resembles the same trend as that of an error that we

21
00:01:41,810 --> 00:01:43,290
plotted before.

22
00:01:43,290 --> 00:01:46,270
Let us run this piece of code first.

23
00:01:46,270 --> 00:01:49,740
As you see, this simply defined what we want to draw.

24
00:01:51,010 --> 00:01:56,460
Using this piece of code we are going to now actually draw the learning curves.

25
00:01:56,460 --> 00:02:00,730
Hopefully you'll have three plots that look similar to this one.

26
00:02:00,730 --> 00:02:03,600
What we have here is that on the X axis we

27
00:02:03,600 --> 00:02:07,250
have plotted the Training examples and on the Y axis we have

28
00:02:07,250 --> 00:02:11,420
plotted the cost which we have defined to be one minus the score.

29
00:02:11,420 --> 00:02:14,380
This is the learning curve for the logistic regression.

30
00:02:14,380 --> 00:02:19,060
The blue line shows you one minus the Training score, and

31
00:02:19,060 --> 00:02:22,260
the red line shows you one minus the test score.

32
00:02:22,260 --> 00:02:25,580
Scrolling down, you can see the learning curves for

33
00:02:25,580 --> 00:02:27,890
the other models that we have built.

34
00:02:27,890 --> 00:02:29,600
The second curve here.

35
00:02:29,600 --> 00:02:34,200
Is the second model in our list, the random forest classifier.

36
00:02:34,200 --> 00:02:36,520
Notice that the Training and

37
00:02:36,520 --> 00:02:42,770
the test scores don't change very much after the 25th Training example.

38
00:02:42,770 --> 00:02:46,850
Here, we can see that using 25 examples we can

39
00:02:46,850 --> 00:02:51,620
do just as well as using larger number of examples.

40
00:02:51,620 --> 00:02:55,480
This is the advantage of drawing a learning curve.

41
00:02:55,480 --> 00:03:00,560
You can see the number of Training examples after which the score does not

42
00:03:00,560 --> 00:03:02,090
improve very much.
