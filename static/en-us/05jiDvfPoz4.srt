1
00:00:00,080 --> 00:00:03,200
We're going to talk about some
possible MDP optimization criteria.

2
00:00:03,200 --> 00:00:06,550
And, well I guess this isn't
really an optimization criterion.

3
00:00:06,550 --> 00:00:10,020
But we talked about exploring
randomly being problematic

4
00:00:10,020 --> 00:00:12,180
because you can have
MDPs that look like this.

5
00:00:12,180 --> 00:00:13,420
Where there's a chain and

6
00:00:13,420 --> 00:00:17,630
you have to keep taking some specific
action to get to the high reward state.

7
00:00:17,630 --> 00:00:20,100
And if you're always
choosing actions randomly,

8
00:00:20,100 --> 00:00:23,710
then the probability you would even ever
reach that is exponentially small, so

9
00:00:23,710 --> 00:00:24,950
we have to be a little bit careful.

10
00:00:24,950 --> 00:00:28,890
For example, in this MDP, it's not
really clear what to do from this state.

11
00:00:28,890 --> 00:00:30,390
>> From which state, that state?

12
00:00:30,390 --> 00:00:32,350
No, it's pretty clear what
to do from that state.

13
00:00:32,350 --> 00:00:33,040
Go left.

14
00:00:33,040 --> 00:00:34,000
>> Oh, okay.

15
00:00:34,000 --> 00:00:34,610
Sure.

16
00:00:34,610 --> 00:00:37,970
No, I mean in the reinforcement learning
setting, we have just two actions.

17
00:00:37,970 --> 00:00:41,990
If we choose wisely, as you just did,
then we end up in this state and

18
00:00:41,990 --> 00:00:44,610
we get reward and that's yea.

19
00:00:44,610 --> 00:00:48,390
If we choose not wisely,
then we get stuck in this trap state and

20
00:00:48,390 --> 00:00:50,620
we get bad reward forever and
theres no way out.

21
00:00:50,620 --> 00:00:54,800
And so in the reinforcement learning
setting, we don't know which of those,

22
00:00:54,800 --> 00:00:57,640
where the different arrows are pointing,
until we try it.

23
00:00:57,640 --> 00:01:01,910
So, it's a little bit unfair to say
what we should be doing is maximizing

24
00:01:03,140 --> 00:01:05,260
all possible reward values, right?

25
00:01:06,400 --> 00:01:09,310
Because it could be that we get
ourselves into a position where we just

26
00:01:09,310 --> 00:01:10,300
can't any more.

27
00:01:10,300 --> 00:01:11,250
So here's what I advocate.

28
00:01:11,250 --> 00:01:13,600
Remember we talked in the bandit
case about a mistake bound.

29
00:01:13,600 --> 00:01:18,330
The idea of the number of times you pull
an arm that's not the right arm to pull,

30
00:01:18,330 --> 00:01:20,200
is bounded by some polynomial.

31
00:01:20,200 --> 00:01:23,330
It turns out that works really
nicely in MDPs as well.

32
00:01:23,330 --> 00:01:25,480
So, the idea is that.

33
00:01:25,480 --> 00:01:28,540
Think about this state
here in this trap example.

34
00:01:28,540 --> 00:01:32,730
It has only one action
choice from the state.

35
00:01:32,730 --> 00:01:34,770
So, what's the right action
to choose from here?

36
00:01:34,770 --> 00:01:35,860
>> Go to one action?

37
00:01:35,860 --> 00:01:36,420
>> Yeah.

38
00:01:36,420 --> 00:01:37,480
So, we're not wrong.

39
00:01:37,480 --> 00:01:38,180
>> True, true.

40
00:01:38,180 --> 00:01:39,740
>> Right?
So, in fact.

41
00:01:39,740 --> 00:01:41,160
No matter what in this particular MDP,

42
00:01:41,160 --> 00:01:43,525
then total number of mistakes
we could ever make is one.

43
00:01:43,525 --> 00:01:48,030
[LAUGH] We might make this
wrong first action, and

44
00:01:48,030 --> 00:01:50,270
then we're making optimal
actions thereafter.

45
00:01:50,270 --> 00:01:52,610
Again, optimal is relative to
the state that we're in, but

46
00:01:52,610 --> 00:01:53,730
we're in the state that we're in.

47
00:01:53,730 --> 00:01:54,820
>> Oh, that's kind of deep.

48
00:01:54,820 --> 00:01:56,120
Okay.
>> Thanks.

49
00:01:56,120 --> 00:01:58,950
So the criterion that I'm
going to advocate for here

50
00:01:58,950 --> 00:02:03,810
is that what we're going to do is count
the number of epsilon suboptimal action.

51
00:02:03,810 --> 00:02:05,610
That is to say the number of
times that we're in a state,

52
00:02:05,610 --> 00:02:09,030
we choose an action and
that action in that state was not

53
00:02:09,030 --> 00:02:11,640
epsilon close to the best
action in that state.

54
00:02:11,640 --> 00:02:14,890
That we want that to be
bounded by some polynomial,

55
00:02:14,890 --> 00:02:16,100
and let's say one over epsilon,

56
00:02:16,100 --> 00:02:21,024
one over delta, the number states n,
and the number actions in each state k.

57
00:02:21,024 --> 00:02:23,740
>> Okay.
[LAUGH] K so, oh, so yeah,

58
00:02:23,740 --> 00:02:25,750
the actions are like bandits.

59
00:02:25,750 --> 00:02:26,880
I like where this is going.

60
00:02:26,880 --> 00:02:28,380
>> A little bit, a little bit.

61
00:02:28,380 --> 00:02:29,350
We're going to bring it all together.

62
00:02:29,350 --> 00:02:31,770
And so this wasn't really
an analysis of different criteria.

63
00:02:31,770 --> 00:02:35,110
This was basically me just
trying to justify one criterion

64
00:02:35,110 --> 00:02:36,580
because it has some nice properties.

65
00:02:36,580 --> 00:02:40,330
It lets you deal with the fact
that there can be trap states and

66
00:02:40,330 --> 00:02:43,240
that doesn't hurt us
in a learning setting.

67
00:02:43,240 --> 00:02:46,290
And well, we're going to have to come up
with an algorithm that's going to deal

68
00:02:46,290 --> 00:02:50,130
with this randomness,
you can't explore randomly idea.

69
00:02:50,130 --> 00:02:52,610
>> Okay, so I think there should be
a fourth criteria in here by the way,

70
00:02:52,610 --> 00:02:56,633
which was inspired by your listening
to William Shatner saying.

71
00:02:56,633 --> 00:02:58,850
Which is the Marusha Criteria,

72
00:02:58,850 --> 00:03:01,320
which is we minimize the number
of times we have to cheat.

73
00:03:01,320 --> 00:03:04,020
I do not remember recommending that
you listen to William Shatner.

74
00:03:04,020 --> 00:03:04,850
>> But everyone should.

75
00:03:04,850 --> 00:03:06,910
>> Okay, but that's a separate issue.

76
00:03:06,910 --> 00:03:11,610
So minimize, in the Kobayashi Maru test,
he reprogrammed it so

77
00:03:11,610 --> 00:03:13,220
that he could win,
even though you couldn't win.

78
00:03:13,220 --> 00:03:17,070
So this is [LAUGH] kind of like that,
in that we're defining winning.

79
00:03:17,070 --> 00:03:18,070
Well, it's not really like that.

80
00:03:18,070 --> 00:03:21,130
But it's defining winning to
be doing the best thing given

81
00:03:21,130 --> 00:03:21,830
the state that you're in.

82
00:03:21,830 --> 00:03:25,690
And it's not actually penalizing you for
having done something really stupid for

83
00:03:25,690 --> 00:03:27,180
having gotten into that state.

84
00:03:27,180 --> 00:03:30,000
As long as the total number of
really stupid things is bounded.

85
00:03:30,000 --> 00:03:31,670
>> Huh, everything really
does come back to Star Trek.

86
00:03:31,670 --> 00:03:32,450
Okay.
I'll buy that.

87
00:03:32,450 --> 00:03:33,537
Three and four are the same.

88
00:03:33,537 --> 00:03:35,530
>> [LAUGH] Well, we can't cheat.

89
00:03:35,530 --> 00:03:37,390
You can't cheat in
reinforcement learning.

90
00:03:37,390 --> 00:03:38,460
>> Oh, I beg to differ.
