1
00:00:00,150 --> 00:00:04,026
What I'd like to do, and [LAUGH] I'm
guessing maybe you don't want me to,

2
00:00:04,026 --> 00:00:05,838
but I'm going to try to do it anyway,

3
00:00:05,838 --> 00:00:09,485
is go through the argument as to
why policy duration actually works.

4
00:00:09,485 --> 00:00:13,449
I think ultimately it's not that bad,
but

5
00:00:13,449 --> 00:00:16,869
we're going to [LAUGH] slog through it.

6
00:00:16,869 --> 00:00:20,900
And so, one of the things we're going to
need is the ability to compare policies

7
00:00:20,900 --> 00:00:24,001
to each other, and
value functions to each other, and so,

8
00:00:24,001 --> 00:00:26,748
we're going to introduce
the concept of domination.

9
00:00:26,748 --> 00:00:27,709
>> Oh, yeah.

10
00:00:27,709 --> 00:00:28,376
>> [LAUGH] So,

11
00:00:28,376 --> 00:00:32,390
this is what domination [LAUGH]
means in the context of policies.

12
00:00:32,390 --> 00:00:37,490
We're going to say that policy one
dominates policy two if it's the case

13
00:00:37,490 --> 00:00:42,230
that for all states the value of
that policy, at that state for

14
00:00:42,230 --> 00:00:46,530
policy one, is bigger than or equal to
the value at that state for policy two.

15
00:00:46,530 --> 00:00:50,570
So, it could be that there's some
policies where policy one does better in

16
00:00:50,570 --> 00:00:53,060
one state, policy two does
better in another state, but

17
00:00:53,060 --> 00:00:57,120
if it dominates it, policy one
has to do no worse in any state.

18
00:00:57,120 --> 00:00:59,603
And in fact, we're going to
say we have strict domination.

19
00:00:59,603 --> 00:01:02,563
[NOISE] If it's the case that
>> [LAUGH]

20
00:01:02,563 --> 00:01:04,507
>> policy one dominates policy two, and

21
00:01:04,507 --> 00:01:05,724
there's some state for

22
00:01:05,724 --> 00:01:08,901
which it's actually strictly better,
it's not equality.

23
00:01:08,901 --> 00:01:15,078
And just for fun, we'll also introduce
the concept of epsilon optimal policies.

24
00:01:15,078 --> 00:01:20,266
So, a policy is epsilon optimal if it's
the case that the value function for

25
00:01:20,266 --> 00:01:23,914
that policy is epsilon close
to the value function for

26
00:01:23,914 --> 00:01:26,440
the optimal policy at all states.

27
00:01:26,440 --> 00:01:29,770
And so what this does is it gives
us a concept of bounding loss or

28
00:01:29,770 --> 00:01:30,805
bounding regret.

29
00:01:30,805 --> 00:01:35,507
So, we'll say that a policy is nearly
optimal if the amount that it loses,

30
00:01:35,507 --> 00:01:38,282
essentially, per time
step is very small.

31
00:01:38,282 --> 00:01:39,240
>> Okay.

32
00:01:39,240 --> 00:01:43,040
So, by the way, does this mean that if
policy one dominates policy two, but

33
00:01:43,040 --> 00:01:46,550
does not strictly dominate it, they
must have the same value everywhere?

34
00:01:46,550 --> 00:01:47,160
>> Yeah, that's right.

35
00:01:47,160 --> 00:01:49,710
They'd have to have- it
would never be greater than,

36
00:01:49,710 --> 00:01:52,400
it would always be greater than or equal
to, which means it's always equal to.

37
00:01:52,400 --> 00:01:55,680
So, two policies that,
well, what you said.

38
00:01:55,680 --> 00:01:56,420
>> Oh, okay, good.

39
00:01:56,420 --> 00:01:57,340
Well, as long as it's what I said.
