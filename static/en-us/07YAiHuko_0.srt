1
00:00:00,000 --> 00:00:03,292
You replace memcachedb with Cassandra
for the precompute stuff?

2
00:00:03,292 --> 00:00:06,000
That solved the right contention issue.

3
00:00:06,646 --> 00:00:08,000
At that point,

4
00:00:11,000 --> 00:00:15,000
Every listing has a lock --you have to lock around.

5
00:00:15,000 --> 00:00:18,000
So you have this listing, which is a set of IDs

6
00:00:18,000 --> 00:00:21,000
of all of the links in a subreddit, right?

7
00:00:21,000 --> 00:00:23,000
And you have to modify that.

8
00:00:23,000 --> 00:00:25,000
You have to lock around that item,

9
00:00:25,000 --> 00:00:27,000
and that's at the subreddit level.

10
00:00:27,000 --> 00:00:30,000
If a lot of people are hanging out in one subreddit,

11
00:00:30,000 --> 00:00:32,000
that thing will have a lot of locking going on.

12
00:00:32,000 --> 00:00:39,000
We talked a little bit just in the
office hours I think for Unit 4,

13
00:00:39,000 --> 00:00:41,353
somebody had asked about

14
00:00:41,353 --> 00:00:46,000
in datastore, if two users tried to register
the same username at the same time,

15
00:00:46,000 --> 00:00:48,815
and datastore doesn't enforce
any uniqueness constraints

16
00:00:48,815 --> 00:00:53,000
on a field in the database.
How do you manage that?

17
00:00:53,000 --> 00:00:55,492
And my answer was

18
00:00:55,492 --> 00:00:59,000
you can either use transactions in the
datastore, which I don't fully understand,

19
00:00:59,000 --> 00:01:02,000
because I'm new to a datastore,
or you can use memcache,

20
00:01:02,000 --> 00:01:04,000
which is one of the 101 uses of memcache,

21
00:01:04,000 --> 00:01:07,000
it has this global lock.

22
00:01:07,000 --> 00:01:11,000
So you guys are moving away from that?
>>Yes.

23
00:01:11,000 --> 00:01:13,000
When you use memcache as a locking service,

24
00:01:13,000 --> 00:01:15,000
the problem is

25
00:01:15,000 --> 00:01:18,000
if you lose a single memcache node,

26
00:01:18,000 --> 00:01:20,000
then you lose the site,

27
00:01:20,000 --> 00:01:22,000
because you can't throw away that node

28
00:01:22,000 --> 00:01:25,000
without potentially,

29
00:01:25,000 --> 00:01:28,000
say half the apps can't see a memcache node,

30
00:01:28,000 --> 00:01:30,000
and the others can.

31
00:01:30,000 --> 00:01:32,000
So the ones that don't see
it decide that they're not

32
00:01:32,000 --> 00:01:34,000
gonna talk to that guy anymore,
and they try

33
00:01:34,000 --> 00:01:35,000
to lock on a different set of servers.

34
00:01:35,000 --> 00:01:38,400
When we store data on multiple
nodes of Cassandra.

35
00:01:38,415 --> 00:01:40,000
It's different from how we store data

36
00:01:40,000 --> 00:01:42,000
across different nodes of memcache,

37
00:01:42,000 --> 00:01:46,000
which is--correct me if I'm wrong--when
you distribute across memcache,

38
00:01:46,000 --> 00:01:49,000
you basically hash your key to
a particular memcache box,

39
00:01:49,000 --> 00:01:51,000
and there's no notion of replication.

40
00:01:51,000 --> 00:01:54,000
Yeah, it's similar in Cassandra.

41
00:01:54,000 --> 00:01:57,000
This node has a key space,

42
00:01:57,000 --> 00:02:01,000
and it just happens to go +1 and -1.

43
00:02:01,000 --> 00:02:04,000
And you can do the same. You can do
replication like that in memcache, as well.

44
00:02:04,000 --> 00:02:07,000
Now, were you guys using--

45
00:02:07,000 --> 00:02:10,000
when I left, we were using a
naive memcache library

46
00:02:10,000 --> 00:02:13,000
that basically took a key and said,

47
00:02:13,000 --> 00:02:18,000
which box is this hashed to, and
would store it on that box.

48
00:02:18,000 --> 00:02:21,000
If you lost that box, it would effect
the hashing of every other key.

49
00:02:21,000 --> 00:02:24,000
Losing a memcache box was really painful.

50
00:02:24,000 --> 00:02:26,000
So we'd replicate memcaches for mostly space.

51
00:02:26,000 --> 00:02:30,000
We had more space--things wouldn't
expire out of the cache fast enough

52
00:02:30,000 --> 00:02:33,000
or would expire out of the cache too fast.

53
00:02:33,000 --> 00:02:35,000
But that had the big balance setup that

54
00:02:35,000 --> 00:02:37,230
we lost a single memcache box

55
00:02:37,276 --> 00:02:40,000
all of the keys would get rearranged.

56
00:02:40,000 --> 00:02:43,323
Right. Module hashing.

57
00:02:43,753 --> 00:02:47,000
We are using consistent hashing now.

58
00:02:47,000 --> 00:02:49,000
The way that works is

59
00:02:49,000 --> 00:02:53,000
it basically builds up this ring similar to this,

60
00:02:53,000 --> 00:02:56,000
and instead of mapping

61
00:02:56,000 --> 00:02:59,000
keys 1 through 1,000 to this box,

62
00:02:59,000 --> 00:03:02,000
it actually sort of assigns them
to a place on a circle

63
00:03:02,000 --> 00:03:06,000
and finds the nearest
server on that circle.

64
00:03:06,000 --> 00:03:09,000
When this node fails, it will
just go to the next one.

65
00:03:09,000 --> 00:03:12,000
We have 10 nodes, we lose 1.

66
00:03:12,000 --> 00:03:15,000
Only 1/10th of your keys get redistributed.

67
00:03:15,000 --> 00:03:18,000
If you're using consistent hashing,

68
00:03:18,000 --> 00:03:21,000
a key might hash to this space on a circle.

69
00:03:21,000 --> 00:03:24,000
And if you lose this guy, the key
still goes in that space,

70
00:03:24,000 --> 00:03:28,000
but all of the keys over here
stay in the same spot.

71
00:03:28,000 --> 00:03:30,000
When you're using modular hashing--

72
00:03:30,000 --> 00:03:32,723
remember we talked about modular in unit 6--

73
00:03:32,723 --> 00:03:36,261
that's a really naive way to distribute keys.

74
00:03:36,261 --> 00:03:41,723
But all of a sudden you go from modular 10,
because you have 10 servers, to modular 9.

75
00:03:41,723 --> 00:03:45,000
Instead of losing 1/10th of your keys,

76
00:03:45,000 --> 00:03:47,600
you lose 9/10th of your keys;
that's a big problem.

77
00:03:47,600 --> 00:03:51,000
We actually had this happen a couple
of times while I was there.

78
00:03:51,000 --> 00:03:53,000
You lose one memcache box,

79
00:03:53,000 --> 00:03:55,000
or more likely, you misconfigure it,

80
00:03:55,000 --> 00:03:58,000
and the app service doesn't see it anymore--

81
00:03:58,000 --> 00:04:01,000
All of the sudden, your cache is not warm anymore,

82
00:04:01,000 --> 00:04:03,000
and you've got users who are just like

83
00:04:03,000 --> 00:04:08,000
hitting the databases, pummeling the
hard cache or the memchache.

84
00:04:08,000 --> 00:04:10,000
We call it the hard cache.

85
00:04:10,000 --> 00:04:13,861
And all of a sudden, your load profile completely
changes, and everything explodes.

86
00:04:14,430 --> 00:04:17,000
That was a really nice improvement.
