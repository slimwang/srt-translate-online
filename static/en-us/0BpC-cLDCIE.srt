1
00:00:00,000 --> 00:00:07,000
And surprisingly, the probability for this message to be spam is 0.

2
00:00:07,000 --> 00:00:11,000
It's not 0.001. It's flat 0.

3
00:00:11,000 --> 00:00:14,000
In other words, it's impossible, according to our model,

4
00:00:14,000 --> 00:00:17,000
that this text could be a spam message.

5
00:00:17,000 --> 00:00:19,000
Why is this?

6
00:00:19,000 --> 00:00:24,000
When we apply the same rule as before, we get the prior for spam which is 3/8.

7
00:00:24,000 --> 00:00:28,000
And we multiple the conditional for each word into this.

8
00:00:28,000 --> 00:00:31,000
For "secret," we know it to be 1/3.

9
00:00:31,000 --> 00:00:39,000
For "is," to be 1/9, but for today, it's 0.

10
00:00:39,000 --> 00:00:45,000
It's 0 because the maximum of the estimate for the probability of "today" in spam is 0.

11
00:00:45,000 --> 00:00:49,000
"Today" just never occurred in a spam message so far.

12
00:00:49,000 --> 00:00:55,000
Now, this 0 is troublesome because as we compute the outcome--

13
00:00:55,000 --> 00:01:00,000
and I'm plugging in all the numbers as before--

14
00:01:00,000 --> 00:01:03,000
none of the words matter anymore, just the 0 matters.

15
00:01:03,000 --> 00:01:10,000
So, we get 0 over something which is plain 0.

16
00:01:10,000 --> 00:01:13,000
Are we overfitting? You bet.

17
00:01:13,000 --> 00:01:15,000
We are clearly overfitting.

18
00:01:15,000 --> 00:01:21,000
It can't be that a single word determines the entire outcome of our analysis.

19
00:01:21,000 --> 00:01:26,000
The reason is that our model, to assign a probability of 0 for the word "today"

20
00:01:26,000 --> 00:01:29,000
to be in the class of spam is just too aggressive.

21
00:01:29,000 --> 00:01:34,000
Let's change this.

22
00:01:34,000 --> 00:01:39,000
One technique to deal with the overfitting problem is called Laplace smoothing.

23
00:01:39,000 --> 00:01:45,000
In maximum likelihood estimation, we assign towards our probability

24
00:01:45,000 --> 00:01:51,000
the quotient of the count of this specific event over all events in our data set.

25
00:01:51,000 --> 00:01:57,000
For example, for the prior probability, we found that 3/8 messages are spam.

26
00:01:57,000 --> 00:02:00,000
Therefore, our maximum likelihood estimate

27
00:02:00,000 --> 00:02:05,000
for the prior probability of spam was 3/8.

28
00:02:05,000 --> 00:02:10,000
In Laplace Smoothing, we use a different estimate.

29
00:02:10,000 --> 00:02:15,000
We add the value k to the count

30
00:02:15,000 --> 00:02:20,000
and normalize as if we added k to every single class

31
00:02:20,000 --> 00:02:23,000
that we've tried to estimate something over.

32
00:02:23,000 --> 00:02:28,000
This is equivalent to assuming we have a couple of fake training examples

33
00:02:28,000 --> 00:02:32,000
where we add k to each observation count.

34
00:02:32,000 --> 00:02:36,000
Now, if k equals 0, we get our maximum likelihood estimator.

35
00:02:36,000 --> 00:02:41,000
But if k is larger than 0 and n is finite, we get different answers.

36
00:02:41,000 --> 00:02:47,000
Let's say k equals 1,

37
00:02:47,000 --> 00:02:51,000
and let's assume we get one message,

38
00:02:51,000 --> 00:02:56,000
and that message was spam, so we're going to write it one message, one spam.

39
00:02:56,000 --> 00:03:03,000
What is p (spam) for the Laplace smoothing of k + 1?

40
00:03:03,000 --> 00:03:09,000
Let's do the same with 10 messages, and we get 6 spam.

41
00:03:09,000 --> 00:03:16,000
And 100 messages, of which 60 are spam.

42
00:03:16,000 --> 99:59:59,999
Please enter your numbers into the boxes over here.
