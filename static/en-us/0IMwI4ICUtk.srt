1
00:00:00,470 --> 00:00:02,780
So, how ideal is an ideal cache?

2
00:00:02,780 --> 00:00:04,765
Does ideal compare to real?

3
00:00:04,765 --> 00:00:09,210
I want to go over some facts that
help justify the ideal cache model.

4
00:00:09,210 --> 00:00:12,820
Remember that the ideal cache model
seems to have some super powers.

5
00:00:12,820 --> 00:00:14,440
So a technical question is,

6
00:00:14,440 --> 00:00:17,650
are the assumptions of the ideal
cache model too strong?

7
00:00:17,650 --> 00:00:21,069
What I want to do now is go over
the justification from the original

8
00:00:21,069 --> 00:00:23,602
cacheable VS model paper,
which was by Frigo,

9
00:00:23,602 --> 00:00:28,187
et al, and appeared in the Foundations
of Computer Science, or FOCS, in 1999.

10
00:00:28,187 --> 00:00:31,650
The first fact is about
the assumption of optimal replacement.

11
00:00:31,650 --> 00:00:33,730
It's a lemma, and here's what it says.

12
00:00:33,730 --> 00:00:36,860
Suppose you take your algorithm and
you count the number of cache misses it

13
00:00:36,860 --> 00:00:40,730
incurs on a machine with
an LRU replacement policy.

14
00:00:40,730 --> 00:00:43,560
Now suppose you're given
a different machine.

15
00:00:43,560 --> 00:00:47,800
This machine has the same line size,
but it only has half the cache.

16
00:00:47,800 --> 00:00:50,490
It also implements an optimal
replacement policy,

17
00:00:50,490 --> 00:00:52,640
rather than an LRU policy.

18
00:00:52,640 --> 00:00:55,290
In other words,
it has a better replacement policy, but

19
00:00:55,290 --> 00:00:57,120
it has less space.

20
00:00:57,120 --> 00:01:00,140
It's both slightly better,
being optimal, but also slightly worse,

21
00:01:00,140 --> 00:01:01,480
having less space.

22
00:01:01,480 --> 00:01:05,180
The lemma says that the number of
transfer on the LRU machine will be

23
00:01:05,180 --> 00:01:09,010
within a factor of 2 of the number of
transfers on this slightly better or

24
00:01:09,010 --> 00:01:11,490
slightly worse optimal machine.

25
00:01:11,490 --> 00:01:13,250
Now, I know what you might be thinking.

26
00:01:13,250 --> 00:01:14,200
Huh?

27
00:01:14,200 --> 00:01:15,338
What the?

28
00:01:15,338 --> 00:01:19,254
[SOUND] At first glance,
this lemma may seem a bit odd, but

29
00:01:19,254 --> 00:01:22,119
here's a very natural interpretation.

30
00:01:22,119 --> 00:01:24,989
Suppose you design an algorithm
assuming optimal replacement,

31
00:01:24,989 --> 00:01:27,340
which is what we're going to be doing.

32
00:01:27,340 --> 00:01:32,080
Then the performance of that algorithm
on a more realistic LRU machine will be

33
00:01:32,080 --> 00:01:34,170
asymptotically close.

34
00:01:34,170 --> 00:01:35,100
In other words,

35
00:01:35,100 --> 00:01:39,158
optimal replacement isn't as strong
an assumption as you might think.

36
00:01:39,158 --> 00:01:43,360
Now, there's a more specific, technical
requirement which is one of regularity.

37
00:01:43,360 --> 00:01:49,330
We say QOPT is regular if it's big
O of QOPT with twice the cache.

38
00:01:49,330 --> 00:01:51,310
In other words,
let's say you design an algorithm and

39
00:01:51,310 --> 00:01:57,160
you find out what QOPT is on a machine
with an optimal replacement policy.

40
00:01:57,160 --> 00:01:58,200
What you calculate for

41
00:01:58,200 --> 00:02:03,470
QOPT is regular if QOPT is big
O of QOPT with twice the cache.

42
00:02:03,470 --> 00:02:07,070
So, if you can show that QOPT
is regular in this sense,

43
00:02:07,070 --> 00:02:10,780
then QLRU will be proportional to QOPT.
