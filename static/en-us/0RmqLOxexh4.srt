1
00:00:00,000 --> 00:00:03,000
This all raises the question,

2
00:00:03,000 --> 00:00:06,000
how to minimize more complicated loss functions

3
00:00:06,000 --> 00:00:09,000
than the one we discussed so far.

4
00:00:09,000 --> 00:00:14,000
Are there closed-form solutions of the type we found for linear regression?

5
00:00:14,000 --> 00:00:17,000
Or do we have to resort to iterative methods?

6
00:00:17,000 --> 00:00:23,000
The general answer is, unfortunantly, we have to resort to iterative methods.

7
00:00:23,000 --> 00:00:28,000
Even though there are special cases in which corresponding solutions may exist,

8
00:00:28,000 --> 00:00:32,000
in general, our loss functions now become complicated enough

9
00:00:32,000 --> 00:00:35,000
that all we can do is iterate.

10
00:00:35,000 --> 00:00:40,000
Here is a prototypical loss function

11
00:00:40,000 --> 00:00:44,000
and the method for interation will be called gradient descent.

12
00:00:44,000 --> 00:00:48,000
In gradient descent, you start with an initial guess,

13
00:00:48,000 --> 00:00:53,000
W-0, where 0 is your iteration number,

14
00:00:53,000 --> 00:00:55,000
and then you up with it iteratively.

15
00:00:55,000 --> 00:01:04,000
Your i plus 1st parameter guess will be obtained by taking your i-th guess

16
00:01:04,000 --> 00:01:10,000
and subtracting from it the gradient of your loss function,

17
00:01:10,000 --> 00:01:15,000
and that guess multiplied by a small learning weight alpha,

18
00:01:15,000 --> 00:01:19,000
where alpha is often as small as 0.01.

19
00:01:19,000 --> 00:01:21,000
I have a couple of questions for you.

20
00:01:21,000 --> 00:01:25,000
Consider the following 3 points.

21
00:01:25,000 --> 00:01:27,000
We call them A, B, C.

22
00:01:27,000 --> 00:01:34,000
I wish to know, for points A, B, and C,

23
00:01:34,000 --> 00:01:40,000
Is the gradient at this point positive, about zero, or negative?

24
00:01:40,000 --> 99:59:59,999
For each of those, check exactly one of those cases.
