1
00:00:00,370 --> 00:00:03,358
Now we focused on the NVIDIA CUDA architecture for this course,

2
00:00:03,358 --> 00:00:06,610
but there are also some Cross Platform solutions that compile not only to CUDA

3
00:00:06,610 --> 00:00:10,947
but to GPUs and multicore CPUs from AMD, Intel, or ARM.

4
00:00:10,947 --> 00:00:13,165
Some of the tools and libraries we've already mentioned,

5
00:00:13,165 --> 00:00:16,658
like Halite and Copperhead, specifically target CPU back ends.

6
00:00:16,658 --> 00:00:23,816
Three other cross platform solutions worth mentioning are openCL, openGL Compute, and openACC.

7
00:00:23,816 --> 00:00:28,266
You'll find that these first 2 are really similar to CUDA in the overall shape.

8
00:00:28,266 --> 00:00:30,965
They share concepts such as thread blocks and shared memory,

9
00:00:30,965 --> 00:00:33,135
even though the names and syntax vary.

10
00:00:33,135 --> 00:00:36,607
For example, openCL refers to work groups rather than thread blocks.

11
00:00:36,607 --> 00:00:38,721
But the basic ideas are isomorphic to CUDA,

12
00:00:38,721 --> 00:00:40,718
and that's one reason why we focused on CUDA

13
00:00:40,718 --> 00:00:43,594
is that what you learn there is really applicable in many places.

14
00:00:43,594 --> 00:00:45,860
OpenGL compute, as the name implies,

15
00:00:45,860 --> 00:00:49,686
is tightly integrative with the extremely popular openGL graphics library,

16
00:00:49,686 --> 00:00:52,692
and this makes it a good choice for developers that are doing graphics

17
00:00:52,692 --> 00:00:55,889
or image processing work who need to support multiple GPU vendors.

18
00:00:55,889 --> 00:00:58,668
Now the third option, openACC, is a little different.

19
00:00:58,668 --> 00:01:00,964
This is a directives based approach.

20
00:01:00,964 --> 00:01:04,379
Directives are annotations that the programmer puts into his or her serial code

21
00:01:04,379 --> 00:01:09,846
that help the compiler figure out how to automatically parallelize some of the loops.

22
00:01:09,846 --> 00:01:12,605
For example, with the appropriate directives,

23
00:01:12,605 --> 00:01:17,201
an openACC compiler can transform a nested for loop into a thread launch on CUDA

24
00:01:17,201 --> 00:01:19,650
or a multi-threaded SIMD routine on a multicore CPU,

25
00:01:19,650 --> 00:01:23,729
so often adding just a few lines to an existing code base can get dramatic speed ups.

26
00:01:23,729 --> 00:01:28,166
And this is what makes openACC a great choice for programmers that have a large legacy code

27
00:01:28,166 --> 00:01:30,182
that they want to parallelize.

28
00:01:30,182 --> 00:01:33,931
OpenACC makes it easy and incremental to add parallelism to an existing code,

29
00:01:33,931 --> 00:01:37,507
and it'll be very familiar to programmers that are used to using openMP.

30
00:01:37,507 --> 00:01:39,710
The ACC stands for accelerators,

31
00:01:39,710 --> 00:01:42,572
and Open ACC is essentially updating of OpenMP

32
00:01:42,572 --> 00:01:46,009
to reflect the evolution of accelerators like the GPU.

33
00:01:46,009 --> 00:01:55,254
Open ACC compilers exist for C, C++, and Fortran.
