1
00:00:00,200 --> 00:00:03,395
So ultimately we're going to run, this
is going to be algorithm c by the way.

2
00:00:03,395 --> 00:00:06,340
We're going to run this algorithm c for
some number of steps.

3
00:00:06,340 --> 00:00:11,750
We're going to call tau prime,
k, epsilon prime, delta prime.

4
00:00:11,750 --> 00:00:14,397
And let's say that the algorithm
that we actually run for

5
00:00:14,397 --> 00:00:16,085
this amount of time is algorithm b.

6
00:00:16,085 --> 00:00:17,360
>> Mm-hm.

7
00:00:17,360 --> 00:00:17,970
>> What do we know?

8
00:00:17,970 --> 00:00:22,950
We know that it could
be that it made tau k,

9
00:00:24,110 --> 00:00:27,640
oh lets say epsilon prime,
delta prime mistakes.

10
00:00:27,640 --> 00:00:31,120
And then all the rest have
to be actual near optimal.

11
00:00:31,120 --> 00:00:33,920
In fact,
well it's going to be epsilon optimal.

12
00:00:33,920 --> 00:00:36,260
So we might need a different
epsilon than epsilon prime.

13
00:00:36,260 --> 00:00:39,705
But, so let's say that it's epsilon
optimal for the rest of these steps.

14
00:00:39,705 --> 00:00:43,195
>> Mm-hm.
>> What do we have to do to make it so

15
00:00:43,195 --> 00:00:46,810
that this average is close to optimal?

16
00:00:46,810 --> 00:00:51,510
>> Well from that point on, we just
pull anything that's epsilon optimal.

17
00:00:51,510 --> 00:00:53,360
And it'll sort of all average out,
right?

18
00:00:53,360 --> 00:00:54,240
>> No, because we don't know

19
00:00:55,635 --> 00:00:57,805
how to pull things that are epsilon
optimal from that point on.

20
00:00:57,805 --> 00:01:01,355
All we know is that this algorithm b,

21
00:01:01,355 --> 00:01:05,770
is going to make it most tau k
epsilon delta prime mistakes.

22
00:01:05,770 --> 00:01:06,495
>> Mm-hm.

23
00:01:06,495 --> 00:01:09,055
>> So, on those trials how close to

24
00:01:09,055 --> 00:01:11,605
optimal is it on the trials where
it actually makes a mistake?

25
00:01:11,605 --> 00:01:12,835
>> Where it makes a mistake?

26
00:01:12,835 --> 00:01:14,140
>> Yeah.

27
00:01:14,140 --> 00:01:15,600
>> Something greater than epsilon.

28
00:01:15,600 --> 00:01:16,380
>> Sure.

29
00:01:16,380 --> 00:01:17,740
What's the largest it can be?

30
00:01:17,740 --> 00:01:18,310
>> One.

31
00:01:18,310 --> 00:01:19,650
>> Good, right.

32
00:01:19,650 --> 00:01:20,150
because remember,

33
00:01:20,150 --> 00:01:24,330
we're talking about a Bernoulli bandit,
which is a bandit you pull the arm and

34
00:01:24,330 --> 00:01:27,620
it either pays off with probably 0 or
1, or something in between.

35
00:01:27,620 --> 00:01:31,440
So the worst we can do is,
we pulled an arm, but

36
00:01:31,440 --> 00:01:34,770
it actually has a payoff of 0, but
the right arm had a payoff of 1.

37
00:01:34,770 --> 00:01:36,230
>> Right.
>> So the maximum error,

38
00:01:36,230 --> 00:01:39,758
the maximum first step reward cost
that we could have in this case is

39
00:01:39,758 --> 00:01:40,600
going to be one.

40
00:01:40,600 --> 00:01:41,350
>> Times tau.

41
00:01:41,350 --> 00:01:42,270
>> For those trials.

42
00:01:42,270 --> 00:01:43,730
>> Right.
>> That's times taro, that's right.

43
00:01:43,730 --> 00:01:44,970
In fact let's write that.

44
00:01:44,970 --> 00:01:48,320
And how close to optimal are we
going to be in these other trials?

45
00:01:48,320 --> 00:01:49,670
>> Epsilon optimal.

46
00:01:49,670 --> 00:01:50,620
>> Epsilon, right.

47
00:01:50,620 --> 00:01:55,505
Alright so this is going to be our
total suboptimality over the end steps.

48
00:01:55,505 --> 00:01:56,080
>> Mm-hm.
>> And

49
00:01:56,080 --> 00:01:58,920
our average per step optimality
is going to be that.

50
00:01:58,920 --> 00:01:59,660
>> Right.

51
00:01:59,660 --> 00:02:03,370
>> And we want this to be
founded by epsilon prime.

52
00:02:03,370 --> 00:02:05,130
>> Yes.
>> And we get to pick two things, right?

53
00:02:05,130 --> 00:02:08,330
We get to pick n and
we get to pick epsilon.

54
00:02:08,330 --> 00:02:09,990
So, can we solve this?

55
00:02:09,990 --> 00:02:10,669
How do we do that?

56
00:02:10,669 --> 00:02:13,675
So I'm going to suggest that
we just pick an epsilon.

57
00:02:13,675 --> 00:02:17,775
It's going to be problematic if we
pick epsilon to be epsilon prime.

58
00:02:17,775 --> 00:02:19,795
Because then we're making
these big mistakes here and

59
00:02:19,795 --> 00:02:21,195
we're making even
biggest mistakes there.

60
00:02:21,195 --> 00:02:22,670
So, on average,
it's not going to work out.

61
00:02:22,670 --> 00:02:23,195
>> Mm-hm.
>> But,

62
00:02:23,195 --> 00:02:26,065
if we set epsilon to epsilon
prime over 2, right.

63
00:02:26,065 --> 00:02:30,335
So, we want our average per step
reward to be epsilon close.

64
00:02:30,335 --> 00:02:30,915
But, to do that,

65
00:02:30,915 --> 00:02:35,000
we're going to run this algorithm B,
with an even tighter epsilon.

66
00:02:35,000 --> 00:02:36,160
>> Okay.
>> Alright.

67
00:02:36,160 --> 00:02:38,810
So, that gives us this inequality.

68
00:02:38,810 --> 00:02:41,310
With only one free variable, which is n.

69
00:02:41,310 --> 00:02:45,280
So how big does n have to be,
so that everything works out?

70
00:02:45,280 --> 00:02:47,090
So, that should be solvable.

71
00:02:47,090 --> 00:02:48,190
>> Yeah.
>> Multiply both sides by n.

72
00:02:48,190 --> 00:02:49,800
You do some algebra.

73
00:02:49,800 --> 00:02:51,640
>> Okay, so
the algebra got us to this point.

74
00:02:51,640 --> 00:02:56,730
Where we say that the n, the number of
steps that we need to run before we have

75
00:02:56,730 --> 00:02:58,750
near optimal reward per step.

76
00:02:58,750 --> 00:03:03,998
Is at least m, the number of mistakes
that the algorithm b is going to make,

77
00:03:03,998 --> 00:03:06,760
times 2 over epsilon prime minus 1.

78
00:03:06,760 --> 00:03:08,020
So epsilon prime,

79
00:03:08,020 --> 00:03:11,520
if you think of epsilon prime
as being some teeny tiny number.

80
00:03:11,520 --> 00:03:15,340
So 2 over epsilon prime is going to
be some fairly big number.

81
00:03:15,340 --> 00:03:17,370
We have to run that minus 1.

82
00:03:17,370 --> 00:03:20,590
But it's still a polynomial in
the epsilon prime that we were

83
00:03:20,590 --> 00:03:21,480
originally given.

84
00:03:21,480 --> 00:03:23,010
>> Right.
Or 1 over epsilon prime.

85
00:03:23,010 --> 00:03:26,890
So epsilon prime has to be,
>> Less than 1.

86
00:03:26,890 --> 00:03:27,710
>> Right.

87
00:03:27,710 --> 00:03:29,390
>> So we're good.

88
00:03:29,390 --> 00:03:30,200
What's going to happen here,

89
00:03:30,200 --> 00:03:34,400
is we could get something like 2 over
1 minus 1 which is going to be 1.

90
00:03:34,400 --> 00:03:38,400
But as epsilon prime gets closer and
closer to 0, this blows up, but

91
00:03:38,400 --> 00:03:41,190
again, it's polynomial
in 1 over epsilon prime.

92
00:03:41,190 --> 00:03:41,870
So we're good.

93
00:03:41,870 --> 00:03:45,070
It just means that we
have to run a bit longer

94
00:03:45,070 --> 00:03:49,110
than the number of mistakes
that we are bounded by.

95
00:03:50,220 --> 00:03:52,020
So that on average, things work out,

96
00:03:52,020 --> 00:03:54,030
because we're going to be
essentially optimal here.

97
00:03:54,030 --> 00:03:58,680
We're going to be epsilon
over 2 optimal here.

98
00:03:58,680 --> 00:04:01,380
And so when we've actually
worked that all the way through,

99
00:04:01,380 --> 00:04:03,900
we're going to get something that,
averaged out

100
00:04:03,900 --> 00:04:08,505
over this very long time interval,
it's nearly optimal per time step.

101
00:04:08,505 --> 00:04:11,320
So that the reward that we're going to
get is actually close to optimal.
