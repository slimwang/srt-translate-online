1
00:00:00,580 --> 00:00:04,120
In the example in the quiz,
the math says the result should be 1.0.

2
00:00:04,120 --> 00:00:06,851
But the code says 0.95.

3
00:00:06,851 --> 00:00:09,080
That's a big difference.

4
00:00:09,080 --> 00:00:12,130
Go ahead, replace the one
billion with just one, and

5
00:00:12,130 --> 00:00:14,840
you'll see that the error becomes tiny.

6
00:00:14,840 --> 00:00:17,610
We're going to want the values
involved in the calculation of this

7
00:00:17,610 --> 00:00:23,270
big loss function that we care about
to never get too big or too small.

8
00:00:23,270 --> 00:00:26,230
One good guiding principle
is that we always want our

9
00:00:26,230 --> 00:00:31,300
variables to have zero mean and
equal variance whenever possible.

10
00:00:31,300 --> 00:00:33,110
On top of the numeric issues,

11
00:00:33,110 --> 00:00:37,100
there are also really good mathematical
reasons to keep values you compute.

12
00:00:37,100 --> 00:00:42,150
Roughly around a mean of zero, an equal
variance when you're doing optimization.

13
00:00:42,150 --> 00:00:45,060
A badly conditioned problem means
that the optimizer has to do

14
00:00:45,060 --> 00:00:48,290
a lot of searching to go and
find a good solution.

15
00:00:48,290 --> 00:00:50,980
A well conditioned problem
makes it a lot easier for

16
00:00:50,980 --> 00:00:53,030
the optimizer to do its job.

17
00:00:53,030 --> 00:00:55,760
If your dealing with images,
it's simple.

18
00:00:55,760 --> 00:00:59,000
You can take the pixel values of your
image, they are typically between 0 and

19
00:00:59,000 --> 00:00:59,710
255.

20
00:00:59,710 --> 00:01:03,750
And simply subtract 128 and
divide by 128.

21
00:01:03,750 --> 00:01:07,610
It doesn't change the content of your
image, but it makes it much easier for

22
00:01:07,610 --> 00:01:09,390
the optimization to proceed numerically.

23
00:01:10,560 --> 00:01:13,730
You also want your weights and
biases to be initialized at

24
00:01:13,730 --> 00:01:16,830
a good enough starting point for
the gradient descent to proceed.

25
00:01:16,830 --> 00:01:20,530
There are lots of fancy schemes to
find good initialization values, but

26
00:01:20,530 --> 00:01:23,940
we're going to focus on a simple,
general method.

27
00:01:23,940 --> 00:01:27,860
Draw the weights randomly from a
Gaussian distribution with mean zero and

28
00:01:27,860 --> 00:01:29,100
standard deviation sigma.

29
00:01:30,190 --> 00:01:34,300
The sigma value determines the order
of magnitude of your outputs

30
00:01:34,300 --> 00:01:37,500
at the initial point
of your optimization.

31
00:01:37,500 --> 00:01:40,770
Because of the soft max on top of it,
the order of magnitude

32
00:01:40,770 --> 00:01:45,510
also determines the peakiness of your
initial probability distribution.

33
00:01:45,510 --> 00:01:49,310
A large sigma means that your
distribution will have large peaks.

34
00:01:49,310 --> 00:01:51,350
It's going to be very opinionated.

35
00:01:51,350 --> 00:01:54,370
A small sigma means that your
distribution is very uncertain

36
00:01:54,370 --> 00:01:55,290
about things.

37
00:01:55,290 --> 00:01:58,930
It's usually better to begin with
an uncertain distribution and

38
00:01:58,930 --> 00:02:02,490
let the optimization become more
confident as the train progress.

39
00:02:02,490 --> 00:02:05,180
So use a small sigma to begin with.

40
00:02:05,180 --> 00:02:05,740
Okay, so

41
00:02:05,740 --> 00:02:09,830
now we actually have everything we need
to actually train this classifier.

42
00:02:09,830 --> 00:02:13,460
We've got our training data, which
is normalized to have zero mean and

43
00:02:13,460 --> 00:02:15,220
unit variance.

44
00:02:15,220 --> 00:02:20,180
We multiply it by a large matrix, which
is initialized with random weights.

45
00:02:20,180 --> 00:02:24,330
We apply the soft max and
then the cross entropy loss and

46
00:02:24,330 --> 00:02:28,180
we calculate the average of this
loss over the entire training data.

47
00:02:29,360 --> 00:02:31,940
Then our magical optimization
package computes

48
00:02:31,940 --> 00:02:36,680
the derivative of this loss with respect
to the weights and to the biases and

49
00:02:36,680 --> 00:02:40,920
takes a step back in the direction
opposite to that derivative.

50
00:02:40,920 --> 00:02:42,541
And then we start all over again, and

51
00:02:42,541 --> 00:02:45,430
repeat the process until we reach
a minimum of the loss function.
