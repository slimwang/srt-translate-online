1
00:00:00,310 --> 00:00:02,430
The next background technology that I'm telling

2
00:00:02,430 --> 00:00:06,680
about is software RAID. I mentioned that hardware

3
00:00:06,680 --> 00:00:09,790
RAID has two problems. The first problem being

4
00:00:09,790 --> 00:00:12,100
small writes and we said we can get

5
00:00:12,100 --> 00:00:14,420
rid of the small write problem by using

6
00:00:14,420 --> 00:00:17,080
log structure file systems. But the hardware RAID

7
00:00:17,080 --> 00:00:19,410
also has another problem and that is it

8
00:00:19,410 --> 00:00:25,100
is employing multiple hardware drives. And hardware RAID,

9
00:00:25,100 --> 00:00:29,390
generally speaking, is very expensive proposition. On the

10
00:00:29,390 --> 00:00:31,760
other hand, in a local area network, we

11
00:00:31,760 --> 00:00:34,090
have lots of compute power distributed over the

12
00:00:34,090 --> 00:00:37,910
LAN and every node on the LAN has

13
00:00:37,910 --> 00:00:41,240
associated with it are disks. So could we

14
00:00:41,240 --> 00:00:44,130
not use the disks that are available on

15
00:00:44,130 --> 00:00:47,670
local data network for doing exactly the same

16
00:00:47,670 --> 00:00:50,240
thing that we did with hardware RAID? And that

17
00:00:50,240 --> 00:00:56,690
is, stripe a file across the disks of all the nodes that are in the

18
00:00:56,690 --> 00:01:02,850
local area network. So that's the idea behind the Zebra file system that was

19
00:01:02,850 --> 00:01:07,840
built at UC Berkeley, which was the first one to experiment with this software

20
00:01:07,840 --> 00:01:11,640
RAID technology. It combines both lock structure

21
00:01:11,640 --> 00:01:15,280
file system and the RAID technology, lock-structured file

22
00:01:15,280 --> 00:01:20,130
system in order to get rid of the small write problem, and the RAID technology

23
00:01:20,130 --> 00:01:27,085
to get the parallelism you want in a file server to be able to read from the

24
00:01:27,085 --> 00:01:30,090
disks in parallel, so that you can reduce

25
00:01:30,090 --> 00:01:33,780
the latency for serving client requests. The idea

26
00:01:33,780 --> 00:01:35,940
here is that you're going to use commodity

27
00:01:35,940 --> 00:01:40,480
hardware available as nodes connected to disks on

28
00:01:40,480 --> 00:01:43,958
a local area network. And since LFS, lock

29
00:01:43,958 --> 00:01:47,400
structured file system, is good for getting rid of

30
00:01:47,400 --> 00:01:49,880
the small write problem, we are going to employ

31
00:01:49,880 --> 00:01:54,150
LFS as the technology for the file server. So

32
00:01:54,150 --> 00:01:57,070
in this case, what we have are not data

33
00:01:57,070 --> 00:02:00,820
files but we have log segments that have to

34
00:02:00,820 --> 00:02:05,500
be written out to the disk in an LFS. And what we're going to do is we're going

35
00:02:05,500 --> 00:02:08,750
to stripe the log segment on multiple nodes

36
00:02:08,750 --> 00:02:11,220
of the disks in software and that is the

37
00:02:11,220 --> 00:02:13,820
RAID technology. So if this is a log segment

38
00:02:13,820 --> 00:02:18,250
that represents the changes made to several different files

39
00:02:18,250 --> 00:02:21,720
on a particular client node, then the software

40
00:02:21,720 --> 00:02:24,640
RAID, what it will do, is then take this

41
00:02:24,640 --> 00:02:27,740
log segment and stripe it. Part one of the

42
00:02:27,740 --> 00:02:30,650
log segment on this node, part two on this,

43
00:02:30,650 --> 00:02:35,730
part three on this, part four on this and the ECC that corresponds to these four

44
00:02:35,730 --> 00:02:41,040
parts of the log segments into a fifth drive. That's the idea

45
00:02:41,040 --> 00:02:43,380
in software RAID. Exactly similar to the

46
00:02:43,380 --> 00:02:46,560
hardware RAID except software is doing this

47
00:02:46,560 --> 00:02:52,570
striping of the log segment on multiple nodes available in a local area network.
