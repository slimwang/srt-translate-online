1
00:00:00,300 --> 00:00:05,140
>> If I say the cat purrs or
this cat hunts mice, it's

2
00:00:05,140 --> 00:00:09,850
perfectly reasonable to also say the
kitty purrs or this kitty hunts mice.

3
00:00:11,030 --> 00:00:15,130
The context gives you a strong
idea that those words are similar.

4
00:00:15,130 --> 00:00:18,850
You have to be catlike to purr and
hunt mice.

5
00:00:18,850 --> 00:00:21,690
So, let's learn to
predict a word's context.

6
00:00:21,690 --> 00:00:25,680
The hope is that a model that's good at
predicting a word's context will have to

7
00:00:25,680 --> 00:00:30,830
treat cat and kitty similarly, and
will tend to bring them closer together.

8
00:00:30,830 --> 00:00:34,190
The beauty of this approach is that you
don't have to worry about what the words

9
00:00:34,190 --> 00:00:39,360
actually mean, giving further meaning
directly by the company they keep.

10
00:00:39,360 --> 00:00:44,350
There are many way to use this idea that
similar words occur in similar contexts.

11
00:00:44,350 --> 00:00:48,050
In our case, we're going to use
it to map words to small vectors

12
00:00:48,050 --> 00:00:51,350
called embeddings which are going
to be close to each other

13
00:00:51,350 --> 00:00:54,700
when words have similar meanings,
and far apart when they don't.

14
00:00:55,820 --> 00:00:58,570
Embedding solves of
the sparsity problem.

15
00:00:58,570 --> 00:01:02,680
Once you have embedded your word into
this small vector, now you have a word

16
00:01:02,680 --> 00:01:07,510
representation where all the catlike
things like cats, kitties, kittens,

17
00:01:07,510 --> 00:01:12,280
pets, lions, are all represented
by vectors that are very similar.

18
00:01:12,280 --> 00:01:15,020
Your model no longer has
to learn new things for

19
00:01:15,020 --> 00:01:17,650
every way there is to talk about a cat.

20
00:01:17,650 --> 00:01:21,260
It can generalize from this
particular pattern of catlike things.
