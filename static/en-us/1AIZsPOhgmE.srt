1
00:00:00,250 --> 00:00:02,890
Okay, Michael, so that was a great conversation, what have we learned?

2
00:00:02,890 --> 00:00:05,460
>> Alright, well we talked about ensemble learning

3
00:00:05,460 --> 00:00:07,010
which was the idea of instead of just

4
00:00:07,010 --> 00:00:08,820
learning one thing, if it's good to learn

5
00:00:08,820 --> 00:00:11,840
once, it's even better to learn multiple times,

6
00:00:11,840 --> 00:00:12,510
>> In multiple ways.

7
00:00:12,510 --> 00:00:15,110
>> The simple version that we concentrated on first was

8
00:00:15,110 --> 00:00:17,840
this notion of bagging. Where what we did is instead

9
00:00:17,840 --> 00:00:20,125
of just learning on the whole data set, we would

10
00:00:20,125 --> 00:00:24,700
sub-sample bunch of examples from the training set, different ways,

11
00:00:24,700 --> 00:00:28,570
and train up different classifiers or different learners on each

12
00:00:28,570 --> 00:00:30,920
of those and then merge them together with the average.

13
00:00:30,920 --> 00:00:32,390
>> Okay, so if I can summarize

14
00:00:32,390 --> 00:00:36,702
that, we learned that ensembles are good. [LAUGH]

15
00:00:36,702 --> 00:00:40,670
>> We learned that even simple ensembles like bagging are good.

16
00:00:40,670 --> 00:00:45,090
>> We talked about the fact that by using this kind

17
00:00:45,090 --> 00:00:51,010
of ensemble approach, you can take simple learners or simple classifiers and

18
00:00:51,010 --> 00:00:53,160
merge them together and get more complicated classifiers.

19
00:00:53,160 --> 00:00:55,750
>> Mm, yeah, so we can take. We

20
00:00:55,750 --> 00:00:59,370
can. Combining simple gives you complex. Anything else?

21
00:00:59,370 --> 00:01:01,960
>> And we talked about the idea of boosting where you

22
00:01:01,960 --> 00:01:04,930
can Oh, maybe this is why it's called boosting. You can take

23
00:01:04,930 --> 00:01:08,570
something that has possibly very high error but always less than

24
00:01:08,570 --> 00:01:11,400
a half, and turn it into something that has very low error.

25
00:01:11,400 --> 00:01:14,080
>> So we learned that boosting is really good.

26
00:01:14,080 --> 00:01:16,360
And, we talked a little bit about why, that's

27
00:01:16,360 --> 00:01:17,700
good. By the way, there's a whole bunch of

28
00:01:17,700 --> 00:01:20,190
other details here too, right? Boosting also has the

29
00:01:20,190 --> 00:01:23,180
advan, as does bagging Not only has these little

30
00:01:23,180 --> 00:01:25,030
properties you've talked about before, but it tends to be

31
00:01:25,030 --> 00:01:28,980
very fast. It's agnostic to the learner. As you

32
00:01:28,980 --> 00:01:31,450
noticed, that in no time, did we say, try

33
00:01:31,450 --> 00:01:33,900
to take advantage of what the actual learner was

34
00:01:33,900 --> 00:01:36,550
doing. Just that it was, in fact, a weak learner.

35
00:01:36,550 --> 00:01:37,060
>> Hm.

36
00:01:37,060 --> 00:01:39,730
>> So I think that's important. It's agnostic.

37
00:01:39,730 --> 00:01:41,680
>> Meaning you can plug in any learner you want?

38
00:01:41,680 --> 00:01:44,900
>> Yeah. So long as it's a weak learner. So there's something

39
00:01:44,900 --> 00:01:47,720
we learned about. We learned about weak learners that we defined with that

40
00:01:47,720 --> 00:01:53,030
meant. And, we also talked about ,um, what error really, really means.

41
00:01:53,030 --> 00:01:56,950
With respect to some kind of underlying distribution. What do you think Michael?

42
00:01:56,950 --> 00:01:58,240
>> That seems like useful stuff.

43
00:01:58,240 --> 00:02:00,650
>> These are useful stuff to me. I'm going to throw one more

44
00:02:00,650 --> 00:02:02,920
thing at you, Michael, before I let you go. Okay, you ready?

45
00:02:02,920 --> 00:02:03,850
>> Yep.

46
00:02:03,850 --> 00:02:06,820
>> Here's a simple fact. About boosting that

47
00:02:06,820 --> 00:02:08,910
turns out in practice. You know our

48
00:02:08,910 --> 00:02:11,620
favorite little over-fitting example. Do you know

49
00:02:11,620 --> 00:02:14,420
how over-fitting works? You have a training

50
00:02:14,420 --> 00:02:16,470
line that tends to get better, and better,

51
00:02:16,470 --> 00:02:21,180
and better. Maybe even going down to zero error. But then you have test

52
00:02:21,180 --> 00:02:25,260
error Which gets better and better and at some point it starts to get worse.

53
00:02:25,260 --> 00:02:26,580
>> Mm.

54
00:02:26,580 --> 00:02:29,320
>> And at that point you have over fitting and I think,

55
00:02:29,320 --> 00:02:32,325
Michael, you asserted it at some point or maybe I asserted that

56
00:02:32,325 --> 00:02:34,980
,you always have to worry about over fitting. Over fitting is

57
00:02:34,980 --> 00:02:36,630
just the kind of fact of life. You got to come

58
00:02:36,630 --> 00:02:38,750
up with ways to deal with it or sort of over

59
00:02:38,750 --> 00:02:42,380
believing your data. Well, what if I told you that in practice

60
00:02:42,380 --> 00:02:45,620
When you run boosting, even as you run it over time

61
00:02:45,620 --> 00:02:49,130
so that your training error keeps getting better and better and

62
00:02:49,130 --> 00:02:52,960
better and better, it also turns out that your testing error

63
00:02:52,960 --> 00:02:57,280
keeps getting better and better and better and better and better and

64
00:02:57,280 --> 00:02:58,250
better and better.

65
00:02:58,250 --> 00:03:00,240
>> That seems too good to be true.

66
00:03:00,240 --> 00:03:03,490
>> It does seem too good to be true. It turns out it's

67
00:03:03,490 --> 00:03:07,020
not too good to be true. And I have an explanation for it.

68
00:03:07,020 --> 00:03:07,830
>> Tell me.

69
00:03:07,830 --> 00:03:10,200
>> Not until next time.

70
00:03:10,200 --> 00:03:11,650
>> alright, see you then.

71
00:03:11,650 --> 00:03:12,990
>> See you then. Bye.
