1
00:00:00,000 --> 00:00:04,007
It is now really easy to explain expectation maximization

2
00:00:04,007 --> 00:00:06,373
as a generalization of K-means.

3
00:00:06,373 --> 00:00:09,076
Again, we have a couple of data points here

4
00:00:09,076 --> 00:00:12,279
and 2 randomly chosen cluster centers.

5
00:00:12,279 --> 00:00:16,383
But in the correspondence step instead of making a hard correspondence

6
00:00:16,383 --> 00:00:18,385
we make a soft correspondence.

7
00:00:18,385 --> 00:00:22,152
Each data point is attracted to a cluster center

8
00:00:22,152 --> 00:00:24,958
in proportion to the posterior likelihood

9
00:00:24,958 --> 00:00:26,096
which we will define in a minute.

10
00:00:26,096 --> 00:00:30,163
In the adjustment step or the maximization step

11
00:00:30,163 --> 00:00:34,535
the cluster centers are being optimized just like before

12
00:00:34,535 --> 00:00:37,037
but now the correspondence is a soft variable

13
00:00:37,037 --> 00:00:39,873
and they correspond to all data points in different strengths

14
00:00:39,873 --> 00:00:41,542
not just the nearest ones.

15
00:00:41,542 --> 00:00:44,077
As a result, in EM the cluster centers

16
00:00:44,077 --> 00:00:46,254
tend not to move as far as in K-means.

17
00:00:46,254 --> 00:00:48,048
Their movement is smooth away.

18
00:00:48,048 --> 00:00:50,005
A new correspondence over here gives us different strength

19
00:00:50,005 --> 00:00:53,387
as indicated by the different coloring of the links

20
00:00:53,387 --> 00:00:57,224
and another relaxation step gives us better cluster centers.

21
00:00:57,224 --> 00:00:59,593
And as you can see over time, gradually

22
00:00:59,593 --> 00:01:03,363
the EM will then converge to about the same solution as K-means.

23
00:01:03,363 --> 00:01:05,899
However, all the correspondences are still alive.

24
00:01:05,899 --> 00:01:08,000
Which means there is not a 0, 1 correspondence.

25
00:01:08,000 --> 00:01:09,883
There is a soft correspondence

26
00:01:09,883 --> 99:59:59,999
which relates to a posterior probability, which I will explain next.
