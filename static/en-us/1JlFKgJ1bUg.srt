1
00:00:00,190 --> 00:00:02,800
Now suppose the starting point is a sequential

2
00:00:02,800 --> 00:00:05,920
program. How can we exploit the cluster? We

3
00:00:05,920 --> 00:00:08,790
have multiple processors, how do we exploit the

4
00:00:08,790 --> 00:00:11,450
cluster if the starting point is a sequential

5
00:00:11,450 --> 00:00:13,882
program? One possibility is to do what is

6
00:00:13,882 --> 00:00:18,130
called automatic parallelization. That is, instead of writing

7
00:00:18,130 --> 00:00:21,480
an explicitly parallel program, we write a sequential

8
00:00:21,480 --> 00:00:25,380
program. And let somebody else do the heavy lifting

9
00:00:25,380 --> 00:00:29,270
in terms of identifying opportunities for parallelism that

10
00:00:29,270 --> 00:00:31,730
exist in the program and map it to

11
00:00:31,730 --> 00:00:34,360
the underlying cluster. And this is what is

12
00:00:34,360 --> 00:00:38,230
called an implicitly parallel program. There are opportunities for

13
00:00:38,230 --> 00:00:40,970
parallelism, but the program itself is not written

14
00:00:40,970 --> 00:00:43,250
as a parallel program. And, now it is

15
00:00:43,250 --> 00:00:46,540
the onus of the tool, in this case

16
00:00:46,540 --> 00:00:50,510
an automatic parallelizing compiler, to look at the sequential

17
00:00:50,510 --> 00:00:54,670
program and identify opportunities for parallelism

18
00:00:54,670 --> 00:00:58,210
and exploit that by using the resources

19
00:00:58,210 --> 00:01:00,295
that are available in the cluster. So

20
00:01:00,295 --> 00:01:03,070
high-performance FORTRAN is an example of a

21
00:01:03,070 --> 00:01:07,320
programming language that does automatic parallelization,

22
00:01:07,320 --> 00:01:09,760
but it is user-assisted parallelization in the

23
00:01:09,760 --> 00:01:12,370
sense that the user who is writing

24
00:01:12,370 --> 00:01:15,880
the sequential program is using directives for

25
00:01:15,880 --> 00:01:18,830
distribution of data and computation. And those

26
00:01:18,830 --> 00:01:23,020
directives are then used by this parallelizing

27
00:01:23,020 --> 00:01:26,560
compiler to say, oh, these are opportunities

28
00:01:26,560 --> 00:01:30,280
for mapping these computations onto the resources of

29
00:01:30,280 --> 00:01:35,320
a cluster. So it puts it on different nodes on the cluster and that

30
00:01:35,320 --> 00:01:38,150
way, it exploits the parallelism that is

31
00:01:38,150 --> 00:01:41,175
there in the hardware, starting from the sequential

32
00:01:41,175 --> 00:01:43,290
program and doing the heavy lifting in

33
00:01:43,290 --> 00:01:45,740
terms of converting the sequential program to a

34
00:01:45,740 --> 00:01:49,420
parallel program to extract performance for this

35
00:01:49,420 --> 00:01:54,310
application. This kind of automatic parallelization, or implicitly

36
00:01:54,310 --> 00:01:57,690
parallel programming, works really well for certain

37
00:01:57,690 --> 00:02:00,840
classes of program called data parallel programs. In

38
00:02:00,840 --> 00:02:06,400
such programs, for the most part, the data accesses are fairly static, and it is

39
00:02:06,400 --> 00:02:11,710
determinable at compile time. So in other words, there is limited potential for

40
00:02:11,710 --> 00:02:16,050
exploiting the available parallelism in the cluster

41
00:02:16,050 --> 00:02:18,885
if we resort to implicitly parallel programming.
