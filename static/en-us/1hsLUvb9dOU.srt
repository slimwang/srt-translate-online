1
00:00:00,160 --> 00:00:02,969
LIke you said,
it's sort of an old school way doing it.

2
00:00:02,969 --> 00:00:06,770
They were the first methods being used
in what's called pattern recognition.

3
00:00:06,770 --> 00:00:08,670
Pattern recognition's entire field or

4
00:00:08,670 --> 00:00:11,980
discipline that looked at making
these kinds of decisions.

5
00:00:11,980 --> 00:00:15,820
And one of the reasons was it was
very easy to model this analytically.

6
00:00:15,820 --> 00:00:17,950
Because you'd use things like Gaussians,
or whatever.

7
00:00:17,950 --> 00:00:22,550
Or mixtures of Gaussians, and you could
do this with modest amounts of data

8
00:00:22,550 --> 00:00:25,960
in a smallish moderate
number of dimensions, and

9
00:00:25,960 --> 00:00:28,520
because a lot of this was done
before we had serious computers, or

10
00:00:28,520 --> 00:00:29,870
before we had serious data.

11
00:00:29,870 --> 00:00:31,320
This was really important.

12
00:00:31,320 --> 00:00:34,280
But sort of in the modern world, or
not even the modern world anymore, but

13
00:00:34,280 --> 00:00:37,750
the world 20 years ago, which I like to
think of as being modern, because back

14
00:00:37,750 --> 00:00:40,730
then I was in my 30s, and I'm hoping
that I'm not like, really that old.

15
00:00:40,730 --> 00:00:43,010
But now I'm like,
man I'm old, okay fine.

16
00:00:43,010 --> 00:00:47,210
But in the modern world there are some
liabilities of the generative method.

17
00:00:47,210 --> 00:00:51,230
First of all, these days, many of our
signals are high-dimensional, right.

18
00:00:51,230 --> 00:00:54,590
So we get some description of a signal,
whether it's stock market data or

19
00:00:54,590 --> 00:00:55,660
whatever it is.

20
00:00:55,660 --> 00:01:00,860
I might have a feature space that's
tens of thousands of dimensions,

21
00:01:00,860 --> 00:01:02,260
or at least many hundreds of dimensions.

22
00:01:02,260 --> 00:01:04,150
It's a very high-dimensional vector.

23
00:01:04,150 --> 00:01:07,690
And if you actually want to represent
the density, in that high-dimensional

24
00:01:07,690 --> 00:01:11,080
space I, I wrote here that
it's called sort of data hard.

25
00:01:11,080 --> 00:01:11,920
And what I mean by that is

26
00:01:11,920 --> 00:01:14,400
the thing becomes exponential in
the number of features that you have.

27
00:01:14,400 --> 00:01:17,650
So you have to have sort of massive,
massive, massive amounts of data.

28
00:01:17,650 --> 00:01:19,070
And even more mass, when, you know,

29
00:01:19,070 --> 00:01:22,840
we tend to think sort of Google
styles data being is massive.

30
00:01:22,840 --> 00:01:23,530
No.

31
00:01:23,530 --> 00:01:27,020
You know, when you've got tens of
thousands of dimensions, it, it's,

32
00:01:27,020 --> 00:01:30,520
the amount of data you need to densely
sample that is, is exponential,

33
00:01:30,520 --> 00:01:32,030
and we just don't have it.

34
00:01:32,030 --> 00:01:34,990
But, there's even a bigger liability,
okay?

35
00:01:34,990 --> 00:01:38,190
The bigger liability is that

36
00:01:38,190 --> 00:01:43,260
we don't actually care about
modelling the coal class.

37
00:01:43,260 --> 00:01:45,980
Right?
If I've got sort of skin things and

38
00:01:45,980 --> 00:01:47,450
not skin things.

39
00:01:47,450 --> 00:01:51,330
Imagine a whole bunch of colored pixels
that have nothing to do with skin,

40
00:01:51,330 --> 00:01:54,060
I don't have to model that as not skin.

41
00:01:54,060 --> 00:01:55,960
We only care about making
the right decisions.

42
00:01:55,960 --> 00:01:59,930
So we should be modelling
the boundaries, the hard cases.

43
00:01:59,930 --> 00:02:02,390
The cases where I'm not really sure.

44
00:02:02,390 --> 00:02:05,400
And when you do a general model
of a category or of a class,

45
00:02:05,400 --> 00:02:10,139
you're worried about the vast majority
of them, so the skin pixels, not this,

46
00:02:10,139 --> 00:02:12,740
just this boundary that's
next to the non-skin ones.

47
00:02:12,740 --> 00:02:16,097
And I'm not even totally sure
what it means to model non-class,

48
00:02:16,097 --> 00:02:20,510
non-skin pixels, because that's
everything from you know, the pixels on

49
00:02:20,510 --> 00:02:25,430
colored shirts, to the pixels on grass,
to the pixels on tree bark.

50
00:02:25,430 --> 00:02:29,150
Yeah, it's kind of a weird, not,
non skin's not really a great class.

51
00:02:29,150 --> 00:02:33,540
What I care about is, find me the pixels
that may be labeled as skin, or

52
00:02:33,540 --> 00:02:34,660
maybe not.

53
00:02:34,660 --> 00:02:37,900
And finally,
I'm going to give you a feature vector,

54
00:02:37,900 --> 00:02:41,060
a bunch of features that
describe an instance.

55
00:02:41,060 --> 00:02:43,170
And we might not have any idea,
a priori,

56
00:02:43,170 --> 00:02:48,650
which features actually are good for
discriminating between the classes.

57
00:02:48,650 --> 00:02:51,490
So part of our building a classifier

58
00:02:51,490 --> 00:02:54,300
is essentially doing feature selection,
right?

59
00:02:54,300 --> 00:02:57,920
We have to figure out what
parts of the features, or

60
00:02:57,920 --> 00:03:03,090
what parts of the signal,
are the sort of informative ones.

61
00:03:03,090 --> 00:03:03,780
And, so, for

62
00:03:03,780 --> 00:03:08,890
training a classifier, our general model
has no way of talking about doing that.

63
00:03:08,890 --> 00:03:11,440
But the methods we're
about to talk about,

64
00:03:11,440 --> 00:03:14,310
which are called discriminative methods,
inherently do that.
