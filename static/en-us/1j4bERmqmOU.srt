1
00:00:00,340 --> 00:00:05,038
>> So we are going to need a learning algorithm that is more robust to

2
00:00:05,038 --> 00:00:10,680
non-linear separability or linear non-separability. Does that sound right?

3
00:00:10,680 --> 00:00:16,320
>> Non-linear separability. Non linear separability.

4
00:00:16,320 --> 00:00:17,280
>> Non?

5
00:00:17,280 --> 00:00:17,770
>> Yeah think of it.

6
00:00:17,770 --> 00:00:20,470
>> Left parenthesis, linear sep, spreadability left parenthesis.

7
00:00:20,470 --> 00:00:22,340
>> There we go, that's right, negating the whole phrase, very good.

8
00:00:22,340 --> 00:00:25,350
So and, Gradient descent is going to give us an algorithm for doing

9
00:00:25,350 --> 00:00:27,920
exactly that. So, what we're going to do now is

10
00:00:29,340 --> 00:00:32,590
think of things this way. So what we did before

11
00:00:32,590 --> 00:00:35,400
was we had a summation over all the different input features

12
00:00:35,400 --> 00:00:38,920
of the activation on that input feature times the weight,

13
00:00:38,920 --> 00:00:41,480
w, for that input feature. And we sum all

14
00:00:41,480 --> 00:00:44,450
those up and we get an activation. And then we

15
00:00:44,450 --> 00:00:48,010
have our estimated output as whether or not that activation

16
00:00:48,010 --> 00:00:50,530
is greater than or equal to zero. So let's imagine

17
00:00:50,530 --> 00:00:53,200
that the output is not thresholded when we're doing the training,

18
00:00:53,200 --> 00:00:55,350
and what we're going to do instead is try to figure out

19
00:00:55,350 --> 00:00:59,380
the weight so that the Not thresholded value is, as close

20
00:00:59,380 --> 00:01:01,810
to the target as we can. So this actually kind of brings

21
00:01:01,810 --> 00:01:04,459
us back to the regression story. We can define an error

22
00:01:04,459 --> 00:01:08,390
metric on the weight vector w. And the form of that's

23
00:01:08,390 --> 00:01:12,120
going to be one half, times the sum over all the data

24
00:01:12,120 --> 00:01:15,750
in the dataset, of what the target was supposed to be for

25
00:01:15,750 --> 00:01:18,790
that particular example minus what the

26
00:01:18,790 --> 00:01:21,120
activation actually was. Right? The activation

27
00:01:21,120 --> 00:01:25,320
being the dot product between the weights and the input and we're

28
00:01:25,320 --> 00:01:27,540
going to square that. We're going to square that error and we want to try

29
00:01:27,540 --> 00:01:30,350
to now minimize that. > Hey Michael, can I ask you a question?

30
00:01:30,350 --> 00:01:31,480
>> Sure.

31
00:01:31,480 --> 00:01:32,590
>> Why one half of that?

32
00:01:32,590 --> 00:01:37,900
>> Mm. Yes. It turns out that it turn, in terms of

33
00:01:37,900 --> 00:01:40,640
minimizing the error this is just a constant and it doesn't matter.

34
00:01:41,690 --> 00:01:45,450
So why do we stick in a half there? Let's get back to that.

35
00:01:45,450 --> 00:01:45,820
>> Okay.

36
00:01:45,820 --> 00:01:47,980
>> Just like in the regression case we're going

37
00:01:47,980 --> 00:01:50,160
to fall back to calculus. Right, calculus is going to

38
00:01:50,160 --> 00:01:53,360
tell us how we can push around these weights,

39
00:01:53,360 --> 00:01:55,820
to try to push this error down. Right, so we

40
00:01:55,820 --> 00:01:58,100
would like to know. How does changing the weight

41
00:01:58,100 --> 00:02:00,810
change the error, and lets push the weight in the

42
00:02:00,810 --> 00:02:03,110
direction that causes the error to go down. So

43
00:02:03,110 --> 00:02:06,990
we're going to take the partial derivative of the, this error metric

44
00:02:06,990 --> 00:02:09,220
with respect to each of the individual weights, so that we'll

45
00:02:09,220 --> 00:02:11,390
know for each weight which way we should push it a

46
00:02:11,390 --> 00:02:13,980
little bit to move in the direction of the gradient. So

47
00:02:13,980 --> 00:02:17,730
that's the partial dif, dif, [INAUDIBLE] So that's the partial derivitive

48
00:02:17,730 --> 00:02:22,540
with respect to weight wi, of exactly this error measure. So

49
00:02:22,540 --> 00:02:25,210
to take this partial derivitive we just use the chain rule

50
00:02:25,210 --> 00:02:28,530
as we always do. And what is it to take the

51
00:02:28,530 --> 00:02:31,920
derivitive of something like this, if you have this quantity here.

52
00:02:31,920 --> 00:02:35,160
We take the power, move it to the front, keep this

53
00:02:35,160 --> 00:02:38,110
thing, and then take the derivitive of this thing. But that,

54
00:02:38,110 --> 00:02:40,460
so this now answers your question, Charles. Why do we put

55
00:02:40,460 --> 00:02:42,780
a half in there? Because down the line, it's going to be

56
00:02:42,780 --> 00:02:45,970
really convenient that two and the half canceled out. So, it's

57
00:02:45,970 --> 00:02:48,700
just going to mean that our partial derivative is going to look simpler,

58
00:02:48,700 --> 00:02:51,760
even though our error measure looked a little bit more complicated.

59
00:02:51,760 --> 00:02:56,990
So so what we're left with then, is exactly what I said,

60
00:02:56,990 --> 00:02:59,020
the sum over all these data points of what

61
00:02:59,020 --> 00:03:02,790
was inside this. Quantity here times the derivative of

62
00:03:02,790 --> 00:03:06,240
that, and here I expanded the a to be,

63
00:03:06,240 --> 00:03:09,700
the definition of the a. Now, we need to take

64
00:03:09,700 --> 00:03:13,720
the partial derivative with respect to weight w i

65
00:03:13,720 --> 00:03:15,750
of this sum that involves a bunch of the ws

66
00:03:15,750 --> 00:03:18,270
in it. So, when don't match the w i,

67
00:03:18,270 --> 00:03:21,690
that derivative is going to be zero because the, you know,

68
00:03:21,690 --> 00:03:24,290
changing the weight won't have any impact on it. The only

69
00:03:24,290 --> 00:03:27,240
place where this, changing this weight has any impact is at

70
00:03:27,240 --> 00:03:31,020
x of i. So that's what we end up carrying down.

71
00:03:31,020 --> 00:03:34,390
This summation disappears. And all that's left is just the one term

72
00:03:34,390 --> 00:03:37,090
that matches the weight that we care about. So this is

73
00:03:37,090 --> 00:03:39,320
what we're left with. Now the derivative of the error with

74
00:03:39,320 --> 00:03:43,000
respect to any weight w sub i. Is exactly this, this

75
00:03:43,000 --> 00:03:46,990
sum. The sum of the difference between the activation and the target

76
00:03:46,990 --> 00:03:51,130
output times the activation on that input unit

77
00:03:51,130 --> 00:03:54,900
>> You know? That looks exactly like, almost exactly

78
00:03:54,900 --> 00:03:57,880
like the rule that we use with the rule that we used perceptron before.

79
00:03:57,880 --> 00:03:59,780
>> It does indeed! What's the

80
00:03:59,780 --> 00:04:03,250
difference? Well, actually let's Let's write this

81
00:04:03,250 --> 00:04:05,670
down. This is now just a derivative, but let's actually write down what

82
00:04:05,670 --> 00:04:08,460
our weight update is going to be because we're going to take a little step

83
00:04:08,460 --> 00:04:10,850
in the direction of this derivative and it's going to involve a learning rate.
