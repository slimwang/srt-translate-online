1
00:00:00,340 --> 00:00:01,340
Okay Michael, are you ready?

2
00:00:01,340 --> 00:00:03,310
>> I am afraid so.

3
00:00:03,310 --> 00:00:06,710
>> All right, which one do you want to fill out first?

4
00:00:06,710 --> 00:00:10,600
>> Let's just do them in order, so ,one nearest neighbor. You, you explained,

5
00:00:10,600 --> 00:00:13,490
how the training works. We just take the assorted list, and leave it there.

6
00:00:13,490 --> 00:00:14,310
>> Mm-hm.

7
00:00:14,310 --> 00:00:19,440
>> And we have the classifier, or the aggressor itself ,has linear space,

8
00:00:19,440 --> 00:00:23,900
and now at query time, we need to find the nearest neighbor, Um-huh.

9
00:00:23,900 --> 00:00:25,600
>> Which we could do by taking the query

10
00:00:25,600 --> 00:00:29,560
point, and running through the whole list ,and seeing which

11
00:00:29,560 --> 00:00:33,140
one it's closest to, but, because, it's sorted I think

12
00:00:33,140 --> 00:00:35,740
we, we outta be able to use binary search and,

13
00:00:35,740 --> 00:00:41,690
and in log time, find the closest, point to the query.

14
00:00:41,690 --> 00:00:43,350
>> That's exactly right, you should be able to do

15
00:00:43,350 --> 00:00:47,000
that in log base, two time. What if it weren't sorted?

16
00:00:47,000 --> 00:00:50,630
>> Yeah then, like I said, I think you could just scan through the whole

17
00:00:50,630 --> 00:00:53,290
list and that would be linear time and that's not a big deal.

18
00:00:53,290 --> 00:00:55,820
>> Right, yeah, we could do linear time, but, because I gave you a

19
00:00:55,820 --> 00:00:58,210
sorted list, because, I'm so helpful, you

20
00:00:58,210 --> 00:00:59,690
can do it in [INAUDIBLE] time, okay, but.

21
00:00:59,690 --> 00:01:01,860
>> That was, that was very, very thoughtful of you.

22
00:01:01,860 --> 00:01:04,500
>> It was I thought, I thought of her, so what about on the, space side?

23
00:01:04,500 --> 00:01:09,960
>> Alright, so the amount of space that you need to process its query is

24
00:01:09,960 --> 00:01:12,430
linear. We don't need to take any special,

25
00:01:12,430 --> 00:01:16,230
set aside, space beyond, a couple simple variables.

26
00:01:16,230 --> 00:01:19,150
And the data that we're given, which we've already accounted for.

27
00:01:19,150 --> 00:01:21,750
>> Right, so then why would it be linear, if we accounted for it?

28
00:01:21,750 --> 00:01:23,530
>> Did I say linear? I meant constant.

29
00:01:23,530 --> 00:01:24,600
>> Yes, yes, that's right,

30
00:01:26,890 --> 00:01:29,240
constant. That's what you meant. That's what you said. That's what happened.

31
00:01:29,240 --> 00:01:30,948
>> [LAUGH]

32
00:01:30,948 --> 00:01:31,220
>> Okay.

33
00:01:31,220 --> 00:01:33,210
>> It's a good thing, this wasn't being recorded,

34
00:01:33,210 --> 00:01:34,695
so we could verify one way or the other.

35
00:01:34,695 --> 00:01:35,800
>> [LAUGH] It is a good thing, maybe we'll

36
00:01:35,800 --> 00:01:38,391
look it up on Wikipedia, and it'll say confusingly [LAUGH]

37
00:01:38,391 --> 00:01:40,400
>> Linear sometimes use to mean constant.

38
00:01:40,400 --> 00:01:44,030
>> Constant. [LAUGH]. Yeah, that is pretty confusing.

39
00:01:44,030 --> 00:01:45,510
>> Okay, what about k and n?

40
00:01:45,510 --> 00:01:48,930
>> Alright, K and n. So k and n, so the training process,

41
00:01:48,930 --> 00:01:52,080
the learning process, is exactly the same, as it is for one year stamper,

42
00:01:52,080 --> 00:01:54,810
which is to say you do nothing and you pass all the data

43
00:01:54,810 --> 00:01:58,890
forward, to the, The query processor. So it's going to be 1 n.

44
00:01:58,890 --> 00:02:00,980
>> That is correct, nice.

45
00:02:00,980 --> 00:02:05,000
>> Now querying, seems like its a little more subtle. So

46
00:02:05,000 --> 00:02:07,640
we can find the single nearest neighbor in log and time.

47
00:02:07,640 --> 00:02:08,241
>> Mm-mm.

48
00:02:08,241 --> 00:02:12,950
>> Where we going to get the other K minus one? So, I'm pretty sure,

49
00:02:12,950 --> 00:02:17,500
that ,once we find the nearest neighbor we can kind of start doing a little,

50
00:02:17,500 --> 00:02:20,700
Spread out search, from there, until we found the k nearest neighbors.

51
00:02:20,700 --> 00:02:22,940
>> Sure. So, you're saying, you know,

52
00:02:22,940 --> 00:02:24,380
you've got these points. They're already in a

53
00:02:24,380 --> 00:02:27,653
line, you find the nearest neighbor. You

54
00:02:27,653 --> 00:02:29,530
know ,the, the next nearest neighbors have to

55
00:02:29,530 --> 00:02:35,360
be within k of the points surrounding it, and so you can just move in

56
00:02:35,360 --> 00:02:39,370
kind of, either direction and pick them up as you go. Yeah, something like that.

57
00:02:39,370 --> 00:02:39,920
>> Okay.

58
00:02:39,920 --> 00:02:40,950
>> I mean the way that, the way that I

59
00:02:40,950 --> 00:02:42,540
was thinking about it is that, I, I think you

60
00:02:42,540 --> 00:02:46,440
can use the same algorithm that you used for merging lists, in merge sort,

61
00:02:46,440 --> 00:02:50,420
but here the lists actually corresponds, to being, to the left of the query

62
00:02:50,420 --> 00:02:53,340
point, and being to the right of the query point and they are both

63
00:02:53,340 --> 00:02:57,920
sorted ,in terms of their distance from the query point. Sure, yeah, I buy that.

64
00:02:57,920 --> 00:03:01,690
>> So, so that ought to give us log n, plus k.

65
00:03:01,690 --> 00:03:04,220
>> Okay, so, do we need to write the k?

66
00:03:04,220 --> 00:03:07,600
>> I'm going to say yes, because, if k is

67
00:03:07,600 --> 00:03:11,530
on the order of like, n over 2, then it's going to dominate. If k is on

68
00:03:11,530 --> 00:03:13,090
the order of log n, then it's not

69
00:03:13,090 --> 00:03:15,180
going to dominate. Mm, that's a good point. So,

70
00:03:15,180 --> 00:03:17,760
yeah, we'll do k. I will point out that if k is on the order of

71
00:03:17,760 --> 00:03:20,960
n over 2, you're right, it will dominate, and then really this is big o of n.

72
00:03:20,960 --> 00:03:21,830
>> That's right.

73
00:03:21,830 --> 00:03:24,240
>> But if it's on the order of log n, then it's

74
00:03:24,240 --> 00:03:26,440
just log n plus n, and so it's a big old long

75
00:03:26,440 --> 00:03:28,660
n, log n. But ,you're right, so we should probably keep the

76
00:03:28,660 --> 00:03:32,740
k around because, we don't know its relationship to n. Okay, fair enough.

77
00:03:32,740 --> 00:03:34,600
Okay, what about the space requirements?

78
00:03:34,600 --> 00:03:37,230
>> We know one bit of relationship, it's smaller than or equal to n.

79
00:03:37,230 --> 00:03:38,350
>> That's true.

80
00:03:38,350 --> 00:03:39,580
>> Because that would be really weird, if I gave you

81
00:03:39,580 --> 00:03:41,510
ten data points and asked you for the 20 nearest neighbors.

82
00:03:41,510 --> 00:03:43,640
>> That's the sort of thing you would do.

83
00:03:43,640 --> 00:03:44,480
>> It's the sort of thing you would

84
00:03:44,480 --> 00:03:46,530
do, but then ,it would be really confusing.

85
00:03:46,530 --> 00:03:47,110
>> No, no, no, it's the sort of

86
00:03:47,110 --> 00:03:48,940
thing YOU would do, again, let's go to Wikipedia.

87
00:03:48,940 --> 00:03:52,136
>> Confusingly, twenty sometimes means ten. [LAUGH]

88
00:03:52,136 --> 00:03:53,970
>> Okay. So what about space?

89
00:03:53,970 --> 00:03:56,630
>> Space, so, I don't understand why,

90
00:03:56,630 --> 00:03:58,170
it would ever need more than constant space.

91
00:03:58,170 --> 00:04:01,680
So, so we're going to, zip around in that. I mean, I guess, if

92
00:04:01,680 --> 00:04:05,550
do it really badly ,we can use K space. To kind of copy over

93
00:04:05,550 --> 00:04:09,178
what those, possible nearest neighbors are. But ,we don't need to keep track

94
00:04:09,178 --> 00:04:12,530
of them. We can just point to them in place, so it's constant.

95
00:04:12,530 --> 00:04:13,030
>> Okay,

96
00:04:15,180 --> 00:04:17,649
yea that's true in fact, because, it's sorted all you really need

97
00:04:17,649 --> 00:04:21,110
to know is the beginning and the ending. So, that's two things

98
00:04:21,110 --> 00:04:23,900
that's constant. Okay, cool, alright, good,

99
00:04:23,900 --> 00:04:25,380
so what about linear regression? Your

100
00:04:25,380 --> 00:04:29,470
favorite little algorithm thing that you did? When we talked about this before.

101
00:04:29,470 --> 00:04:32,030
>> I do like, linear regression. The learning in

102
00:04:32,030 --> 00:04:35,070
this case, is what we are mapping, real number

103
00:04:35,070 --> 00:04:37,730
input to a real number output. The way we

104
00:04:37,730 --> 00:04:41,020
are doing that is we are taking, its probably

105
00:04:41,020 --> 00:04:44,310
M X plus B sort of form [CROSSTALK]. We

106
00:04:44,310 --> 00:04:48,020
need to find, the multiplier and the additive constant. which,

107
00:04:48,020 --> 00:04:52,450
It seems, like, in general doing a regression involves,

108
00:04:52,450 --> 00:04:54,920
inverting a matrix. But, in this case,I think the matrix

109
00:04:54,920 --> 00:04:57,190
that we're talking about is of constant size. So

110
00:04:57,190 --> 00:05:00,152
inverting, is, constant time. I think, it's as easy ,as

111
00:05:00,152 --> 00:05:03,014
basically just scanning through the list ,to populate that,

112
00:05:03,014 --> 00:05:06,030
that ,constant size matrix. So, I'm going to say order n.

113
00:05:06,030 --> 00:05:06,650
>> Yep.

114
00:05:06,650 --> 00:05:08,060
>> To process the data.

115
00:05:08,060 --> 00:05:09,110
>> That is correct.

116
00:05:09,110 --> 00:05:12,190
>> There's probably a really nice algorithm for that.

117
00:05:12,190 --> 00:05:13,940
>> Yeah, probably some kind of linear regression algorithm.

118
00:05:13,940 --> 00:05:16,620
>> Yeah. [LAUGH] No, I mean like the general

119
00:05:16,620 --> 00:05:20,130
linear regression algorithm is, is involves inverting a matrix.

120
00:05:20,130 --> 00:05:20,550
>> Right.

121
00:05:20,550 --> 00:05:24,060
>> Or something like it, something equivalent to it. But here

122
00:05:24,060 --> 00:05:27,730
because, we're, it's all in scaler land, I think it's simpler.

123
00:05:27,730 --> 00:05:30,060
>> Yeah, I think that's right. Okay ,so what about space?

124
00:05:30,060 --> 00:05:31,270
>> All right, so space interpreted

125
00:05:31,270 --> 00:05:32,960
the data that you passed forward from

126
00:05:32,960 --> 00:05:37,960
the learning algorithm, to the regressor, is just,

127
00:05:37,960 --> 00:05:41,960
well MX plus B. It's just M and B, which is constant. There's the two numbers.

128
00:05:41,960 --> 00:05:43,936
>> Right, that's 2, 2 is like 1,

129
00:05:43,936 --> 00:05:46,740
[UNKNOWN] large values [LAUGH] of 1, so it's constant.

130
00:05:46,740 --> 00:05:51,530
>> Yeah, All right, now at query time, you give me an X.

131
00:05:51,530 --> 00:05:56,070
I multiply, it by M and add B, so that's constant time [CROSSTALK]. So,

132
00:05:56,070 --> 00:05:59,080
before the query, cost was expensive and the learning

133
00:05:59,080 --> 00:06:01,310
cost was cheap. And now we've kind of swapped that around.

134
00:06:02,490 --> 00:06:05,470
>> Yeah, we have, so space would be.

135
00:06:05,470 --> 00:06:07,060
>> Space, oh, that you're asking me?

136
00:06:07,060 --> 00:06:07,390
>> Yeah.

137
00:06:07,390 --> 00:06:10,130
>> Space for the query would be constant as well.

138
00:06:10,130 --> 00:06:11,860
>> Right, exactly, so yeah, so you made

139
00:06:11,860 --> 00:06:13,830
a good point here. So earlier on, we had

140
00:06:13,830 --> 00:06:17,360
the situation where learning was fast, was constant, and

141
00:06:17,360 --> 00:06:21,070
querying was, you know, not as fast. It was,

142
00:06:21,070 --> 00:06:24,480
you know, probably logarithmic. But, in the regression case,

143
00:06:24,480 --> 00:06:29,290
learning was expensive and the querying was easy, so

144
00:06:29,290 --> 00:06:31,580
we swapped around, that's exactly right. So, why would

145
00:06:31,580 --> 00:06:34,650
you care about that, well, let's see, I'll point

146
00:06:34,650 --> 00:06:36,510
out something, which is though, even though we swapped

147
00:06:36,510 --> 00:06:38,370
out what was expensive in terms of time and

148
00:06:38,370 --> 00:06:42,910
what wasn't, you'll notice that. It's only logarithmic at

149
00:06:42,910 --> 00:06:47,120
query time for these first two but it's linear for

150
00:06:47,120 --> 00:06:49,240
the learning time, in, linear regression, so doesn't that

151
00:06:49,240 --> 00:06:52,110
mean that linear regression is always slower and worse?

152
00:06:52,110 --> 00:06:55,940
>> No really, because we only have to

153
00:06:55,940 --> 00:06:58,430
learn once, but we can query many, many times.

154
00:06:58,430 --> 00:07:03,210
>> Right, right. So I guess if we query more than, you know,

155
00:07:03,210 --> 00:07:05,520
n times for example ,it'll certainly be,

156
00:07:05,520 --> 00:07:08,040
worse overall. In terms of running time.

157
00:07:08,040 --> 00:07:08,990
>> That's right.

158
00:07:08,990 --> 00:07:09,440
>> Okay.

159
00:07:09,440 --> 00:07:12,190
>> Though, it's, though it's interesting, because like when I see numbers

160
00:07:12,190 --> 00:07:15,470
like this, my, my algorithm, hat tells me that

161
00:07:15,470 --> 00:07:17,520
I should try to balance them a little bit more.

162
00:07:17,520 --> 00:07:20,140
Like, here it's, it's you can make, learning ,essentially

163
00:07:20,140 --> 00:07:22,280
free and the other one you can make querying essentially

164
00:07:22,280 --> 00:07:26,180
free. Really you want to split those, somewhat evenly. Though

165
00:07:26,180 --> 00:07:27,720
it's, not obvious to me how you would do that.

166
00:07:27,720 --> 00:07:30,700
Like, square root of n, learning time and then

167
00:07:30,700 --> 00:07:32,470
square root of n query time or something like that.

168
00:07:32,470 --> 00:07:33,910
>> Yeah, but you did say something else that was

169
00:07:33,910 --> 00:07:35,779
important right? Which is that you only have to learn once.

170
00:07:37,210 --> 00:07:38,420
>> Yeah, It's true.

171
00:07:38,420 --> 00:07:40,880
>> So, the balance you know, really depends

172
00:07:40,880 --> 00:07:42,070
on how often you're going to do querying and

173
00:07:42,070 --> 00:07:43,640
exactly, what power it gives you. I mean,

174
00:07:43,640 --> 00:07:45,190
the trade off is really there. Don't you think?

175
00:07:45,190 --> 00:07:47,660
>> Yeah, I guess so. I mean so, so specifically,

176
00:07:47,660 --> 00:07:50,200
in the, in the version where you just query ones.

177
00:07:50,200 --> 00:07:50,820
>> Right.

178
00:07:50,820 --> 00:07:53,240
>> Then the balance thing could be more interesting.

179
00:07:53,240 --> 00:07:54,940
>> Sure, okay, cool. All right anything else

180
00:07:54,940 --> 00:07:57,230
you want to observe about this. Let's see,

181
00:07:57,230 --> 00:07:59,060
we got the trade off between learning versus

182
00:07:59,060 --> 00:08:00,990
querying. So, either you do all your work upfront,

183
00:08:02,030 --> 00:08:07,380
or, you put it ,off ,and do your work only, when you're forced to at query time.

184
00:08:07,380 --> 00:08:10,330
>> Yeah, I guess, well one thing, is, I want to point out that there's

185
00:08:10,330 --> 00:08:13,320
a nice Mr. Rodgers, reference in the title. That was, that was very cool.

186
00:08:13,320 --> 00:08:16,110
>> Thank you very much. And the second thing, is

187
00:08:16,110 --> 00:08:19,390
that it does strike me, in a sense, that what's going

188
00:08:19,390 --> 00:08:21,700
on here for the nearest neighbor algorithms, is that you

189
00:08:21,700 --> 00:08:24,440
just put off ,doing any work until you absolutely have to.

190
00:08:24,440 --> 00:08:25,641
>> Mm-hm.

191
00:08:25,641 --> 00:08:29,450
>> Which strikes me as kind of a procrastinatory approach.

192
00:08:29,450 --> 00:08:32,919
>> So that's a good word, that you used there, procrastinate.

193
00:08:32,919 --> 00:08:35,539
The words that people use, in the literature, are ,uh, lazy.

194
00:08:35,539 --> 00:08:37,669
>> Mm.

195
00:08:37,669 --> 00:08:40,169
>> They say that these are lazy learners, versus

196
00:08:40,169 --> 00:08:42,900
something like linear regression, which is an eager learner.

197
00:08:42,900 --> 00:08:44,290
>> Eager.

198
00:08:44,290 --> 00:08:47,870
>> Yes. So, linear regression, is eager, it wants to learn right away, and it

199
00:08:47,870 --> 00:08:50,510
does, but nearest neighbor algorithms are lazy. They

200
00:08:50,510 --> 00:08:54,566
want to put off, learning until they absolutely ,have

201
00:08:54,566 --> 00:08:58,029
to, and so we refer to this class ,as lazy and this class as eager.

202
00:08:58,029 --> 00:09:00,950
>> I See, so I guess its the case, that, if

203
00:09:00,950 --> 00:09:04,340
we never query it, then the lazy learner definitely comes out ahead.

204
00:09:04,340 --> 00:09:05,690
>> Right, that makes sense.

205
00:09:05,690 --> 00:09:06,600
>> Yeah.

206
00:09:06,600 --> 00:09:10,800
>> It's just in time learning, or JIL. [LAUGH]

207
00:09:10,800 --> 00:09:12,750
Or JITL, I guess, which doesn't roll off the tongue

208
00:09:12,750 --> 00:09:15,050
anywhere near as well. Okay, cool. So we've gotten

209
00:09:15,050 --> 00:09:16,790
through this quiz. Would you like to do another one?

210
00:09:16,790 --> 00:09:18,860
>> Yeah, I just have to get this JITL off my tongue.

211
00:09:18,860 --> 00:09:21,090
>> [LAUGH]
