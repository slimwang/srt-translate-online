1
00:00:00,160 --> 00:00:03,865
In unit five, remember that we learned the, the GPU hardware processes groups

2
00:00:03,865 --> 00:00:07,767
of threads called warps. And at every thread in a warp, performs the same

3
00:00:07,767 --> 00:00:10,926
instruction at the same time. And this means, that is some threads are not

4
00:00:10,926 --> 00:00:13,427
actively doing computation, they will, they're still going to have to wait

5
00:00:13,427 --> 00:00:18,284
while the active threads in warp do there thing. Now todays GPUs, in fact all

6
00:00:18,284 --> 00:00:24,708
GPU that Nvidia has ever made, have 32 threads per warp. In this first case,

7
00:00:24,708 --> 00:00:29,313
only four out of every 32 threads are actually doing something. And, the rest

8
00:00:29,313 --> 00:00:33,508
of the threads are just along for the ride. In a compacted case, if we can

9
00:00:33,508 --> 00:00:37,245
compact this down to a dense array. Where all four of these elements, are next

10
00:00:37,245 --> 00:00:39,997
to each other and then there's, the remaining elements from elsewhere in the

11
00:00:39,997 --> 00:00:43,896
array all kind of compacted in into one dense array. In that case, all 32

12
00:00:43,896 --> 00:00:47,700
threads in the warp are going to be doing active work and so you'll finish the

13
00:00:47,700 --> 00:00:51,736
total work eight times faster. So that's our answer. We're going to go eight

14
00:00:51,736 --> 00:00:56,880
times faster, if every eighth element is active in the original array, and we

15
00:00:56,880 --> 00:01:02,838
compact it and operate and still in the compacted array. And by the same

16
00:01:02,838 --> 00:01:06,496
reasoning, if only one of these threads in a warp operating on the original

17
00:01:06,496 --> 00:01:10,154
array, has anything useful to do, then you'll go 32 times faster, which is a

18
00:01:10,154 --> 00:01:16,920
big deal. You can see why compaction can lead to big speed ups. But the other

19
00:01:16,920 --> 00:01:19,368
subtlety to remember about warps, is that if all the threads in the warp take

20
00:01:19,368 --> 00:01:24,149
the same branch, so in the code snippet I showed before. If all 32 threads in

21
00:01:24,149 --> 00:01:27,217
the warp check to see if they were active, decided they weren't and then

22
00:01:27,217 --> 00:01:31,760
exited. Then you pay almost no penalty for that. And so that, that warp is

23
00:01:31,760 --> 00:01:34,120
going to fire up, all threads are going to see they have nothing to do, it's

24
00:01:34,120 --> 00:01:38,993
going to disappear. And so, actually the warps that are primarily empty or that

25
00:01:38,993 --> 00:01:44,733
are entirely empty we'll exit immediately. And so, the case where only one in

26
00:01:44,733 --> 00:01:49,631
128 threads is doing something useful, actually still goes about 32 times

27
00:01:49,631 --> 00:01:54,607
faster. because there's one warp that's going to have some work to do. And 31

28
00:01:54,607 --> 00:01:57,380
of the threads in that warp, are going to be sitting around waiting for the

29
00:01:57,380 --> 00:02:01,327
single thread that has some useful work to do. But, all of the, all of the

30
00:02:01,327 --> 00:02:04,382
threads and the entirely empty warps, that have nothing to do, they exit quite

31
00:02:04,382 --> 00:02:08,846
quickly. So you'd pay a very slight penalty for having launched those warps and

32
00:02:08,846 --> 00:02:13,830
having them immediately exist. But, unbalance is still are going to be close to

33
00:02:13,830 --> 00:02:16,463
32 times as fast to operate on the compacted array than it would have been to

34
00:02:16,463 --> 00:02:19,443
operate on the original array.
