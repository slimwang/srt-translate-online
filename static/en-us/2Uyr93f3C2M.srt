1
00:00:00,300 --> 00:00:03,750
In other words, if you increase
the size of your outputs,

2
00:00:03,750 --> 00:00:07,060
your classifier becomes very
confident about its predictions.

3
00:00:07,060 --> 00:00:09,440
But if you reduce
the size of your outputs,

4
00:00:09,440 --> 00:00:11,810
your classifier becomes very unsure.

5
00:00:11,810 --> 00:00:13,390
Keep this in mind for later.

6
00:00:13,390 --> 00:00:17,410
We'll want our classifier to not be
too sure of itself in the beginning.

7
00:00:17,410 --> 00:00:21,480
And then over time,
it will gain confidence as it learns.

8
00:00:21,480 --> 00:00:25,350
Next, we need a way to represent
our labels mathematically.

9
00:00:25,350 --> 00:00:28,750
We just said, let's have the
probabilities for the correct class be

10
00:00:28,750 --> 00:00:33,410
close to 1, and the probability for
all the others be close to 0.

11
00:00:33,410 --> 00:00:35,400
We can write exactly with that.

12
00:00:35,400 --> 00:00:37,920
Each label will be
represented by a vector

13
00:00:37,920 --> 00:00:40,460
that is as long as there are classes and

14
00:00:40,460 --> 00:00:45,780
it has the value 1.0 for the correct
class and 0 every where else.

15
00:00:45,780 --> 00:00:47,710
This is often called one-hot encoding.
