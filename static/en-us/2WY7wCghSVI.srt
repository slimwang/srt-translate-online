1
00:00:00,080 --> 00:00:02,580
Okay Michael, so the second algorithm that we're going to

2
00:00:02,580 --> 00:00:05,270
look at it just like principle components analysis, except it's

3
00:00:05,270 --> 00:00:08,780
called independent components analysis. Okay, and the major difference is

4
00:00:08,780 --> 00:00:12,240
really the difference between the first word principle and independent.

5
00:00:12,240 --> 00:00:15,810
So the main idea here is that PCA is about

6
00:00:15,810 --> 00:00:18,500
finding correlation. And the way it does that is by

7
00:00:18,500 --> 00:00:21,990
maximizing variance. And what that gives you, is the ability

8
00:00:21,990 --> 00:00:25,330
to do reconstruction. What independent components analysis is doing, or

9
00:00:25,330 --> 00:00:31,090
often called ICA by those in the know, is it's trying to maximize independence.

10
00:00:31,090 --> 00:00:33,330
Very simply put, it tries to find

11
00:00:33,330 --> 00:00:36,690
a linear transformation of your feature space, into

12
00:00:36,690 --> 00:00:42,810
a new feature space, such that each of the individual new features are mutually

13
00:00:42,810 --> 00:00:44,470
independent and I mean that in a

14
00:00:44,470 --> 00:00:49,290
statistical sense. So, you are converting your XI,

15
00:00:49,290 --> 00:00:55,850
your X1, X2, XI... And there's some new features space let's call it I don't

16
00:00:55,850 --> 00:01:04,019
know let's call it a Y, Y1,Y2 YI... Such that each

17
00:01:04,019 --> 00:01:09,280
one of the new features are statistically independent of one another, that is

18
00:01:09,280 --> 00:01:13,510
to say their mutual information is equal to zero. Does that make sense?

19
00:01:13,510 --> 00:01:14,390
>> And

20
00:01:14,390 --> 00:01:16,320
this is going to be a linear transformation?

21
00:01:16,320 --> 00:01:18,360
>> It's going to be a linear transformation.

22
00:01:18,360 --> 00:01:21,930
>> T, to make them statistically independent.

23
00:01:21,930 --> 00:01:26,010
>> So, I find some linear transformation here, which is going to take my

24
00:01:26,010 --> 00:01:28,640
original feature space, which I'm representing with

25
00:01:28,640 --> 00:01:30,760
these X's, and transform it into a

26
00:01:30,760 --> 00:01:34,400
new feature space, such that, if I were to treat each of these new

27
00:01:34,400 --> 00:01:36,800
features as random variables and compute their

28
00:01:36,800 --> 00:01:39,590
mutual information, I would get for all

29
00:01:39,590 --> 00:01:43,730
pairs a mutual information of zero. That's part one and

30
00:01:43,730 --> 00:01:45,330
the second thing that it's trying to do is that

31
00:01:45,330 --> 00:01:49,240
it's trying to make certain that the mutual information between

32
00:01:49,240 --> 00:01:54,280
all of the features, y and the original feature space x

33
00:01:54,280 --> 00:01:56,540
is as high as possible. So in other words, we

34
00:01:56,540 --> 00:01:58,910
want to be able to reconstruct the data. We want

35
00:01:58,910 --> 00:02:04,840
to be able to predict an X from a Y and a Y from a X. While at the same time

36
00:02:04,840 --> 00:02:07,200
making sure that each of the new dimensions is in

37
00:02:07,200 --> 00:02:10,620
>> fact mutually independent. In a statistical sense.

38
00:02:10,620 --> 00:02:12,490
>> I think I'm going to need an example.
