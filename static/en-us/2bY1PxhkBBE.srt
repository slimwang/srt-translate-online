1
00:00:00,340 --> 00:00:03,810
So we write the program as a truly parallel program, or in other words,

2
00:00:03,810 --> 00:00:06,920
the application programmer is going to think

3
00:00:06,920 --> 00:00:11,440
about his application and write the program as

4
00:00:11,440 --> 00:00:14,290
an explicitly parallel program. And there are

5
00:00:14,290 --> 00:00:18,920
two styles of writing explicitly parallel programs. And

6
00:00:18,920 --> 00:00:22,330
correspondingly, system support for those two tiles

7
00:00:22,330 --> 00:00:25,580
of explicitly pavalled programs. One is called message

8
00:00:25,580 --> 00:00:28,690
passing style of explicitly pavalled program. The

9
00:00:28,690 --> 00:00:31,430
run time system is going to provide a message

10
00:00:31,430 --> 00:00:35,430
passing laterally which has primitives for. An

11
00:00:35,430 --> 00:00:39,130
application thread to do sends and receives to

12
00:00:39,130 --> 00:00:41,660
its peers that are executing on other

13
00:00:41,660 --> 00:00:44,380
nodes of the cluster. So this message passing

14
00:00:44,380 --> 00:00:47,830
style of explicitly parallel program is true to

15
00:00:47,830 --> 00:00:50,650
the physical nature of the cluster. The physical

16
00:00:50,650 --> 00:00:52,400
nature of the cluster is the fact that

17
00:00:52,400 --> 00:00:56,320
every processor has its private memory. And this memory

18
00:00:56,320 --> 00:00:59,330
is not shared across all the processors. So

19
00:00:59,330 --> 00:01:03,440
the only way a processor can communicate with, another

20
00:01:03,440 --> 00:01:05,840
processor is by sending a message through the

21
00:01:05,840 --> 00:01:09,650
network that this processor can receive. This processor cannot

22
00:01:09,650 --> 00:01:12,400
directly reach into the memory of this processor. Because

23
00:01:12,400 --> 00:01:15,680
that is not the way a cluster is architected.

24
00:01:15,680 --> 00:01:19,050
So, the messaging passing library is true

25
00:01:19,050 --> 00:01:21,860
to the physical nature of the cluster. That

26
00:01:21,860 --> 00:01:24,070
there is no physically shared memory. And

27
00:01:24,070 --> 00:01:26,790
lots of examples of message passing libraries that

28
00:01:26,790 --> 00:01:29,980
have been written to support explicit parallel

29
00:01:29,980 --> 00:01:33,950
programming. In a cluster they include MPI, message

30
00:01:33,950 --> 00:01:38,510
passing interface, MPI for short, PVM, CLF

31
00:01:38,510 --> 00:01:40,990
from digital equipment corporations. So these are all

32
00:01:40,990 --> 00:01:46,320
examples of message passing libraries that have been built with the intent of

33
00:01:46,320 --> 00:01:50,430
allowing application programmer to write explicitly parallel

34
00:01:50,430 --> 00:01:53,400
programs using this message passing style. And

35
00:01:53,400 --> 00:01:56,350
to this day, many scientific applications

36
00:01:56,350 --> 00:02:00,300
running on large scale clusters in national

37
00:02:00,300 --> 00:02:03,400
labs like Lawrence Livermore, and Argonne National

38
00:02:03,400 --> 00:02:06,110
Labs and so on, use this style

39
00:02:06,110 --> 00:02:08,669
of programming using MPI as the message

40
00:02:08,669 --> 00:02:11,895
passing fabric. Now, the only downside to the

41
00:02:11,895 --> 00:02:15,350
message-passing style of programming is that it is

42
00:02:15,350 --> 00:02:18,190
difficult to program using this style. If you're

43
00:02:18,190 --> 00:02:22,150
a programmer who's written sequential programs, the transitions

44
00:02:22,150 --> 00:02:25,560
paths to writing an explicitly parallel program is

45
00:02:25,560 --> 00:02:28,940
easier if there is this. Notion of shared

46
00:02:28,940 --> 00:02:31,190
memory, because it is natural to think of

47
00:02:31,190 --> 00:02:34,300
shared data structures among different threads of

48
00:02:34,300 --> 00:02:37,180
an application. And that's the reason making the

49
00:02:37,180 --> 00:02:40,560
transition from sequential program to parallel programming, using

50
00:02:40,560 --> 00:02:43,490
for instance the P thread library or SMP

51
00:02:43,490 --> 00:02:46,680
is fairly intuitive and easy pathway. On

52
00:02:46,680 --> 00:02:49,160
the other hand, If the programmer has to

53
00:02:49,160 --> 00:02:52,310
think in terms of coordinating the activities on

54
00:02:52,310 --> 00:02:56,600
different processes by explicitly descending and desisting messages

55
00:02:56,600 --> 00:03:01,320
from their peers. That is calling for a failuratical change

56
00:03:01,320 --> 00:03:03,600
of thinking in terms of how to structure a program.
