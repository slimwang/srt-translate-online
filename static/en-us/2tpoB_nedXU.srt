1
00:00:00,310 --> 00:00:04,729
So what's RAID 1 reliability if we
do replace failed disks as soon as

2
00:00:04,729 --> 00:00:06,240
possible?

3
00:00:06,240 --> 00:00:11,500
We have the situation that both disks
are okay for this amount of time.

4
00:00:11,500 --> 00:00:16,450
Then a disk fails, and then we have
only one okay disk for the mean time to

5
00:00:16,450 --> 00:00:21,340
repair for one disk, so this is how long
does it take us to replace the disk?

6
00:00:21,340 --> 00:00:24,800
After that,
both disks are okay again, and

7
00:00:24,800 --> 00:00:27,820
then we again have this long
until the next failure.

8
00:00:27,820 --> 00:00:33,430
So what's the mean time to data loss for
RAID 1 with two disks?

9
00:00:33,430 --> 00:00:37,690
The question really is,
how many times can we have this

10
00:00:37,690 --> 00:00:41,800
until we encounter the period when
a single disk has failed, and

11
00:00:41,800 --> 00:00:46,800
during the time it takes to replace it,
another disk fails?

12
00:00:46,800 --> 00:00:49,826
Well, it's equal to
the length of this interval.

13
00:00:49,826 --> 00:00:55,090
This is how how long we survive
with both disks working, times,

14
00:00:55,090 --> 00:00:59,130
how many times can we get this until
we encounter a repair period where

15
00:00:59,130 --> 00:01:01,440
the second disk failed during it.

16
00:01:01,440 --> 00:01:06,130
So, the question is really what's the
probability of the second disk failing

17
00:01:06,130 --> 00:01:08,890
during the time to repair one disk.

18
00:01:08,890 --> 00:01:12,420
Now, normally, to calculate this
probability, we would have to compute

19
00:01:12,420 --> 00:01:17,380
the distribution for this function and
then see what's the probability for

20
00:01:17,380 --> 00:01:20,760
this distribution being under MTTR 1,
and so on.

21
00:01:20,760 --> 00:01:25,755
But when the time to repair
is a lot smaller then

22
00:01:25,755 --> 00:01:30,225
the time to failure of a single disk,
then we can approximate this

23
00:01:30,225 --> 00:01:34,015
probability of the second disk
failing during the MTTR for

24
00:01:34,015 --> 00:01:39,952
the first disk simply as
the MTTR divided by the MTTF.

25
00:01:39,952 --> 00:01:44,862
So, for example, if the MTTR is,
let's say, a single day, and

26
00:01:44,862 --> 00:01:50,002
MTTF is 100 days, then
the probability of the second failure

27
00:01:50,002 --> 00:01:54,902
during the repair period is
simply the length of the repair

28
00:01:54,902 --> 00:02:00,402
period divided by the period that
we expected this to survive.

29
00:02:00,402 --> 00:02:04,810
So for example, if the time to
repair is, for example, one day, and

30
00:02:04,810 --> 00:02:09,250
the time to failure is, let's say,
a hundred days, then we can expect that

31
00:02:09,250 --> 00:02:15,390
there is a one in 100 chance off of
this failing during the one day.

32
00:02:15,390 --> 00:02:20,760
So now how many times can we repair
this before we encounter one of these?

33
00:02:20,760 --> 00:02:24,100
Well, the number of
times we can try this

34
00:02:24,100 --> 00:02:28,550
until we encounter a failure is
going to be 1 over this probability.

35
00:02:28,550 --> 00:02:31,790
So what we finally get is this.

36
00:02:31,790 --> 00:02:37,500
The mean time to data loss is
MTTF of a single disk squared,

37
00:02:37,500 --> 00:02:41,410
divided by 2 times the mean
to repair for one disk.

38
00:02:41,410 --> 00:02:44,730
How long does it take us to replace
the disk and copy the data on it so

39
00:02:44,730 --> 00:02:47,430
that now we have a fully
functional RAID 1 array?

40
00:02:47,430 --> 00:02:50,820
And as a reminder, this is how long

41
00:02:50,820 --> 00:02:54,870
do we get a working array with
both these functional, and

42
00:02:54,870 --> 00:02:59,910
this is how many times do we do a repair
until we can expect that the second

43
00:02:59,910 --> 00:03:04,910
disk that is covering us during
the repair is going to fail on us.

44
00:03:04,910 --> 00:03:10,670
Not that because the MTTF Is
much larger than the MTTR,

45
00:03:10,670 --> 00:03:15,600
this is measured in years, usually,
this hopefully is hours or days.

46
00:03:15,600 --> 00:03:17,981
It's just how long does it
take us to replace the disk.

47
00:03:17,981 --> 00:03:22,530
This factor here is very,
very large, and now we get many,

48
00:03:22,530 --> 00:03:26,340
many times the MTTF of
a single disk over 2.

49
00:03:26,340 --> 00:03:30,530
This factor way larger than 2, which
means that the time to data loss with

50
00:03:30,530 --> 00:03:36,400
a RAID 1 array with only two disks is
way higher than the time to data loss

51
00:03:36,400 --> 00:03:41,290
for a single disk, which means that RAID
1 is dramatically improving reliability.
