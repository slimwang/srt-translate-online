1
00:00:00,170 --> 00:00:04,040
There's a really nice little a graphical
illustration of tracking with

2
00:00:04,040 --> 00:00:04,810
Kalman filters.

3
00:00:04,810 --> 00:00:07,939
So let's suppose we have
some current estimate

4
00:00:07,939 --> 00:00:09,570
of where we think things are, right.

5
00:00:09,570 --> 00:00:12,710
And that's what's represented by this
little circle here in the x y plane.

6
00:00:13,980 --> 00:00:18,850
Our prediction shifts and grows, and
that's why the, the size if you will.

7
00:00:18,850 --> 00:00:23,450
And you can think of this as being
a a single, say, some number du,

8
00:00:23,450 --> 00:00:25,210
of standard deviations of uncertainty.

9
00:00:25,210 --> 00:00:28,340
So our, our uncertainty matrix
got significantly bigger, and

10
00:00:28,340 --> 00:00:30,450
the mean may have shifted as well as,
also.

11
00:00:30,450 --> 00:00:31,380
So that's our prediction.

12
00:00:32,390 --> 00:00:33,690
We then take a measurement, and

13
00:00:33,690 --> 00:00:38,480
you'll notice that the measurement
here is offset a little bit.

14
00:00:38,480 --> 00:00:41,290
I mean, and it happens, you know, it's,
it's mean is right in the middle.

15
00:00:41,290 --> 00:00:42,390
And it's actually tighter.

16
00:00:42,390 --> 00:00:46,550
So it actually has a tighter
co-variance than the prediction, okay?

17
00:00:46,550 --> 00:00:50,880
But the thing that's important
to realize is the correction

18
00:00:50,880 --> 00:00:54,500
has a tighter co-variance
than either one of them.

19
00:00:54,500 --> 00:00:59,310
So you'll see that the mean of
the final correction is somewhere

20
00:00:59,310 --> 00:01:03,210
between the mean of the measurement and
the mean of the prediction.

21
00:01:03,210 --> 00:01:05,820
Remember, it's going to be
a weighting between them.

22
00:01:05,820 --> 00:01:10,880
And also notice that the co-variance
of the final corrected estimate

23
00:01:10,880 --> 00:01:13,650
is smaller than either of the two,
all right?

24
00:01:13,650 --> 00:01:17,370
And this might seem a little
weird when you look just at, say,

25
00:01:17,370 --> 00:01:21,420
the, the co-variance of the measurement
and the co-variance of the prediction,

26
00:01:21,420 --> 00:01:26,560
because they're both pretty big and
you might ask, how is it that

27
00:01:26,560 --> 00:01:30,880
the co-variance goes down even
tighter than the measurement?

28
00:01:30,880 --> 00:01:35,660
And again, the answer is, information
can only reduce your uncertainty.

29
00:01:35,660 --> 00:01:38,730
So whether you think of as measurement
as reducing your uncertainty about

30
00:01:38,730 --> 00:01:41,150
the prediction or

31
00:01:41,150 --> 00:01:44,600
the prediction is reducing
the uncertainty about the measurement.

32
00:01:44,600 --> 00:01:47,800
What that means is your final
co-variance will be less.

33
00:01:47,800 --> 00:01:51,240
You'd be amazed at the number of PhD
qualifying students who when asked that

34
00:01:51,240 --> 00:01:54,580
question standing in front of
the board kind of flunk through it.

35
00:01:54,580 --> 00:01:58,350
So if you're thinking about doing a PhD
and I'm going to be on your qualifying

36
00:01:58,350 --> 00:02:00,440
committee and
you're doing something in robotics.

37
00:02:00,440 --> 00:02:02,860
Get to know and lerve,
love, your Kalman filters.
