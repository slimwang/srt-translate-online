1
00:00:00,310 --> 00:00:02,850
Hey Charles, this is when we get to
talk about reinforcement learning.

2
00:00:02,850 --> 00:00:04,280
>> Hi Michael.
This is when I get to

3
00:00:04,280 --> 00:00:05,700
hear about reinforcement learning.

4
00:00:05,700 --> 00:00:07,410
>> Wow.
I'm glad we're on the same page.

5
00:00:07,410 --> 00:00:08,520
So-
>> Are we on the same page?

6
00:00:08,520 --> 00:00:09,970
Is this all of reinforcement learning or

7
00:00:09,970 --> 00:00:11,490
is it just the reinforcement
learning basics?

8
00:00:11,490 --> 00:00:13,050
>> We're going to start with the basics.

9
00:00:13,050 --> 00:00:14,370
>> Oh, okay.
I can't wait to hear what that is.

10
00:00:14,370 --> 00:00:17,170
>> So the first concept to try
to understand when you're doing

11
00:00:17,170 --> 00:00:20,730
reinforcement learning is that a lot of
it takes place as a conversation between

12
00:00:20,730 --> 00:00:22,650
an agent and an environment.

13
00:00:22,650 --> 00:00:26,430
>> Okay, so like right now, you're
the agent and I'm the environment.

14
00:00:27,430 --> 00:00:29,670
>> Actually I think I'm
going to have you be the agent.

15
00:00:29,670 --> 00:00:30,570
>> Okay.

16
00:00:30,570 --> 00:00:32,400
>> And we'll just imagine some kind of,
I don't know,

17
00:00:32,400 --> 00:00:33,520
like a video game environment.

18
00:00:33,520 --> 00:00:35,010
>> That sounds reasonable.

19
00:00:35,010 --> 00:00:36,345
By the way did you
notice I've lost weight?

20
00:00:36,345 --> 00:00:39,010
>> [LAUGH] Oh, good job,
how did you do that?

21
00:00:39,010 --> 00:00:40,375
>> Well, I got drawn as a stick figure.

22
00:00:40,375 --> 00:00:42,580
>> [LAUGH] That's fair.

23
00:00:42,580 --> 00:00:46,300
So here we are, the agent and
the environment, and

24
00:00:46,300 --> 00:00:50,460
the conversation basically talks
about what is going back and

25
00:00:50,460 --> 00:00:52,030
forth between the agent and
the environment.

26
00:00:52,030 --> 00:00:54,620
So, the environment is going
to reveal itself to you,

27
00:00:54,620 --> 00:00:57,550
to the agent, in the form of states, S.

28
00:00:58,940 --> 00:01:04,480
You then get to have some influence
on the environment by taking actions,

29
00:01:04,480 --> 00:01:09,550
a, and then you also receive back,
before the next state,

30
00:01:10,640 --> 00:01:15,710
some kind of reward for the most
recent state action combination.

31
00:01:15,710 --> 00:01:16,420
>> Okay.
Fair enough.

32
00:01:17,530 --> 00:01:20,430
>> So this is the same kind of
elements that we have in an MDP.

33
00:01:20,430 --> 00:01:23,310
But the important thing is that instead
of just being given an MDP as some kind

34
00:01:23,310 --> 00:01:25,415
of a graph structure and
then we get to compute on it.

35
00:01:25,415 --> 00:01:29,750
Really the computation's happening
inside the head of the agent and

36
00:01:29,750 --> 00:01:33,070
the information about the environment
is really only available through

37
00:01:33,070 --> 00:01:35,020
the course of this interaction.

38
00:01:35,020 --> 00:01:36,510
So does that make some sense?

39
00:01:36,510 --> 00:01:42,110
>> It does make some sense but I guess
how is that any different from the MDP?

40
00:01:42,110 --> 00:01:45,150
Well, it's the same story as how
a policy interacts with an MDP.

41
00:01:45,150 --> 00:01:48,820
Right, where this is playing
the role of the MDP,

42
00:01:48,820 --> 00:01:52,175
and this is playing
the role of the policy, pi.

43
00:01:52,175 --> 00:01:52,810
>> Mm-hm.

44
00:01:52,810 --> 00:01:56,260
>> But now, again, the computational
aspect of the system here,

45
00:01:56,260 --> 00:02:00,520
the agent doesn't know the environment,
it's not living inside the agent's head.

46
00:02:00,520 --> 00:02:04,760
Instead the agent is just experiencing
the environment by interacting with it.

47
00:02:04,760 --> 00:02:07,860
It can then you know if it so
chose build

48
00:02:07,860 --> 00:02:11,020
some kind of a model of the environment
in its head and then think about that.

49
00:02:11,020 --> 00:02:12,720
But what's in the agent's head and

50
00:02:12,720 --> 00:02:15,810
what's in the environment are two
different things in this set up.

51
00:02:15,810 --> 00:02:16,400
>> Okay fair enough.

52
00:02:16,400 --> 00:02:17,570
I get that.

53
00:02:17,570 --> 00:02:20,380
>> So maybe I can make
this a little bit clearer.

54
00:02:20,380 --> 00:02:22,760
So let's actually put
you in this environment.

55
00:02:22,760 --> 00:02:23,700
What do you say?

56
00:02:23,700 --> 00:02:25,490
>> Okay, metaphorically?

57
00:02:25,490 --> 00:02:26,670
>> No, let's just do it.

58
00:02:26,670 --> 00:02:27,170
>> Sure.
