1
00:00:00,000 --> 00:00:06,000
In this case, the problem really comes down to the naive Bayes assumption is

2
00:00:06,000 --> 00:00:09,000
a weak one, and the Markov assumption would do much better.

3
00:00:09,000 --> 00:00:12,000
It wouldn't really help to have more data or to do a better job of smoothing,

4
00:00:12,000 --> 00:00:16,000
because I already have good counts for words like "in" and "significant"

5
00:00:16,000 --> 00:00:18,000
as well as words like "small" and "and."

6
00:00:18,000 --> 00:00:22,000
They're all common enough that I have a good representation of how often they occur

7
00:00:22,000 --> 00:00:25,000
as a unigram as a single word.

8
00:00:25,000 --> 00:00:30,000
The problem is that we would like to know that the word "small" goes very well

9
00:00:30,000 --> 00:00:35,000
with the word "insignificant" but does not goes very well with the word "significant."

10
00:00:35,000 --> 00:00:40,000
So if we had a Markov model where the probability of "insignificant" depended

11
00:00:40,000 --> 00:00:44,000
on the probability of "small," then we could catch that,

12
00:00:44,000 --> 99:59:59,999
and we could get this segmentation correct.
