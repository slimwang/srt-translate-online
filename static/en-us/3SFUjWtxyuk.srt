1
00:00:00,190 --> 00:00:02,800
What about those discriminative
methods we were talking about?

2
00:00:04,250 --> 00:00:08,670
As we said before they find a division,
a surface between the classes.

3
00:00:08,670 --> 00:00:11,190
We're going to talk about
a couple of different methods.

4
00:00:11,190 --> 00:00:13,920
The one that I want to talk about
today is nearest neighbors.

5
00:00:13,920 --> 00:00:16,341
And then, we'll do separate lessons for
each of the, the other two.

6
00:00:17,520 --> 00:00:19,940
So the simplest method we're going to
talk about is nearest neighbor

7
00:00:19,940 --> 00:00:21,070
classification.

8
00:00:21,070 --> 00:00:24,450
And it is a discriminative method
because we are using the boundaries

9
00:00:24,450 --> 00:00:29,020
between I wont say classes,
I'll say examples, right.

10
00:00:29,020 --> 00:00:31,400
The boundary is always between
examples of the classes.

11
00:00:31,400 --> 00:00:33,940
But we actually don't have
to do very much training.

12
00:00:33,940 --> 00:00:36,789
So the idea is we have some future
space here labeled as blank.

13
00:00:37,940 --> 00:00:39,770
It should be x1 and x2.

14
00:00:39,770 --> 00:00:43,250
This picture by the way is taken
from a classic book, due to Hart.

15
00:00:43,250 --> 00:00:44,467
It was, the original was due to Hart,

16
00:00:44,467 --> 00:00:46,060
the new version is due to Hart and
Stork.

17
00:00:46,060 --> 00:00:50,600
And it's a good way of learning sort
of classical pattern recognition.

18
00:00:50,600 --> 00:00:52,910
So the idea is I've got
a bunch of examples.

19
00:00:52,910 --> 00:00:56,590
The negatives, so the not a's
are these little black points, here.

20
00:00:56,590 --> 00:00:57,870
So the, the, the black thing.

21
00:00:57,870 --> 00:00:59,970
And then I've got some
positive examples, right?

22
00:00:59,970 --> 00:01:01,660
And those are the little red ones,
there.

23
00:01:01,660 --> 00:01:05,000
And when I come up with a new point,
all right?

24
00:01:05,000 --> 00:01:06,410
So there it is in the green.

25
00:01:06,410 --> 00:01:09,780
Basically, that's my
new novel test example.

26
00:01:09,780 --> 00:01:10,982
And how am I going to label that?

27
00:01:10,982 --> 00:01:15,150
I'm going to find the closest
training example, so in this

28
00:01:15,150 --> 00:01:19,410
case the closest training example
is that one, and so I say, ah-ha.

29
00:01:19,410 --> 00:01:24,062
The closest one is a positive, so I'm
going to label this new one a positive.

30
00:01:24,062 --> 00:01:28,770
All right, now drawn on this picture
actually are the divisions themselves,

31
00:01:28,770 --> 00:01:32,180
and the computer scientists
among you will recognize that as

32
00:01:32,180 --> 00:01:33,480
a Voronoi partitioning.

33
00:01:33,480 --> 00:01:37,100
Right, when you partition a space
using a, a Voronoi method,

34
00:01:37,100 --> 00:01:41,490
you essentially carve the space
up into these little chunks.

35
00:01:41,490 --> 00:01:43,990
Where this chunk means that
if you're in that chunk,

36
00:01:43,990 --> 00:01:46,580
this black point is
the closest one to you.

37
00:01:46,580 --> 00:01:49,988
And so all nearest neighbor
is doing is giving you

38
00:01:49,988 --> 00:01:53,710
a Voronoi partitioning of the space.

39
00:01:53,710 --> 00:01:57,280
It's shall we say pretty
easy to implement.

40
00:01:57,280 --> 00:01:59,670
It has a couple problems,
one is it doesn't work very well.

41
00:01:59,670 --> 00:02:00,910
We'll get to that in a second.

42
00:02:00,910 --> 00:02:05,910
It's also, very data intensive in terms
of the memory of what you need to know.

43
00:02:05,910 --> 00:02:08,710
Right, so if I give you
a million trainee examples,

44
00:02:08,710 --> 00:02:10,710
how many of them do you
have to remember, Megan?

45
00:02:10,710 --> 00:02:11,420
>> A million.

46
00:02:11,420 --> 00:02:13,620
>> A million, that's exactly right.

47
00:02:13,620 --> 00:02:20,360
Furthermore, every time a new point
comes in I gotta find the closest one.

48
00:02:20,360 --> 00:02:21,850
So if I was really dumb,

49
00:02:21,850 --> 00:02:24,590
you know I don't have a compute, Masters
of Computer Science Degree from Georgia,

50
00:02:24,590 --> 00:02:26,580
I might list go through
all of them one at a time.

51
00:02:26,580 --> 00:02:29,490
If I'm a little smarter I'll use
something called a KD tree or

52
00:02:29,490 --> 00:02:31,680
some other hierarchical
representation but

53
00:02:31,680 --> 00:02:33,940
you're still searching through
a lot of these things.

54
00:02:33,940 --> 00:02:36,448
So at test time,
it's very painful as well so

55
00:02:36,448 --> 00:02:39,550
not only is it a lot of memory stored,
etc.

56
00:02:39,550 --> 00:02:42,700
But let's get back to that
doesn't work all that well thing.

57
00:02:42,700 --> 00:02:47,080
Well one of the things that might
happen is that I might have, you know,

58
00:02:47,080 --> 00:02:49,647
an occasional kind of spurious point.

59
00:02:49,647 --> 00:02:54,313
Or I might be in an area where I really
don't have too many points nearby.

60
00:02:54,313 --> 00:02:58,740
And what I'd like to do is be able
to make a more robust decision.

61
00:02:58,740 --> 00:03:02,318
Okay, and the way of doing
that is referred to as k-NN or

62
00:03:02,318 --> 00:03:04,190
K-Nearest Neighbor.

63
00:03:04,190 --> 00:03:05,770
And it's really very simple.

64
00:03:05,770 --> 00:03:08,950
It's basically the idea, if I've got
some new point, and it's written here as

65
00:03:08,950 --> 00:03:14,680
an x, so that point right there,
I don't just find the nearest point.

66
00:03:14,680 --> 00:03:16,810
You gotta think like
a computer scientist.

67
00:03:16,810 --> 00:03:17,420
You find k.

68
00:03:18,600 --> 00:03:21,480
So k might be one, might be three,
might be five, might be seven.

69
00:03:21,480 --> 00:03:24,660
Whatever choice you choose to make,
you would look for k.

70
00:03:24,660 --> 00:03:30,650
So, in k-NN, in 5-NN, for example,
you know, if I have this point x, okay?

71
00:03:30,650 --> 00:03:32,740
What I'm going to do is,
I'm going to look for

72
00:03:32,740 --> 00:03:34,700
the five nearest neighbors, right?

73
00:03:34,700 --> 00:03:38,840
And I'll just, one,
two, three, four, five.

74
00:03:38,840 --> 00:03:44,700
Okay, and in this particular case three
of them are black, two of them are red,

75
00:03:44,700 --> 00:03:48,840
black is negative, red is positive so
I would classify it as what?

76
00:03:48,840 --> 00:03:49,670
Negative.

77
00:03:49,670 --> 00:03:50,170
Okay?

78
00:03:51,360 --> 00:03:53,088
One of the funny things, well,

79
00:03:53,088 --> 00:03:57,480
one of the interesting things about
k-NN is it works really well, okay?

80
00:03:57,480 --> 00:04:00,810
It does have this data intensive
problem, and there are methods that we

81
00:04:00,810 --> 00:04:04,520
now use that are kind of,
they're sort of related to k-NN.

82
00:04:04,520 --> 00:04:10,062
But this idea of getting a loose
consensus is very effective.

83
00:04:10,062 --> 00:04:12,930
We're not actually going to talk about
a thing called random forest later,

84
00:04:12,930 --> 00:04:16,230
but this notion of consensus.

85
00:04:16,230 --> 00:04:20,970
So I don't get the support from just one
place or even one classification method

86
00:04:20,970 --> 00:04:24,260
little classifier, this notion
of consensus is very powerful.
