1
00:00:00,690 --> 00:00:02,469
We've covered a number of approaches for

2
00:00:02,469 --> 00:00:06,680
cleaning up data within a field, but
after you've completed that step,

3
00:00:06,680 --> 00:00:10,000
you can still have an issue
with duplicate records.

4
00:00:10,000 --> 00:00:14,460
Duplicate records can end up in your
dataset because of a manual mistake,

5
00:00:14,460 --> 00:00:17,200
where a record may have
been entered twice.

6
00:00:17,200 --> 00:00:19,580
Or it could be some
kind of program error,

7
00:00:19,580 --> 00:00:22,340
where some data was submitted twice.

8
00:00:22,340 --> 00:00:26,463
Just as I've mentioned earlier about
getting a piece of mail with your name

9
00:00:26,463 --> 00:00:30,659
spelled incorrectly, have you ever
received the same piece of mail twice?

10
00:00:30,659 --> 00:00:35,397
That's a costly mistake, that should
never happen, since removing duplicates

11
00:00:35,397 --> 00:00:38,910
from your data set or de-duping,
can be done quite easily.
