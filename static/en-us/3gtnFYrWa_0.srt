1
00:00:00,330 --> 00:00:02,310
Now I'm going to introduce you to a

2
00:00:02,310 --> 00:00:05,380
lazy version of the RC memory model. And it's

3
00:00:05,380 --> 00:00:09,460
called LRC, stands for lazy RC. Now this is

4
00:00:09,460 --> 00:00:12,290
the structure of your pilot program so a thread

5
00:00:12,290 --> 00:00:14,860
acquires a lot, does the data accessing, releases the

6
00:00:14,860 --> 00:00:17,970
lock. Another thread may acquire the same lock. And

7
00:00:17,970 --> 00:00:21,080
if the critical section for P1 Precedes the critical

8
00:00:21,080 --> 00:00:25,720
section for P2, then the RC memory model requires

9
00:00:25,720 --> 00:00:28,210
that, at the point of release, you

10
00:00:28,210 --> 00:00:31,740
ensure that all the modifications that have been

11
00:00:31,740 --> 00:00:34,740
made on processor P1, that is, the

12
00:00:34,740 --> 00:00:39,535
coherence actions. That are commensurate with those data

13
00:00:39,535 --> 00:00:42,550
acceses, are all communicated to all the

14
00:00:42,550 --> 00:00:45,629
peer processes including P2. Then you release the

15
00:00:45,629 --> 00:00:47,670
lock. That's the semantic. And this is

16
00:00:47,670 --> 00:00:51,050
what is called eager release consistency, meaning that

17
00:00:51,050 --> 00:00:53,910
at the point of release you're insuring that the

18
00:00:53,910 --> 00:00:57,640
whole system is cache coherent The whole system is

19
00:00:57,640 --> 00:01:00,450
cache coherent at the point of release, then you

20
00:01:00,450 --> 00:01:03,050
release from the lock. And the cache coherence is

21
00:01:03,050 --> 00:01:06,540
with respect to the set of data accesses that

22
00:01:06,540 --> 00:01:08,830
have gone on on this process up to this

23
00:01:08,830 --> 00:01:12,710
point, that's what we are ensuring has been communicated

24
00:01:12,710 --> 00:01:16,070
and made consistent on all the processes. Now let's

25
00:01:16,070 --> 00:01:18,730
say that the timeline looks like this and P1's

26
00:01:18,730 --> 00:01:22,880
release happened at this point. And P2's acquire of

27
00:01:22,880 --> 00:01:25,600
the same lock happened at a much later point

28
00:01:25,600 --> 00:01:28,000
in time. That's the luck of the draw in

29
00:01:28,000 --> 00:01:30,600
terms of how the computation went, and so there

30
00:01:30,600 --> 00:01:34,510
is this time window between P1's release of the

31
00:01:34,510 --> 00:01:37,560
lock and P2's acquisition of the same lock. Now

32
00:01:37,560 --> 00:01:41,300
if you think about it there's an opportunity for procrastination.

33
00:01:41,300 --> 00:01:45,360
Now we saw that procrastination often helps in

34
00:01:45,360 --> 00:01:49,210
system design. We've seen this in mutual exclusion locks.

35
00:01:49,210 --> 00:01:52,100
If you insert delay between successive trials of

36
00:01:52,100 --> 00:01:54,470
trying to get the lock, that actually results in

37
00:01:54,470 --> 00:01:57,310
better performance. We saw that in processes scheduling

38
00:01:57,310 --> 00:02:01,040
too. Instead of eagerly scheduling the next available task

39
00:02:01,040 --> 00:02:03,170
Maybe you want to wait for a task

40
00:02:03,170 --> 00:02:06,450
that has more affinity to your processor. That results

41
00:02:06,450 --> 00:02:11,190
in performance advantage. So procrastination often is

42
00:02:11,190 --> 00:02:13,520
a good friend of system design. So

43
00:02:13,520 --> 00:02:15,210
here again there is an opportunity for

44
00:02:15,210 --> 00:02:19,120
procrastination. So Lazy RC is another instance

45
00:02:19,120 --> 00:02:22,480
where procrastination may actually help in optimizing

46
00:02:22,480 --> 00:02:25,700
the system performance. The idea is that,

47
00:02:25,700 --> 00:02:28,940
rather than performing all the coherence actions

48
00:02:28,940 --> 00:02:32,200
at the point of release. Don't do it,

49
00:02:32,200 --> 00:02:36,540
procrastinate. Wait till the acquire actually happens.

50
00:02:36,540 --> 00:02:38,560
At the point of acquire, take all the

51
00:02:38,560 --> 00:02:42,550
coherence actions before allowing this acquire to succeed.

52
00:02:42,550 --> 00:02:45,730
So the key point is that you're deafening

53
00:02:45,730 --> 00:02:48,260
the point at which you ensure that all

54
00:02:48,260 --> 00:02:51,050
the coherence actions are complete to the point

55
00:02:51,050 --> 00:02:54,020
of acquisition as opposed to the point of

56
00:02:54,020 --> 00:02:57,500
release. Even if all the coherence actions commensurate

57
00:02:57,500 --> 00:02:59,300
with the data accesses that have gone on

58
00:02:59,300 --> 00:03:01,840
up until this release point, are not yet

59
00:03:01,840 --> 00:03:04,590
complete when we hit the release, go ahead.

60
00:03:04,590 --> 00:03:09,070
Release the lock, but if the next lock acquisition

61
00:03:09,070 --> 00:03:11,970
happens, at that point, make sure that all

62
00:03:11,970 --> 00:03:14,720
the coherence actions are complete. So in other words,

63
00:03:14,720 --> 00:03:18,579
you get an opportunity to overlap computation with

64
00:03:18,579 --> 00:03:23,540
communication once again in this window of time between

65
00:03:23,540 --> 00:03:26,800
release of a lock, and the acquisition of the same lock.
