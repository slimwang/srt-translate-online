1
00:00:00,350 --> 00:00:02,390
Let's talk about
Dimensionality Reduction.

2
00:00:02,390 --> 00:00:04,770
So, here,
I've got a bunch of points, okay?

3
00:00:04,770 --> 00:00:07,300
And these points are in 2D, all right?

4
00:00:07,300 --> 00:00:09,860
It's called kind of green and
red, but that's sort of a joke.

5
00:00:09,860 --> 00:00:12,320
We're not going to worry too
much about why it was done that.

6
00:00:12,320 --> 00:00:14,960
What you should notice is
that in the middle here

7
00:00:14,960 --> 00:00:17,260
are these orangeish yellowish points.

8
00:00:17,260 --> 00:00:19,160
Do you see why it's R and G?

9
00:00:19,160 --> 00:00:21,890
Red plus green makes sort of yellowish,
or so, so

10
00:00:21,890 --> 00:00:24,580
along that diagonal
it'll be kind of yellow.

11
00:00:24,580 --> 00:00:27,190
I don't know who did this originally,
but it's kind of cool.

12
00:00:27,190 --> 00:00:29,370
All right,
if you think about these orange points,

13
00:00:29,370 --> 00:00:32,600
let's think about how
they're distributed, okay?

14
00:00:32,600 --> 00:00:37,140
So, they have a have a mean sit
somewhere, and they've got a.

15
00:00:37,140 --> 00:00:38,930
Set of eigenvectors, right?

16
00:00:38,930 --> 00:00:41,710
They've got a co-variants
matrix that describes them.

17
00:00:41,710 --> 00:00:45,570
They have a axis of least inertia and
then the next axis.

18
00:00:45,570 --> 00:00:47,120
So here you're seeing the points,

19
00:00:47,120 --> 00:00:51,790
the orange points are x, x bar here
is supposed to be their, their mean.

20
00:00:51,790 --> 00:00:56,570
And then we've got the big eigenvector
v1 and then the smaller one v2.

21
00:00:56,570 --> 00:00:59,900
And the idea is.

22
00:00:59,900 --> 00:01:05,170
We can represent those orange points

23
00:01:05,170 --> 00:01:09,140
by only their v1 coordinates,
plus the mean.

24
00:01:09,140 --> 00:01:11,640
And what that would mean is haha.

25
00:01:11,640 --> 00:01:14,010
Essentially that we're going to

26
00:01:14,010 --> 00:01:17,620
think of all the orange points
as just being on that line.

27
00:01:17,620 --> 00:01:21,180
And all I'm going to tell you
is where on the line they are.

28
00:01:21,180 --> 00:01:26,060
And we're going to essentially ignore
the amount that they're off that line.

29
00:01:27,080 --> 00:01:30,440
So we've just reduced
the dimensions from how many?

30
00:01:30,440 --> 00:01:31,170
Two.

31
00:01:31,170 --> 00:01:32,420
To how many?

32
00:01:32,420 --> 00:01:33,110
One.

33
00:01:33,110 --> 00:01:36,040
Okay, that doesn't sound like
that big a deal, nor is it.

34
00:01:36,040 --> 00:01:38,270
But in higher dimensions,
this could be a huge deal.

35
00:01:38,270 --> 00:01:42,220
Imagine you've got something in
10,000-dimensional space, and yes,

36
00:01:42,220 --> 00:01:45,130
in just a minute, we're going to
do a 10,000-dimensional space.

37
00:01:45,130 --> 00:01:48,960
If you said, well, I'm going to
represent them by one number, or

38
00:01:48,960 --> 00:01:51,890
even 30, that would be a huge reduction.

39
00:01:51,890 --> 00:01:54,150
So if I say, well,
what direction does it vary the most in?

40
00:01:54,150 --> 00:01:56,630
And I just give you that value.

41
00:01:56,630 --> 00:01:59,350
If that's good enough for
what we want to do, you've reduced

42
00:01:59,350 --> 00:02:02,660
the description from being a lot of
numbers to being a much smaller number.

43
00:02:03,780 --> 00:02:08,000
In fact, we can sort of express that
here algebraically in terms of just,

44
00:02:08,000 --> 00:02:10,810
thinking about it,
whatever dimension x happens to be in.

45
00:02:10,810 --> 00:02:14,850
So if I've got a whole bunch of data
points in some N-dimensional space,

46
00:02:14,850 --> 00:02:17,590
what I want to know is
the direction of projection.

47
00:02:17,590 --> 00:02:19,300
And we'll just say it's v.

48
00:02:19,300 --> 00:02:21,210
All right,
that if I projected those points,

49
00:02:21,210 --> 00:02:24,660
after subtracting out the mean,
that I'd have the greatest amount.

50
00:02:24,660 --> 00:02:26,370
Right, the greatest variation.

51
00:02:26,370 --> 00:02:28,210
And that's what that says here, right?

52
00:02:28,210 --> 00:02:30,975
So take x, subtract out the mean,
dotted with the v,

53
00:02:30,975 --> 00:02:33,190
summed over all the x's, take the norm.

54
00:02:33,190 --> 00:02:34,280
Take the square.

55
00:02:34,280 --> 00:02:37,170
Well that can be written as
an expression like this of just v

56
00:02:37,170 --> 00:02:41,580
transpose Av,
where A is just this outer product.

57
00:02:41,580 --> 00:02:45,760
Okay, that's the co-variants matrix
that we're, we're familiar with before.

58
00:02:45,760 --> 00:02:48,000
And as we said before.

59
00:02:48,000 --> 00:02:50,840
The eigenvector with
the largest eigenvalue lambda

60
00:02:50,840 --> 00:02:53,610
is going to be the one that
captures that greatest variation.

61
00:02:53,610 --> 00:02:55,990
In a minute I'll give you a little
argument about why it's the largest

62
00:02:55,990 --> 00:02:58,250
eigenvector with the largest eigenvalue.

63
00:02:58,250 --> 00:02:59,760
Or you can just take my word for it.

64
00:02:59,760 --> 00:03:03,650
And, in fact, the smallest eigenvalue
would be the least amount of dimension,

65
00:03:03,650 --> 00:03:06,110
so basically, what we're
going to have to do at some point

66
00:03:06,110 --> 00:03:09,760
is take the eigenvectors
of this co-variance matrix.
