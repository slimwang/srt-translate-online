1
00:00:00,000 --> 00:00:04,000
Another problem with linear regression has to do with the regularization

2
00:00:04,000 --> 00:00:06,000
or complexity control.

3
00:00:06,000 --> 00:00:08,000
Just like before, we sometimes wish to have

4
00:00:08,000 --> 00:00:10,000
a less complex model.

5
00:00:10,000 --> 00:00:15,000
So in regularization, the loss function is either the sum

6
00:00:15,000 --> 00:00:21,000
of the loss of a data function and a complexity control term,

7
00:00:21,000 --> 00:00:24,000
which is often called the loss of the parameters.

8
00:00:24,000 --> 00:00:29,000
The loss of the data is simply curvatic loss, as we discussed before.

9
00:00:29,000 --> 00:00:35,000
The loss of parameters might just be a function that penalizes

10
00:00:35,000 --> 00:00:37,000
the parameters to become large

11
00:00:37,000 --> 00:00:43,000
up to some known P, where P is usually either 1 or 2.

12
00:00:43,000 --> 00:00:46,000
If you draw this graphically,

13
00:00:46,000 --> 00:00:49,000
in a parameter space comprised of 2 parameters,

14
00:00:49,000 --> 00:00:53,000
your curvatic term for minimizing the data error

15
00:00:53,000 --> 00:00:57,000
might look like this, where the minimum sits over here.

16
00:00:57,000 --> 00:01:02,000
Your term for regularization might pull these parameters toward 0.

17
00:01:02,000 --> 00:01:09,000
It pulls it toward 0, along the circle if you use curvatic error,

18
00:01:09,000 --> 00:01:14,000
and it does it in a diamond-shaped way.

19
00:01:14,000 --> 00:01:20,000
For L1 regularization--either one works well.

20
00:01:20,000 --> 00:01:24,000
L1 has the advantage in that parameters tend to get really sparse.

21
00:01:24,000 --> 00:01:30,000
If you look at this diagram, there is a tradeoff between W-0 and W-1.

22
00:01:30,000 --> 00:01:33,000
In the L1 case, that allows one of them to be driven to 0.

23
00:01:33,000 --> 00:01:37,000
In the L2 case, parameters tend not to be as sparse.

24
00:01:37,000 --> 99:59:59,999
So L1 is often preferred.
