1
00:00:00,470 --> 00:00:03,180
So it's important to understand these two concepts

2
00:00:03,180 --> 00:00:06,700
of latency and throughput. Latency is the elapsed time

3
00:00:06,700 --> 00:00:09,280
from an event. If it takes me one minute

4
00:00:09,280 --> 00:00:12,300
to walk from my office to the classroom, that's

5
00:00:12,300 --> 00:00:14,710
the latency that I'm going to experience for that event

6
00:00:14,710 --> 00:00:18,200
of walking from my office to the classroom. That's

7
00:00:18,200 --> 00:00:22,410
the elapsed time. Throughput is the number of events

8
00:00:22,410 --> 00:00:25,510
that can be executed per unit time. Bandwidth is

9
00:00:25,510 --> 00:00:28,090
a measure of throughput. So once again with this

10
00:00:28,090 --> 00:00:31,870
analogy of walking to the classroom from my office, if

11
00:00:31,870 --> 00:00:36,380
the hallway is wide enough to allow five, ten

12
00:00:36,380 --> 00:00:39,020
of us to walk in parallel side by side to

13
00:00:39,020 --> 00:00:42,660
the classroom, increases the throughput but it does nothing

14
00:00:42,660 --> 00:00:44,940
to the latency. The latency is going to be determined by

15
00:00:44,940 --> 00:00:46,650
how fast I can walk from my office to

16
00:00:46,650 --> 00:00:50,630
the classroom. So, the difference between latency and, and throughput

17
00:00:50,630 --> 00:00:52,870
is very important to understand. In other

18
00:00:52,870 --> 00:00:56,670
words, I can increase the bandwidth and that'll

19
00:00:56,670 --> 00:00:58,840
improve the throughput but it is not going

20
00:00:58,840 --> 00:01:01,160
to do anything to the latency itself. So

21
00:01:01,160 --> 00:01:04,030
in other words, higher bandwidth does not

22
00:01:04,030 --> 00:01:07,430
necessarily imply lower latency. You'll work hard to

23
00:01:07,430 --> 00:01:12,040
lower the latency. RPC is a basis for

24
00:01:12,040 --> 00:01:15,770
client server based distributed systems. And performance of

25
00:01:15,770 --> 00:01:24,060
RPC is crucial, specifically in the context of this lesson. Latency refers to

26
00:01:24,060 --> 00:01:26,120
the time it takes for an application

27
00:01:26,120 --> 00:01:29,070
generated message to reach it's destination. So

28
00:01:29,070 --> 00:01:34,310
for instance, if you're doing an RPC call from a client to the server, then the

29
00:01:34,310 --> 00:01:40,870
RPC call entails sending the argument from the client to the server.

30
00:01:40,870 --> 00:01:43,160
And there is work to be done here, work

31
00:01:43,160 --> 00:01:45,190
to be done in sending the message, work to

32
00:01:45,190 --> 00:01:48,180
be done here before the server can actually execute

33
00:01:48,180 --> 00:01:51,125
the server procedure. So it's the latency that we

34
00:01:51,125 --> 00:01:54,150
are concerned about. And what we will see is

35
00:01:54,150 --> 00:01:58,640
all the software components that comprise the latency for

36
00:01:58,640 --> 00:02:02,280
RPC based communication. And performance of RPC is very

37
00:02:02,280 --> 00:02:06,710
crucial in building client server systems. There are two components

38
00:02:06,710 --> 00:02:11,220
to the latency that is observed for message communication in a distributive

39
00:02:11,220 --> 00:02:15,390
system. The first component is the hardware overhead and the second component is

40
00:02:15,390 --> 00:02:20,430
the software overhead. The hardware overhead is really dependent on how the

41
00:02:20,430 --> 00:02:26,460
network is interfaced to the computer. So, typically in

42
00:02:26,460 --> 00:02:32,680
any computer, what you have is a network controller that interfaces

43
00:02:32,680 --> 00:02:36,050
the network to the CPU. And typically, the

44
00:02:36,050 --> 00:02:39,310
network controller operates by moving the bits of

45
00:02:39,310 --> 00:02:43,310
the message from the system memory of the

46
00:02:43,310 --> 00:02:46,980
node into it's private buffer, which is inside the

47
00:02:46,980 --> 00:02:49,540
network controller. And this part of it, moving

48
00:02:49,540 --> 00:02:52,550
the bits from the memory of the node

49
00:02:52,550 --> 00:02:55,230
into the internal buffer of the network controller

50
00:02:55,230 --> 00:02:58,380
is accomplished using what is called direct memory access.

51
00:02:58,380 --> 00:03:01,140
Meaning the network controller is smart enough to move the

52
00:03:01,140 --> 00:03:04,690
bits directly using the bus that connects the memory to

53
00:03:04,690 --> 00:03:07,862
the network controller without the intervention of the CPU. And

54
00:03:07,862 --> 00:03:11,190
this is what is called dir, direct memory access. And that's

55
00:03:11,190 --> 00:03:14,320
how the bits are moved from the memory of the

56
00:03:14,320 --> 00:03:18,000
system into the buffer of the network controller, and once it

57
00:03:18,000 --> 00:03:20,920
comes here, the network controller can then put the bits

58
00:03:20,920 --> 00:03:23,530
out on the wire, and this is where the bandwidth that

59
00:03:23,530 --> 00:03:27,650
you have connecting your node to the network comes

60
00:03:27,650 --> 00:03:30,140
into play. But there are also other types of

61
00:03:30,140 --> 00:03:33,820
network controllers where the CPU may actually be involved

62
00:03:33,820 --> 00:03:36,200
in the data movement, and in that case, the

63
00:03:36,200 --> 00:03:39,890
CPU does program I/O to move the bits from

64
00:03:39,890 --> 00:03:43,130
the memory into the buffer of the network controller,

65
00:03:43,130 --> 00:03:45,040
from which the network controller will then put it

66
00:03:45,040 --> 00:03:48,540
out on the network. But modern network controllers tend to

67
00:03:48,540 --> 00:03:51,874
be built using DMA technique, meaning that the

68
00:03:51,874 --> 00:03:55,430
network controller, once the CPU tells the network controller

69
00:03:55,430 --> 00:03:58,760
were in memory the messages to be sent

70
00:03:58,760 --> 00:04:01,200
on the wire, network controller does the rest in

71
00:04:01,200 --> 00:04:03,478
terms of moving the bits into it's internal

72
00:04:03,478 --> 00:04:05,468
buffer, and then from the buffer putting it out

73
00:04:05,468 --> 00:04:09,680
onto the network. The software overhead is what the

74
00:04:09,680 --> 00:04:14,030
operating system tax on to the hardware overhead of

75
00:04:14,030 --> 00:04:17,560
moving the bits out onto the network. So

76
00:04:17,560 --> 00:04:19,790
the latency, if you think about the latency

77
00:04:19,790 --> 00:04:23,560
as a whole for doing a network transmission,

78
00:04:23,560 --> 00:04:26,540
there is the software overhead incurred in the

79
00:04:26,540 --> 00:04:29,470
layers of the operating system to make the

80
00:04:29,470 --> 00:04:33,380
message available in the memory of the processor,

81
00:04:33,380 --> 00:04:36,050
ready for transmission. Once it is ready for

82
00:04:36,050 --> 00:04:39,110
transmission, the hardware overhead kicks in, and the

83
00:04:39,110 --> 00:04:42,840
hardware, the network controller in particular, moves the bits

84
00:04:42,840 --> 00:04:45,260
from the memory into it's buffer and then out on

85
00:04:45,260 --> 00:04:47,890
the wire. The focus of course being an operating

86
00:04:47,890 --> 00:04:54,170
system designers work, is to reduce the software overhead and

87
00:04:54,170 --> 00:04:56,500
take what the hardware gives you and think about

88
00:04:56,500 --> 00:04:58,810
how you can reduce the software overhead so that we

89
00:04:58,810 --> 00:05:02,224
can overall reduce the latency involved in transmission. Which

90
00:05:02,224 --> 00:05:05,085
is a sum of the hardware overhead and software overhead.
