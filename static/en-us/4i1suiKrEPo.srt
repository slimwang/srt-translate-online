1
00:00:00,170 --> 00:00:04,330
Now let's look at the relationships
between SMT and cache performance.

2
00:00:04,330 --> 00:00:06,110
The cache of the core,

3
00:00:06,110 --> 00:00:10,520
is shared by all of the SMT threads
that are currently active on for

4
00:00:10,520 --> 00:00:15,180
example, if you have 2 SMT then the
cache is shared by both of the threads.

5
00:00:15,180 --> 00:00:18,510
That gives us some good and
some bad properties.

6
00:00:18,510 --> 00:00:23,720
The good property Is that we get
fast data sharing among the threads.

7
00:00:23,720 --> 00:00:26,370
When Thread 0 stores something, and

8
00:00:26,370 --> 00:00:30,490
then Thread 1 wants to read it,
it gets to get a cache shared.

9
00:00:30,490 --> 00:00:34,530
So any communication between these
threads that happens within a short span

10
00:00:34,530 --> 00:00:37,355
of time like we store and
then we quickly load in another thread,

11
00:00:37,355 --> 00:00:40,260
will go through the cache and
have really,

12
00:00:40,260 --> 00:00:44,190
really good performance because the load
here will always be a cache hit,

13
00:00:44,190 --> 00:00:47,120
assuming that the store
happened recently.

14
00:00:47,120 --> 00:00:50,840
So the communication now no
longer goes through shared memory

15
00:00:50,840 --> 00:00:52,260
because we have a cache.

16
00:00:52,260 --> 00:00:55,530
The communication tends to
go through just the cache.

17
00:00:55,530 --> 00:01:00,770
But when we have an SMT processor,
the cache capacity that we have and

18
00:01:00,770 --> 00:01:04,650
its associativity is shared
by all of the threads.

19
00:01:04,650 --> 00:01:09,470
So if the working set of thread
0 plus the working set of

20
00:01:09,470 --> 00:01:15,140
thread 1 minus the part of the working
set that they have in common,

21
00:01:15,140 --> 00:01:17,820
which will be counted
twice if we did this,

22
00:01:17,820 --> 00:01:23,330
exceeds the data size,
we get what is called cache trashing.

23
00:01:23,330 --> 00:01:26,200
Pretty much the data no
longer fits in the cache, and

24
00:01:26,200 --> 00:01:28,670
we get a lot of cache misses.

25
00:01:28,670 --> 00:01:34,070
If each of the working sets would fit
individually in the data cache size,

26
00:01:34,070 --> 00:01:37,650
then the performance
of the SMT can be and

27
00:01:37,650 --> 00:01:42,200
usually is significantly worse than
doing the threads one at a time.

28
00:01:42,200 --> 00:01:47,310
So we were assuming that when we did
SMT that we will get some benefits from

29
00:01:47,310 --> 00:01:51,200
using resources that would otherwise be
underutilized like the issue slots and

30
00:01:51,200 --> 00:01:53,460
the reservation stations and so on.

31
00:01:53,460 --> 00:01:56,630
But that assumes we are getting
similar cash performance.

32
00:01:56,630 --> 00:02:00,360
However, if the cash
has a given size and

33
00:02:00,360 --> 00:02:05,540
threads are written to fit in that cash,
then running two such threads

34
00:02:05,540 --> 00:02:10,530
if they don't share enough data,
might exceed the cash size in which case

35
00:02:10,530 --> 00:02:15,400
suddenly we have a lot more misses
then we expected and we make payback.

36
00:02:15,400 --> 00:02:19,330
In cache misses a lot more than
we gain by overlapping execution

37
00:02:19,330 --> 00:02:20,580
in the SMT fashion.
