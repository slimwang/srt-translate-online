1
00:00:00,260 --> 00:00:03,400
So the process of producing
the dual of a linear program

2
00:00:03,400 --> 00:00:05,580
is just a mechanical process.

3
00:00:05,580 --> 00:00:08,290
There's a series of steps that
you go through where each

4
00:00:08,290 --> 00:00:11,890
constraint from the primal becomes
a variable in the dual and

5
00:00:11,890 --> 00:00:14,890
each variable in the primal
becomes a constraint in the dual.

6
00:00:14,890 --> 00:00:21,515
And certain maxes become mins and
bounds become objective function things.

7
00:00:21,515 --> 00:00:24,863
So what I've done here is I've
actually gone through that process,

8
00:00:24,863 --> 00:00:28,451
which we could go through in detail,
but let's just pretend that we did.

9
00:00:28,451 --> 00:00:32,952
[LAUGH] And we create a new linear
program from the old linear program and

10
00:00:32,952 --> 00:00:34,940
the new one is called the dual.

11
00:00:34,940 --> 00:00:39,835
So let's take a look at
this dual in detail.

12
00:00:39,835 --> 00:00:42,580
Because it turns out that the variables
and the constraints actually have some

13
00:00:42,580 --> 00:00:45,550
very nice interpretations that
are worth thinking about.

14
00:00:45,550 --> 00:00:49,330
Here in the dual, the thing that we're
trying to optimize is we're trying to

15
00:00:49,330 --> 00:00:55,130
maximize the sum over all
state-action pairs of the reward for

16
00:00:55,130 --> 00:01:00,140
that state-action pair times
this value little qsa.

17
00:01:00,140 --> 00:01:02,410
So little qsa,
probably not a great name for

18
00:01:02,410 --> 00:01:04,376
it because it's not
the same as a Q value.

19
00:01:04,376 --> 00:01:05,096
But it's something.

20
00:01:05,096 --> 00:01:08,065
It's something that we're
multiplying by the rewards and

21
00:01:08,065 --> 00:01:09,616
we're trying to maximize it.

22
00:01:09,616 --> 00:01:14,886
So one way I like to think
about qsa is as policy flow.

23
00:01:14,886 --> 00:01:19,960
Sort of how much [LAUGH] agentness is
flowing through each state-action pair.

24
00:01:19,960 --> 00:01:22,990
If it follows the policy it's
going to spend some time

25
00:01:22,990 --> 00:01:27,150
running around in the environment,
passing through each state-action pair.

26
00:01:27,150 --> 00:01:29,428
And each time it passes
through a state-action pair,

27
00:01:29,428 --> 00:01:32,990
we're going to get the reward
associated with that state-action pair.

28
00:01:32,990 --> 00:01:34,980
And what we want to do
is maximize that reward.

29
00:01:34,980 --> 00:01:37,562
So from this sort of,
policy flow concept,

30
00:01:37,562 --> 00:01:39,630
it sort of makes sense what it
is we're trying to maximize.

31
00:01:39,630 --> 00:01:42,200
It's in some ways even more
intuitive than in the primal,

32
00:01:42,200 --> 00:01:44,420
which we were saying we're
trying the minimize the value,

33
00:01:44,420 --> 00:01:46,620
which of course we're not really
trying to minimize the value.

34
00:01:46,620 --> 00:01:47,630
We're just trying to minimize it so

35
00:01:47,630 --> 00:01:50,720
that it doesn't end up being
an upper bound on the value.

36
00:01:50,720 --> 00:01:53,170
Here we're actually trying
to maximize expected reward,

37
00:01:53,170 --> 00:01:54,870
which seems like a really good thing.

38
00:01:54,870 --> 00:01:56,000
>> Right, right, right.

39
00:01:56,000 --> 00:01:59,480
>> Okay so this is now going to
be subject to the following idea,

40
00:01:59,480 --> 00:02:03,830
that for each state, in this case it's
easier to think of them as next states.

41
00:02:03,830 --> 00:02:05,800
For each possible next state,

42
00:02:05,800 --> 00:02:11,210
we want it to be true that the amount
of policy flow through that next state

43
00:02:11,210 --> 00:02:14,070
summed up over all the actions
that are going through it.

44
00:02:14,070 --> 00:02:16,900
That should be equal to in some
sense the number of times that

45
00:02:16,900 --> 00:02:20,460
next state is visited,
which we can get by summing over all

46
00:02:20,460 --> 00:02:24,190
states we might have started at, and
actions we might have taken form there.

47
00:02:24,190 --> 00:02:27,100
The policy flow through that
state-action pair times

48
00:02:27,100 --> 00:02:31,260
the transition probability that would
send us to s' as a result of that.

49
00:02:31,260 --> 00:02:34,500
And we're going to also
include in this equation

50
00:02:34,500 --> 00:02:37,900
sort of the sense that we can also
start a given state-action pair.

51
00:02:37,900 --> 00:02:39,630
So we're just going to stick a 1 in for
that.

52
00:02:39,630 --> 00:02:42,530
So there's some policy
flow that we're injecting

53
00:02:42,530 --> 00:02:45,090
into each state-action
pair of the system.

54
00:02:45,090 --> 00:02:47,700
And we're going to add to that any
policy flow that would be coming through

55
00:02:47,700 --> 00:02:49,330
it through other parts of the system.

56
00:02:49,330 --> 00:02:53,290
So it's kind of like we're dumping
policy flow all over our MDP.

57
00:02:53,290 --> 00:02:55,990
And then letting the MDP kind
of pump around what's left.

58
00:02:55,990 --> 00:02:58,080
And it's discounted,
there's a little discount here.

59
00:02:58,080 --> 00:03:01,980
So there's actually evaporation or
something happening all over the place.

60
00:03:01,980 --> 00:03:06,455
And what we want to know is what's
the way of letting the flow go through,

61
00:03:06,455 --> 00:03:10,655
kind of deciding at each state which
action it should flow through, so

62
00:03:10,655 --> 00:03:13,415
that we ultimately get
the maximum possible reward.

63
00:03:14,545 --> 00:03:16,875
And I think just one
additional constraint here,

64
00:03:16,875 --> 00:03:18,985
just to make sure that the policy
flow is not negative, so

65
00:03:18,985 --> 00:03:22,190
that we can't introduce a whole
lot of policy flow in one place.

66
00:03:22,190 --> 00:03:24,780
And then drain it out some other
place to make things balance.

67
00:03:24,780 --> 00:03:27,520
You could think of these constraints
as actually being a kind of

68
00:03:27,520 --> 00:03:29,480
conservation of flow.

69
00:03:29,480 --> 00:03:31,920
So the amount of stuff going into s'

70
00:03:31,920 --> 00:03:34,010
has to equal the amount of
stuff coming out of it.

71
00:03:34,010 --> 00:03:35,060
And that's all this is saying.

72
00:03:35,060 --> 00:03:36,590
It's just saying that
that has to balance.

73
00:03:36,590 --> 00:03:37,910
It has to be a meaningful flow.

74
00:03:37,910 --> 00:03:40,460
Subject to that constraint,
maximize reward.

75
00:03:40,460 --> 00:03:44,140
So, again, this is kind of a neat
interpretation of what it means to

76
00:03:44,140 --> 00:03:45,980
solve a mark off decision process.

77
00:03:45,980 --> 00:03:49,490
But it just comes mechanically
out of the primal version of

78
00:03:49,490 --> 00:03:51,390
setting up this as a linear program.

79
00:03:51,390 --> 00:03:54,330
>> Okay, does that suggest an algorithm

80
00:03:54,330 --> 00:03:56,985
that's different from the sort of
algorithms you've been using so far?

81
00:03:56,985 --> 00:03:59,040
>> Hm, interesting thought.

82
00:03:59,040 --> 00:04:02,660
So, well for one thing, it is an
algorithm in the sense that once we set

83
00:04:02,660 --> 00:04:05,520
things up this way, we can hit
it over the head with the linear

84
00:04:05,520 --> 00:04:08,560
programming stick and
a solution will come out.

85
00:04:08,560 --> 00:04:11,760
>> And in fact, it'll come out in
polynomial time, which is nice.

86
00:04:11,760 --> 00:04:12,690
But you're right.

87
00:04:12,690 --> 00:04:15,429
I mean in some sense, if you think
about value iteration as something

88
00:04:15,429 --> 00:04:18,279
that's propagating these values around,
this has a different form.

89
00:04:18,279 --> 00:04:23,105
This is really concentrating more
on the policy aspect of the MDP.

90
00:04:23,105 --> 00:04:28,103
And maybe we could make an algorithm
that more directly kind of

91
00:04:28,103 --> 00:04:33,007
searched in policy space for
the best behavior for the MDP.

92
00:04:33,007 --> 00:04:33,600
>> Let's do that.
