1
00:00:00,270 --> 00:00:05,750
So, what I'm claiming in step two here
is that, if we end up choosing a policy

2
00:00:05,750 --> 00:00:09,530
that is not based on knowing all
of the transitions that there

3
00:00:09,530 --> 00:00:12,620
are still some unknown transitions out
there, but we end up taking a policy

4
00:00:12,620 --> 00:00:16,180
that puts us into a loop where
we never visit any of those,

5
00:00:16,180 --> 00:00:19,970
then that still has to be an optimal
loop, we're not making any mistakes.

6
00:00:19,970 --> 00:00:23,570
It could be that we could do as well
by uncovering those other values but

7
00:00:23,570 --> 00:00:28,170
we can't do any better because
we're assuming in step two

8
00:00:28,170 --> 00:00:32,020
that anything that we don't know
has the largest possible value and

9
00:00:32,020 --> 00:00:33,910
so, if it has the largest
possible value and

10
00:00:33,910 --> 00:00:37,730
we still don't want to go there, then
we're doing at least as well as optimal.

11
00:00:37,730 --> 00:00:38,510
>> Yeah, that's kind of cute.

12
00:00:38,510 --> 00:00:40,350
>> All right, but now,
we need step three.

13
00:00:40,350 --> 00:00:44,980
So now, step three says,
okay, we solve the MDP,

14
00:00:44,980 --> 00:00:50,250
this sort of imagined MDP that has
these optimistic estimates in it,

15
00:00:50,250 --> 00:00:54,140
and let's say that that
actually takes us to a state

16
00:00:54,140 --> 00:00:57,150
action pair that we haven't been
to before, and we learn its value.

17
00:00:57,150 --> 00:00:57,710
Mm-hm.

18
00:00:57,710 --> 00:01:00,530
>> How many mistakes might
we make on route to that?

19
00:01:00,530 --> 00:01:02,620
So, when we actually get to that state,
and

20
00:01:02,620 --> 00:01:06,580
traverse an edge that we didn't know
about, that could have been a mistake.

21
00:01:06,580 --> 00:01:07,310
>> Right.

22
00:01:07,310 --> 00:01:09,340
>> So, that's definitely, one.

23
00:01:09,340 --> 00:01:12,890
And how many mistakes might we make
seeking out the state that actually

24
00:01:12,890 --> 00:01:15,000
turned out to be a really bad state?

25
00:01:15,000 --> 00:01:16,070
>> From a particular state?

26
00:01:16,070 --> 00:01:18,170
I guess, the number of states.

27
00:01:18,170 --> 00:01:19,260
>> Yes, right.

28
00:01:19,260 --> 00:01:22,830
So, the number of transitions that we
might take just to find out that we were

29
00:01:22,830 --> 00:01:26,340
wrong, could be as big as N.

30
00:01:26,340 --> 00:01:28,906
>> N minus one or
something like that but yeah.

31
00:01:28,906 --> 00:01:31,853
>> Yeah, so,
this isn't a deterministic M.B.P.

32
00:01:31,853 --> 00:01:35,484
under to any state our path is
going to be either bounded by N, or

33
00:01:35,484 --> 00:01:39,666
if it's bigger than N, it's infinity
because we're actually looping

34
00:01:39,666 --> 00:01:42,750
it without actually getting
to that other place.

35
00:01:42,750 --> 00:01:45,140
>> Right.
>> So the most it can be is in N right?

36
00:01:45,140 --> 00:01:45,930
>> Mm hm.
>> All right.

37
00:01:45,930 --> 00:01:51,650
So, given that, we might make N mistakes
to find out something new, to learn

38
00:01:51,650 --> 00:01:54,710
something new, to visit a state action
pair that we've never seen before.

39
00:01:54,710 --> 00:01:56,480
How many times can that happen?

40
00:01:56,480 --> 00:02:00,500
How many times can it be that
there's a policy that brings us to

41
00:02:00,500 --> 00:02:02,510
a state action pair that
we haven't been to before?

42
00:02:03,640 --> 00:02:05,520
>> How many times can that be?

43
00:02:06,570 --> 00:02:08,220
>> Yeah, so each time.

44
00:02:08,220 --> 00:02:09,810
This is sort of step three.

45
00:02:09,810 --> 00:02:11,150
Maybe I should write in English.

46
00:02:11,150 --> 00:02:13,300
All right, so what this step three is,

47
00:02:13,300 --> 00:02:17,380
is that what our our max
algorithm tells us to do is to

48
00:02:17,380 --> 00:02:20,430
execute a path that actually brings
us to an unknown state action pair.

49
00:02:20,430 --> 00:02:23,420
Then, we go traverse that edge
find out what it really is and

50
00:02:23,420 --> 00:02:25,090
generate a new policy.

51
00:02:25,090 --> 00:02:27,970
So, the number of steps that it might
take to get to that new state action

52
00:02:27,970 --> 00:02:29,140
pair is N.

53
00:02:29,140 --> 00:02:30,780
>> Right.
>> And the number of times that we might

54
00:02:30,780 --> 00:02:33,340
discover a new state
action pair is what?

55
00:02:33,340 --> 00:02:34,710
>> N times the number of actions.

56
00:02:34,710 --> 00:02:37,330
n times the number of actions because

57
00:02:37,330 --> 00:02:41,080
that's the total number of unknown
things in the graph to begin with.

58
00:02:41,080 --> 00:02:42,940
>> Right.
>> Right, so we're exploring the MDP and

59
00:02:42,940 --> 00:02:45,800
each time we visit state and
take an action we

60
00:02:45,800 --> 00:02:48,610
haven't taken before that's
going to reduce one from the count.

61
00:02:48,610 --> 00:02:52,670
So, the largest that count is
in times K and so, the total

62
00:02:52,670 --> 00:02:57,290
number of mistakes that this algorithm
can make is bounded by N squared K.

63
00:02:57,290 --> 00:02:58,610
>> Hey, that's polynomial.

64
00:02:58,610 --> 00:02:59,320
N, N, and K.

65
00:02:59,320 --> 00:03:01,360
>> It is and
it also doesn't depend on Epsilon, and

66
00:03:01,360 --> 00:03:02,750
it also doesn't depend on Delta.

67
00:03:02,750 --> 00:03:04,050
>> That's even better.

68
00:03:04,050 --> 00:03:05,720
So, wait,
if it doesn't depend on Epsilon, and

69
00:03:05,720 --> 00:03:09,500
it doesn't depend on Delta, then it
means it's polynomial in those things.

70
00:03:09,500 --> 00:03:10,020
>> Sure.
Yes,

71
00:03:10,020 --> 00:03:11,750
it's a very simple
polynomial in those things.

72
00:03:11,750 --> 00:03:13,570
>> So,
then we're polynomial in everything.

73
00:03:13,570 --> 00:03:14,110
Yeah.
Yes, so,

74
00:03:14,110 --> 00:03:15,900
this is an efficient
exploration algorithm for

75
00:03:15,900 --> 00:03:17,340
deterministic MDP's, right.

76
00:03:17,340 --> 00:03:19,400
All it's saying is,
whenever we don't know something,

77
00:03:19,400 --> 00:03:22,120
assume it's awesome and
behave accordingly.

78
00:03:22,120 --> 00:03:23,090
If we learn something new.

79
00:03:23,090 --> 00:03:24,030
Make note of it.

80
00:03:24,030 --> 00:03:27,190
And so, the number of times that we take
a step that might not be the best thing

81
00:03:27,190 --> 00:03:30,240
to do for that state,
it can't be any bigger than N squared K.

82
00:03:30,240 --> 00:03:31,360
>> That's beautiful.

83
00:03:31,360 --> 00:03:35,270
>> Yeah, it's pretty cool, this kind
of analysis comes from a paper that

84
00:03:35,270 --> 00:03:39,380
Koenig wrote with Reed Simmons where
they actually analyzed Q learning in

85
00:03:39,380 --> 00:03:43,360
this kind of setting and they got
a worse bound like N cubed times K.

86
00:03:43,360 --> 00:03:48,170
But this algorithm is sort of pretty
simple, and gives us a nice bound and

87
00:03:48,170 --> 00:03:49,640
it sort of does what you want it to do.

88
00:03:51,070 --> 00:03:54,720
>> It's cool, I will point out that
in cube case, also a polynomial.

89
00:03:54,720 --> 00:03:57,540
>> Yes, yes, but it's a slightly worse
polynomial, you're absolutely right.

90
00:03:57,540 --> 00:04:00,150
I mean, he showed that Q learning has
a polynomial bound in these kinds of

91
00:04:00,150 --> 00:04:01,090
environments.

92
00:04:01,090 --> 00:04:02,910
If you set up the rewards correctly, and

93
00:04:02,910 --> 00:04:04,890
if you set up the initial
value functions correctly.

94
00:04:04,890 --> 00:04:10,079
All right, so, that gets us through
stochastic MDPs that have no state, and

95
00:04:10,079 --> 00:04:16,209
state MDPs that have no stochastic, and
the last thing we want to dive into is,

96
00:04:16,209 --> 00:04:19,790
what happens if we do something like
our max except for on stochastic MDP's.

97
00:04:19,790 --> 00:04:22,500
>> Well, we still have another case,
we have to worry about the case where

98
00:04:22,500 --> 00:04:24,460
we have no state and
we have no stochastic's.

99
00:04:24,460 --> 00:04:25,000
>> Ahh.

100
00:04:26,500 --> 00:04:28,510
So, okay, let's do that one really fast.

101
00:04:28,510 --> 00:04:29,480
>> All right.

102
00:04:29,480 --> 00:04:32,050
So, you're talking about
an environment has one state and

103
00:04:32,050 --> 00:04:33,330
all the actions are deterministic.

104
00:04:33,330 --> 00:04:36,450
They give you a deterministic reward and
return you back to that state.

105
00:04:36,450 --> 00:04:36,990
>> Right.
>> So,

106
00:04:36,990 --> 00:04:39,350
this is essentially
deterministic bandit problem.

107
00:04:39,350 --> 00:04:40,830
>> Right.
>> So, how many mistakes might,

108
00:04:40,830 --> 00:04:44,730
like what would be an algorithm for this
and how many mistakes would it make?

109
00:04:44,730 --> 00:04:48,000
>> The dumbest one, or the simplest one
would just be taking every action once,

110
00:04:48,000 --> 00:04:49,130
and then you know everything.

111
00:04:49,130 --> 00:04:52,390
>> Right, and each of those, so K minus
1 of those are mistakes possibly.

112
00:04:53,460 --> 00:04:54,030
And then,

113
00:04:54,030 --> 00:04:58,060
we only know that we just teach using
the optimal action at their after.

114
00:04:58,060 --> 00:05:00,680
>> Yeah, you could do a little bit
better view assume that everything is

115
00:05:00,680 --> 00:05:04,750
our max because as you come
across our RMX, you ca stop.

116
00:05:04,750 --> 00:05:09,100
>> Very good, yes, right, if you end up
finding along the way, an action that

117
00:05:09,100 --> 00:05:11,940
actually gives you the full value,
the full RMX value, then we don't

118
00:05:11,940 --> 00:05:14,690
have to explore the rest of them because
they're not going to be any better.

119
00:05:14,690 --> 00:05:15,820
Yeah, okay, so that gives us.

120
00:05:15,820 --> 00:05:18,150
It doesn't give us a better bound, but

121
00:05:18,150 --> 00:05:20,950
it does probably give us a better
performance if we keep track of that.

122
00:05:20,950 --> 00:05:22,835
So, yes, so this is a silly case.

123
00:05:22,835 --> 00:05:23,640
[LAUGH]
>> Well, but

124
00:05:23,640 --> 00:05:26,440
it does allow us to say we
solved three of the four.

125
00:05:26,440 --> 00:05:27,370
>> Great.
All right, so

126
00:05:27,370 --> 00:05:29,140
let's dive into number four.
