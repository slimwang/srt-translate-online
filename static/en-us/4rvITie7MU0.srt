1
00:00:00,110 --> 00:00:00,650
All right.
So there's

2
00:00:00,650 --> 00:00:02,740
going to be a couple key
ideas to make this work.

3
00:00:02,740 --> 00:00:05,180
And one of them is the simulation lemma.

4
00:00:05,180 --> 00:00:07,820
The idea of the simulation
lemma is that if we have

5
00:00:07,820 --> 00:00:12,630
a pretty good estimate of the real MDP,
then optimizing our rewards in that

6
00:00:12,630 --> 00:00:15,220
estimate is going to be
pretty good in the real MDP.

7
00:00:15,220 --> 00:00:18,640
>> This is a lemma about simulations,
not a simulation about a lemma.

8
00:00:18,640 --> 00:00:20,190
>> That's right.
It's not a simulated lemma.

9
00:00:21,240 --> 00:00:22,780
It's a lemma about simulation, right?

10
00:00:22,780 --> 00:00:26,720
And so the idea is that
we've got transitions and

11
00:00:26,720 --> 00:00:29,550
rewards that are off by alpha or
less, and

12
00:00:29,550 --> 00:00:33,270
we want to think about what happens
if we adopt some policy pi.

13
00:00:33,270 --> 00:00:33,950
Any policy pi.

14
00:00:33,950 --> 00:00:39,010
We want to compare the value we get for
following pi in MDP 1.

15
00:00:39,010 --> 00:00:42,700
That has transition function T1 and
reward function R1,

16
00:00:42,700 --> 00:00:47,620
to the value that we get, return that
we get for following policy pi in

17
00:00:47,620 --> 00:00:52,592
mdp V2 which has transition
function T2 and reward function R2.

18
00:00:52,592 --> 00:00:53,630
>> Okay.

19
00:00:53,630 --> 00:00:58,130
>> And if those are going to be near
each other given that T1 is near T2 and

20
00:00:58,130 --> 00:00:58,840
R1 is near R2,

21
00:00:58,840 --> 00:01:02,670
then that's going to give us the ability
to have accurate simulations, right.

22
00:01:02,670 --> 00:01:05,660
We can use our model of
the MVP to stimulate.

23
00:01:05,660 --> 00:01:07,570
What's going to happen in the real MVP.

24
00:01:07,570 --> 00:01:11,250
And just to be concrete, this is what I
mean by the transition functions, reward

25
00:01:11,250 --> 00:01:14,270
functions not being too different,
than we have this value alpha.

26
00:01:14,270 --> 00:01:19,040
And if we take the maximum over all
state action x state triples, that

27
00:01:19,040 --> 00:01:23,250
the probability assigned by transition
function T1 and the probability assigned

28
00:01:23,250 --> 00:01:27,510
by transaction function T2 are not
different by more than alpha.

29
00:01:27,510 --> 00:01:28,940
And, same thing with the rewards.

30
00:01:28,940 --> 00:01:29,960
>> So that's a little weird.

31
00:01:29,960 --> 00:01:32,840
>> How so?
>> Well, because the transition model

32
00:01:32,840 --> 00:01:36,170
is probability, so
that can't ever be off by more than one.

33
00:01:36,170 --> 00:01:36,820
>> Sure.

34
00:01:36,820 --> 00:01:40,450
>> And the rewards can be gigantic
numbers that can be off by billions.

35
00:01:40,450 --> 00:01:41,020
>> Sure.

36
00:01:41,020 --> 00:01:42,020
>> Billions and billions.

37
00:01:42,020 --> 00:01:43,300
So, it just seems a little weird.

38
00:01:43,300 --> 00:01:45,250
>> You feel like it should
be two different alphas.

39
00:01:45,250 --> 00:01:48,340
Yeah, alpha T and
alpha R, but maybe not.

40
00:01:48,340 --> 00:01:51,470
>> Yeah, I mean I didn't want
to proliferate variables

41
00:01:51,470 --> 00:01:53,460
when we didn't really need to,
but you're right.

42
00:01:53,460 --> 00:01:55,400
The scale of these could
be very different.

43
00:01:55,400 --> 00:02:00,995
A lot of times in the proofs that
people use for these kinds of MBPs.

44
00:02:00,995 --> 00:02:05,005
They first assume that rewards are all
in the range zero to one, also.

45
00:02:05,005 --> 00:02:07,955
It doesn't change the fact that you
still might want a different alpha for

46
00:02:07,955 --> 00:02:12,320
the two cases, but it does at least get
the scale More approximately correctly.

47
00:02:12,320 --> 00:02:14,100
>> Okay, so
then without loss of generality,

48
00:02:14,100 --> 00:02:16,580
assume that all your rewards
are between minus one and one.

49
00:02:16,580 --> 00:02:17,350
>> Or zero and one.

50
00:02:17,350 --> 00:02:18,130
>> Or zero and one.

51
00:02:18,130 --> 00:02:18,790
Why not?

52
00:02:18,790 --> 00:02:20,355
Because it's all the same MDP.

53
00:02:20,355 --> 00:02:22,970
>> [LAUGH] Yeah, they look more
like probabilities or something.

54
00:02:22,970 --> 00:02:24,140
>> Okay, cool.
So,

55
00:02:24,140 --> 00:02:26,720
without loss of generality assume that
your rewards are between zero and

56
00:02:26,720 --> 00:02:29,490
one and your transition probabilities
which Would have to be here between

57
00:02:29,490 --> 00:02:33,040
zero and one so that we can use the same
alpha and everything kind of works out.

58
00:02:33,040 --> 00:02:33,710
>> Yeah.
Again,

59
00:02:33,710 --> 00:02:35,340
it still might be worth
having different alphas.

60
00:02:35,340 --> 00:02:36,620
But yeah, you're right.

61
00:02:36,620 --> 00:02:38,980
Again, it puts us in
the right ball park.

62
00:02:38,980 --> 00:02:42,690
>> Okay.
So this is a ball park simulation.

63
00:02:42,690 --> 00:02:44,375
I'm perfectly fine with that.

64
00:02:44,375 --> 00:02:46,080
>> [LAUGH]
>> Now we're out of the ballpark.

65
00:02:46,080 --> 00:02:50,050
So that's either a vacuum cleaner or
a metal detector,

66
00:02:50,050 --> 00:02:53,960
or it's a gigantic baseball
being knocked out of a park.

67
00:02:53,960 --> 00:02:57,650
>> It was supposed to be a ballpark,
but it looks to me more like

68
00:02:57,650 --> 00:03:00,280
some kind of terrible
>> Cruel alien.

69
00:03:00,280 --> 00:03:01,890
>> [LAUGH]
>> So

70
00:03:01,890 --> 00:03:02,670
I
>> With

71
00:03:02,670 --> 00:03:04,340
>> I don't know.

72
00:03:04,340 --> 00:03:05,050
>> Head issues.
>> Anyway,

73
00:03:05,050 --> 00:03:08,780
the point is that, it's.it's alpha
close to a reasonable pictures.

74
00:03:08,780 --> 00:03:09,620
>> And that's alpha.

75
00:03:09,620 --> 00:03:10,120
Okay.
