1
00:00:00,133 --> 00:00:02,732
And here are the answers that I would give to these two questions.

2
00:00:02,733 --> 00:00:07,266
Obviously, dimensionally the data is two. There's an X dimension and a Y dimension.

3
00:00:07,267 --> 00:00:12,866
But I believe you can represent this all in one dimension. If you pick this dimension over here,

4
00:00:12,867 --> 00:00:18,266
then the major variation of the data goes along this dimension.

5
00:00:18,267 --> 00:00:21,732
If you were, for example, to look at the orthogonal dimension in this direction over here,

6
00:00:21,733 --> 00:00:25,699
you would find that there's very little variation in the second dimension.

7
00:00:25,700 --> 00:00:30,566
So if you only graphed your data along one dimension, you'd be doing well.

8
00:00:30,567 --> 00:00:37,266
Now, this is an example of unsupervised learning where you'll find structure in terms of lower dimensionality of data.

9
00:00:37,267 --> 00:00:42,766
And especially for very high dimensional data like image data, this will be an important technique.

10
00:00:42,767 --> 00:00:47,732
So I'll talk a little bit later in this unit about dimensionality reduction.

11
00:00:47,733 --> 00:00:55,767
So in this class we've learned about clustering, dimensionality reduction, and we'll apply it to a number of interesting problems.
