1
00:00:00,288 --> 00:00:10,288
[BLANK_AUDIO]

2
00:00:22,073 --> 00:00:23,680
Live stream is starting soon.

3
00:00:24,850 --> 00:00:27,360
It's starting soon, it's about to start.

4
00:00:27,360 --> 00:00:28,400
It's about to go down.

5
00:00:28,400 --> 00:00:30,600
Live stream is about to go down.

6
00:00:30,600 --> 00:00:32,259
In fact it's probably
already going down.

7
00:00:33,410 --> 00:00:34,660
There I am.

8
00:00:34,660 --> 00:00:35,770
Hello world, it's Raj.

9
00:00:35,770 --> 00:00:37,210
Good to see you guys.

10
00:00:37,210 --> 00:00:40,650
ML Squad is in the building.

11
00:00:40,650 --> 00:00:42,340
Let me shout out some names.

12
00:00:42,340 --> 00:00:49,410
Egor, Glorious, Patchy, Mick,
Rodge, Chudumshu, Brandon.

13
00:00:49,410 --> 00:00:50,290
Yo!

14
00:00:50,290 --> 00:00:53,370
We got the whole machine running
squad in the house right now.

15
00:00:53,370 --> 00:00:54,820
Who is ready to learn some TensorFlow.

16
00:00:54,820 --> 00:00:57,820
I'm so excited for TensorFlow.

17
00:00:57,820 --> 00:01:00,510
Asude, Gora, Spencer.

18
00:01:00,510 --> 00:01:02,820
All right, all right,
all right, okay cool.

19
00:01:02,820 --> 00:01:05,630
Hi everybody, hi, hi, hi, hi.

20
00:01:05,630 --> 00:01:09,050
Okay, so let's get down to
business as they say in Mulan,

21
00:01:09,050 --> 00:01:10,706
let's get down to business.

22
00:01:10,706 --> 00:01:16,392
[SOUND] And
defeat [SOUND] the loss function.

23
00:01:16,392 --> 00:01:23,870
Okay, so anyway, let's start
off with a five minute Q and A.

24
00:01:23,870 --> 00:01:27,900
So hit me up with your most intense
machine learning questions, or

25
00:01:27,900 --> 00:01:29,630
you just want to know anything.

26
00:01:29,630 --> 00:01:30,880
I'm just, We'll go for it.

27
00:01:30,880 --> 00:01:34,265
And then we're going to use TensorFlow
to classify housing prices.

28
00:01:34,265 --> 00:01:34,420
Okay.

29
00:01:34,420 --> 00:01:35,300
It's going to be dope.

30
00:01:35,300 --> 00:01:38,240
It's going to be the first
time using TensorFlow from

31
00:01:38,240 --> 00:01:40,030
scratch in a live session.

32
00:01:40,030 --> 00:01:44,060
It's going to be awesome,
Okay, so here we go.

33
00:01:44,060 --> 00:01:45,291
Hi, hi, hi, hi, let's begin.

34
00:01:45,291 --> 00:01:49,293
[BLANK_AUDIO]

35
00:01:49,293 --> 00:01:51,150
I'm doing good, I'm doing good.

36
00:01:51,150 --> 00:01:54,010
I'm so down, I do have a sore throat,

37
00:01:54,010 --> 00:01:58,690
but I'm a human, I know, it's crazy,
but it's cool, it's cool.

38
00:01:58,690 --> 00:02:01,600
What kind of method approach do
you use to manage your time?

39
00:02:01,600 --> 00:02:02,530
Nick, great question.

40
00:02:05,080 --> 00:02:08,949
So what do I use to manage my time?

41
00:02:08,949 --> 00:02:12,750
I write down what I'm going to do for
the whole week, before the week starts.

42
00:02:12,750 --> 00:02:16,600
So every Sunday I'll write
down the weekly goals.

43
00:02:16,600 --> 00:02:21,630
So I have a weekly to-do, I have
a daily to-do, I have a monthly to-do,

44
00:02:21,630 --> 00:02:24,130
I have a yearly to-do, and
then I have a five year to-do.

45
00:02:24,130 --> 00:02:27,290
So I plan things out in
intervals like that.

46
00:02:27,290 --> 00:02:31,330
I make sure that my goals
are accomplishable goals so

47
00:02:31,330 --> 00:02:34,802
that I get that positive feed back so
I can get going.

48
00:02:34,802 --> 00:02:37,350
Is google release its own OS?

49
00:02:39,040 --> 00:02:40,905
Chrome OS yes.

50
00:02:40,905 --> 00:02:44,500
Are there any recommended
books on computer science or

51
00:02:44,500 --> 00:02:48,470
chritogrophy Yeah,
cryptography is so dope.

52
00:02:48,470 --> 00:02:50,410
I wish I could just deep
dive into cryptography.

53
00:02:50,410 --> 00:02:53,370
That is like some ancient Egyptian,
pharaoh, mummy,

54
00:02:53,370 --> 00:02:56,650
you know what I'm saying,
like you have to.

55
00:02:56,650 --> 00:02:59,600
A good book on cryptography is my book,
Decentralized Applications,

56
00:02:59,600 --> 00:03:02,230
the best selling software
engineering book on Amazon for 2016.

57
00:03:02,230 --> 00:03:05,120
And a good book for
machine learning is Machine Learning,

58
00:03:05,120 --> 00:03:08,700
A Probabilistic Approach I
forgot the name.

59
00:03:08,700 --> 00:03:09,680
But that's the name of it.

60
00:03:09,680 --> 00:03:10,590
What do you think of Star Wars?

61
00:03:10,590 --> 00:03:11,580
I love Star Wars.

62
00:03:11,580 --> 00:03:13,570
I'm actually not like...

63
00:03:13,570 --> 00:03:14,134
I love the old ones.

64
00:03:14,134 --> 00:03:17,101
The new ones, like, I was kind of
disappointed by episode seven, so

65
00:03:17,101 --> 00:03:20,239
it's like, whatever, it's kind of
conservative on Disney's part.

66
00:03:20,239 --> 00:03:22,695
We have to make our own stories,
guys, we are a new generation.

67
00:03:22,695 --> 00:03:25,517
We're going to make our own stories and
they're going to be as good,

68
00:03:25,517 --> 00:03:26,833
if not better than Star Wars.

69
00:03:26,833 --> 00:03:28,751
And we're going to have machine
learning, it's going to be dope.

70
00:03:28,751 --> 00:03:31,864
When we make stories it's going to
be technically art accurate.

71
00:03:31,864 --> 00:03:41,160
What if there are more [INAUDIBLE].

72
00:03:41,160 --> 00:03:41,770
Great question.

73
00:03:41,770 --> 00:03:45,220
So, it's like a 3D Graph with the value,
right?

74
00:03:45,220 --> 00:03:47,840
We want to, remember what I talked about
last lecture, we want to drop a ball

75
00:03:47,840 --> 00:03:50,350
into a bowl and see where it fits,
but what if we have multiple values?

76
00:03:50,350 --> 00:03:54,530
Well then that's where a second order
optimization function comes in.

77
00:03:54,530 --> 00:03:58,320
We have to find the local
minima that is closest to,

78
00:04:00,110 --> 00:04:04,840
that is going to best
optimize our function.

79
00:04:04,840 --> 00:04:05,820
We're going to talk about that later,

80
00:04:05,820 --> 00:04:09,330
but the key words to remember is
a second order optimization function.

81
00:04:09,330 --> 00:04:12,183
Tensor calculus,
that's exactly what we're going to do.

82
00:04:12,183 --> 00:04:14,118
Not at a low level, but at a high level,

83
00:04:14,118 --> 00:04:17,579
because TensorFlow is going to
do a little bit of magic for us.

84
00:04:17,579 --> 00:04:19,300
Can you explain math behind TensorFlow?

85
00:04:19,300 --> 00:04:21,709
I will absolutely explain
the math behind TensorFlow.

86
00:04:21,709 --> 00:04:24,180
TensorFlow is better than
Torch exactly Hey Sraj,

87
00:04:24,180 --> 00:04:27,170
are more layers always
better in a neural network?

88
00:04:27,170 --> 00:04:28,005
Great question, Tamar.

89
00:04:28,005 --> 00:04:31,860
No.M Not always, not always.

90
00:04:31,860 --> 00:04:33,210
Usually it is.

91
00:04:33,210 --> 00:04:36,490
So if you have a smaller data set,
you don't really need that many layers.

92
00:04:36,490 --> 00:04:40,470
So remember, with machine learning,
there's always a tradeoff.

93
00:04:40,470 --> 00:04:41,830
There's always a tradeoff.

94
00:04:41,830 --> 00:04:44,030
Any engineering in general,
there's always In life,

95
00:04:44,030 --> 00:04:45,660
there's a tradeoff as well.

96
00:04:45,660 --> 00:04:49,300
But we want to, when we optimize for.

97
00:04:49,300 --> 00:04:53,925
[BLANK_AUDIO]

98
00:04:53,925 --> 00:04:55,282
So when we increase the layers,

99
00:04:55,282 --> 00:04:59,000
we increase the computational complexity
but we also increase the accuracy.

100
00:04:59,000 --> 00:05:00,150
So there's a tradeoff.

101
00:05:00,150 --> 00:05:04,280
How should I prepare for Google Brain
residency program interview?

102
00:05:04,280 --> 00:05:05,950
Kaggle challenges are great.

103
00:05:05,950 --> 00:05:07,430
And the tension flow intradocs.

104
00:05:08,430 --> 00:05:10,420
Watch all my videos but
you probably already have.

105
00:05:10,420 --> 00:05:13,400
If you do all that, if you apply
the code, you'll be good to go.

106
00:05:13,400 --> 00:05:14,380
Essential.

107
00:05:14,380 --> 00:05:17,310
Can you give new research idea
that can implement a tension flow?

108
00:05:17,310 --> 00:05:19,300
Yeah, so here's an idea I've had for
a while.

109
00:05:19,300 --> 00:05:21,200
Somebody should roll with this.

110
00:05:21,200 --> 00:05:23,250
Take the synthetic radiance paper from.

111
00:05:23,250 --> 00:05:24,230
Synthetic radiance.

112
00:05:24,230 --> 00:05:27,080
So they stopped using that
propagation just for this paper.

113
00:05:27,080 --> 00:05:28,700
And then take the idea
of one shot learning,

114
00:05:28,700 --> 00:05:32,160
so probabilistic programming, and
then combine these two ideas together.

115
00:05:32,160 --> 00:05:34,350
So, synthetic radiance and
one shot learning.

116
00:05:34,350 --> 00:05:36,100
Boom, there's your landmark paper for
the year.

117
00:05:36,100 --> 00:05:38,472
I don't have time to do it, so
someone just roll with that.

118
00:05:38,472 --> 00:05:40,700
Is usable in Raspberry Pi?

119
00:05:41,780 --> 00:05:42,500
Yeah, actually.

120
00:05:42,500 --> 00:05:44,370
Somebody made a rap for our team now.

121
00:05:44,370 --> 00:05:45,980
So two more questions and
then we're going to get started.

122
00:05:48,050 --> 00:05:50,230
How did genetic algorithms
fit into machine running?

123
00:05:50,230 --> 00:05:53,770
Genetic algorithms are good for
optimizing hyper parameters for us.

124
00:05:53,770 --> 00:05:55,400
And let me talk about that for a second.

125
00:05:55,400 --> 00:05:58,540
I've got some emails from some people
who are like, and also articles from

126
00:05:58,540 --> 00:06:02,770
people who are like uneasy about this
where Google are using machinery.

127
00:06:02,770 --> 00:06:05,510
Google's using machine running
to optimize machine running.

128
00:06:05,510 --> 00:06:06,610
So what do we do?

129
00:06:06,610 --> 00:06:06,800
Guys.

130
00:06:07,970 --> 00:06:10,630
Machine running is not going
to take your jobs away.

131
00:06:10,630 --> 00:06:11,280
Okay.

132
00:06:11,280 --> 00:06:12,220
We will be.

133
00:06:14,380 --> 00:06:16,360
Even if there's a job apocalypse,

134
00:06:16,360 --> 00:06:20,870
machine running engineers will
be the last People to have jobs.

135
00:06:20,870 --> 00:06:21,830
Not that that's going to happen,

136
00:06:21,830 --> 00:06:24,200
we're going to create new
jobs in the big data economy.

137
00:06:24,200 --> 00:06:26,780
And we're going to have basic income and
things like that.

138
00:06:26,780 --> 00:06:29,620
Hopefully and crypt currency,
check out my post social point,

139
00:06:29,620 --> 00:06:32,800
it's already possible,
and also Grant's point.

140
00:06:32,800 --> 00:06:35,500
But Don't worry about
Machine Learning replacing

141
00:06:35,500 --> 00:06:37,550
your job as a Machine Learning engineer.

142
00:06:37,550 --> 00:06:40,690
If that actually happens, that we solve
intelligence and then there's no?

143
00:06:40,690 --> 00:06:43,540
The idea of jobs and money and
scarcity all goes away anyway, okay?

144
00:06:43,540 --> 00:06:46,940
So, let's just go ahead and get started.

145
00:06:46,940 --> 00:06:49,170
The best way to start deep
warning is to start my course,

146
00:06:49,170 --> 00:06:51,532
which is this course, okay?

147
00:06:51,532 --> 00:06:51,763
Okay.

148
00:06:51,763 --> 00:06:54,310
[COUGH] So that's the [INAUDIBLE],
here's what we're going to do.

149
00:06:55,470 --> 00:06:58,390
We are going to use to
classify housing prices.

150
00:06:58,390 --> 00:07:02,310
Okay so that's what we're going to do
and I'm going to use a notebook for

151
00:07:02,310 --> 00:07:02,540
a while.

152
00:07:02,540 --> 00:07:06,010
So let me go ahead and
start screen sharing immediately and

153
00:07:06,010 --> 00:07:08,700
we'll get right on into this.

154
00:07:08,700 --> 00:07:09,403
Here we go.

155
00:07:09,403 --> 00:07:19,403
[BLANK_AUDIO]

156
00:07:20,684 --> 00:07:21,260
Okay.

157
00:07:21,260 --> 00:07:26,740
So, let me move this chat window over
here so I can still see you guys.

158
00:07:26,740 --> 00:07:28,990
I don't need to see myself though.

159
00:07:28,990 --> 00:07:30,530
I just need to see you guys.

160
00:07:30,530 --> 00:07:32,126
And then I'll also have,

161
00:07:32,126 --> 00:07:34,866
[BLANK_AUDIO]

162
00:07:35,927 --> 00:07:38,808
up so I can see myself.

163
00:07:38,808 --> 00:07:41,110
[BLANK_AUDIO]

164
00:07:41,110 --> 00:07:44,355
All right, new movie recording.

165
00:07:44,355 --> 00:07:47,352
[BLANK_AUDIO]

166
00:07:47,352 --> 00:07:52,861
All right, and then that goes away and

167
00:07:52,861 --> 00:07:56,140
I'm right here Okay.

168
00:07:56,140 --> 00:07:57,100
Minimize myself.

169
00:07:57,100 --> 00:07:59,714
[BLANK_AUDIO]

170
00:07:59,714 --> 00:08:01,421
Yeah, the one is kind of blurred.

171
00:08:01,421 --> 00:08:02,840
I'm going to get a better one.

172
00:08:03,940 --> 00:08:07,076
Okay, so here's what we're
going to do guys, okay?

173
00:08:07,076 --> 00:08:08,763
So we have a housing data sets.

174
00:08:08,763 --> 00:08:11,079
[BLANK_AUDIO]

175
00:08:11,079 --> 00:08:13,186
One was the name, was the other thing.

176
00:08:13,186 --> 00:08:15,941
Okay, so that someone knew that.

177
00:08:15,941 --> 00:08:18,280
So and
just in general one shot learning.

178
00:08:18,280 --> 00:08:22,070
So we need more one
shot learning progress,

179
00:08:22,070 --> 00:08:25,521
because not everybody has huge data
sets and huge computing power.

180
00:08:25,521 --> 00:08:31,199
I'm going to explain things
slowly this time, okay?

181
00:08:31,199 --> 00:08:32,715
Thanks Alexander, much love.

182
00:08:32,715 --> 00:08:36,470
So, I'm going to explain
things slowly this time, okay?

183
00:08:36,470 --> 00:08:41,100
I want to start off by talking
about a neural network.

184
00:08:42,100 --> 00:08:42,960
It's a lot.

185
00:08:42,960 --> 00:08:45,500
Okay, we see this image a lot
of a neural network, right?

186
00:08:45,500 --> 00:08:48,940
But the one thing I want to say
is you see all these circles?

187
00:08:48,940 --> 00:08:50,780
We call them neurons.

188
00:08:50,780 --> 00:08:53,950
They're not actually objects,
they're not classes.

189
00:08:53,950 --> 00:08:56,480
These are just
representations of numbers.

190
00:08:56,480 --> 00:08:57,480
Let me zoom in a little bit.

191
00:08:58,770 --> 00:09:00,830
They just represent numbers, okay?

192
00:09:00,830 --> 00:09:04,366
Because when we input data
into a neural network,

193
00:09:04,366 --> 00:09:06,468
flow we call it a computation graph

194
00:09:06,468 --> 00:09:09,655
[BLANK_AUDIO]

195
00:09:09,655 --> 00:09:15,855
And these are not actually like neural,
they're not class objects.

196
00:09:15,855 --> 00:09:20,125
When we take an input,
let's say like what we're going to do,

197
00:09:20,125 --> 00:09:24,061
housing prices and
when we apply some matrix math to it,

198
00:09:24,061 --> 00:09:27,608
it's going to take that value and
it's going to do.

199
00:09:27,608 --> 00:09:31,610
So if it's a one value and we multiply
by a matrix of say, three values,

200
00:09:31,610 --> 00:09:34,860
it's going to then become three or
four values.

201
00:09:34,860 --> 00:09:38,610
Those values are what we represent as
neurons, they're not actually objects.

202
00:09:38,610 --> 00:09:42,490
Then we take those values and apply
a set of matrix operations to them and

203
00:09:42,490 --> 00:09:44,100
then, boom,
it's the next set of neurons.

204
00:09:44,100 --> 00:09:47,653
But these neurons are not actually
classes, they are values.

205
00:09:47,653 --> 00:09:51,340
Okay, so I just want to start
off with that, so that's that.

206
00:09:51,340 --> 00:09:53,620
Now let me show you guys the data first.

207
00:09:53,620 --> 00:09:56,155
So here's the data set.

208
00:09:56,155 --> 00:09:56,965
Let me zoom in on this.

209
00:09:56,965 --> 00:09:58,341
[BLANK_AUDIO]

210
00:09:58,341 --> 00:10:01,410
Okay, so
this is a set of housing prices, okay?

211
00:10:01,410 --> 00:10:05,300
So the first one is the index, which is
the number, then next one is the area,

212
00:10:05,300 --> 00:10:10,680
which is the size of the house,
the next one is the number of bathrooms,

213
00:10:10,680 --> 00:10:18,490
then the price, and
then the price per square inch.

214
00:10:18,490 --> 00:10:21,250
So, that's what that is.

215
00:10:21,250 --> 00:10:23,680
What we want to do is we
want to classify this,

216
00:10:23,680 --> 00:10:26,220
but we don't have labels right now.

217
00:10:26,220 --> 00:10:28,030
We're going to add labels.

218
00:10:28,030 --> 00:10:31,910
We're going to add labels where it's
going to be either a good buy or

219
00:10:31,910 --> 00:10:36,400
bad buy, because what we want to
do is given one of these features

220
00:10:36,400 --> 00:10:38,540
we're going to decide
which features to use.

221
00:10:38,540 --> 00:10:39,630
But given one of these features,

222
00:10:39,630 --> 00:10:45,720
we want to then classify the house as
either a good buy or a bad buy, okay?

223
00:10:45,720 --> 00:10:48,536
So let me shut down my screen for
a second so

224
00:10:48,536 --> 00:10:50,800
the bit rate goes up a little bit.

225
00:10:50,800 --> 00:10:54,980
I want it to be not that laggy.

226
00:10:54,980 --> 00:10:57,510
So that's what we're going to do,
okay, so that's our data.

227
00:10:57,510 --> 00:11:00,940
So let's go ahead and
start doing this in a Jupyter notebook.

228
00:11:00,940 --> 00:11:02,410
Okay, so I'm going to do
it in a Jupyter notebook.

229
00:11:02,410 --> 00:11:07,510
All right, so
the first thing I want to do

230
00:11:07,510 --> 00:11:12,310
is talk a little bit about, let me
just make sure that this works first.

231
00:11:12,310 --> 00:11:12,600
Yo, let's see.

232
00:11:13,810 --> 00:11:13,927
Yo.

233
00:11:13,927 --> 00:11:15,000
Okay, it works, great.

234
00:11:15,000 --> 00:11:18,269
Okay, so TensorFlow, why are we
using TensorFlow in the first place?

235
00:11:18,269 --> 00:11:22,290
So, let me make the font bigger.

236
00:11:22,290 --> 00:11:23,378
Actually, let me see.

237
00:11:23,378 --> 00:11:30,410
We're using TensorFlow because
Google created it for,

238
00:11:30,410 --> 00:11:34,420
this is going to be a supervise problem,
okay?

239
00:11:34,420 --> 00:11:36,090
Yeah, it's price per square foot.

240
00:11:36,090 --> 00:11:39,170
This is a supervised problem,
we're using TensorFlow because

241
00:11:39,170 --> 00:11:44,100
TensorFlow is created by Google and
it was built to scale.

242
00:11:44,100 --> 00:11:45,730
They've used it for many years.

243
00:11:45,730 --> 00:11:48,890
They released it, it was kind of like
Zeus coming down from Olympus and

244
00:11:48,890 --> 00:11:51,220
giving fire to the humans, right?

245
00:11:51,220 --> 00:11:53,190
We know that TensorFlow can scale,

246
00:11:53,190 --> 00:11:56,590
we know that it's reliable because
it's used in production by Google.

247
00:11:56,590 --> 00:11:58,450
I mean,
Google search uses TensorFlow, okay?

248
00:11:58,450 --> 00:12:00,890
So this is a reliable library.

249
00:12:00,890 --> 00:12:03,130
Although, High Porch is pretty cool too.

250
00:12:03,130 --> 00:12:04,510
It just came out this month, and

251
00:12:04,510 --> 00:12:07,810
it introduces the concept of
dynamic computation graphs.

252
00:12:07,810 --> 00:12:09,400
Usually, we have two interpreters,
right?

253
00:12:09,400 --> 00:12:15,530
We have a python interpreter, and then
we have computation graph interpreter.

254
00:12:15,530 --> 00:12:19,515
But what High Porch does is it has
one interpreter for everything.

255
00:12:19,515 --> 00:12:25,327
Okay so Jupyter Notebook is
really is a great concept.

256
00:12:25,327 --> 00:12:32,730
Basically, it allows us to view our
code and add mark down to our code.

257
00:12:32,730 --> 00:12:37,440
It lets us add a bunch of
pretty things to our code.

258
00:12:37,440 --> 00:12:41,490
Yes, Prometheus is the right
answer that I was looking for.

259
00:12:41,490 --> 00:12:41,700
Okay.

260
00:12:41,700 --> 00:12:44,750
So let's go ahead and
start by importing our dependencies.

261
00:12:44,750 --> 00:12:47,080
Okay so,
we want to classify our problem.

262
00:12:47,080 --> 00:12:49,800
And also let me show you guys
what our URL net looks like.

263
00:12:49,800 --> 00:12:52,360
I didn't show you guys what
this is going to look like.

264
00:12:52,360 --> 00:12:53,610
So this what it's going to look like.

265
00:12:54,850 --> 00:12:56,710
Okay, so
we're going to feed in two inputs.

266
00:12:56,710 --> 00:13:00,850
And we're going to apply
a set of weights and biases,

267
00:13:00,850 --> 00:13:06,020
use the softmax function,
and then output the housing

268
00:13:06,020 --> 00:13:11,430
price as well as the label,
which is good or bad.

269
00:13:11,430 --> 00:13:14,610
Okay, so
we'll talk about that in a second.

270
00:13:14,610 --> 00:13:17,650
Let's go ahead and
first import our dependencies.

271
00:13:17,650 --> 00:13:20,950
All right, so
yes I'm focused on the content.

272
00:13:20,950 --> 00:13:21,420
Thank you.

273
00:13:21,420 --> 00:13:22,340
Hole watch.

274
00:13:22,340 --> 00:13:22,710
Here we go.

275
00:13:24,000 --> 00:13:25,986
Import pandas as pd.

276
00:13:25,986 --> 00:13:33,174
Pandas is our beautiful library
to work with data s tables.

277
00:13:33,174 --> 00:13:34,980
Work with data s tables.

278
00:13:34,980 --> 00:13:36,710
That's our first library, okay?

279
00:13:36,710 --> 00:13:38,060
So the next one is numpy.

280
00:13:38,060 --> 00:13:45,230
Numpy is going to help
us use number matrices.

281
00:13:45,230 --> 00:13:50,990
Both pandas and numpy need it,
so use number matrices, okay.

282
00:13:50,990 --> 00:13:54,977
The next thing is natplot library
because we're going to show this graph.

283
00:13:54,977 --> 00:13:57,660
We're going to show this
graph in this section.

284
00:13:57,660 --> 00:13:57,710
And,

285
00:13:57,710 --> 00:14:01,354
[BLANK_AUDIO]

286
00:14:01,354 --> 00:14:01,872
What else?

287
00:14:01,872 --> 00:14:04,965
We want natplotlib's pyplot as plt.

288
00:14:04,965 --> 00:14:08,724
[BLANK_AUDIO]

289
00:14:08,724 --> 00:14:10,034
I prefer TensorFlow.

290
00:14:10,034 --> 00:14:16,932
Pyplot to and finally tensorflow as tf.

291
00:14:18,900 --> 00:14:19,520
Okay.

292
00:14:19,520 --> 00:14:19,850
That's it.

293
00:14:19,850 --> 00:14:21,240
Those are our four libraries.

294
00:14:21,240 --> 00:14:22,470
Now we can go ahead and compile it.

295
00:14:22,470 --> 00:14:23,220
Boom.

296
00:14:23,220 --> 00:14:25,501
So because there are no errors,
we can keep going, right?

297
00:14:25,501 --> 00:14:30,188
So that's the great thing about
jupyter/ipython notebooks is that

298
00:14:30,188 --> 00:14:35,320
they let you compile as you go so
you can see what is happening.

299
00:14:35,320 --> 00:14:36,620
Okay so that's what we did there.

300
00:14:36,620 --> 00:14:37,940
Now we're going to load the data.

301
00:14:37,940 --> 00:14:41,700
That was, Step 1 is to load data.

302
00:14:41,700 --> 00:14:42,960
Load data, okay.

303
00:14:42,960 --> 00:14:44,120
So let's go ahead and load data.

304
00:14:44,120 --> 00:14:44,970
How are we going to load data?

305
00:14:44,970 --> 00:14:51,280
Well we are going to use the pandas
library to help us do that.

306
00:14:51,280 --> 00:14:55,230
So the first thing we're going to do is
we're going to use the pandas read csv

307
00:14:55,230 --> 00:14:59,240
function because we have a csv file,
okay?

308
00:15:01,820 --> 00:15:05,370
So we're going to read this
in as a dataframe object.

309
00:15:05,370 --> 00:15:06,730
So what is a dataframe object?

310
00:15:06,730 --> 00:15:11,330
A dataframe object is an object
in memory that pandas

311
00:15:11,330 --> 00:15:14,630
creates from the csv file that
we can then easily parse, and

312
00:15:14,630 --> 00:15:16,240
I'm going to show you what
I mean by easily parse.

313
00:15:17,370 --> 00:15:22,685
So we're going to take this dataframe
variables, that we just created, and

314
00:15:22,685 --> 00:15:28,190
we're going to list what we're going to
remove the columns we don't care about.

315
00:15:28,190 --> 00:15:30,305
So how do we know which columns
we don't care about, right?

316
00:15:30,305 --> 00:15:31,905
So let's go back to our data for
a second.

317
00:15:31,905 --> 00:15:33,765
We have several columns here.

318
00:15:33,765 --> 00:15:39,145
Which are the ones that we want to use
to predict if a house is good or bad?

319
00:15:39,145 --> 00:15:41,465
Well, what we're going to do, we're
going to take out the index because

320
00:15:41,465 --> 00:15:42,875
that's not going to
really give us anything.

321
00:15:42,875 --> 00:15:46,765
It's just a numbering
of what each column is.

322
00:15:46,765 --> 00:15:52,700
The next one is the,
let's say area and bathrooms, right?

323
00:15:52,700 --> 00:15:56,670
So those are the two features that, like
We're going to use area and bathroom.

324
00:15:56,670 --> 00:16:00,570
We're not going to use the price or
the sq_price.

325
00:16:00,570 --> 00:16:05,413
Because when I look for a house,
I want to make sure that it has, and

326
00:16:05,413 --> 00:16:08,335
that's just what I'm using, right?

327
00:16:08,335 --> 00:16:15,011
Deciding which features to use is
its own discipline in and of itself.

328
00:16:15,011 --> 00:16:16,390
What feature should we use?

329
00:16:16,390 --> 00:16:19,060
And a good rule of thumb to go by, is

330
00:16:21,450 --> 00:16:27,196
what feature would you personally
use to make a prediction, okay?

331
00:16:27,196 --> 00:16:34,540
Now let me give you guys this data,
how I got this data.

332
00:16:34,540 --> 00:16:36,210
I could create a gist.

333
00:16:36,210 --> 00:16:40,100
So in a gist, I'll say,
let's just take all this.

334
00:16:40,100 --> 00:16:41,522
I'm going to give you guys
this data real quick and

335
00:16:41,522 --> 00:16:43,171
then I'll paste it into the chat so
you guys can see it.

336
00:16:43,171 --> 00:16:48,270
So data.txt,
that will create a public gist, boom.

337
00:16:48,270 --> 00:16:49,640
And I'll paste it into the chat, ready?

338
00:16:49,640 --> 00:16:51,880
Get ready for this guys,
I'm going to paste it in.

339
00:16:51,880 --> 00:16:54,330
Boom, there's your data.

340
00:16:54,330 --> 00:16:55,479
That's the data file, all right?

341
00:16:55,479 --> 00:16:57,350
So now you guys can do that along with.

342
00:16:59,300 --> 00:17:03,160
All right, so, we are going to remove
the features we don't care about.

343
00:17:05,180 --> 00:17:07,130
Okay, we're going to remove
the features we don't care about.

344
00:17:07,130 --> 00:17:10,000
We don't care about the price, we're
not going to care about the sq_price.

345
00:17:10,000 --> 00:17:14,839
We only care about the number bathrooms
and what was the second thing?

346
00:17:14,839 --> 00:17:18,829
The area, that's what we're
going to be using right now.

347
00:17:21,050 --> 00:17:25,729
And axis is 1 because we're
only going to use the first

348
00:17:25,729 --> 00:17:27,874
axis from the data sets.

349
00:17:27,874 --> 00:17:33,982
All right, so dataframe.drop,
let me say what we just did here.

350
00:17:33,982 --> 00:17:39,517
We removed the features

351
00:17:39,517 --> 00:17:44,780
we don't care about.

352
00:17:44,780 --> 00:17:46,639
So we removed the features
we don't care about.

353
00:17:46,639 --> 00:17:50,224
Now we're going to only use the first
ten rows of the data set in this

354
00:17:50,224 --> 00:17:50,810
example.

355
00:17:50,810 --> 00:17:55,180
So we only want to use the first
ten of these, right up to here.

356
00:17:55,180 --> 00:17:56,435
Right up to here, okay?

357
00:17:56,435 --> 00:17:58,842
No Oxford, I don't want to define that.

358
00:17:58,842 --> 00:18:00,486
Okay, so how do we say that?

359
00:18:00,486 --> 00:18:05,389
Well, easily enough,
the great thing about a dataframe

360
00:18:05,389 --> 00:18:10,402
object is we can say,
I want from row 0 to row 10, boom.

361
00:18:10,402 --> 00:18:16,210
So that's going to tell us that
we only use the first ten rows.

362
00:18:18,160 --> 00:18:18,790
That's it, okay?

363
00:18:19,960 --> 00:18:21,830
That's it, and so
now we have our dataframe object.

364
00:18:21,830 --> 00:18:23,530
So let's go ahead and print this.

365
00:18:23,530 --> 00:18:26,730
And here's a great thing about Python
notebooks, is we can now print this and

366
00:18:26,730 --> 00:18:27,212
see what we have.

367
00:18:27,212 --> 00:18:29,390
And it'll give us an error
if it doesn't work.

368
00:18:29,390 --> 00:18:31,066
Okay, so there we go, invalid syntax.

369
00:18:31,066 --> 00:18:32,157
So let's see what I did here.

370
00:18:32,157 --> 00:18:37,262
So, on this line, dataframe.drop,

371
00:18:37,262 --> 00:18:41,756
it's pointing to this axis = 1.

372
00:18:41,756 --> 00:18:46,516
And the problem here is invalid syntax,

373
00:18:46,516 --> 00:18:51,140
so we have index, sq_price, yes.

374
00:18:51,140 --> 00:18:55,310
So, okay, so
this bracket actually goes here.

375
00:18:55,310 --> 00:18:55,911
That's where the bracket goes.

376
00:18:55,911 --> 00:18:57,357
Now let's compile.

377
00:18:57,357 --> 00:18:58,731
Okay, so there we go.

378
00:18:58,731 --> 00:19:01,438
There is our beautiful,
clean, formatted data.

379
00:19:01,438 --> 00:19:04,855
And we're only using
the first ten columns, and

380
00:19:04,855 --> 00:19:09,590
we're just using the area and
bathrooms as our features.

381
00:19:09,590 --> 00:19:13,680
And remember, feature selection is
an entire discipline in and of itself.

382
00:19:13,680 --> 00:19:15,580
And we could have used the square price,
and

383
00:19:15,580 --> 00:19:17,950
we could have used
the price as features.

384
00:19:17,950 --> 00:19:21,300
But what I'm looking for
are the area and bathrooms.

385
00:19:21,300 --> 00:19:25,786
That's what I look for
when I try to predict if

386
00:19:25,786 --> 00:19:29,808
a house is a good buy or
not for me, okay?

387
00:19:29,808 --> 00:19:33,610
So now we have our features,
so let's introduce our labels.

388
00:19:33,610 --> 00:19:36,700
So what we just did, so
step one was to load the data, and

389
00:19:36,700 --> 00:19:39,040
step two is going to be
to introduce labels.

390
00:19:39,040 --> 00:19:40,810
Because right now, there are no labels.

391
00:19:40,810 --> 00:19:43,360
So we could just do a regression, right?

392
00:19:43,360 --> 00:19:45,180
Because we could then
predict the next value.

393
00:19:45,180 --> 00:19:46,980
And what do we use regression for?

394
00:19:46,980 --> 00:19:49,180
For predicting the next
value in a continuous set.

395
00:19:51,310 --> 00:19:53,090
So that's what we do for that.

396
00:19:53,090 --> 00:19:55,330
But what we're going to do is
we're going to convert this into

397
00:19:55,330 --> 00:19:56,940
a classification problem.

398
00:19:56,940 --> 00:20:00,167
So add labels, so that's our step 2.

399
00:20:00,167 --> 00:20:02,871
So we're going to take
a data print object, and

400
00:20:02,871 --> 00:20:06,285
if anybody doesn't have the data,
go ahead and ask, and

401
00:20:06,285 --> 00:20:10,170
someone's going to paste it into
the chat, the link to that gist.

402
00:20:10,170 --> 00:20:12,895
All right, so what we're going to do
is we're to introduce the labels.

403
00:20:12,895 --> 00:20:17,030
So we want to say, our labels are going
to be either a good buy or a bad buy.

404
00:20:17,030 --> 00:20:18,570
And so how do we represent that?

405
00:20:18,570 --> 00:20:21,372
Well, for simplicity's sake,
let's just use binary numbers, right?

406
00:20:21,372 --> 00:20:24,287
So a good buy is a 1 and a bad buy is 0.

407
00:20:24,287 --> 00:20:30,781
So let's just randomly add a set of,
for our ten values.

408
00:20:30,781 --> 00:20:32,390
So we have to count ten, right?

409
00:20:32,390 --> 00:20:37,047
So we want 1, 1, 1, 0 and
then we'll add some variety in there.

410
00:20:37,047 --> 00:20:38,389
So let's see how many that is.

411
00:20:38,389 --> 00:20:40,758
That's 3, 6, 9.

412
00:20:40,758 --> 00:20:45,728
But we want, we have 1,1,0,0,1.

413
00:20:45,728 --> 00:20:47,316
And then 0, and then 1.

414
00:20:47,316 --> 00:20:48,845
Okay, so yeah, it's ten values.

415
00:20:48,845 --> 00:20:52,728
So we're going to add our labels.

416
00:20:52,728 --> 00:20:57,851
And so, 1 is good and 0 is bad.

417
00:20:57,851 --> 00:21:03,494
So 1 is good buy and 0 is bad buy, okay?

418
00:21:03,494 --> 00:21:05,592
So 1 is good buy and 0 is bad buy.

419
00:21:05,592 --> 00:21:10,120
Now, you can't see
the first part of the code.

420
00:21:10,120 --> 00:21:11,081
Let me repaste the link.

421
00:21:11,081 --> 00:21:12,895
I'm going to repaste the link, boom.

422
00:21:12,895 --> 00:21:14,581
Okay, [INAUDIBLE] paste the link.

423
00:21:14,581 --> 00:21:16,948
Okay, so now you can't see
the first part of the code.

424
00:21:16,948 --> 00:21:19,581
Let me go up.

425
00:21:19,581 --> 00:21:21,672
I can't go back,
we have to keep going, okay?

426
00:21:21,672 --> 00:21:24,320
So 1 is the good buy and 0 is a bad buy.

427
00:21:24,320 --> 00:21:25,860
So we've defined our set of labels.

428
00:21:25,860 --> 00:21:32,260
Now, the next step is to turn
TRUE/FALSE values into 1's and 0's.

429
00:21:32,260 --> 00:21:36,152
So we're going to say,
that the location variable is

430
00:21:36,152 --> 00:21:39,570
going to define those labels for us.

431
00:21:40,710 --> 00:21:45,588
All right, so let's go ahead and

432
00:21:45,588 --> 00:21:52,219
add the y2 values which will,
hold on a sec.

433
00:21:52,219 --> 00:21:56,921
So Y2 and we're going to use

434
00:21:56,921 --> 00:22:03,130
the dataframe( 'y1 ) == 0.

435
00:22:03,130 --> 00:22:05,117
So what did we just do?

436
00:22:05,117 --> 00:22:09,077
So what we just did is say
y2 is a negation of y1.

437
00:22:09,077 --> 00:22:15,342
Right, so, we defined y2 as a negation,
as a y2 is a negation of y1.

438
00:22:15,342 --> 00:22:18,535
It's the opposite, opposite, okay?

439
00:22:18,535 --> 00:22:22,354
[BLANK_AUDIO]

440
00:22:22,354 --> 00:22:27,562
Right, so axis 1 is the column, okay?

441
00:22:27,562 --> 00:22:32,306
So y2 means we don't like a house.

442
00:22:32,306 --> 00:22:35,300
Okay, so let's go ahead and define y2.

443
00:22:36,320 --> 00:22:39,440
Okay, dataframe.loc, location.

444
00:22:39,440 --> 00:22:42,600
And we're saying, okay, so
what is that value that we want?

445
00:22:42,600 --> 00:22:46,344
[BLANK_AUDIO]

446
00:22:46,344 --> 00:22:48,979
y2, and then we'll say, okay, so

447
00:22:48,979 --> 00:22:52,550
we're going to convert it
to a int value, right?

448
00:22:52,550 --> 00:22:56,050
So we want to remember that the type
we're converting to is an int.

449
00:22:56,050 --> 00:22:58,770
So how do we convert it to a int value?

450
00:22:58,770 --> 00:23:01,544
So we're going to take astype(int).

451
00:23:01,544 --> 00:23:07,135
And so what this says is, it says turn
TRUE/FALSE values into 1s or 0's.

452
00:23:07,135 --> 00:23:12,788
So turn TRUE/FALSE values

453
00:23:12,788 --> 00:23:16,971
to ones and zeroes.

454
00:23:16,971 --> 00:23:19,949
And now we've done that.

455
00:23:19,949 --> 00:23:22,745
Now let's print out what we have here,
let's see what this looks like.

456
00:23:22,745 --> 00:23:25,025
Thing about Python notebooks,
we can just print what we have here.

457
00:23:25,025 --> 00:23:27,745
We definitely have a syntax error,
so there it is.

458
00:23:27,745 --> 00:23:31,145
And I think Cyber
mentioned this earlier.

459
00:23:31,145 --> 00:23:35,945
So we are going to add to line three.

460
00:23:35,945 --> 00:23:36,680
So which one is that?

461
00:23:36,680 --> 00:23:41,380
Our brackets, yes, just like that.

462
00:23:43,690 --> 00:23:46,190
Okay, and so
now do we have an extra one?

463
00:23:46,190 --> 00:23:46,500
Okay.

464
00:23:47,660 --> 00:23:48,850
Okay, so what did we do?

465
00:23:48,850 --> 00:23:53,610
So we added labels, but instead of
saying good or bad, we added two labels.

466
00:23:53,610 --> 00:23:55,740
So this is just for simplicity's sake,

467
00:23:55,740 --> 00:23:58,240
we don't want to convert
Text to integers.

468
00:23:58,240 --> 00:23:59,560
We're just going to say a raw value.

469
00:23:59,560 --> 00:24:04,360
So if y1 has a value,
that means that is a good bot.

470
00:24:04,360 --> 00:24:09,800
And every time y1 has a value,
every time y1 is 1, y2 would be zero.

471
00:24:09,800 --> 00:24:12,093
Every time y2 is one,
y1 would be zero, okay?

472
00:24:12,093 --> 00:24:16,580
So that's just our way of of
labeling our data for now.

473
00:24:16,580 --> 00:24:18,070
So we've added our labels.

474
00:24:18,070 --> 00:24:19,560
That was step two, remember?

475
00:24:19,560 --> 00:24:21,510
Step one was to load the data,

476
00:24:21,510 --> 00:24:23,900
which we did into a data
frame object we parsed it.

477
00:24:23,900 --> 00:24:27,240
And then step two was to add
the labels to the data, right.

478
00:24:27,240 --> 00:24:29,440
Which makes this
a classification problem,

479
00:24:29,440 --> 00:24:31,650
if we're going to classify this data.

480
00:24:31,650 --> 00:24:31,940
Okay.

481
00:24:33,280 --> 00:24:34,990
We're going to classify housing.

482
00:24:36,560 --> 00:24:39,800
If a house is a good buy or
a bad buy based on the area and

483
00:24:39,800 --> 00:24:41,292
the number of bathrooms, Okay?

484
00:24:41,292 --> 00:24:43,870
Not bedrooms, right.

485
00:24:43,870 --> 00:24:46,220
Well I guess the data didn't
have bedrooms, did it?

486
00:24:47,520 --> 00:24:50,970
Okay, so, but yeah bedrooms would
probably be better if we have that.

487
00:24:50,970 --> 00:24:52,350
So now we have all our
data in the dataframe,

488
00:24:52,350 --> 00:24:54,610
we are to shape the end matrices that.

489
00:24:54,610 --> 00:25:03,480
Now step three is to prepare data for
TensorFlow.

490
00:25:03,480 --> 00:25:09,178
So depending on which machine
running library we're using,

491
00:25:09,178 --> 00:25:12,210
we can,
we will prepare it a certain way.

492
00:25:12,210 --> 00:25:13,870
But in general,

493
00:25:13,870 --> 00:25:18,030
no matter what data we have we're
going to convert it into tensors.

494
00:25:18,030 --> 00:25:19,470
So let me explain what tensors are.

495
00:25:19,470 --> 00:25:26,584
So tensors are it's a high level,

496
00:25:26,584 --> 00:25:32,719
tensors are a generic version

497
00:25:32,719 --> 00:25:38,130
of vectors and matrices.

498
00:25:38,130 --> 00:25:44,030
A vector is a list of numbers,
then Vector is.

499
00:25:44,030 --> 00:25:45,570
We just write something.

500
00:25:45,570 --> 00:25:46,090
This is important.

501
00:25:46,090 --> 00:25:47,260
Because we see these worth a lot.

502
00:25:47,260 --> 00:25:51,833
Is a list of numbers, then a matrix is

503
00:25:51,833 --> 00:25:56,260
a list of lists of numbers, okay?

504
00:25:56,260 --> 00:26:02,000
And so a vector would be a 1 D sensor,
Matrix would be a 2D tensor, and

505
00:26:03,330 --> 00:26:10,650
then whatever a list of list, of list
of numbers is would be a 3D tensor.

506
00:26:10,650 --> 00:26:13,160
And then that just continues, okay?

507
00:26:13,160 --> 00:26:16,980
So a tensor is a very generic term for
both matrices and vectors.

508
00:26:16,980 --> 00:26:21,680
And these tensors are how we
represent data in tensor flows.

509
00:26:21,680 --> 00:26:24,230
And guys, machine learning,
neural networks.

510
00:26:24,230 --> 00:26:26,940
It's all just matrix math.

511
00:26:26,940 --> 00:26:31,110
It's a collection of operations
that we apply to some input matrix.

512
00:26:31,110 --> 00:26:33,960
And we continually apply them
like a chain of these operations

513
00:26:33,960 --> 00:26:35,520
until we get to an output.

514
00:26:35,520 --> 00:26:41,350
And then we apply an optimization
function To minimize the loss.

515
00:26:41,350 --> 00:26:43,090
And I'm going to talk
about that in a second.

516
00:26:43,090 --> 00:26:43,710
We're going to do that.

517
00:26:43,710 --> 00:26:44,680
But that's what this is.

518
00:26:44,680 --> 00:26:49,415
So that was a little short,
the thing on tensors for a second.

519
00:26:49,415 --> 00:26:52,780
But to get back to what we we're doing,
so we're going to take our data frame.

520
00:26:55,300 --> 00:27:01,640
And we're going to say, Let's see.

521
00:27:01,640 --> 00:27:04,680
We want to take the area and
the bathroom.

522
00:27:04,680 --> 00:27:07,310
So we're going to take our features,
right, we're going to take our features

523
00:27:07,310 --> 00:27:09,550
and we're going to convert
them into tensors.

524
00:27:09,550 --> 00:27:11,660
We're going to convert our
features into tensors.

525
00:27:11,660 --> 00:27:18,480
So, convert features to input tensor.

526
00:27:18,480 --> 00:27:19,070
A simple gram.

527
00:27:19,070 --> 00:27:22,690
We're going to convert our
features into a info sensor.

528
00:27:23,960 --> 00:27:29,130
And this session with a few minutes or
probably an hour, we'll see.

529
00:27:29,130 --> 00:27:34,147
As matrix and b.

530
00:27:34,147 --> 00:27:36,450
We want to say as a matrix.

531
00:27:36,450 --> 00:27:44,190
Now we're going to convert our
labels To input tensors as well.

532
00:27:44,190 --> 00:27:47,650
So let me show you guys this energy gap,
okay?

533
00:27:47,650 --> 00:27:54,940
All right, so see this right here.

534
00:27:54,940 --> 00:27:59,360
These two purple circles, those are our
input sensors, our features and

535
00:27:59,360 --> 00:27:59,650
our label.

536
00:27:59,650 --> 00:28:01,210
So we're going to feed
that into a [INAUDIBLE].

537
00:28:01,210 --> 00:28:04,930
We haven't created our weights yet,
If you just block out everything but

538
00:28:04,930 --> 00:28:07,820
these two purple circles.

539
00:28:07,820 --> 00:28:09,410
That's what we just created.

540
00:28:09,410 --> 00:28:09,820
Okay.

541
00:28:09,820 --> 00:28:17,000
So that's what we did for that.

542
00:28:17,000 --> 00:28:22,900
Now we're going to convert our
Labels into features, okay?

543
00:28:22,900 --> 00:28:24,850
So I'm sorry, our labels into tensors.

544
00:28:24,850 --> 00:28:26,425
So we say out dataframe.loc.

545
00:28:26,425 --> 00:28:28,660
Sometimes is probably going to happen.

546
00:28:28,660 --> 00:28:32,050
So let's say, okay, so
we got location and

547
00:28:32,050 --> 00:28:34,050
then we'll those labels that we created.

548
00:28:34,050 --> 00:28:39,710
And then guys,
we can easily have One label as well.

549
00:28:39,710 --> 00:28:45,176
Okay so, I gotta focus on this.

550
00:28:45,176 --> 00:28:48,070
Let's see if this compiles.

551
00:28:48,070 --> 00:28:52,500
Okay so syntax error,
what do we have here?

552
00:28:52,500 --> 00:28:55,790
Right, so it's an extra bracket,
all right.

553
00:28:55,790 --> 00:29:01,240
So area, bathrooms, We have location.

554
00:29:01,240 --> 00:29:08,258
Okay, so this extra bracket here,
and then what else do I have here?

555
00:29:08,258 --> 00:29:10,170
So for y1,

556
00:29:10,170 --> 00:29:15,050
I have a bracket, I need another
bracket, and then we'll compile that.

557
00:29:16,330 --> 00:29:18,790
Let's see what else we've got here.

558
00:29:18,790 --> 00:29:20,030
Comma.

559
00:29:20,030 --> 00:29:22,280
Aren't Jupyter Notebooks so great guys?

560
00:29:22,280 --> 00:29:26,730
We can just compile as we go.

561
00:29:26,730 --> 00:29:28,420
Input our location.

562
00:29:28,420 --> 00:29:28,570
Y1, Y2.

563
00:29:28,570 --> 00:29:29,526
What's the deal.

564
00:29:29,526 --> 00:29:32,400
We're going to remove that.

565
00:29:35,390 --> 00:29:38,790
We want to say, what is going on here?

566
00:29:38,790 --> 00:29:44,480
We've got y1, y2 dot location,
and this is in brackets.

567
00:29:45,840 --> 00:29:46,010
Boom.

568
00:29:49,470 --> 00:29:50,240
Let's see.

569
00:29:50,240 --> 00:29:52,820
Sh, okay,
let's see what's going on here.

570
00:29:52,820 --> 00:29:53,670
Whoa, that's a lot.

571
00:29:53,670 --> 00:29:55,850
None of the area bathrooms
are in the columns.

572
00:29:56,870 --> 00:29:57,930
Hold on a second.

573
00:29:57,930 --> 00:30:00,950
Area bathroom are in the columns.

574
00:30:00,950 --> 00:30:02,020
Let's see what we did here.

575
00:30:04,260 --> 00:30:11,560
So we said in dataframe.location we
converted area bathrooms as a matrix.

576
00:30:11,560 --> 00:30:19,710
And then we said okay,
as_matrix, interesting, okay.

577
00:30:19,710 --> 00:30:23,190
So we did have area bathrooms,

578
00:30:27,030 --> 00:30:29,285
remove the closing brackets.

579
00:30:29,285 --> 00:30:29,604
Remove the closing brackets on

580
00:30:29,604 --> 00:30:34,466
[BLANK_AUDIO]

581
00:30:34,466 --> 00:30:38,543
Or move this one and
we want to add, interesting.

582
00:30:38,543 --> 00:30:40,591
[BLANK_AUDIO]

583
00:30:40,591 --> 00:30:45,620
Missing quotes, there it is, right
missing quotes, that's what it was.

584
00:30:45,620 --> 00:30:50,120
My god, okay, yes, all right.

585
00:30:50,120 --> 00:30:50,520
There we go.

586
00:30:51,940 --> 00:30:54,180
Okay, you scared me for a second.

587
00:30:54,180 --> 00:30:55,750
Let's print out what we had here.

588
00:30:55,750 --> 00:30:57,640
So, what did we just create?

589
00:30:57,640 --> 00:30:58,250
Boom.

590
00:30:58,250 --> 00:30:59,340
So, what is this?

591
00:30:59,340 --> 00:31:02,070
This is what our input
matrix looks like.

592
00:31:02,070 --> 00:31:02,220
Okay.

593
00:31:02,220 --> 00:31:03,350
So, we have the area and the bathrooms.

594
00:31:03,350 --> 00:31:05,530
And this is what our
labels matrix looks like.

595
00:31:05,530 --> 00:31:06,568
Let's print out our label matrix.

596
00:31:06,568 --> 00:31:12,182
[BLANK_AUDIO]

597
00:31:12,182 --> 00:31:13,410
Okay.

598
00:31:13,410 --> 00:31:15,340
So that's our input matrix and
that's our label matrix.

599
00:31:15,340 --> 00:31:20,090
Guys, we have our inputs, now we're
going to prepare our parameters, and so

600
00:31:20,090 --> 00:31:21,070
what step are we on now?

601
00:31:21,070 --> 00:31:21,930
We're on step four.

602
00:31:21,930 --> 00:31:25,540
So step three was to prepare
the data for tensor flow.

603
00:31:25,540 --> 00:31:27,460
So we've formatted our data for
tensor flow and

604
00:31:27,460 --> 00:31:31,540
now we're going to step four is
to write out our Hyperparameters.

605
00:31:34,280 --> 00:31:35,940
Okay so what are our hyper parameters?

606
00:31:35,940 --> 00:31:38,370
So the first one is going
to be our learning rate.

607
00:31:38,370 --> 00:31:42,040
And so
when did we last see our learning rates?

608
00:31:42,040 --> 00:31:43,760
We used the learning rates when we,

609
00:31:43,760 --> 00:31:46,550
in the last live session
we summed that up we.

610
00:31:46,550 --> 00:31:49,820
The learning rate controls
the rate at which we learn.

611
00:31:49,820 --> 00:31:51,240
I know that's kind of.

612
00:31:51,240 --> 00:31:52,850
So what's a better explanation?

613
00:31:52,850 --> 00:31:58,230
The learning rate defines How fast we
reach convergence, to be technical.

614
00:31:58,230 --> 00:32:00,350
Convergence is when our
model is at its optimal fit,

615
00:32:00,350 --> 00:32:04,420
when we have that optimal fit
where the error is minimized.

616
00:32:04,420 --> 00:32:07,390
And the learning rate defines
how fast we get to convergence.

617
00:32:07,390 --> 00:32:11,398
We can't just say, can't just say,
make the learning rate a million and

618
00:32:11,398 --> 00:32:13,390
just get there as fast as possible.

619
00:32:13,390 --> 00:32:16,740
Because if you go too high,
then your model's not going to converge.

620
00:32:16,740 --> 00:32:18,290
But if you go too low,
it's going to be too slow.

621
00:32:18,290 --> 00:32:20,750
So remember, like all things,
it is a tradeoff.

622
00:32:20,750 --> 00:32:22,230
That is the word of the day.

623
00:32:22,230 --> 00:32:25,414
Machine learning is
a series of tradeoffs.

624
00:32:25,414 --> 00:32:29,016
And epochs have tradeoffs as well.

625
00:32:29,016 --> 00:32:29,700
Okay, so
the next thing is the training epoch.

626
00:32:29,700 --> 00:32:32,640
We're going to train
this thing 2,000 times.

627
00:32:32,640 --> 00:32:33,073
And why 2,000?

628
00:32:33,073 --> 00:32:36,549
I don't know,
I mean we could try 10,000 in a second.

629
00:32:36,549 --> 00:32:38,540
I don't know, you know what I'm saying?

630
00:32:38,540 --> 00:32:39,140
And that's what we do.

631
00:32:39,140 --> 00:32:43,838
We train for a number of epochs,
we see our results, and then we say,

632
00:32:43,838 --> 00:32:46,844
well, is this prediction accurate or
not?

633
00:32:46,844 --> 00:32:48,696
And then, if not, then we'll change,

634
00:32:48,696 --> 00:32:51,720
we'll tune our hyper
parameter to this step again.

635
00:32:51,720 --> 00:32:54,790
Okay, so then,
the number of display steps.

636
00:32:54,790 --> 00:33:00,515
So how often do we want
to display the process

637
00:33:00,515 --> 00:33:05,480
of trading and the number of samples?

638
00:33:05,480 --> 00:33:09,406
Which is going to be the size
of number label which is 10.

639
00:33:09,406 --> 00:33:10,115
Okay?

640
00:33:10,115 --> 00:33:13,080
So those were our hyper parameters.

641
00:33:13,080 --> 00:33:16,720
And now guys, we are ready to
compile and make sure that runs,

642
00:33:16,720 --> 00:33:19,042
is to create our computation bracket.

643
00:33:19,042 --> 00:33:20,909
So, a bit on computation.

644
00:33:20,909 --> 00:33:25,090
Create our computation
graph/neural network, okay?

645
00:33:25,090 --> 00:33:28,151
So computation graph,
neural network, are the same thing.

646
00:33:28,151 --> 00:33:32,480
But before we get started, we do one
quick review, of what we've done so far.

647
00:33:32,480 --> 00:33:36,620
We imported our dependencies, then we
loaded our data which is CSV file.

648
00:33:36,620 --> 00:33:40,863
The data contained a bunch of features,
but we only wanted to use the area and

649
00:33:40,863 --> 00:33:42,980
the bathrooms as our features.

650
00:33:42,980 --> 00:33:44,590
We removed everything else.

651
00:33:44,590 --> 00:33:48,060
Using those two things we're going to
predict if a housing price is good or

652
00:33:48,060 --> 00:33:48,170
not.

653
00:33:48,170 --> 00:33:49,020
But guess what?

654
00:33:49,020 --> 00:33:51,146
There are no labels in our data,
so what did we do?

655
00:33:51,146 --> 00:33:55,553
We added labels, because data frame
objects are really easy to parse,

656
00:33:55,553 --> 00:33:59,161
add data from, remove data from,
and so we added labels.

657
00:33:59,161 --> 00:34:03,190
Y1 is, when Y1 has a value 1,
it means it's a good buy.

658
00:34:03,190 --> 00:34:12,130
When Y2 has good value that
is when it's not a good buy.

659
00:34:12,130 --> 00:34:13,620
Then we prepare our data for
tensor flow.

660
00:34:13,620 --> 00:34:14,780
How do we do that?

661
00:34:14,780 --> 00:34:16,170
We look at our data print object and

662
00:34:16,170 --> 00:34:19,170
we converted our features
into an input tensor.

663
00:34:19,170 --> 00:34:22,289
And then we convert our labels
into input tensor as well.

664
00:34:22,289 --> 00:34:26,192
We printed them out,
we defined our hyper parameters, and

665
00:34:26,192 --> 00:34:31,750
now we're going to do, we're going to
write out our computation graph, okay?

666
00:34:31,750 --> 00:34:35,817
So, let's go ahead and
write out our computation graph.

667
00:34:35,817 --> 00:34:37,320
So let's go ahead and do this.

668
00:34:37,320 --> 00:34:39,800
So the first thing I'm going to do is,
I'm going to create a placeholder for

669
00:34:39,800 --> 00:34:43,989
our feature input tensor that
we just defined, input x.

670
00:34:43,989 --> 00:34:49,047
And what this is doing is, so,

671
00:34:49,047 --> 00:34:54,310
for our feature input tensors.

672
00:34:54,310 --> 00:34:57,821
[BLANK_AUDIO]

673
00:34:57,821 --> 00:35:01,440
So the tensor flow will feed
it an array of examples.

674
00:35:01,440 --> 00:35:04,007
Each example will be an array
of two flow values, area and

675
00:35:04,007 --> 00:35:05,610
number of bathrooms.

676
00:35:05,610 --> 00:35:09,835
And so
none means any number of examples,

677
00:35:09,835 --> 00:35:13,224
which is usually the backsides.

678
00:35:13,224 --> 00:35:16,350
So we're just going to say none,
so it's just generic.

679
00:35:16,350 --> 00:35:18,550
We're going to say however many we want.

680
00:35:18,550 --> 00:35:21,350
We allocated that number beforehand.

681
00:35:21,350 --> 00:35:24,655
And this allows us to have
any number of examples.

682
00:35:24,655 --> 00:35:31,480
And 32 because that's the size
of this input tensor.

683
00:35:31,480 --> 00:35:33,620
Okay, so, I'm sorry, not 32, 2.

684
00:35:33,620 --> 00:35:36,335
My mistake.

685
00:35:36,335 --> 00:35:39,200
So, two because we have two features,
okay?

686
00:35:39,200 --> 00:35:43,018
And it's a 2D matrix, right,
because we have two features.

687
00:35:43,018 --> 00:35:48,060
So we've done that, and so the next
step is to now create our weights.

688
00:35:48,060 --> 00:35:50,720
So create weight.

689
00:35:50,720 --> 00:35:53,780
And so another thing,
placeholder objects are gateways.

690
00:35:53,780 --> 00:35:57,150
So placeholders in tensor
flow are gateways for data.

691
00:35:57,150 --> 00:35:59,820
It's how we feed data into
our computation graph.

692
00:35:59,820 --> 00:36:03,212
Our gateway for
data into our computation graph.

693
00:36:03,212 --> 00:36:05,201
All right, so
the next step is create weights.

694
00:36:05,201 --> 00:36:07,140
So how are we going to
create our weights?

695
00:36:07,140 --> 00:36:10,568
So we're going to define
our weights as W.

696
00:36:10,568 --> 00:36:16,170
And we're going to say tf.variable, and
so it means- let me just write this out.

697
00:36:16,170 --> 00:36:18,380
So we'll define our weights, and

698
00:36:18,380 --> 00:36:22,730
it's going to start off as a set
of zeroes, because we haven't, and

699
00:36:22,730 --> 00:36:26,550
that's how you should start off your
weights for a simple example like this.

700
00:36:27,700 --> 00:36:31,516
Although it's transfer learning,
your weights aren't zero.

701
00:36:31,516 --> 00:36:33,630
They've been trained beforehand.

702
00:36:33,630 --> 00:36:37,670
So this is a 2x2 float matrix,
2x2 float matrix.

703
00:36:39,380 --> 00:36:41,611
And we're going to keep updating that.

704
00:36:41,611 --> 00:36:49,190
That will keep updating
through the training process.

705
00:36:49,190 --> 00:36:52,612
That's what we're going to do.

706
00:36:52,612 --> 00:36:53,750
Okay, why do we use a variable?

707
00:36:53,750 --> 00:37:00,735
Well in tensor flow a variable is,
variables hold and update parameters.

708
00:37:00,735 --> 00:37:06,534
So variables in tf hold and
update parameters.

709
00:37:06,534 --> 00:37:13,706
And we have weights or
any other things we want to.

710
00:37:13,706 --> 00:37:18,199
So they're basically in memory
buffers containing tensors,

711
00:37:18,199 --> 00:37:21,514
okay, to be a little
more technical about it.

712
00:37:21,514 --> 00:37:24,540
But these are our weights.

713
00:37:24,540 --> 00:37:27,210
Okay, so
the next step is to add our biases.

714
00:37:27,210 --> 00:37:29,930
So we're going to add our biases.

715
00:37:29,930 --> 00:37:31,780
So, add biases.

716
00:37:31,780 --> 00:37:33,483
And so, we want two biases, right?

717
00:37:33,483 --> 00:37:37,510
Because we have two inputs.

718
00:37:37,510 --> 00:37:39,388
We want two biases because
we have two inputs.

719
00:37:39,388 --> 00:37:44,220
And this is also going
to be a tf variable.

720
00:37:46,320 --> 00:37:48,850
And this is a 2x2 float matrix.

721
00:37:48,850 --> 00:37:50,400
Okay, I already typed that out.

722
00:37:50,400 --> 00:37:55,042
And, we're going to close the bracket
here and add a closing parentheses.

723
00:37:55,042 --> 00:37:58,640
Okay, so let's see what this,
actually we're going to do one.

724
00:37:58,640 --> 00:38:00,220
So let me show you guys what we have so
far.

725
00:38:00,220 --> 00:38:03,983
So now what we've done is we've
created our input tensor.

726
00:38:03,983 --> 00:38:07,160
We have, actually,
it uses the little square thing.

727
00:38:07,160 --> 00:38:08,590
And we've created our weights.

728
00:38:08,590 --> 00:38:09,740
And now we're going to add our biases.

729
00:38:09,740 --> 00:38:11,070
This is what we're about to do.

730
00:38:11,070 --> 00:38:15,657
So we're going to add these other
purple square looking dots.

731
00:38:15,657 --> 00:38:15,983
So yes,

732
00:38:15,983 --> 00:38:18,710
we're going to use back propagation
to update our weights in a second.

733
00:38:18,710 --> 00:38:20,890
But first we have to define our biases.

734
00:38:20,890 --> 00:38:22,360
And why do we use biases?

735
00:38:22,360 --> 00:38:27,380
Well, biases are going to
help our model fit better.

736
00:38:27,380 --> 00:38:34,494
So a good example is,
b in the y = mx + b equation.

737
00:38:34,494 --> 00:38:36,888
B is the bias, like that.

738
00:38:36,888 --> 00:38:39,080
So, right?

739
00:38:39,080 --> 00:38:40,640
So, because b takes that line, and

740
00:38:40,640 --> 00:38:44,340
it adds the y-intercept,
which makes it fit better.

741
00:38:44,340 --> 00:38:45,325
So that's what biases do.

742
00:38:45,325 --> 00:38:52,550
They're a part of a larger goal.

743
00:38:52,550 --> 00:38:55,158
It's like if you didn't
have b in the y = mx +b,

744
00:38:55,158 --> 00:38:58,865
the line wouldn't be the red line
that you're looking for, right?

745
00:38:58,865 --> 00:39:06,420
So okay, and I'll go over everything
we've done at the end guys.

746
00:39:06,420 --> 00:39:08,420
So right now we've added our biases.

747
00:39:08,420 --> 00:39:12,020
And so the next step is to calculate.

748
00:39:12,020 --> 00:39:13,453
We're going to perform
some matrix tasks.

749
00:39:13,453 --> 00:39:18,250
So now we're going to multiply
our weights by our inputs.

750
00:39:18,250 --> 00:39:22,170
So this is our first calculation
that's happening here, right?

751
00:39:22,170 --> 00:39:24,580
Because weights are what we multiply,

752
00:39:24,580 --> 00:39:28,360
weights are how we govern how
data flows in our compute.

753
00:39:28,360 --> 00:39:33,630
Weights are how we govern how data
flows in our computation graph, okay?

754
00:39:33,630 --> 00:39:35,490
Okay, so let's go ahead and
perform this.

755
00:39:35,490 --> 00:39:37,670
We'll say,
we'll define this as y values.

756
00:39:39,520 --> 00:39:45,290
And we'll say, and yes, we can
initialize weights as random as well.

757
00:39:46,440 --> 00:39:50,980
And depending on what you're doing,
it can be different,

758
00:39:50,980 --> 00:39:53,200
but right now we're
initializing them as zeroes.

759
00:39:53,200 --> 00:39:56,058
And to perform this actual
matrix multiplication step,

760
00:39:56,058 --> 00:39:59,810
the tensor flow has this great built-in
matrix multiplication function.

761
00:39:59,810 --> 00:40:03,314
[BLANK_AUDIO]

762
00:40:03,314 --> 00:40:06,989
That we can say, okay, so take our
[INAUDIBLE] and then our weights, and

763
00:40:06,989 --> 00:40:09,840
then our biases and
what are we going to do here?

764
00:40:09,840 --> 00:40:14,150
We're going to calculate the prediction
and to do that, we're going to multiply

765
00:40:14,150 --> 00:40:16,720
the input matrix by the weight
matrix and then add the biases.

766
00:40:16,720 --> 00:40:17,410
So let me write that out.

767
00:40:18,610 --> 00:40:25,542
Multiply inputs by weights and
add biases, okay?

768
00:40:25,542 --> 00:40:28,689
So, it's kind of like y = mx + b.

769
00:40:28,689 --> 00:40:31,840
So similar, kind of operations.

770
00:40:31,840 --> 00:40:36,732
Okay, so we've gone back, and now we're
going to do our softmax function,

771
00:40:36,732 --> 00:40:38,175
which is our sigmoid.

772
00:40:38,175 --> 00:40:39,502
Softmax is another word for sigmoid.

773
00:40:39,502 --> 00:40:41,100
So we've done this part, right?

774
00:40:41,100 --> 00:40:44,080
And now we're going to do this part,
which is we're going to take those

775
00:40:44,080 --> 00:40:46,400
values, and
we're going to tie a softmax to it.

776
00:40:46,400 --> 00:40:48,080
And what does softmax do?

777
00:40:48,080 --> 00:40:48,819
Does anyone remember?

778
00:40:48,819 --> 00:40:50,738
Shout it out in the comments and
I'm going to explain it in a second.

779
00:40:50,738 --> 00:40:52,363
I'm going to type it out, and

780
00:40:52,363 --> 00:40:55,430
If someone can shout that out
I will give you a shout out.

781
00:40:55,430 --> 00:40:56,870
Tell me what the softmax function does?

782
00:40:56,870 --> 00:41:00,015
We're going to say, softmax,

783
00:41:00,015 --> 00:41:05,265
apply softmax to this value
that we created, right?

784
00:41:05,265 --> 00:41:07,403
We going to apply our
softmax to the value input.

785
00:41:07,403 --> 00:41:08,382
Apply,

786
00:41:08,382 --> 00:41:12,171
[BLANK_AUDIO]

787
00:41:12,171 --> 00:41:15,480
Softmax to value we just created, okay?

788
00:41:17,750 --> 00:41:19,560
It's an activation function.

789
00:41:19,560 --> 00:41:22,060
So it's is our activation function.

790
00:41:23,910 --> 00:41:27,905
And it does some things, shoot.

791
00:41:27,905 --> 00:41:29,835
Okay, so let me go ahead and,

792
00:41:29,835 --> 00:41:33,154
[BLANK_AUDIO]

793
00:41:33,154 --> 00:41:34,933
Yes, it normalizes our values.

794
00:41:34,933 --> 00:41:40,859
So what softmax does is
it normalizes our value.

795
00:41:40,859 --> 00:41:41,970
And what do I mean by that?

796
00:41:41,970 --> 00:41:44,500
It takes our value and
it converts to a probability,

797
00:41:44,500 --> 00:41:46,160
that we can then feed to our output.

798
00:41:46,160 --> 00:41:48,500
So it's the last step before
we feed to our output.

799
00:41:48,500 --> 00:41:52,630
So let me make sure that I have
typed all this ish out perfectly.

800
00:41:52,630 --> 00:41:56,111
So on line 14, tf.Variable.zeros.

801
00:41:56,111 --> 00:42:00,228
I said tf.Variable.zeros.

802
00:42:00,228 --> 00:42:05,035
I need a bracket here, okay.

803
00:42:05,035 --> 00:42:10,170
tf.zeros, be laser focused
on what I've done here.

804
00:42:10,170 --> 00:42:10,852
Sp,

805
00:42:10,852 --> 00:42:13,839
[BLANK_AUDIO]

806
00:42:13,839 --> 00:42:15,299
Let's see.

807
00:42:15,299 --> 00:42:16,740
[BLANK_AUDIO]

808
00:42:16,740 --> 00:42:22,380
Interesting, so
there is an extra value here.

809
00:42:22,380 --> 00:42:25,260
[BLANK_AUDIO]

810
00:42:25,260 --> 00:42:26,798
Let's see what's going down here?

811
00:42:26,798 --> 00:42:30,444
[BLANK_AUDIO]

812
00:42:30,444 --> 00:42:34,224
tf.variable, tf.zeros.

813
00:42:34,224 --> 00:42:35,865
Extra dot, okay.

814
00:42:35,865 --> 00:42:38,693
[BLANK_AUDIO]

815
00:42:38,693 --> 00:42:41,933
Okay, so now the next step is to say-

816
00:42:41,933 --> 00:42:43,549
[BLANK_AUDIO]

817
00:42:43,549 --> 00:42:44,531
It's good, you know what it is.

818
00:42:44,531 --> 00:42:45,160
It's all good.

819
00:42:45,160 --> 00:42:49,610
Okay, so I need to add a bracket there,
I think.

820
00:42:49,610 --> 00:42:53,194
Right, yeah, so adding one there,
and once you add one here, and

821
00:42:53,194 --> 00:42:55,052
then we're going to say compile.

822
00:42:55,052 --> 00:42:57,129
[BLANK_AUDIO]

823
00:42:57,129 --> 00:43:00,840
And so then for the Y values,
it's telling me what I've done wrong.

824
00:43:00,840 --> 00:43:03,930
So it's saying, hey,
Serraj, listen, buddy.

825
00:43:03,930 --> 00:43:09,993
Listen, you need to apply your matrix

826
00:43:09,993 --> 00:43:15,302
multiply, x w v tf.add, okay.

827
00:43:15,302 --> 00:43:16,970
[BLANK_AUDIO]

828
00:43:16,970 --> 00:43:17,862
What else we got here?

829
00:43:17,862 --> 00:43:19,064
y values,

830
00:43:19,064 --> 00:43:21,945
[BLANK_AUDIO]

831
00:43:21,945 --> 00:43:26,632
And let's see, maybe x is not defined.

832
00:43:26,632 --> 00:43:29,494
x is not defined?

833
00:43:29,494 --> 00:43:31,585
Because I didn't define it, yes.

834
00:43:31,585 --> 00:43:34,449
So x is that value, okay?

835
00:43:34,449 --> 00:43:37,005
Great, great.

836
00:43:37,005 --> 00:43:42,443
And I hit Enter, so I need to
go ahead and add one more thing.

837
00:43:42,443 --> 00:43:43,820
Okay, so what did we just do?

838
00:43:43,820 --> 00:43:46,770
We added- the code is
going to go online, yes,

839
00:43:46,770 --> 00:43:49,140
I'm going to post it in
the comments when I'm done.

840
00:43:49,140 --> 00:43:50,361
We've got 15 minutes.

841
00:43:50,361 --> 00:43:54,403
Okay, so what we're going to
do is we're going to say,

842
00:43:54,403 --> 00:43:59,500
we've got that softmax value and
then we can get to our outputs.

843
00:43:59,500 --> 00:44:00,310
So let me type that out.

844
00:44:02,240 --> 00:44:03,660
So what are we doing here?

845
00:44:04,780 --> 00:44:08,285
We're going to feed, so
we never did this part, so

846
00:44:08,285 --> 00:44:11,313
we're going to say feed
in a matrix of labels.

847
00:44:11,313 --> 00:44:13,833
So let me go over what
I've just done here.

848
00:44:13,833 --> 00:44:19,805
Okay so,
these are our two placeholders right?

849
00:44:19,805 --> 00:44:22,717
So x is up here and
y is down here, right?

850
00:44:22,717 --> 00:44:27,796
So, one is our labels, and
then one is our set of features.

851
00:44:27,796 --> 00:44:30,315
We created our weights,
we created our biases, and

852
00:44:30,315 --> 00:44:32,760
this is our computation
stuff right here.

853
00:44:32,760 --> 00:44:38,690
We multiplied our weights by our inputs,
and then we added our biases, and then

854
00:44:38,690 --> 00:44:43,700
we used that value that we calculated
and we input it into an softmax player.

855
00:44:43,700 --> 00:44:46,590
Which converted it into
a set of probabilities.

856
00:44:46,590 --> 00:44:49,712
Okay, so there's that.

857
00:44:49,712 --> 00:44:52,401
And now we're going to apply,

858
00:44:52,401 --> 00:44:57,890
we're going to perform our training set,
perform training.

859
00:44:57,890 --> 00:44:59,190
So we've done these steps.

860
00:44:59,190 --> 00:45:01,760
And now step six is to perform training.

861
00:45:01,760 --> 00:45:04,140
Step six is to perform training.

862
00:45:04,140 --> 00:45:09,710
So let's go ahead and, so
this is the magic part flow.

863
00:45:09,710 --> 00:45:14,350
So if you were here in the live section
last week, then we did this manually.

864
00:45:14,350 --> 00:45:16,898
So let me just write out
what we're about to do.

865
00:45:16,898 --> 00:45:20,488
What we're going to do is
create our cost function,

866
00:45:20,488 --> 00:45:25,480
and the cost function we're going to
be using is the mean squared error.

867
00:45:26,482 --> 00:45:28,640
All right, so
we did this last lab section.

868
00:45:29,930 --> 00:45:33,300
One of the great things about
quartz is that we have continuity.

869
00:45:33,300 --> 00:45:39,485
And so we can say, we can say minus y.

870
00:45:39,485 --> 00:45:41,044
So this is going to be the equation.

871
00:45:41,044 --> 00:45:44,530
Now I'm going to show
the equation in a second,

872
00:45:44,530 --> 00:45:49,315
but what this does is it says,
we want to calculate the error.

873
00:45:49,315 --> 00:45:54,864
So between our two y values, and we're
going to divide by the number of- So

874
00:45:54,864 --> 00:46:00,764
the mean squared error is going to take
the average of the difference in values,

875
00:46:00,764 --> 00:46:04,374
which is our error,
the square difference, and

876
00:46:04,374 --> 00:46:06,510
that's what this is doing.

877
00:46:08,720 --> 00:46:11,167
So reduce sum basically
computes the element,

878
00:46:11,167 --> 00:46:13,196
the sum of elements across the matrix.

879
00:46:13,196 --> 00:46:14,286
So let me write that out.

880
00:46:14,286 --> 00:46:21,170
Reduce sum, computes the sum of elements

881
00:46:21,170 --> 00:46:26,485
across dimensions of a tensor.

882
00:46:26,485 --> 00:46:30,345
And so then, let me show the equation
for the ten seconds, but we missed it.

883
00:46:31,835 --> 00:46:35,893
And so the two different
values we're using are the,

884
00:46:35,893 --> 00:46:40,555
[BLANK_AUDIO]

885
00:46:40,555 --> 00:46:44,874
The two different values we are using
are the predictive values and

886
00:46:44,874 --> 00:46:48,130
then the actual values for the outputs.

887
00:46:48,130 --> 00:46:52,020
That is our cost function, we want to
minimize this cost function over time

888
00:46:52,020 --> 00:46:58,050
and to do that we're going
to perform gradiant descent.

889
00:46:58,050 --> 00:47:01,328
So, we're not, and
the great thing about textual flow,

890
00:47:01,328 --> 00:47:05,300
we can just define gradient descent
as our optimization function.

891
00:47:05,300 --> 00:47:07,651
We don't have to actually
write out every step.

892
00:47:07,651 --> 00:47:13,005
And in gradient descent every
step means we're computing

893
00:47:13,005 --> 00:47:18,905
the partial derivative,
with respect to our input variables,

894
00:47:18,905 --> 00:47:24,494
which in our case would be the set
of weights and the y biases.

895
00:47:24,494 --> 00:47:25,379
So, that's it.

896
00:47:25,379 --> 00:47:27,050
We can just say,
we have a cost function.

897
00:47:27,050 --> 00:47:29,000
We want to minimize that
cost using gradient descent,

898
00:47:29,000 --> 00:47:32,990
and our learning rate will define
how fast we want to do that, okay.

899
00:47:36,690 --> 00:47:41,110
So, do not call me
during a live session.

900
00:47:41,110 --> 00:47:44,655
Okay, so let's see what we've got here.

901
00:47:44,655 --> 00:47:51,138
We're going to say y is here, right?

902
00:47:51,138 --> 00:47:56,620
And this should be y underscore,
so that's why that didn't work.

903
00:47:56,620 --> 00:48:01,250
And so the next thing is you want to
make sure that Name wide is not defined.

904
00:48:01,250 --> 00:48:02,549
How about now?

905
00:48:02,549 --> 00:48:05,391
No, it is defined,
because I just updated it.

906
00:48:05,391 --> 00:48:06,161
There we go.

907
00:48:06,161 --> 00:48:08,170
Great.

908
00:48:08,170 --> 00:48:13,430
I updated the white and
now we're going to create our section.

909
00:48:13,430 --> 00:48:14,840
Let's initialize our section.

910
00:48:14,840 --> 00:48:15,980
This is still part of the training.

911
00:48:15,980 --> 00:48:19,340
We're going to initialize our
variables and tensorflow section.

912
00:48:19,340 --> 00:48:23,496
So in ternsorflow we encapsulate
our computation graph.

913
00:48:23,496 --> 00:48:26,333
[BLANK_AUDIO]

914
00:48:26,333 --> 00:48:31,141
We should re-encapsulate
our competition graph using

915
00:48:31,141 --> 00:48:36,458
initialize_all_variables, using
a session object.

916
00:48:36,458 --> 00:48:39,230
So in tensorflow, whenever you
want to do any kind of training,

917
00:48:39,230 --> 00:48:41,770
you have to first initialize
all the variables.

918
00:48:41,770 --> 00:48:45,640
And what initialize_all_variables does,
it just it does exactly what it says,

919
00:48:45,640 --> 00:48:47,780
initializes every variable
you've declared beforehand.

920
00:48:47,780 --> 00:48:52,370
That means those "tf dot" variables
we declared beforehand and

921
00:48:52,370 --> 00:48:54,400
the placeholder objects
we did beforehand.

922
00:48:54,400 --> 00:48:58,020
So every tensorflow,
generic variable that we

923
00:48:58,020 --> 00:49:01,410
defined beforehand is initialized for
this section.

924
00:49:01,410 --> 00:49:04,780
So this could be done under
the hood as well, but for

925
00:49:04,780 --> 00:49:09,260
now we are going to have to define.

926
00:49:09,260 --> 00:49:14,140
So we created our extension our
session and we just rerun it by

927
00:49:14,140 --> 00:49:17,070
using the other variables that
we've initialized as our input.

928
00:49:17,070 --> 00:49:23,630
Okay, and of course there's a so
no attributions of our variable.

929
00:49:24,840 --> 00:49:26,270
So what was that?

930
00:49:26,270 --> 00:49:30,592
So initialize all variables with a set

931
00:49:30,592 --> 00:49:35,330
of parentheses, and then tf.Session.

932
00:49:35,330 --> 00:49:39,624
Let's see what else we've got here,
tf.initialize.

933
00:49:39,624 --> 00:49:43,792
I misspelled initialize, okay.

934
00:49:43,792 --> 00:49:46,910
Initialize, great.

935
00:49:46,910 --> 00:49:48,310
Okay, so here we go.

936
00:49:48,310 --> 00:49:48,780
Let's do this.

937
00:49:48,780 --> 00:49:51,160
This is the last bit
that we're going to do.

938
00:49:51,160 --> 00:49:53,110
This is the actual training loop.

939
00:49:53,110 --> 00:49:54,280
This is the training loop.

940
00:49:54,280 --> 00:49:56,580
Okay, so let's go ahead and do this.

941
00:49:56,580 --> 00:50:00,510
For every number, every epoch that
we have, for training epochs,

942
00:50:00,510 --> 00:50:03,788
how many people do we have
in this live session?

943
00:50:03,788 --> 00:50:07,170
300 people going, awesome.

944
00:50:07,170 --> 00:50:12,470
So for every training epoch,

945
00:50:13,960 --> 00:50:16,135
we're going to find this 300,
what was the number?

946
00:50:16,135 --> 00:50:16,712
2,000?

947
00:50:16,712 --> 00:50:17,580
We define as 2,000.

948
00:50:17,580 --> 00:50:23,061
We're going to run our session given our
optimizer which is gradient descent.

949
00:50:23,061 --> 00:50:25,650
So this is where we're
performing gradient descent.

950
00:50:25,650 --> 00:50:27,320
And we're going to feed in.

951
00:50:27,320 --> 00:50:30,820
This is were we actually feed into
the placeholders where we find earlier.

952
00:50:30,820 --> 00:50:36,880
The first is going to be that x value
which is going to be our features.

953
00:50:36,880 --> 00:50:37,550
R for x.

954
00:50:37,550 --> 00:50:39,640
And the next is going to be for
thelabels,

955
00:50:39,640 --> 00:50:45,000
which is going to be our r equal y.

956
00:50:46,930 --> 00:50:49,830
So does that, and so that's it,
and so now that's it really, and

957
00:50:49,830 --> 00:50:53,150
we can just write out
our debugging method.

958
00:50:53,150 --> 00:51:00,061
Which is going to be write
out logs of training.

959
00:51:02,260 --> 00:51:09,369
If I display_step = 0, and
then let's say, let's run this session,

960
00:51:09,369 --> 00:51:14,225
given our cost function and
then we're going to.

961
00:51:14,225 --> 00:51:17,987
And we actually already just did this,

962
00:51:17,987 --> 00:51:22,889
but we're doing this so
we can print it out, just so

963
00:51:22,889 --> 00:51:26,995
we can see what's
happening at each step.

964
00:51:26,995 --> 00:51:32,378
Okay, so our input is going
to be that first tensor.

965
00:51:32,378 --> 00:51:40,090
And our other input is going to be
that other tensor that we defined.

966
00:51:40,090 --> 00:51:44,923
So then training stat.

967
00:51:46,750 --> 00:51:52,900
It's going to be, now.

968
00:51:52,900 --> 00:51:58,030
So, this part, I'm just going to,
it's this part right here.

969
00:51:58,030 --> 00:51:58,880
Because we don't actually.

970
00:52:01,690 --> 00:52:02,560
It's just logs.

971
00:52:02,560 --> 00:52:03,360
That's it really.

972
00:52:03,360 --> 00:52:05,830
So what we're doing is,
we're printing out the training stat.

973
00:52:05,830 --> 00:52:07,592
And let me just go ahead and
run this now.

974
00:52:07,592 --> 00:52:11,212
[BLANK_AUDIO]

975
00:52:11,212 --> 00:52:17,675
Okay, so
our first syntax error is going to be x.

976
00:52:17,675 --> 00:52:18,458
It doesn't need that.

977
00:52:18,458 --> 00:52:20,736
[BLANK_AUDIO]

978
00:52:20,736 --> 00:52:24,229
I do, what else we got here?

979
00:52:24,229 --> 00:52:27,108
X, input x, y,

980
00:52:27,108 --> 00:52:32,190
[BLANK_AUDIO]

981
00:52:32,190 --> 00:52:37,967
feed_dict equals session.run feed_dict

982
00:52:37,967 --> 00:52:43,050
equals, yo, okay, yes, okay.

983
00:52:43,050 --> 00:52:44,267
Okay, awesome!

984
00:52:44,267 --> 00:52:45,130
So, boom!

985
00:52:45,130 --> 00:52:51,000
That's how fast it trains,
because it was only ten values.

986
00:52:51,000 --> 00:52:52,530
Okay, it was only ten values.

987
00:52:52,530 --> 00:52:55,010
And so what is happening here,
let's look at this.

988
00:52:55,010 --> 00:52:58,810
The first column is our set
of training steps, right?

989
00:53:00,570 --> 00:53:02,970
And the next one is our cost function,
and

990
00:53:02,970 --> 00:53:05,820
the cost function is
minimized over time.

991
00:53:05,820 --> 00:53:08,570
Okay, and
then eventually it's finished training.

992
00:53:08,570 --> 00:53:13,860
So what is, so it ends up with a cost
function of .109 whatever, right?

993
00:53:13,860 --> 00:53:14,790
So is this good or bad?

994
00:53:14,790 --> 00:53:18,320
I don't really know, but it's better
than the first cost value, that's for

995
00:53:18,320 --> 00:53:18,810
sure.

996
00:53:18,810 --> 00:53:22,140
We don't know if it's a good cost
function or not because we kind of

997
00:53:22,140 --> 00:53:28,200
arbitrarily defined what
those labels were ourselves.

998
00:53:28,200 --> 00:53:29,750
So let's go ahead and test this out.

999
00:53:29,750 --> 00:53:30,693
And how are we going to test it?

1000
00:53:30,693 --> 00:53:37,848
[SOUND] I have a cough, slash sore
throat and I'm also really sick.

1001
00:53:37,848 --> 00:53:40,533
Which is like,
I don't have time for that crap,

1002
00:53:40,533 --> 00:53:42,760
I'm teaching this dope ass court.

1003
00:53:42,760 --> 00:53:48,640
So we're going to feed in our input.

1004
00:53:48,640 --> 00:53:49,430
And we're going to test it now.

1005
00:53:49,430 --> 00:53:50,790
We're going to see what happens now.

1006
00:53:51,880 --> 00:53:52,900
So this is our output.

1007
00:53:52,900 --> 00:53:54,120
So what is it here?

1008
00:53:54,120 --> 00:53:55,410
So this is the prediction, right?

1009
00:53:55,410 --> 00:53:58,000
So we train our model and
now we're going to do our prediction.

1010
00:53:58,000 --> 00:54:01,440
And so it's guessing that
they're all good houses,

1011
00:54:01,440 --> 00:54:04,130
because these values are all above.

1012
00:54:04,130 --> 00:54:08,060
So remember I said we have y1 and y0?

1013
00:54:08,060 --> 00:54:11,574
So it's saying,
let me go back up for a second, so

1014
00:54:11,574 --> 00:54:14,863
you guys can see what
I'm talking about here.

1015
00:54:14,863 --> 00:54:17,561
So we defined, remember this y1 and y2?

1016
00:54:17,561 --> 00:54:21,241
That's what it's outputting as,
just the y1 and the y2.

1017
00:54:21,241 --> 00:54:26,758
It's saying these values rounded
are closest to one on the left side.

1018
00:54:26,758 --> 00:54:27,840
So that means it's all one.

1019
00:54:27,840 --> 00:54:30,190
And the values on the right
are all closest to zero.

1020
00:54:30,190 --> 00:54:30,780
Which means it's zero.

1021
00:54:30,780 --> 00:54:37,530
So what it's saying is it's saying,
all houses are a good buy.

1022
00:54:37,530 --> 00:54:38,710
But that's not the case.

1023
00:54:38,710 --> 00:54:40,870
So it's got seven out of ten correct.

1024
00:54:40,870 --> 00:54:42,552
But the actual thing is,

1025
00:54:42,552 --> 00:54:46,572
they weren't all good just
some of them were good, right?

1026
00:54:46,572 --> 00:54:49,660
So only three,
only some of them were good.

1027
00:54:49,660 --> 00:54:53,200
So seven out of ten
correct which is not bad.

1028
00:54:53,200 --> 00:54:54,670
How to improve?

1029
00:54:54,670 --> 00:54:56,330
Maybe we can add a hidden layer.

1030
00:54:56,330 --> 00:54:58,710
Add a hidden layer.

1031
00:54:58,710 --> 00:55:00,430
That's what I would do I
would add a hidden layer and

1032
00:55:00,430 --> 00:55:01,640
then I would try it again.

1033
00:55:01,640 --> 00:55:06,260
So that's my example for this, okay?

1034
00:55:06,260 --> 00:55:09,300
So let's go back to screen sharing,
okay?

1035
00:55:11,960 --> 00:55:13,027
Stop screen sharing.

1036
00:55:13,027 --> 00:55:13,614
Go back to me.

1037
00:55:13,614 --> 00:55:17,105
Hi, okay so that's what we did for that.

1038
00:55:17,105 --> 00:55:21,800
I'm going to add the code
to the GitHub link.

1039
00:55:21,800 --> 00:55:24,220
Five minute ending to an A,
then we're good to go.

1040
00:55:24,220 --> 00:55:26,280
Then I have a video for
you guys coming out for

1041
00:55:26,280 --> 00:55:28,940
you Friday which I'm
super excited about.

1042
00:55:28,940 --> 00:55:31,150
Hit me up with your questions, let's go.

1043
00:55:31,150 --> 00:55:31,520
Anything.

1044
00:55:34,680 --> 00:55:36,620
Why are you using Jupiter,
any special perks?

1045
00:55:37,920 --> 00:55:43,650
Jupiter because it allows you to compile
and see what you're doing in real time.

1046
00:55:43,650 --> 00:55:45,830
It's great for visually looking at,
especially for

1047
00:55:45,830 --> 00:55:49,840
data, it's great for
visually looking at what you're doing.

1048
00:55:49,840 --> 00:55:51,000
Exactly what Jake said.

1049
00:55:51,000 --> 00:55:53,140
How do you choose the number
of hidden layers or

1050
00:55:53,140 --> 00:55:57,680
not, of unites while trying
to develop a neural network?

1051
00:55:57,680 --> 00:55:59,720
It depends on how big your data is.

1052
00:55:59,720 --> 00:56:02,860
The more data you have,
generally the more hidden layers.

1053
00:56:02,860 --> 00:56:11,710
The more data you have the better it
is to add more hidden layers, Right?

1054
00:56:11,710 --> 00:56:13,550
Pieforce vs tensorflow for newcomer?

1055
00:56:13,550 --> 00:56:18,500
Absolutely tensorflow don't even trip,
but Pieforce has some great

1056
00:56:18,500 --> 00:56:22,710
architectural ideas that I think we can,
we'll see where that goes.

1057
00:56:22,710 --> 00:56:24,310
It's a really exciting space, guys.

1058
00:56:24,310 --> 00:56:28,810
It's only January, and we're already
seeing so much innovation in this field.

1059
00:56:28,810 --> 00:56:33,330
In fact, this pace of innovation is
accelerating itself, it's insane.

1060
00:56:33,330 --> 00:56:35,610
How many layers should we add?

1061
00:56:35,610 --> 00:56:38,670
For this case, I just mentioned that.

1062
00:56:38,670 --> 00:56:41,970
Can you say something about Google's
deep mind machine learning?

1063
00:56:44,120 --> 00:56:48,220
Well I mean they haven't
done anything this year yet.

1064
00:56:48,220 --> 00:56:51,680
Give them about two weeks I
expect to see some state of

1065
00:56:51,680 --> 00:56:54,950
the art something in
paper in like two weeks.

1066
00:56:54,950 --> 00:56:56,850
How to classify an image data set?

1067
00:56:56,850 --> 00:57:00,030
We are going to do that in
two episodes for this course.

1068
00:57:00,030 --> 00:57:01,050
High level.

1069
00:57:01,050 --> 00:57:05,640
A label data set a supervise
approach with labels,

1070
00:57:05,640 --> 00:57:07,190
I would look at Google's inception.

1071
00:57:07,190 --> 00:57:08,430
You don't even have
to train it yourself,

1072
00:57:08,430 --> 00:57:10,830
they've already trained
it on a million images.

1073
00:57:10,830 --> 00:57:13,120
And you can use transfer
learning to apply that, and

1074
00:57:13,120 --> 00:57:16,430
then add in whatever other
images you want to classify.

1075
00:57:16,430 --> 00:57:18,250
And because of transfer learning,

1076
00:57:18,250 --> 00:57:21,440
you don't need a backdated dataset to
train it, and I have video on that

1077
00:57:21,440 --> 00:57:24,460
called, build a TensorFlow Image
Classifier in five minutes.

1078
00:57:24,460 --> 00:57:25,580
How much coffee did you have?

1079
00:57:25,580 --> 00:57:26,340
I had a cup.

1080
00:57:28,290 --> 00:57:31,361
It is the same one as the one we
were doing with audacity, yes.

1081
00:57:31,361 --> 00:57:35,706
Anybody who is ever current on that
plan yes it's coming out in three days.

1082
00:57:35,706 --> 00:57:41,120
Do you think that will prepare me for
a deep learning job?

1083
00:57:41,120 --> 00:57:42,250
Yes.

1084
00:57:42,250 --> 00:57:45,080
What event, if you do the assignments.

1085
00:57:45,080 --> 00:57:47,350
Okay, that doesn't, copy that.

1086
00:57:47,350 --> 00:57:48,610
You have to do the assignment.

1087
00:57:48,610 --> 00:57:49,870
You can't just watch the video.

1088
00:57:49,870 --> 00:57:51,740
You have to do it.

1089
00:57:51,740 --> 00:57:51,860
Do it.

1090
00:57:51,860 --> 00:57:53,490
Like Shilo LeBuff.

1091
00:57:53,490 --> 00:57:53,780
Do it.

1092
00:57:53,780 --> 00:57:56,510
I'm a huge Shilo LeBuff fan,
by the way guys.

1093
00:57:56,510 --> 00:57:57,280
Recently.

1094
00:57:57,280 --> 00:58:00,170
Because he does art for the sake of art.

1095
00:58:00,170 --> 00:58:00,940
Did you age?

1096
00:58:00,940 --> 00:58:01,170
I'm 25.

1097
00:58:01,170 --> 00:58:02,810
Did you study at university?

1098
00:58:02,810 --> 00:58:03,460
Yes.

1099
00:58:03,460 --> 00:58:03,730
What?

1100
00:58:03,730 --> 00:58:04,870
Computer Science.

1101
00:58:04,870 --> 00:58:06,010
Specifically robotics.

1102
00:58:07,090 --> 00:58:08,700
Anything on memory networks.

1103
00:58:08,700 --> 00:58:13,040
Yo, I want to do memory networks,
that's some advanced shit,

1104
00:58:13,040 --> 00:58:14,560
that's like generative
adversarial network,

1105
00:58:14,560 --> 00:58:17,900
shit that I find super interesting
at the bleeding edge of the field.

1106
00:58:17,900 --> 00:58:20,130
But we've gotta get our basics down.

1107
00:58:20,130 --> 00:58:24,064
If you should take something out
of this [INAUDIBLE] session,

1108
00:58:24,064 --> 00:58:27,640
what you should take out is,
are three things,

1109
00:58:27,640 --> 00:58:32,630
One, how to do gradient descent and
that propagation.

1110
00:58:32,630 --> 00:58:40,200
Two, a basic, an idea of what the main
variables are with tension flow.

1111
00:58:40,200 --> 00:58:45,000
That means place holders, tf variables,
the optimizer function, weights and

1112
00:58:45,000 --> 00:58:46,580
biases.

1113
00:58:46,580 --> 00:58:47,940
because everything's
going to build off of that.

1114
00:58:47,940 --> 00:58:51,480
If you get those simple things then
everything else is going to be much,

1115
00:58:51,480 --> 00:58:52,010
much easier.

1116
00:58:53,090 --> 00:58:59,580
The first thing is a softmax function.

1117
00:58:59,580 --> 00:59:01,090
Sorry, activation function.

1118
00:59:01,090 --> 00:59:04,130
An activation function is what we use
to convert numbers to probabilities.

1119
00:59:04,130 --> 00:59:05,980
There are several types
of activation functions.

1120
00:59:05,980 --> 00:59:07,880
In this case we use softmax.

1121
00:59:07,880 --> 00:59:08,480
Which university?

1122
00:59:08,480 --> 00:59:09,650
Columbia University.

1123
00:59:09,650 --> 00:59:14,340
But guys, I want to end this
with one piece of advice, and

1124
00:59:14,340 --> 00:59:17,990
then, we are good to go.

1125
00:59:17,990 --> 00:59:19,740
A lot of what we learn,

1126
00:59:19,740 --> 00:59:24,700
is, what we tell our selves we are
capable of doing, that what degrees are.

1127
00:59:24,700 --> 00:59:28,400
Degrees tells us,
they allow us to tell ourselves, hey,

1128
00:59:28,400 --> 00:59:30,340
I now know this, I can do it.

1129
00:59:30,340 --> 00:59:32,430
Well we have to inflict
that way of thinking.

1130
00:59:32,430 --> 00:59:36,010
It's not about, I have a degree,
now I'm a computer scientist.

1131
00:59:36,010 --> 00:59:37,970
I have a degree, now I can do X.

1132
00:59:37,970 --> 00:59:42,510
No, if you study something and
you do it, if you create the code and

1133
00:59:42,510 --> 00:59:46,620
it compiles and it runs and
gets you help with what you wanted,

1134
00:59:46,620 --> 00:59:48,370
you would now have
the ability to do that.

1135
00:59:48,370 --> 00:59:52,450
So don't be restricted by the dogma of,
I need to read this or

1136
00:59:52,450 --> 00:59:54,470
I need to this to get to this.

1137
00:59:54,470 --> 00:59:56,960
Think of something you want to build,
start building, and

1138
00:59:56,960 --> 00:59:59,980
along the way learn what
you need to to build it.

1139
00:59:59,980 --> 01:00:01,530
By the end you are going
to get really good.

1140
01:00:01,530 --> 01:00:05,600
Because if you've ever engineered
anything before then you know that

1141
01:00:05,600 --> 01:00:08,980
if anybody asks you a problem about it,
you will know every answer, why?

1142
01:00:08,980 --> 01:00:13,900
Because you had to deep dive into
it to be able to build that, okay.

1143
01:00:13,900 --> 01:00:14,950
So that's it.

1144
01:00:14,950 --> 01:00:18,290
All right, so
believe in yourself, basically.

1145
01:00:18,290 --> 01:00:20,550
Believe in yourselves,
the world needs you guys, okay?

1146
01:00:20,550 --> 01:00:23,750
We need you,
we are starting a revolution okay,

1147
01:00:23,750 --> 01:00:28,360
we are in the midst of a revolution
unlike anything the world has ever

1148
01:00:28,360 --> 01:00:32,000
seen before and we're going to
do some awesome shit, okay?

1149
01:00:32,000 --> 01:00:33,500
We're going to do some awesome shit,
okay.

1150
01:00:34,500 --> 01:00:35,640
That's it.

1151
01:00:35,640 --> 01:00:37,020
Thanks guys for watching.

1152
01:00:37,020 --> 01:00:38,340
The video is coming out on Friday.

1153
01:00:38,340 --> 01:00:40,780
It's going to be dope.

1154
01:00:40,780 --> 01:00:42,940
Continue the conversation
on site channel.

1155
01:00:42,940 --> 01:00:48,705
For now I'm going to do
the math tutorial in a second.

1156
01:00:48,705 --> 01:00:49,793
Okay.

1157
01:00:49,793 --> 01:00:57,610
For now I've gotta work
with Udacity on something,

1158
01:00:58,630 --> 01:01:02,710
I want to go down to Udacity and just
hang with them and see what we can do.

1159
01:01:02,710 --> 01:01:03,900
There's some really cool people.

1160
01:01:03,900 --> 01:01:07,721
For now, that's what I got to do,
so thanks for watching.

1161
01:01:07,721 --> 01:01:08,529
Love you guys.

1162
01:01:08,529 --> 01:01:15,380
[BLANK_AUDIO]