1
00:00:00,070 --> 00:00:03,050
So how can we change the MDP
reward function without changing

2
00:00:03,050 --> 00:00:04,030
the optimal policy?

3
00:00:04,030 --> 00:00:08,430
So in an MDP, we usually define
it with the states, the actions,

4
00:00:08,430 --> 00:00:12,615
the rewards, the transitions,
and our friend gamma.

5
00:00:12,615 --> 00:00:15,535
>> Mm-hm.
[LAUGH] What we're going to mess

6
00:00:15,535 --> 00:00:18,345
with here is the R, but we're going
to leave everything else the same.

7
00:00:18,345 --> 00:00:20,715
So what are some ways that we might be
able to change a reward function that

8
00:00:20,715 --> 00:00:23,015
actually wouldn't change
what the agent would want to

9
00:00:23,015 --> 00:00:24,735
do to maximize the reward function.

10
00:00:24,735 --> 00:00:27,465
>> Well there's a couple of I think,
obvious ones.

11
00:00:27,465 --> 00:00:29,665
So one is if you just take
the reward function and

12
00:00:29,665 --> 00:00:33,080
multiply it by any positive constant.

13
00:00:33,080 --> 00:00:34,240
Everything should be the same.

14
00:00:34,240 --> 00:00:36,146
Right?
>> Yes, exactly so.

15
00:00:36,146 --> 00:00:39,240
So in some sense the rewards
are kind of unit less, so

16
00:00:39,240 --> 00:00:42,870
if you multiply them by 1,000,
it still encourages the same behavior.

17
00:00:42,870 --> 00:00:45,931
It just seems like you're making
more reward points, but they're not,

18
00:00:45,931 --> 00:00:50,415
it doesn't change the relative
strength of different behaviors.

19
00:00:50,415 --> 00:00:52,385
So, good, good.
That's definitely one.

20
00:00:52,385 --> 00:00:53,923
>> And I want to say this is true.

21
00:00:53,923 --> 00:00:56,607
Just like by multiplying by a positive
constant doesn't make a difference,

22
00:00:56,607 --> 00:00:59,602
it seems to me that if you just
shifted everything over by one or

23
00:00:59,602 --> 00:01:01,642
by two or by three,

24
00:01:01,642 --> 00:01:04,581
it would make a difference of adding
a constant to the reward function.

25
00:01:04,581 --> 00:01:07,002
Adding a constant as opposed
to multiplying a constant.

26
00:01:07,002 --> 00:01:07,802
>> Good, okay, good.

27
00:01:07,802 --> 00:01:09,052
We'll talk about that one as well.

28
00:01:09,052 --> 00:01:12,102
And there's a third one that we're
going to talk about, that may not be so

29
00:01:12,102 --> 00:01:12,612
obvious.

30
00:01:12,612 --> 00:01:14,362
But do you have any other guesses?

31
00:01:14,362 --> 00:01:17,192
>> This makes me think it
desperately looks like what we did

32
00:01:17,192 --> 00:01:19,602
in the other machine learning
class with clustering.

33
00:01:20,910 --> 00:01:24,880
There has to be non-linear ways of
messing with the reward function to get

34
00:01:24,880 --> 00:01:25,810
you the same answer.

35
00:01:25,810 --> 00:01:28,240
But I can't think of a compact
way of saying what I'm thinking.

36
00:01:28,240 --> 00:01:29,700
>> Okay, so that's, that's fair.

37
00:01:29,700 --> 00:01:31,110
And we'll get into that in more detail.

38
00:01:31,110 --> 00:01:34,744
Ultimately, we're going to call it
potential based modifications, or

39
00:01:34,744 --> 00:01:36,142
potential based rewards.

40
00:01:37,400 --> 00:01:39,730
And I'll define in a bit, but
first I want to go through the,

41
00:01:39,730 --> 00:01:44,530
as you pointed out, the simpler cases
and develop a kind of methodology for

42
00:01:44,530 --> 00:01:48,230
proving that we're not going
to change the optimal policy.

43
00:01:48,230 --> 00:01:48,730
>> Okay.
