1
00:00:00,380 --> 00:00:02,880
All right.
So, the last topic I'd like to cover in

2
00:00:02,880 --> 00:00:08,950
palm DP land is a line of work
that that I am somewhat fond of,

3
00:00:08,950 --> 00:00:11,180
that is called predictive
state representation.

4
00:00:11,180 --> 00:00:15,140
So let me let me say a little bit about
the motivation behind studying this, and

5
00:00:15,140 --> 00:00:17,080
we'll talk a little bit about
the representation itself.

6
00:00:17,080 --> 00:00:19,700
But we're not going to talk much
about algorithms that actually make

7
00:00:19,700 --> 00:00:20,272
use of this.

8
00:00:20,272 --> 00:00:22,630
There is a literature
if you want to dive in.

9
00:00:22,630 --> 00:00:23,260
All right.
So,

10
00:00:23,260 --> 00:00:26,410
here's the basic premise
of this line of work.

11
00:00:26,410 --> 00:00:27,902
So, we talk about POMDPs.

12
00:00:27,902 --> 00:00:31,560
So, POMDPs, when we're moving around in
a POMDP we're tracking the belief state,

13
00:00:31,560 --> 00:00:33,320
which is a distribution over states,
right?

14
00:00:33,320 --> 00:00:34,500
>> Right.
So, here's the thing.

15
00:00:34,500 --> 00:00:36,450
The states are not observable, right?

16
00:00:36,450 --> 00:00:37,320
>> Right.
So,

17
00:00:37,320 --> 00:00:40,110
how can we ever thought
hope to learn in a PomDP,

18
00:00:40,110 --> 00:00:42,830
when we never actually get to
see these underlying states?

19
00:00:42,830 --> 00:00:44,290
They're never observed.

20
00:00:44,290 --> 00:00:46,758
So, you could ask yourself
do they even exist, or

21
00:00:46,758 --> 00:00:49,440
are they just kind of useful
fictions of our imagination?

22
00:00:49,440 --> 00:00:51,740
There's nothing that we
can ever do to test hey,

23
00:00:51,740 --> 00:00:54,110
was this the state we were actually in?

24
00:00:54,110 --> 00:00:55,750
The answer is I don't know.

25
00:00:55,750 --> 00:00:58,300
I mean, if we're learning in a POMDP,
we're just creating this notion of

26
00:00:58,300 --> 00:01:01,470
states to provide some structure to
the observations that we've seen.

27
00:01:01,470 --> 00:01:03,070
>> Is this like a tree
in the forest thing?

28
00:01:03,070 --> 00:01:05,750
>> Yeah, yeah.
If a POMDP is solved in a forest,

29
00:01:05,750 --> 00:01:07,510
does anybody know whether or
not they exist?

30
00:01:07,510 --> 00:01:08,940
And the answer is me.

31
00:01:08,940 --> 00:01:10,370
>> No, I think you're right.

32
00:01:10,370 --> 00:01:12,775
I mean, we're making this
stuff up anyway, right?

33
00:01:12,775 --> 00:01:13,650
>> Yeah.
And in the case that

34
00:01:13,650 --> 00:01:16,820
somebody gives us a POMDP and
says would you please plan in this,

35
00:01:16,820 --> 00:01:19,310
it's a reasonable
representation to work with.

36
00:01:19,310 --> 00:01:21,450
But if you're in a reinforcement
learning setting,

37
00:01:21,450 --> 00:01:23,220
you never get to see
these underlying states.

38
00:01:23,220 --> 00:01:25,540
You can never actually collect
statistics about them.

39
00:01:25,540 --> 00:01:29,530
In fact, most of the good work
that I've seen that uses POMDPs,

40
00:01:29,530 --> 00:01:30,430
they don't learn the POMDP.

41
00:01:30,430 --> 00:01:34,980
They actually send, I want to say
grad students out into the field

42
00:01:34,980 --> 00:01:39,110
to measure underlying states, and
then measure the observation function.

43
00:01:39,110 --> 00:01:42,180
They actually kind of instrument
the world to build a POMDP,

44
00:01:42,180 --> 00:01:45,270
and then they can act in
the POMDP successfully later.

45
00:01:45,270 --> 00:01:47,910
But it's just there's really no
good way to learn about the states.

46
00:01:47,910 --> 00:01:49,070
>> Or even that they're real even.

47
00:01:49,070 --> 00:01:49,600
>> Eve that they're real.

48
00:01:50,730 --> 00:01:53,682
>> Okay.
So, now that you've blown my mind,

49
00:01:53,682 --> 00:01:55,465
what does it have to do with?

50
00:01:55,465 --> 00:01:56,180
>> Right.
So, so,

51
00:01:56,180 --> 00:01:57,940
the idea of a predictive
state representation,

52
00:01:57,940 --> 00:02:02,250
or P.S.R., is that the thing that we're
going to use for representation isn't

53
00:02:02,250 --> 00:02:06,690
going to be probability distributions
over these possibly fictional states.

54
00:02:06,690 --> 00:02:07,400
But instead,

55
00:02:07,400 --> 00:02:11,670
they're going to be probabilities of
the outcomes of future predictions.

56
00:02:11,670 --> 00:02:16,620
So, I'm in a state where it's about 80%
chance it's going to rain tomorrow.

57
00:02:16,620 --> 00:02:17,190
>> Okay.

58
00:02:17,190 --> 00:02:19,400
Which doesn't require an actual state.

59
00:02:19,400 --> 00:02:21,220
>> Well, and
that is maybe not a great example.

60
00:02:21,220 --> 00:02:24,720
So, let me give a more concrete example
to kind of ground this idea out.
