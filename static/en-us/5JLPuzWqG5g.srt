1
00:00:00,460 --> 00:00:03,016
You may ask, what if the protection domain is

2
00:00:03,016 --> 00:00:06,510
so large that it needs all of the hardware address

3
00:00:06,510 --> 00:00:09,080
space? Maybe the file system code base is so

4
00:00:09,080 --> 00:00:12,580
big that it needs the entire hardware address space. Cannot

5
00:00:12,580 --> 00:00:16,020
share it with anybody else. Similarly, the code base

6
00:00:16,020 --> 00:00:18,560
for the storage module is so big that it may

7
00:00:18,560 --> 00:00:21,740
occupy the entire hardware address space. Now, if this were

8
00:00:21,740 --> 00:00:25,530
the situation, that is, if the protection domains are so

9
00:00:25,530 --> 00:00:28,880
large that they pretty much occupy the entire hardware

10
00:00:28,880 --> 00:00:33,260
address space, and if the TLB does not support

11
00:00:33,260 --> 00:00:35,830
address space tagging, then you have to do a

12
00:00:35,830 --> 00:00:38,500
TLB flush on a context switch. If you go

13
00:00:38,500 --> 00:00:42,200
from this service to this service, you need to

14
00:00:42,200 --> 00:00:45,360
do a TLB flush because the memory footprint of

15
00:00:45,360 --> 00:00:48,240
each of these service is as big as the

16
00:00:48,240 --> 00:00:51,360
hardware address space that's available on the processor. Or in

17
00:00:51,360 --> 00:00:55,020
other words, the segments overlap and therefore you have

18
00:00:55,020 --> 00:00:58,390
to do a TLB flush when you do a

19
00:00:58,390 --> 00:01:01,500
context switch. The cost that we've been talking about

20
00:01:01,500 --> 00:01:04,390
so far is the explicit cost, that is, the

21
00:01:04,390 --> 00:01:08,800
cost that is incurred for flushing the TLB at

22
00:01:08,800 --> 00:01:11,540
the point of a context switch. And for small

23
00:01:11,540 --> 00:01:14,620
protection domains that we want to go across, we

24
00:01:14,620 --> 00:01:16,960
want to avoid that cost and we can do that

25
00:01:16,960 --> 00:01:19,620
by packing them in the linear address space

26
00:01:19,620 --> 00:01:22,770
provided by the hardware. But if the memory

27
00:01:22,770 --> 00:01:24,770
footprint of the service is so big that

28
00:01:24,770 --> 00:01:28,860
it occupies the hardware address space of the

29
00:01:28,860 --> 00:01:33,380
architecture completely, the explicit cost we're going to incur,

30
00:01:33,380 --> 00:01:35,840
because we have to do a TLB flush,

31
00:01:35,840 --> 00:01:38,380
if the TLB is not address space tagged.

32
00:01:38,380 --> 00:01:42,260
But that explicit cost is very insignificant compared

33
00:01:42,260 --> 00:01:46,540
to the implicit cost that is going to be incurred. Now what do we mean by

34
00:01:46,540 --> 00:01:50,540
implicit cost? We mean cache effects, that is,

35
00:01:50,540 --> 00:01:54,510
the loss of locality going from this service

36
00:01:54,510 --> 00:01:56,580
to this service, that the cache is not

37
00:01:56,580 --> 00:02:00,840
going to have the working set of this service,

38
00:02:00,840 --> 00:02:07,066
when we go from here to here. That impact is much more that the explicit cost.

39
00:02:07,066 --> 00:02:10,720
For example, LeKay shows that on the

40
00:02:10,720 --> 00:02:14,460
specific architecture in which they implemented L3, which

41
00:02:14,460 --> 00:02:16,750
is a Pentium architecture. It had 32

42
00:02:16,750 --> 00:02:20,880
entries for kernel translations and 64 entries for

43
00:02:20,880 --> 00:02:26,430
user translations. Even if you want to flush the entire TLB, all the entries in

44
00:02:26,430 --> 00:02:32,450
the TLB, it takes 864 cycles to do that. But, the loss of locality, when a

45
00:02:32,450 --> 00:02:36,510
service goes from this to this, in terms of cache

46
00:02:36,510 --> 00:02:39,690
effects is going to be much more significant because, with a

47
00:02:39,690 --> 00:02:44,060
large address space you expect that you're doing more work

48
00:02:44,060 --> 00:02:47,520
in the subsystem. And the implicit costs are going to dominate.
