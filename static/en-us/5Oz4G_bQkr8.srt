1
00:00:00,000 --> 00:00:04,000
In class we studied parsing using memoization,

2
00:00:04,000 --> 00:00:09,000
but I've heard in the real world many tools use techniques such as LALR(1).

3
00:00:09,000 --> 00:00:11,000
Why didn't we learn those?

4
00:00:11,000 --> 00:00:16,000
That is a good question, and actually there are a number of answers to it.

5
00:00:16,000 --> 00:00:18,000
I'll enumerate maybe five of them.

6
00:00:18,000 --> 00:00:25,000
One of the first is that this memoization or chart parsing technique that we learned

7
00:00:25,000 --> 00:00:30,000
is actually very similar to--almost isomorphic to-- some of the more advanced techniques

8
00:00:30,000 --> 00:00:32,000
used in practice.

9
00:00:32,000 --> 00:00:37,000
You can read up on or you may have heard of GLR--generalized LR parsing.

10
00:00:37,000 --> 00:00:41,000
That's commonly used in tools such as Oink--the real name for a software project,

11
00:00:41,000 --> 00:00:46,000
go check it out--which can be used to parse complicated languages like C++.

12
00:00:46,000 --> 00:00:53,000
It turns out that C++'s grammar doesn't really easily fit into these

13
00:00:53,000 --> 00:00:58,000
restrictive little cubbyholes that some of the faster tools prefer.

14
00:00:58,000 --> 00:01:01,000
It can really be a contortion that distorts meaning

15
00:01:01,000 --> 00:01:04,000
to try to get C++ to fit into one of these other tools.

16
00:01:04,000 --> 00:01:10,000
By teaching you a very general technique earl on that can handle any context-free grammar

17
00:01:10,000 --> 00:01:13,000
you know how parsing works even in these obscure corner cases,

18
00:01:13,000 --> 00:01:18,000
and I'm teaching you the approach that used in the more advanced tools.

19
00:01:18,000 --> 00:01:25,000
Another reason related to this is that all of these tools have the same interface.

20
00:01:25,000 --> 00:01:29,000
If you want to go try Oink out later on, it accepts this same sort of context-free grammar format

21
00:01:29,000 --> 00:01:31,000
that we're used to.

22
00:01:31,000 --> 00:01:40,000
So does Yacc. So does Bison. So does Ocamlyacc. So does Java CUP. So does RE Yacc, Ruby Yacc.

23
00:01:40,000 --> 00:01:42,000
I've taught you the way to phrase it,

24
00:01:42,000 --> 00:01:47,000
and then the particular tool under the hood can use whatever parsing algorithm it prefers.

25
00:01:47,000 --> 00:01:51,000
In the same way that we write regular expressions--ab+--

26
00:01:51,000 --> 00:01:54,000
and then depending on whether we're writing a PERL program or a Python program

27
00:01:54,000 --> 00:02:00,000
or some other program, it might convert that regular expression into a finite state machine

28
00:02:00,000 --> 00:02:03,000
slightly differently, but we know what the final output is going to be.

29
00:02:03,000 --> 00:02:05,000
We know what strings are going to be matched.

30
00:02:05,000 --> 00:02:10,000
Similarly here, when you write your grammar, regardless of what parser-generating algorithm

31
00:02:10,000 --> 00:02:14,000
is eventually used, we know what the behavior is going to be.

32
00:02:14,000 --> 00:02:17,000
That's what we've covered in this class. There's a related argument.

33
00:02:17,000 --> 00:02:22,000
You might say, well, why are we using Python in this class

34
00:02:22,000 --> 00:02:27,000
when the real world uses C or Java or C++

35
00:02:27,000 --> 00:02:31,000
or whichever language you think is commonly used in the real world--perhaps C#.

36
00:02:31,000 --> 00:02:33,000
That's the same argument here.

37
00:02:33,000 --> 00:02:37,000
Why are we using this chart parsing, when the real world uses these slightly different tools.

38
00:02:37,000 --> 00:02:41,000
This is a big question in education and pedagogy.

39
00:02:41,000 --> 00:02:44,000
I honestly believe as a practitioner and a researcher

40
00:02:44,000 --> 00:02:48,000
that there is a key difference between knowing how to program

41
00:02:48,000 --> 00:02:51,000
and knowing how to program in a particular language.

42
00:02:51,000 --> 00:02:53,000
The former is much more important.

43
00:02:53,000 --> 00:02:56,000
If you know how to program in Java, if you survived this class,

44
00:02:56,000 --> 00:03:03,000
you can pick up another language like Ruby, Java, Python, and do the same thing--

45
00:03:03,000 --> 00:03:08,000
knowing how to program, thinking about recursion, thinking about laying out data,

46
00:03:08,000 --> 00:03:10,000
thinking about algorithms.

47
00:03:10,000 --> 00:03:15,000
That's much more important than knowing the particular syntax for any language.

48
00:03:15,000 --> 00:03:20,000
Similarly here, I believe that really understanding context-free grammars and parsing

49
00:03:20,000 --> 00:03:25,000
is more important than knowing which particular parser generator tool implementation

50
00:03:25,000 --> 00:03:27,000
is being used under the hood.

51
00:03:27,000 --> 00:03:30,000
You know one particular algorithm for creating parsers,

52
00:03:30,000 --> 00:03:34,000
and that may as well be the one that's used in practice.

53
00:03:34,000 --> 00:03:39,000
Another reason that I focused on this algorithm is that it is simple.

54
00:03:39,000 --> 00:03:42,000
It relates grammars to parsing, which is one of the concepts in this class.

55
00:03:42,000 --> 00:03:45,000
We lay out context-free grammars, but now we need to be able to tell

56
00:03:45,000 --> 00:03:49,000
if a string or an utterance is in the language of that grammar and build up its parse tree.

57
00:03:49,000 --> 00:03:55,000
It's only a few lines long, and it allows us to introduce lovely concepts like memoization

58
00:03:55,000 --> 00:03:57,000
or list comprehensions.

59
00:03:57,000 --> 00:04:01,000
A number of you might wonder, if we were to write our parser in another way,

60
00:04:01,000 --> 00:04:06,000
maybe you've heard of techniques like recursive dissent or even using LALR(1) grammars.

61
00:04:06,000 --> 00:04:08,000
Would it be just as convenient?

62
00:04:08,000 --> 00:04:11,000
I have done them all, and the answer is no.

63
00:04:11,000 --> 00:04:17,000
By no means is a recursive dissent parser or an LALR(1) grammar

64
00:04:17,000 --> 00:04:20,000
for the language fragment that we're considering here,

65
00:04:20,000 --> 00:04:22,000
as easy as the parsing algorithm that I showed you.

66
00:04:22,000 --> 00:04:26,000
Now, this is in fact the most elegant approach.

67
00:04:26,000 --> 00:04:31,000
Then finally a number of students were concerned about the time taken by our parser,

68
00:04:31,000 --> 00:04:39,000
which is cubic or runs in time proportional to the size times the size times the size of the input program,

69
00:04:39,000 --> 00:04:41,000
in the worst case for an ambiguous grammar.

70
00:04:41,000 --> 00:04:44,000
Now, this is bit beyond the scope of this course,

71
00:04:44,000 --> 00:04:48,000
but it's worth pointing out that these other tools that claim to be faster,

72
00:04:48,000 --> 00:04:50,000
it's not actually a fair comparison.

73
00:04:50,000 --> 00:04:53,000
The parsing algorithm I taught you handles any grammar.

74
00:04:53,000 --> 00:04:56,000
The tools that claim to be faster don't.

75
00:04:56,000 --> 00:04:59,000
They only handle special restricted grammars.

76
00:04:59,000 --> 00:05:02,000
An interesting note that I didn't cover in this course,

77
00:05:02,000 --> 00:05:05,000
but that's nonetheless true, is that the particular parsing algorithm

78
00:05:05,000 --> 00:05:09,000
that we covered also runs in linear time,

79
00:05:09,000 --> 00:05:16,000
is very fast for the same small class of grammars--LRK grammars they're called--

80
00:05:16,000 --> 00:05:18,000
that are accepted by these faster tools.

81
00:05:18,000 --> 00:05:22,000
If you were the sort of person who would take all the work and time required

82
00:05:22,000 --> 00:05:27,000
to shoehorn your language's description into the particular straight jacket format

83
00:05:27,000 --> 00:05:29,000
favored by these other faster tools.

84
00:05:29,000 --> 00:05:34,000
The faster tools are actually just as slow or just as fast--your choice--

85
00:05:34,000 --> 00:05:38,000
as the algorithm I taught you in the limit. Their asymptotic complexity is the same.

86
00:05:38,000 --> 00:05:42,000
They're both linear time. You don't loose anything in terms of time either.

87
00:05:42,000 --> 00:05:46,000
Again, getting back to the first point, I think it's more of an historical accident

88
00:05:46,000 --> 00:05:53,000
that most of these tools happen to use LALR(1) or other hacks for speed in practice.

89
00:05:53,000 --> 00:05:57,000
In the last few years--the last 10 years, say--research has focused increasingly

90
00:05:57,000 --> 00:06:01,000
on GLR parsing or this sort of chart parsing that I've shown you

91
00:06:01,000 --> 00:06:06,000
as a way of allowing people to write the grammars that they want,

92
00:06:06,000 --> 00:06:10,000
get linear time, super fast performance most of the time,

93
00:06:10,000 --> 00:06:14,000
pay a little bit when things are ambiguous, and then get on with their lives.

94
00:06:14,000 --> 00:06:18,000
If foresee that languages like C++ or Java or whatnot--

95
00:06:18,000 --> 00:06:23,000
more complicated languages--will be analyzed using tools like that in the future.

96
00:06:23,000 --> 00:06:27,000
It's my honest believe as an educator that I'm teaching you the right approach,

97
00:06:27,000 --> 00:06:32,000
teaching something like LALR(1)--that information is really not necessary in the modern world,

98
00:06:32,000 --> 00:06:36,000
and it's log--those lectures feel like they go on forever.

99
00:06:36,000 --> 00:06:40,000
By contrast, this chart-based approach is elegant, simple,

100
00:06:40,000 --> 00:06:43,000
allows us to introduce fun concepts, handles all grammars,

101
00:06:43,000 --> 00:06:46,000
which the other approaches do not,

102
00:06:46,000 --> 00:06:53,000
fits in just a few slides, and is just as fast in the limit. What's not to like?
