1
00:00:00,530 --> 00:00:02,420
Here's another example.

2
00:00:02,420 --> 00:00:03,640
You have an image and

3
00:00:03,640 --> 00:00:07,170
you want your network to say
it's an image with a cat in it.

4
00:00:07,170 --> 00:00:10,480
it doesn't really matter where the cat
is, it's still an image with a cat.

5
00:00:11,850 --> 00:00:14,990
If your network has to learn about
kittens in the left corner and

6
00:00:14,990 --> 00:00:16,760
about kittens in the right corner,

7
00:00:16,760 --> 00:00:20,420
independently, that's a lot
of work that it has to do.

8
00:00:20,420 --> 00:00:24,700
How about you telling it instead,
explicitly, that objects and images

9
00:00:24,700 --> 00:00:28,388
are largely the same whether they're on
the left or on the right of the picture.

10
00:00:28,388 --> 00:00:31,450
That's what's called
translation invariance,.

11
00:00:31,450 --> 00:00:33,200
Different positions, same kitten.

12
00:00:34,480 --> 00:00:36,210
Yet another example.

13
00:00:36,210 --> 00:00:39,770
Imagine you have a long text
that talks about kittens.

14
00:00:39,770 --> 00:00:42,580
Does the meaning of kitten
change depending on whether

15
00:00:42,580 --> 00:00:45,730
it's in the first sentence or
in the second one?

16
00:00:45,730 --> 00:00:47,290
Mostly not.

17
00:00:47,290 --> 00:00:50,430
So if you're trying to network on text,
maybe you want the part of

18
00:00:50,430 --> 00:00:54,150
the network that learns what
a kitten is to be reused

19
00:00:54,150 --> 00:00:58,110
every time you see the word kitten,
and not have to relearn it every time.

20
00:00:59,620 --> 00:01:02,580
The way you achieve this in your own
networks is using what is called

21
00:01:02,580 --> 00:01:03,890
weight sharing.

22
00:01:03,890 --> 00:01:07,820
When you know that two inputs can
contain the same kind of information,

23
00:01:07,820 --> 00:01:12,650
then you share the weights and train
the weights jointly for those inputs.

24
00:01:12,650 --> 00:01:14,510
it is a very important idea.

25
00:01:14,510 --> 00:01:19,260
Statistical invariants, things that
don't change on average across time or

26
00:01:19,260 --> 00:01:20,830
space, are everywhere.

27
00:01:21,840 --> 00:01:23,030
For images,

28
00:01:23,030 --> 00:01:27,080
the idea of weight sharing will get
us to study convolutional networks.

29
00:01:27,080 --> 00:01:30,690
For text and sequences in general,
it will lead us to embeddings and

30
00:01:30,690 --> 00:01:31,940
recurrent neural networks.
