1
00:00:00,600 --> 00:00:02,788
This lesson is about Q-learning.

2
00:00:02,788 --> 00:00:08,460
Recall that Q-learning is a model-free
approach, meaning that it does not know

3
00:00:08,460 --> 00:00:14,580
about or use models of
the transitions T or the rewards R.

4
00:00:14,580 --> 00:00:18,630
Instead, Q-learning builds
a table of utility values

5
00:00:18,630 --> 00:00:21,250
as the agent interacts with the world.

6
00:00:21,250 --> 00:00:26,300
These Q-values can be used at each
step to select the best action

7
00:00:26,300 --> 00:00:29,200
based on what it has learned so far.

8
00:00:29,200 --> 00:00:31,930
The fantastic thing about Q-learning

9
00:00:31,930 --> 00:00:35,440
is that it is guaranteed to
provide an optimal policy.

10
00:00:35,440 --> 00:00:37,900
There is a hitch, however,
that we'll cover later.
