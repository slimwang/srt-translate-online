1
00:00:00,310 --> 00:00:02,570
Now that I've given some
kind of motivation for

2
00:00:02,570 --> 00:00:06,610
this notion of generalization, it might
be worth talking about what exactly

3
00:00:06,610 --> 00:00:10,670
we are trying to generalize between
different learning opportunities.

4
00:00:10,670 --> 00:00:13,790
So what are the functions that we're
actually making use of when we're doing

5
00:00:13,790 --> 00:00:14,740
reinforcement learning?

6
00:00:14,740 --> 00:00:15,540
>> What do you mean?

7
00:00:15,540 --> 00:00:18,430
>> When we're learning we're actually
generally learning some mapping right.

8
00:00:18,430 --> 00:00:20,280
So this is true in reinforcement
learning in general.

9
00:00:20,280 --> 00:00:22,800
There's some mapping
from input to output.

10
00:00:22,800 --> 00:00:23,480
>> I see what you mean.

11
00:00:23,480 --> 00:00:25,270
So, well, the whole goal,

12
00:00:25,270 --> 00:00:29,140
of course, is to learn a policy, which
is a mapping from states to action.

13
00:00:29,140 --> 00:00:30,320
>> Great.
Okay, that seems like a really good

14
00:00:30,320 --> 00:00:31,380
place to start.

15
00:00:31,380 --> 00:00:34,370
So policy maps states to actions.

16
00:00:34,370 --> 00:00:35,850
>> But that's usually hard.

17
00:00:35,850 --> 00:00:38,570
So we usually learn some
kind of intermediate thing,

18
00:00:38,570 --> 00:00:40,650
like a value function.

19
00:00:40,650 --> 00:00:42,950
>> Well, let's just stick with
policy here for a second.

20
00:00:42,950 --> 00:00:44,130
So in terms of the policy.

21
00:00:44,130 --> 00:00:47,720
Generalization would mean that to
the extent that similar states

22
00:00:47,720 --> 00:00:49,430
have similar actions.

23
00:00:49,430 --> 00:00:53,090
Then we could learn what the right
action is for some states and then

24
00:00:53,090 --> 00:00:56,580
guess what the right action is for other
states, using some kind of I don't know.

25
00:00:56,580 --> 00:00:58,730
Function approximation,
some kind of supervised learning.

26
00:00:58,730 --> 00:01:01,860
>> Right, so if you're in a state
where an anvil is about to hit you on

27
00:01:01,860 --> 00:01:04,500
the head,
you should probably jump out of the way.

28
00:01:04,500 --> 00:01:05,459
That's probably true for

29
00:01:05,459 --> 00:01:07,860
every state where an anvil's
about to hit you in the head.

30
00:01:07,860 --> 00:01:10,460
Even if you never seen the specific
details of that state before.

31
00:01:10,460 --> 00:01:11,850
>> Yeah,
I mean it could matter whether or

32
00:01:11,850 --> 00:01:13,760
not there also is a shark
tank right in front of you.

33
00:01:13,760 --> 00:01:15,630
So when you jump out of
the way of the anvil,

34
00:01:15,630 --> 00:01:17,060
you actually jump into the shark tank.

35
00:01:17,060 --> 00:01:18,340
But yeah, I think that's exactly right.

36
00:01:18,340 --> 00:01:21,110
That we want to be able to
say across lots and lots and

37
00:01:21,110 --> 00:01:24,690
lots of related states, we could
be using the same kind of action.

38
00:01:24,690 --> 00:01:26,820
But I think what you're pointing
out is that we don't know how to

39
00:01:26,820 --> 00:01:28,740
learn the policy directly.

40
00:01:28,740 --> 00:01:29,440
Anyway, when

41
00:01:29,440 --> 00:01:32,100
we're talking about these kinds of
reinforcement learning algorithms.

42
00:01:32,100 --> 00:01:35,440
So, we usually learn a kind
of a stand in for it.

43
00:01:35,440 --> 00:01:38,100
>> And
my favorite one is the value function,

44
00:01:38,100 --> 00:01:42,030
which is where you are now mapping
states to some number to r.

45
00:01:42,030 --> 00:01:43,750
>> Right.
So we can map, for example, states and

46
00:01:43,750 --> 00:01:45,700
actions to the estimated return.

47
00:01:45,700 --> 00:01:48,880
That's what the q function does and
that's another function that we could be

48
00:01:48,880 --> 00:01:52,230
trying to learn and this is another
function where generalization might be

49
00:01:52,230 --> 00:01:53,000
an option, right.

50
00:01:53,000 --> 00:01:57,780
So to the extent that similar state
action pairs have similar returns,

51
00:01:57,780 --> 00:02:00,410
again, because if an anvil's
about to hit you on the head.

52
00:02:00,410 --> 00:02:05,150
Probably that's a bad state,
you're in a bad situation across

53
00:02:05,150 --> 00:02:07,623
all the different kinds of states where
handles about to hit you in the head.

54
00:02:07,623 --> 00:02:08,470
>> There we go that you.

55
00:02:08,470 --> 00:02:12,570
You should draw a picture of me
standing off to the side to your left.

56
00:02:12,570 --> 00:02:14,480
I have the world's tallest neck.

57
00:02:14,480 --> 00:02:15,120
>> Yeah.
So

58
00:02:15,120 --> 00:02:17,230
say I'm in a state where enamels
about to hit me on the head,

59
00:02:17,230 --> 00:02:19,670
there's lots of other details that
are sort of not that important

60
00:02:19,670 --> 00:02:22,050
in predicting what the estimated
returns going to be.

61
00:02:22,050 --> 00:02:24,150
So you even look kind of concerned.

62
00:02:24,150 --> 00:02:25,220
>> That's supposed to be me?

63
00:02:25,220 --> 00:02:26,800
>> Yeah you're tall enough.

64
00:02:26,800 --> 00:02:27,480
>> Okay, fair enough.

65
00:02:27,480 --> 00:02:30,100
>> So this is another place where we
could be doing function approximation,

66
00:02:30,100 --> 00:02:32,780
is in the actual q function itself.

67
00:02:32,780 --> 00:02:36,020
And there's a third one that I think
is actually really important and

68
00:02:36,020 --> 00:02:38,730
interesting, and that is the model.

69
00:02:38,730 --> 00:02:43,010
To the extent that you can actually do
a good job of predicting next states,

70
00:02:43,010 --> 00:02:46,400
transitions for related states.

71
00:02:46,400 --> 00:02:48,400
Then you could actually be
doing function approximation or

72
00:02:48,400 --> 00:02:50,145
generalization in the model as well.

73
00:02:50,145 --> 00:02:50,696
>> Right.

74
00:02:50,696 --> 00:02:54,190
In fact, that's probably very,
very important often.

75
00:02:54,190 --> 00:02:55,530
>> Yes.
Not only is it important but

76
00:02:55,530 --> 00:02:59,890
it's actually often really natural place
to do it because when you're learning

77
00:02:59,890 --> 00:03:02,870
a model right so
what is it about learning a model?

78
00:03:02,870 --> 00:03:04,808
You're in some state,
you take some action, and

79
00:03:04,808 --> 00:03:06,570
you observe what net stage you're in.

80
00:03:06,570 --> 00:03:09,880
These are actually supervised
examples of transitions.

81
00:03:09,880 --> 00:03:11,880
So unlike learning the policy or

82
00:03:11,880 --> 00:03:14,890
learning the value function,
when you're trying to learn the model,

83
00:03:14,890 --> 00:03:18,000
you're actually getting
standard supervised examples.

84
00:03:18,000 --> 00:03:22,860
So that being said using function
approximation models it has been

85
00:03:22,860 --> 00:03:25,840
done in the field but
it's not that well understood.

86
00:03:25,840 --> 00:03:30,430
In particular, you need models that can
actually predict many many steps ahead

87
00:03:30,430 --> 00:03:33,890
to be able to be used effectively for
planning and decision making.

88
00:03:33,890 --> 00:03:38,470
Mostly what researchers have focused on
is function approximation in the value

89
00:03:38,470 --> 00:03:40,910
function, the generalization
of the value function.

90
00:03:40,910 --> 00:03:43,830
So that's what we're going to look
at in the rest of this lesson.

91
00:03:43,830 --> 00:03:44,730
>> Okay cool, look forward to it.
