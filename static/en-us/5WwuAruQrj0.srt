1
00:00:00,380 --> 00:00:03,390
Okay so Michael let's see if we can do something that

2
00:00:03,390 --> 00:00:06,520
might be faster and just as good as what we've been

3
00:00:06,520 --> 00:00:09,820
doing with value iterations. And what we're going to do is we're

4
00:00:09,820 --> 00:00:14,200
going to emphasize the fact that we care about policies, not values.

5
00:00:14,200 --> 00:00:16,540
Now it's true given the true utilities we can find a

6
00:00:16,540 --> 00:00:20,590
policy, but maybe we don't have to find the true utilities. In

7
00:00:20,590 --> 00:00:23,070
order to find the optimal policy. So, here's a little sketch

8
00:00:23,070 --> 00:00:25,500
of an algorithm okay, so it's going to look a lot like value

9
00:00:25,500 --> 00:00:28,290
to ratio. We are going to start with some initial

10
00:00:28,290 --> 00:00:31,330
policy, let's call it pi sub zero and that's just

11
00:00:31,330 --> 00:00:32,549
going to be a guess, so it's just going to be an

12
00:00:32,549 --> 00:00:35,470
arbitrary setting of actions that you should take in different

13
00:00:35,470 --> 00:00:40,540
states. Then we're going to evaluate how good that policy is, and

14
00:00:40,540 --> 00:00:45,660
the way we're going to it at time T. Is to calculate its utility,

15
00:00:45,660 --> 00:00:50,500
which I'm going to call U sub t. Which is equal to the utility you get by

16
00:00:50,500 --> 00:00:53,830
following that policy. Okay? And I, I'll show you in a moment

17
00:00:53,830 --> 00:00:56,390
exactly how you do that, alright? But I just want to make certain

18
00:00:56,390 --> 00:01:00,330
that you, that you, you kind of buy that maybe we can

19
00:01:00,330 --> 00:01:03,570
do that. So, We have, given a policy, we ought to be able

20
00:01:03,570 --> 00:01:06,030
to evaluate it by figuring out what the utility of that policy

21
00:01:06,030 --> 00:01:08,790
is. And, again, we'll talk about that in a second. And then, after

22
00:01:08,790 --> 00:01:11,690
we know what the utility of that policy is, we're actually going to

23
00:01:11,690 --> 00:01:15,630
improve that policy in a way similar to what we did with value

24
00:01:15,630 --> 00:01:19,330
iteration, we're going to update our policy, Time T plus one, to

25
00:01:19,330 --> 00:01:23,910
be the policy. That takes the actions that maximizes the expected

26
00:01:23,910 --> 00:01:27,920
utility based of what we just calculated for [INAUDIBLE] of T.

27
00:01:27,920 --> 00:01:30,370
Now notice it will allow us to change Pi over time

28
00:01:30,370 --> 00:01:34,020
because image that we discover that in some state that actually

29
00:01:34,020 --> 00:01:36,880
there is an action that is very good in that state.

30
00:01:36,880 --> 00:01:38,470
That gets you some place really nice gives you a really

31
00:01:38,470 --> 00:01:41,480
big reward and that went on and you do fairly well.

32
00:01:41,480 --> 00:01:44,800
Well then all other states that can reach that state. Might

33
00:01:44,800 --> 00:01:47,630
end up taking a different action than they did before, because

34
00:01:47,630 --> 00:01:50,420
now the best action would be to move towards that state.

35
00:01:50,420 --> 00:01:54,300
So these two steps will actually, or can actually lead to some

36
00:01:54,300 --> 00:01:58,150
improvement of the policy over time. But the key thing here

37
00:01:58,150 --> 00:02:00,470
is we have to figure out exactly how to compute u sub

38
00:02:00,470 --> 00:02:02,770
t. Well, the good news is we know how to do

39
00:02:02,770 --> 00:02:06,760
that, and it's actually pretty easy. And it boils down to our

40
00:02:06,760 --> 00:02:08,380
favorite equation, Doman's equation.

41
00:02:08,380 --> 00:02:09,520
>> [LAUGH]

42
00:02:09,520 --> 00:02:14,020
>> So our utility at time t, that is the utility that we

43
00:02:14,020 --> 00:02:18,060
get by following a policy at time t, is just well, the true reward

44
00:02:18,060 --> 00:02:19,220
that we're going to get by entering that

45
00:02:19,220 --> 00:02:24,330
state plus gamma times the expected utility,

46
00:02:25,360 --> 00:02:28,130
which now looks like this. There, does that make sense? Do you see that?

47
00:02:28,130 --> 00:02:29,830
>> Okay, hang on, it looks a little

48
00:02:29,830 --> 00:02:31,850
different from the other equation. So did you

49
00:02:31,850 --> 00:02:37,050
mean for it to have T UT is defined in terms of UT and not UT minus one?

50
00:02:37,050 --> 00:02:37,550
>> Yes.

51
00:02:37,550 --> 00:02:41,600
>> Okay, that's interesting. And the max is gone but

52
00:02:41,600 --> 00:02:45,410
instead of max, there's a policy. Stuck into the transition function.

53
00:02:45,410 --> 00:02:46,000
>> Yep.

54
00:02:46,000 --> 00:02:48,090
>> A choice of action is determined by the policy.

55
00:02:48,090 --> 00:02:52,280
>> Right. And that's actually the only difference between what we were doing

56
00:02:52,280 --> 00:02:56,860
before is that rather than having this max over actions, we already know what

57
00:02:56,860 --> 00:02:58,100
action we're going to take. It's determined

58
00:02:58,100 --> 00:02:59,600
by the policy we're currently following.

59
00:02:59,600 --> 00:03:01,380
>> Okay, but isn't this just as hard

60
00:03:01,380 --> 00:03:03,660
as solving? The thing with the max, you said?

61
00:03:03,660 --> 00:03:05,780
>> Well, what was the problem that we were solving before with the max?

62
00:03:05,780 --> 00:03:07,560
>> That was the Bellman equation.

63
00:03:07,560 --> 00:03:11,290
>> Yes, but we were solving a bunch of equations. How many of them?

64
00:03:11,290 --> 00:03:11,540
>> N.

65
00:03:11,540 --> 00:03:15,050
>> So we were solving n equations, and how many unknowns?

66
00:03:15,050 --> 00:03:15,380
>> N.

67
00:03:15,380 --> 00:03:18,260
>> What's the difference between this, n equations and

68
00:03:18,260 --> 00:03:20,870
n unknowns, and the other n equations and n unknowns?

69
00:03:20,870 --> 00:03:22,870
>> Well, n is the same.

70
00:03:22,870 --> 00:03:23,180
Mm hm.

71
00:03:23,180 --> 00:03:24,990
>> There's no max, though.

72
00:03:24,990 --> 00:03:28,500
>> There's no max. And what was it that made solving that hard before?

73
00:03:29,850 --> 00:03:31,950
>> It made it, the max made it non linear. The max is

74
00:03:31,950 --> 00:03:36,660
gone now. You're saying this is, this is a set of linear equations?

75
00:03:36,660 --> 00:03:40,490
>> Yeah. Because, well, there's, there's just a bunch of sums. And the

76
00:03:40,490 --> 00:03:44,250
pi is not like some weird function. This is just effectively a constant.

77
00:03:44,250 --> 00:03:45,640
>> I see.

78
00:03:45,640 --> 00:03:48,100
>> So now, I have n equations, and n unknowns.

79
00:03:48,100 --> 00:03:50,440
But it's in linear equations. And now that I

80
00:03:50,440 --> 00:03:53,310
have in linear equations and unknowns, I can actually compute

81
00:03:53,310 --> 00:03:55,600
this is a reasonable amount of time, by doing

82
00:03:55,600 --> 00:03:59,340
matrix inversions, and regression, and other magic hand wavey things.

83
00:03:59,340 --> 00:04:00,710
>> That's very slick.

84
00:04:00,710 --> 00:04:00,800
>> Yeah.

85
00:04:00,800 --> 00:04:03,180
>> It's seems, it's still more expensive than doing the

86
00:04:03,180 --> 00:04:07,960
valued [UNKNOWN], I guess. Yeah, but you don't have to, perhaps,

87
00:04:07,960 --> 00:04:10,840
do as many iterations as you were doing before. So

88
00:04:10,840 --> 00:04:13,360
once you've evaluated it, which we now know how to do,

89
00:04:13,360 --> 00:04:15,090
and you've improved it, you just keep

90
00:04:15,090 --> 00:04:16,829
doing that until your policy doesn't change.

91
00:04:16,829 --> 00:04:17,890
>> Very cool.

92
00:04:17,890 --> 00:04:21,480
>> Mmhm. And this does look a lot like value iteration to you, doesn't it?

93
00:04:21,480 --> 00:04:25,350
>> Yeah, though is seems like it's making bigger jumps somehow.

94
00:04:25,350 --> 00:04:26,970
>> It is, and that's because instead

95
00:04:26,970 --> 00:04:29,650
of making jumps. In value space, it's making

96
00:04:29,650 --> 00:04:31,830
jumps in policy space. Which is why

97
00:04:31,830 --> 00:04:34,290
we call this class of algorithms Policy Iteration.

98
00:04:34,290 --> 00:04:35,724
>> Cool.

99
00:04:35,724 --> 00:04:37,533
>> Right.

100
00:04:37,533 --> 00:04:43,350
Now, this inversion can still be fairly painful, it's, you

101
00:04:43,350 --> 00:04:46,010
know, if we don't worry about being highly efficient, you know,

102
00:04:46,010 --> 00:04:48,070
it's roughly n cubed, and if there are a lot of

103
00:04:48,070 --> 00:04:51,270
states, this can be kind of painful. But it turns out there's

104
00:04:51,270 --> 00:04:54,100
little tricks you can do, like do a little step

105
00:04:54,100 --> 00:04:58,020
evaluate iteration here for a while to get an estimate and

106
00:04:58,020 --> 00:05:00,550
then, you know, kind of cycle through. So there's all kinds of

107
00:05:00,550 --> 00:05:02,790
clever things you might want to do, but at a high level

108
00:05:02,790 --> 00:05:06,700
without worrying about, you know, the, the details of constants, this general

109
00:05:06,700 --> 00:05:10,620
process of moving through policy space. And taking advantage of the fact

110
00:05:10,620 --> 00:05:15,490
that by picking a specific policy you're able to turn your nonlinear

111
00:05:15,490 --> 00:05:18,470
equations into linear equations turns out to often to be very helpful.

112
00:05:18,470 --> 00:05:21,030
>> So, is it guaranteed to converge?

113
00:05:21,030 --> 00:05:21,810
>> Yes.

114
00:05:21,810 --> 00:05:22,760
>> Nice.

115
00:05:22,760 --> 00:05:25,770
>> Well there, that was easy. I'm, I'm not going

116
00:05:25,770 --> 00:05:27,910
to go into it but, you know, there's a finite number

117
00:05:27,910 --> 00:05:32,000
of policies. You're always getting better so eventually

118
00:05:32,000 --> 00:05:33,390
you have to converge. It's very similar to

119
00:05:33,390 --> 00:05:35,520
the, or at least intuitively it's very similar

120
00:05:35,520 --> 00:05:37,270
to the argument you might make for [UNKNOWN]. Cool.
