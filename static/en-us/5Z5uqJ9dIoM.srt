1
00:00:00,440 --> 00:00:05,430
If we want to build a linear classifier,
so we want to find some function

2
00:00:05,430 --> 00:00:09,330
to separate the positive and negative
values, and the idea is that we want

3
00:00:09,330 --> 00:00:14,600
a line x dot w plus b such that,
if the xi is positive, so if that value

4
00:00:14,600 --> 00:00:18,620
is greater than zero we're going to
call it a positive, so that's here.

5
00:00:18,620 --> 00:00:22,930
So if it's greater than zero we'll call
it positive, and if it's less than zero,

6
00:00:22,930 --> 00:00:24,700
we'll call it negative.

7
00:00:24,700 --> 00:00:26,910
But let's take a look at these data.

8
00:00:26,910 --> 00:00:30,900
So here's a line that separates
the blues from the pinks.

9
00:00:30,900 --> 00:00:32,270
That's a good line.

10
00:00:32,270 --> 00:00:32,780
Good line?

11
00:00:32,780 --> 00:00:34,010
Sure, it's a great line.

12
00:00:34,010 --> 00:00:38,140
But here's another line, all right,
and here's another line, and

13
00:00:38,140 --> 00:00:43,620
here's another line, and all of these
lines separate the blues from the pinks.

14
00:00:43,620 --> 00:00:47,570
So the question is, of course,
which line is best?

15
00:00:47,570 --> 00:00:48,330
All right?

16
00:00:48,330 --> 00:00:51,420
And support vector machines
are a particular take

17
00:00:51,420 --> 00:00:53,770
on how to decide which line is best.

18
00:00:53,770 --> 00:00:56,020
And then we'll go to hyper planes and
higher dimensions there.

19
00:00:56,020 --> 00:01:01,680
But it's basically having to do with
which classifying separator is best.

20
00:01:01,680 --> 00:01:06,960
So, Support Vector Machines, SVMs,
are really this idea of the optimal or

21
00:01:06,960 --> 00:01:10,850
the best possible separating line,
or in higher dimensions, plane.

22
00:01:10,850 --> 00:01:14,300
And the way we do that is,
so you see this line here?

23
00:01:14,300 --> 00:01:16,960
This line is drawn
down the middle there.

24
00:01:16,960 --> 00:01:18,830
Well, sort of down the middle, okay.

25
00:01:18,830 --> 00:01:21,530
Well, great, now it's a ribbon.

26
00:01:21,530 --> 00:01:22,560
Okay, fine.

27
00:01:22,560 --> 00:01:24,090
That line down the middle.

28
00:01:24,090 --> 00:01:26,440
And the dotted lines here, right,

29
00:01:26,440 --> 00:01:32,090
this is sort of the closest
distance of the, the points.

30
00:01:32,090 --> 00:01:36,830
So those two lines are drawn such
that this line is the closest blue,

31
00:01:36,830 --> 00:01:41,140
here's the closest red, and
this separator line is down the middle.

32
00:01:41,140 --> 00:01:43,370
The idea, the Support Vector Machines,

33
00:01:43,370 --> 00:01:47,710
is that we want to maximize this
distance between those dotted lines.

34
00:01:47,710 --> 00:01:50,310
And that distance,
that's called the margin.

35
00:01:50,310 --> 00:01:51,470
Okay, it's written in green here.

36
00:01:51,470 --> 00:01:56,270
That's the margin between the positive
and negative training examples.

37
00:01:56,270 --> 00:01:59,942
Right?
So, here, so it's from here to there,

38
00:01:59,942 --> 00:02:04,740
okay, and the idea is to try to find
the line that gives us the best margin.
