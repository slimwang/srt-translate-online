1
00:00:00,140 --> 00:00:02,120
We ended at the following iteration.

2
00:00:02,120 --> 00:00:06,320
We looked at how linear regression performed on our initial feature set and

3
00:00:06,320 --> 00:00:11,200
decided to reduce the feature set to a simpler selection, specifically the top

4
00:00:11,200 --> 00:00:15,000
three performing features, Hershey's, Kit Kat, and Airheads.

5
00:00:15,000 --> 00:00:17,620
Simpler models are usually more effective.

6
00:00:17,620 --> 00:00:19,160
Than complex models.

7
00:00:19,160 --> 00:00:22,970
In particular, reducing the number of features as much as possible to

8
00:00:22,970 --> 00:00:26,130
those that matter most is almost always beneficial.

9
00:00:26,130 --> 00:00:30,580
Moreover, lower numbers of features avoid the curse of dimensionality.

10
00:00:30,580 --> 00:00:34,230
Which is something that we'll talk about much more later on this course.

11
00:00:34,230 --> 00:00:37,790
Let's now evaluate the newer model with the reduced feature set.

12
00:00:37,790 --> 00:00:41,180
And in this plot just as before, the green line refers to

13
00:00:41,180 --> 00:00:44,960
the true inter selection time and the blue line refers to our model.

14
00:00:44,960 --> 00:00:46,225
Now, in this model.

15
00:00:46,225 --> 00:00:50,380
Our predictors using less information to predict the demand for Starburst.

16
00:00:50,380 --> 00:00:52,590
You can see here the result of doing so.

17
00:00:52,590 --> 00:00:55,920
Notice that our predicted line is a bit less jagged than the line that

18
00:00:55,920 --> 00:00:57,520
uses all of the features.

19
00:00:57,520 --> 00:00:59,690
Which indicates that we're over-fitting less.

20
00:00:59,690 --> 00:01:04,069
Revising the feature set choice from the question phase successfully resulted in

21
00:01:04,069 --> 00:01:05,560
reducing overfitting.

22
00:01:05,560 --> 00:01:09,190
We can also see now the new means square error of our new model.

23
00:01:09,190 --> 00:01:11,510
Now let's make a plot on the test set.

24
00:01:11,510 --> 00:01:14,860
The overfitting issue has been somewhat resolved, but now we need to

25
00:01:14,860 --> 00:01:18,910
examine the issue of our model missing the spikes in interselection time.

26
00:01:18,910 --> 00:01:22,630
Clearly this time we cannot avoid having to make a different choice at

27
00:01:22,630 --> 00:01:23,240
the model phase
