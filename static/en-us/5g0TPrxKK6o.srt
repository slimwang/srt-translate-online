1
00:00:00,560 --> 00:00:03,080
Alright. So in the examples up to this point, we've be

2
00:00:03,080 --> 00:00:07,689
setting the weights by hand to make various functions happen. And that's

3
00:00:07,689 --> 00:00:10,470
not really that useful in the context of machine learning. We'd really

4
00:00:10,470 --> 00:00:14,330
like a system, that given examples, finds weights that map the inputs

5
00:00:14,330 --> 00:00:16,940
to the outputs. And we're going to actually look at two different

6
00:00:16,940 --> 00:00:20,300
rules that have been developed for doing exactly that, to figuring out

7
00:00:20,300 --> 00:00:23,300
what the weights ought to be from training examples. One is called

8
00:00:23,300 --> 00:00:25,880
the the Perceptron Rule, and the other is called gradient descent or

9
00:00:25,880 --> 00:00:29,840
the Delta Rule. And the difference between them is the

10
00:00:29,840 --> 00:00:32,689
perception rule is going to make use of the threshold

11
00:00:32,689 --> 00:00:36,500
outputs, and the, the other mechanism is going to use

12
00:00:36,500 --> 00:00:39,400
unthreshold values. Alright so what we need to talk about

13
00:00:39,400 --> 00:00:41,530
now is the perception rule for, which is, how to

14
00:00:41,530 --> 00:00:45,690
set the weights of a single unit. So that it

15
00:00:45,690 --> 00:00:48,080
matches some training set. So we've got a training set,

16
00:00:48,080 --> 00:00:51,410
which is a bunch of examples of x, these are vectors,

17
00:00:51,410 --> 00:00:54,270
and we have y's which are zeros and ones which are the,

18
00:00:54,270 --> 00:00:57,150
the output that we want to hit. And what we want to do is

19
00:00:57,150 --> 00:01:02,300
set the, set the weights so that we capture this, this same data

20
00:01:02,300 --> 00:01:07,970
set. And we're going to do that by, modifying the weights over time.

21
00:01:07,970 --> 00:01:11,930
>> Oh, Michiel, what's the series of dashes over on the left.

22
00:01:11,930 --> 00:01:14,800
>> Oh, sorry, right. I should mention that, so

23
00:01:14,800 --> 00:01:16,430
one of the things that we're going to do here is

24
00:01:16,430 --> 00:01:18,910
were going to give a learning rate for the weights W,

25
00:01:18,910 --> 00:01:22,260
and not give a learning rule for Theta But we do

26
00:01:22,260 --> 00:01:24,500
need to learn the theta. So there's a, there's a very

27
00:01:24,500 --> 00:01:29,040
convenient trick for actually learning them by just treating it as

28
00:01:29,040 --> 00:01:32,310
a, as another kind of weight. So if you think about

29
00:01:32,310 --> 00:01:34,790
the way that the, the thresholding function works. We're taking a

30
00:01:34,790 --> 00:01:37,960
linear combination of the W's and X's, then we're comparing it

31
00:01:37,960 --> 00:01:41,440
to theta,but if you think about just subtracting theta from both

32
00:01:41,440 --> 00:01:45,820
sides, then, in some sense theta just becomes another

33
00:01:45,820 --> 00:01:48,250
one of the weights, and we're just comparing to

34
00:01:48,250 --> 00:01:50,540
zero. So what, what I did here was took

35
00:01:50,540 --> 00:01:53,400
the actual data, the x's, and I added what is

36
00:01:53,400 --> 00:01:55,920
sometimes called a, a bias unit to it. So

37
00:01:55,920 --> 00:02:00,550
basically, the input is one always to that. And the

38
00:02:00,550 --> 00:02:03,120
weight corresponding to it is going to correspond to

39
00:02:03,120 --> 00:02:06,890
negative theta ultimately. So, just, just again, this just simplifies

40
00:02:06,890 --> 00:02:09,660
things so that the threshold can be treated the same

41
00:02:09,660 --> 00:02:11,550
as the weights. So from now on, we don't have

42
00:02:11,550 --> 00:02:13,370
to worry about the threshold. It just gets folded into

43
00:02:13,370 --> 00:02:16,480
the weights, and all our comparisons are going to be just

44
00:02:16,480 --> 00:02:20,170
to zero instead of some, instead of theta. Centric, yeah.

45
00:02:21,550 --> 00:02:24,720
It certainly makes the math shorter. So okay, so this

46
00:02:24,720 --> 00:02:27,230
is what we're going to do. We're going to iterate over this

47
00:02:27,230 --> 00:02:31,900
training set, grabbing an x, which includes the bias piece,

48
00:02:31,900 --> 00:02:35,550
and the y. Where y is our target X is our

49
00:02:35,550 --> 00:02:38,460
input. And what we're going to do is we're going to

50
00:02:38,460 --> 00:02:43,070
change weight i, the, the, the weight corresponding to the ith

51
00:02:43,070 --> 00:02:46,580
unit, by the amount that we're changing the weight by. So

52
00:02:46,580 --> 00:02:49,580
this is sort of a tautology, right. This is truly just

53
00:02:49,580 --> 00:02:52,270
saying the amount we've changed the weight by is exactly delta

54
00:02:52,270 --> 00:02:54,716
W - in other words the amount we've changed the weight

55
00:02:54,716 --> 00:02:56,796
by. So we need to define that what that weight change is.

56
00:02:56,796 --> 00:02:58,972
The weight change is going to be defined as

57
00:02:58,972 --> 00:03:03,520
follows. We're going to take the target, the thing that

58
00:03:03,520 --> 00:03:05,860
we want the output to be. And compare it

59
00:03:05,860 --> 00:03:09,780
to, what the network with the current weight actually spits

60
00:03:09,780 --> 00:03:12,580
out. So we compute this, this y hat. This

61
00:03:12,580 --> 00:03:17,240
approximate output y. By again summing up the inputs according

62
00:03:17,240 --> 00:03:18,880
to the weights and comparing it to zero. That gets

63
00:03:18,880 --> 00:03:22,050
us a zero one value.So we're now comparing that to

64
00:03:22,050 --> 00:03:24,810
what the actual value is. So what's going to happen here, if

65
00:03:24,810 --> 00:03:28,600
they are both zero so let's, let's look at this. Each of

66
00:03:28,600 --> 00:03:30,840
y and y hat can only be zero and one. If they

67
00:03:30,840 --> 00:03:34,350
are both zeros then this y minus y hat is zero. If

68
00:03:34,350 --> 00:03:37,010
they're both ones and what does that mean? It means the output

69
00:03:37,010 --> 00:03:39,970
should have been zero and the output of our current. Network really

70
00:03:39,970 --> 00:03:44,850
was zero, so that's, that's kind of good. If they are both ones,

71
00:03:44,850 --> 00:03:47,770
it means the output was supposed to be one and our network outputted

72
00:03:47,770 --> 00:03:50,570
one, and the difference between them is going to be zero. But in

73
00:03:50,570 --> 00:03:53,770
this other case, y minus y hat, if the output was supposed to

74
00:03:53,770 --> 00:03:56,140
be zero, but we said one, our network says one, then we

75
00:03:56,140 --> 00:03:59,490
get a negative one. If the output was supposed to be one and

76
00:03:59,490 --> 00:04:02,030
we said zero, then we get a positive one. Okay, so those

77
00:04:02,030 --> 00:04:06,150
are the four cases for what's happening here. We're going to take that value

78
00:04:06,150 --> 00:04:10,430
multiply it by the current input to that unit i, scale it

79
00:04:10,430 --> 00:04:12,790
down by the sort of thing that is going to be cut the learning

80
00:04:12,790 --> 00:04:15,660
rate and use that as the the weight update

81
00:04:15,660 --> 00:04:18,390
change. So essentially what we are saying is if the

82
00:04:18,390 --> 00:04:21,290
output is already correct either both on or both

83
00:04:21,290 --> 00:04:23,490
off. Then there's gong to be no change to the

84
00:04:23,490 --> 00:04:28,800
weights. But, if our output is wrong. Let's say

85
00:04:28,800 --> 00:04:32,540
that we are giving a one when we should have

86
00:04:32,540 --> 00:04:35,120
been giving a zero. That means our, the total

87
00:04:35,120 --> 00:04:37,820
here is too large. And so we need to make

88
00:04:37,820 --> 00:04:38,920
it smaller. How are we going to make it

89
00:04:38,920 --> 00:04:45,530
smaller? Which ever input XI's correspond too, very large

90
00:04:45,530 --> 00:04:48,550
values, we're going to move those weights very far in

91
00:04:48,550 --> 00:04:51,080
a negative direction. We're taking this negative one times that

92
00:04:51,080 --> 00:04:54,520
value times this, this little learning rate. Alright, the

93
00:04:54,520 --> 00:04:56,950
other case is if the output was supposed to one

94
00:04:56,950 --> 00:04:59,160
but we're outputting a zero, that means our total

95
00:04:59,160 --> 00:05:03,110
is too small. And what this rule says is increase

96
00:05:03,110 --> 00:05:06,320
the weights essentially to try to make the sum bigger. Now, we

97
00:05:06,320 --> 00:05:08,590
don't want to kind of overdo it, and that's what this learning rate

98
00:05:08,590 --> 00:05:11,610
is about. Learning rate basically says we'll figure out the direction that

99
00:05:11,610 --> 00:05:13,810
we want to move things and just take a little step in that

100
00:05:13,810 --> 00:05:18,950
direction. We'll keep repeating over all of the, the input output pairs.

101
00:05:18,950 --> 00:05:21,330
So, we'll have a chance to get in to really build things up,

102
00:05:21,330 --> 00:05:23,000
but we're going to do it a little bit at a time so

103
00:05:23,000 --> 00:05:26,190
we don't overshoot. And that's the

104
00:05:26,190 --> 00:05:28,050
rule. It's actually extremely simple. Like, you,

105
00:05:28,050 --> 00:05:31,790
actually writing this in code is, is quite trivial. And and

106
00:05:31,790 --> 00:05:35,630
yet, it does some remarkable things. So let's imagine for a

107
00:05:35,630 --> 00:05:37,500
second that we have a training set that looks like this.

108
00:05:37,500 --> 00:05:40,600
It's in two dimensions, again, so that it's easy to visualize.

109
00:05:40,600 --> 00:05:43,960
That we've got. A bunch of positive examples, these green x's

110
00:05:43,960 --> 00:05:46,620
and we've got a bunch of negative examples these red x's,

111
00:05:46,620 --> 00:05:50,410
and were trying to learn basically a half plane right? Were

112
00:05:50,410 --> 00:05:53,070
trying to learn a half plane that separates the positive from the

113
00:05:53,070 --> 00:05:57,500
negative examples. So Charles do you see a, a, half plane

114
00:05:57,500 --> 00:05:58,900
that we could put in here that would do the trick?

115
00:05:58,900 --> 00:05:59,870
>> I do.

116
00:06:00,980 --> 00:06:01,630
>> What would it look like?

117
00:06:01,630 --> 00:06:02,470
>> It's that one.

118
00:06:02,470 --> 00:06:06,115
>> By that one do you mean, this one?

119
00:06:06,115 --> 00:06:08,880
>> Yeah. That's exactly what I was thinking, Michael.

120
00:06:08,880 --> 00:06:11,600
>> That's awesome! Yeah, there are isn't, isn't a whole lot

121
00:06:11,600 --> 00:06:13,940
of flexibility in what the answer is in this case, if

122
00:06:13,940 --> 00:06:16,180
we really want to get all greens on one side and all

123
00:06:16,180 --> 00:06:18,300
the reds on the other. If there is such a half

124
00:06:18,300 --> 00:06:20,870
plane that separates the positive from the negative examples, then

125
00:06:20,870 --> 00:06:24,750
we say that the data set is linearly separable, right? That

126
00:06:24,750 --> 00:06:27,590
there is a way of separating the positives and negatives with

127
00:06:27,590 --> 00:06:31,710
a line. And what's cool about the perception rule, is that

128
00:06:31,710 --> 00:06:35,530
if we have data that is linearly separable. The Perceptron Rule

129
00:06:35,530 --> 00:06:39,080
will find it. It only needs a finite number of iterations

130
00:06:39,080 --> 00:06:41,200
to find it. In fact, which I guess is really the

131
00:06:41,200 --> 00:06:43,770
same as saying that it will actually find it. It won't

132
00:06:43,770 --> 00:06:47,070
eventually get around to getting to something close to it.

133
00:06:47,070 --> 00:06:49,080
It will actually find a line, and it will stop

134
00:06:49,080 --> 00:06:52,010
saying okay I now have a set of weights that,

135
00:06:52,010 --> 00:06:55,520
that do the trick. So that's happens if the data set

136
00:06:55,520 --> 00:06:59,150
is in fact linearly separable and that's pretty cool. It's

137
00:06:59,150 --> 00:07:01,130
pretty amazing that it can do that, it's a very

138
00:07:01,130 --> 00:07:03,440
simple rule and it just goes through and iterates and,

139
00:07:03,440 --> 00:07:07,380
and solves the problem. So. Charles did you see any and solve the problem. So.

140
00:07:07,380 --> 00:07:09,340
>> I can think of one.

141
00:07:09,340 --> 00:07:10,830
What if it is not linearly separable?

142
00:07:10,830 --> 00:07:15,060
>> Hmm, I see. So, if the data is

143
00:07:15,060 --> 00:07:17,530
linearlly separable, then the algorithm works, so the algorithm

144
00:07:17,530 --> 00:07:20,300
simply needs to only be run when the data

145
00:07:20,300 --> 00:07:23,810
is linearlly separable. It's generally not that easy tell actually,

146
00:07:23,810 --> 00:07:25,770
when your data is linearly separable especially, here we

147
00:07:25,770 --> 00:07:28,710
have it in two dimensions, if it's in 50 dimensions,

148
00:07:28,710 --> 00:07:30,720
know whether or not there is a setting of

149
00:07:30,720 --> 00:07:34,740
those parameters that makes it linearly separable, not so clear.

150
00:07:34,740 --> 00:07:37,820
>> Well there is one way you could do it.

151
00:07:37,820 --> 00:07:38,110
>> Whats that?

152
00:07:38,110 --> 00:07:44,170
>> You could run this algorithm, and see if it ever stops. I see,

153
00:07:44,170 --> 00:07:49,280
yes of course, there's a problem with that particular scheme, right, which says,

154
00:07:49,280 --> 00:07:52,140
well for one thing this algorithm never stops, so wait, we need to, we

155
00:07:52,140 --> 00:08:00,414
need to address that. But, but really we should be running this loop here,

156
00:08:00,414 --> 00:08:05,210
while, there's some error so I neglected to say that

157
00:08:05,210 --> 00:08:07,350
before. But what you'll notice is if you continue to run

158
00:08:07,350 --> 00:08:10,470
this after the point where it's getting all the answers right.

159
00:08:10,470 --> 00:08:13,110
It found a set of weights that lineally separate the positive

160
00:08:13,110 --> 00:08:15,640
and negative instances what will happen is when it gets

161
00:08:15,640 --> 00:08:19,290
to this delta w line that y minus y hat will

162
00:08:19,290 --> 00:08:22,370
always be zero the weights will never change we'll go back

163
00:08:22,370 --> 00:08:25,510
and update them by adding zero to them repeatedly over and

164
00:08:25,510 --> 00:08:29,300
over again. So. If it ever does reach zero

165
00:08:29,300 --> 00:08:31,850
error, if it ever does separate the data set

166
00:08:31,850 --> 00:08:33,250
then we can just put a little condition in

167
00:08:33,250 --> 00:08:35,020
there and tell it to stop for loop. So what you

168
00:08:35,020 --> 00:08:39,190
are suggesting is that we could run this algorithm

169
00:08:39,190 --> 00:08:40,429
and if it stops then we know that it is

170
00:08:40,429 --> 00:08:43,950
linearly separable and if it doesn't stop Then we

171
00:08:43,950 --> 00:08:46,760
know that it's not linearly separable, right? By this guarantee.

172
00:08:46,760 --> 00:08:46,980
>> Sure.

173
00:08:46,980 --> 00:08:50,420
>> The problem is we, we don't know when finite is done, right?

174
00:08:50,420 --> 00:08:54,890
If, if this were like 1,000 iterations, we could run it for 1,000 if it wasn't

175
00:08:54,890 --> 00:08:58,550
done. It's not done, but all we know at this point is that it's a finite number

176
00:08:58,550 --> 00:09:00,350
of iterations, and so that could be a

177
00:09:00,350 --> 00:09:02,440
thousand, 10 thousand, a million, ten million, we

178
00:09:02,440 --> 00:09:04,100
don't know, so we never know when to

179
00:09:04,100 --> 00:09:06,980
stop and declare the data set not linearly separable.

180
00:09:06,980 --> 00:09:08,200
>> Hmm, so if we could do that,

181
00:09:08,200 --> 00:09:10,460
then we would have solved the halting problem, and

182
00:09:10,460 --> 00:09:13,190
we would all have nobel prizes Well, that's

183
00:09:13,190 --> 00:09:15,490
not necessarily the case. But it's certainly the other

184
00:09:15,490 --> 00:09:17,260
direction is true. That if we could solve

185
00:09:17,260 --> 00:09:18,800
the halting problem, then we could solve this.

186
00:09:18,800 --> 00:09:19,540
>> Hm.

187
00:09:19,540 --> 00:09:20,990
>> But it could be that this problem

188
00:09:20,990 --> 00:09:23,160
might be solvable even without solving the halting problem.

189
00:09:23,160 --> 00:09:24,640
>> Fair enough. Okay.
