1
00:00:00,350 --> 00:00:05,710
What it basically says is
that it is not possible to

2
00:00:05,710 --> 00:00:07,740
construct a fair voting system.

3
00:00:07,740 --> 00:00:11,130
The way that you can think about this
in very simple mathematical terms.

4
00:00:11,130 --> 00:00:13,240
That I know that I think you'll like,
Michael,

5
00:00:13,240 --> 00:00:17,580
is that all of these things
assume that everyone is

6
00:00:17,580 --> 00:00:22,590
using exactly the same units.

7
00:00:22,590 --> 00:00:26,600
Right, but if one of the agents is using
the metric system and someone else is

8
00:00:26,600 --> 00:00:30,160
using the imperial system it turns
out you can't even compare the two.

9
00:00:30,160 --> 00:00:33,570
There's absolutely no reason to believe,

10
00:00:33,570 --> 00:00:39,008
that the values that each of these
individual agents are developing,

11
00:00:39,008 --> 00:00:43,264
follow the same linear notion
of ordering of Q values.

12
00:00:43,264 --> 00:00:46,544
And if that's not the case then
all of these things are basically

13
00:00:46,544 --> 00:00:47,282
going to break.

14
00:00:47,282 --> 00:00:50,486
Because all of them boil down to doing
what looks like some kind of addition or

15
00:00:50,486 --> 00:00:51,750
averaging..

16
00:00:51,750 --> 00:00:52,480
>> Or maxing.

17
00:00:52,480 --> 00:00:54,690
>> Or maxing but maxing is just
kind of a weighted average.

18
00:00:55,760 --> 00:00:57,130
Kind of an exponential averaging.

19
00:00:58,140 --> 00:00:59,560
And so, you end up with a max that way.

20
00:00:59,560 --> 00:01:02,340
Same thing with negotiated W learning
you're subtracting things and

21
00:01:02,340 --> 00:01:03,600
taking mins.

22
00:01:03,600 --> 00:01:07,150
In the end you're basically assuming
that I can compare your seven with my

23
00:01:07,150 --> 00:01:09,350
seven and it means the same thing.

24
00:01:09,350 --> 00:01:12,340
Why would your seven of my
seven mean the same thing?

25
00:01:12,340 --> 00:01:14,320
So all of these have
that kind of problem.

26
00:01:14,320 --> 00:01:17,720
And in fact there's a paper that
written by a student of mine,

27
00:01:17,720 --> 00:01:21,643
that is a part of the readings,
that delve into this in more detail.

28
00:01:21,643 --> 00:01:24,950
And we could spend a lot of time on it,
but actually the idea is very simple.

29
00:01:24,950 --> 00:01:28,240
Which is really that once you
start thinking about goals, and

30
00:01:28,240 --> 00:01:32,400
you think about these
individual modules or

31
00:01:32,400 --> 00:01:35,450
individual options as trying to
accomplish different things.

32
00:01:35,450 --> 00:01:38,660
You have to deal with the fact that
their goals may not be completely

33
00:01:38,660 --> 00:01:39,840
compatible with one another.

34
00:01:39,840 --> 00:01:43,370
And you have to come up with some
way of making them compatible.

35
00:01:43,370 --> 00:01:46,040
And you can either assume that
compatibility by just pretending your

36
00:01:46,040 --> 00:01:48,320
seven and my seven are the same.

37
00:01:48,320 --> 00:01:52,210
Or you can say well there's some larger
goal you're trying to accomplish

38
00:01:52,210 --> 00:01:53,950
that the arbitrator knows about.

39
00:01:53,950 --> 00:01:56,800
The arbitrator is going to
make decisions based upon

40
00:01:56,800 --> 00:01:58,460
whether it accomplishes that big goal.

41
00:01:58,460 --> 00:01:59,870
In the predator prey case, for

42
00:01:59,870 --> 00:02:04,180
example, the big goal might be
living long enough to reproduce.

43
00:02:04,180 --> 00:02:07,690
And so I'm going to pick the subgoals
that allow me to live long enough

44
00:02:07,690 --> 00:02:08,630
to reproduce.

45
00:02:08,630 --> 00:02:10,150
>> I see, I think.

46
00:02:10,150 --> 00:02:11,670
So there's a high level goal, and

47
00:02:11,670 --> 00:02:16,720
then these subgoals that are chosen to
try to bring about the high level goal.

48
00:02:16,720 --> 00:02:20,740
>> Right, and so it's the arbitrator
that has to decide what's going on.

49
00:02:20,740 --> 00:02:23,040
So this is, this seems like
a kind of straightforward thing.

50
00:02:23,040 --> 00:02:24,440
But I actually think it's kind of cool,
right.

51
00:02:24,440 --> 00:02:29,410
That basically we went from actions
to abstract actions these options.

52
00:02:29,410 --> 00:02:32,200
Those abstract actions
actually turn out to be.

53
00:02:32,200 --> 00:02:34,460
Well at least one could
look at it this way.

54
00:02:34,460 --> 00:02:35,910
Ways of accomplishing goals.

55
00:02:37,360 --> 00:02:38,260
Once you have these ways of

56
00:02:38,260 --> 00:02:41,170
accomplishing goals then suddenly
you realize well these goals don't

57
00:02:41,170 --> 00:02:43,530
necessarily have to be
compatible with one another.

58
00:02:43,530 --> 00:02:45,140
And so
I have to deal with that compatibility.

59
00:02:45,140 --> 00:02:48,760
And so you move from an MDP
with primitive actions

60
00:02:48,760 --> 00:02:52,160
to this larger notion
of balancing tradeoffs.

61
00:02:52,160 --> 00:02:55,730
As you're trying to accomplish some
large goal for the entire system.

62
00:02:55,730 --> 00:02:59,050
And in the end, they're still all
MDPs because they're SNDPs and

63
00:02:59,050 --> 00:03:03,640
SNDPS are MDPs and in the end,
all of that reinforcement learning

64
00:03:03,640 --> 00:03:07,130
stuff that we've been talking about
throughout the entire term still applies

65
00:03:07,130 --> 00:03:09,040
>> And you've kind of brought together.

66
00:03:09,040 --> 00:03:11,740
Generalization with multi agent.

67
00:03:11,740 --> 00:03:13,940
You know type of thinking.

68
00:03:13,940 --> 00:03:16,794
>> Yeah, so we got multi-agents, we got
single agents, we got multiple goals,

69
00:03:16,794 --> 00:03:17,870
we got conflicting goals.

70
00:03:17,870 --> 00:03:21,900
We've got abstraction and this gives
us scale and everything's wonderful.

71
00:03:21,900 --> 00:03:22,400
>> Hallelujah.
