1
00:00:00,280 --> 00:00:03,510
Let's imagine, for example, that there
is truly only one optimal action.

2
00:00:03,510 --> 00:00:07,900
Then, as we spend an infinite amount of
time running around in the world, our

3
00:00:07,900 --> 00:00:11,160
notion of which action is optimal over
here, if we can convert them into actual

4
00:00:11,160 --> 00:00:15,770
probabilities, will drive whatever
the optimal action is towards one.

5
00:00:15,770 --> 00:00:17,900
And that just comes from all the stuff
we've been talking about with

6
00:00:17,900 --> 00:00:20,680
reinforcement learning for
all the time in this class.

7
00:00:20,680 --> 00:00:24,630
If we can get to an optimal policy by
just experiencing things in the world,

8
00:00:24,630 --> 00:00:26,610
then eventually,
these numbers will reflect that.

9
00:00:26,610 --> 00:00:28,900
And it doesn't matter
what the human thinks,

10
00:00:28,900 --> 00:00:31,850
the real world will drive
the correct answer towards one.

11
00:00:31,850 --> 00:00:35,210
>> So, you're saying that c
doesn't change over time?

12
00:00:35,210 --> 00:00:37,900
>> c could change over time, but
let's just imagine it doesn't.

13
00:00:37,900 --> 00:00:41,950
Let's just say, we pick a value for
c and it comes out this way.

14
00:00:41,950 --> 00:00:44,690
If you want to say that c changes
over time that's okay, but

15
00:00:44,690 --> 00:00:49,820
then we're going to deal with that
question by having these numbers here

16
00:00:49,820 --> 00:00:53,090
reflect whatever we
think c happens to be.

17
00:00:53,090 --> 00:00:55,941
Remember, we can keep track of all
the labels that we've seen and for any

18
00:00:55,941 --> 00:00:59,092
given value of c that we have, we can
just change these numbers appropriately.

19
00:00:59,092 --> 00:00:59,625
>> Okay.

20
00:00:59,625 --> 00:01:00,980
>> So, it sort of doesn't matter.

21
00:01:00,980 --> 00:01:02,560
At the time we ask this question,

22
00:01:02,560 --> 00:01:05,010
I've got a distribution of
reactions according to the human.

23
00:01:05,010 --> 00:01:08,838
I've got a distribution of actions
according to my experience in the world.

24
00:01:08,838 --> 00:01:11,528
It no longer matters what I think
about the human because that is

25
00:01:11,528 --> 00:01:13,300
captured by this distribution.

26
00:01:13,300 --> 00:01:15,660
So I can just ask the question,
what should I do?

27
00:01:15,660 --> 00:01:17,251
And here you think the answer is x.

28
00:01:17,251 --> 00:01:17,878
>> Yeah.

29
00:01:17,878 --> 00:01:20,510
>> Now,
what if I had different numbers here?

30
00:01:20,510 --> 00:01:21,563
>> Then I still think it's x.

31
00:01:21,563 --> 00:01:22,320
>> Yeah.
[LAUGH]

32
00:01:22,320 --> 00:01:24,050
>> What if it is this?

33
00:01:24,050 --> 00:01:26,090
Well now I could certainly justify why.

34
00:01:26,090 --> 00:01:26,845
>> Why.
>> Yeah.

35
00:01:26,845 --> 00:01:29,052
>> [LAUGH] Tell me why you think y.

36
00:01:29,052 --> 00:01:32,640
>> So okay, so are we supposed
to imagine that the pi sub A is,

37
00:01:32,640 --> 00:01:34,415
can the algorithm be wrong?

38
00:01:34,415 --> 00:01:36,240
>> The algorithm could be wrong.

39
00:01:36,240 --> 00:01:39,650
But in the same way that we
don't have to worry about

40
00:01:39,650 --> 00:01:43,640
what's wrong with the human because
the distribution captures it,

41
00:01:43,640 --> 00:01:45,008
let's make the same assumption here.

42
00:01:45,008 --> 00:01:48,100
The agent, well this distribution
is going to capture all of

43
00:01:48,100 --> 00:01:50,520
the uncertainty about
the agent's beliefs.

44
00:01:50,520 --> 00:01:53,704
So maybe we'll never actually see 0, 1,
0, but we'll see numbers that are close

45
00:01:53,704 --> 00:01:55,930
enough that I'm just
going to write out 0, 1, 0.

46
00:01:55,930 --> 00:01:59,390
>> Boy, there's a lot of ways to
combine these numbers together

47
00:01:59,390 --> 00:02:00,990
in which y ends up being the winner.

48
00:02:02,940 --> 00:02:05,160
But it strikes me that in particular,

49
00:02:05,160 --> 00:02:09,509
if we believe pi sub A, then pi sub
A is absolutely certain that it's y.

50
00:02:09,509 --> 00:02:12,520
So it almost doesn't matter at
this point what pi sub H thinks,

51
00:02:12,520 --> 00:02:14,235
what the human thinks.

52
00:02:14,235 --> 00:02:18,010
>> Mm-hm, and by the way, with the same
argument, if it's the case that c

53
00:02:18,010 --> 00:02:22,220
is equal to 1 for the human, that the
human is infallible, then it really is

54
00:02:22,220 --> 00:02:24,600
not going to matter what the agent
thinks, because the human's infallible.

55
00:02:24,600 --> 00:02:26,540
>> Well, unless they,
[LAUGH] unless they disagree.

56
00:02:26,540 --> 00:02:29,218
But then we have to really believe
that probability is correct.

57
00:02:29,218 --> 00:02:29,820
>> Right.

58
00:02:29,820 --> 00:02:31,530
Either the probability is correct or
it's incorrect.

59
00:02:31,530 --> 00:02:33,820
And for
the purposes of answering this question,

60
00:02:33,820 --> 00:02:35,490
let's just say
the probabilities are correct.

61
00:02:35,490 --> 00:02:37,810
In so far they are not it will be
reflected by these distributions.

62
00:02:37,810 --> 00:02:40,901
>> So we can never have 1, 0,
0 on the top row and then 0, 1,

63
00:02:40,901 --> 00:02:44,178
0 on the bottom row, because that
just violates the laws of reality.

64
00:02:44,178 --> 00:02:44,759
>> Right.

65
00:02:44,759 --> 00:02:45,977
>> So, you would argue for y?

66
00:02:45,977 --> 00:02:46,662
>> I would.

67
00:02:46,662 --> 00:02:49,696
>> I think that you've been using
intuition so far and I want you to do

68
00:02:49,696 --> 00:02:53,300
a little bit more than intuition, so I'm
going to do that by writing out a quiz.
