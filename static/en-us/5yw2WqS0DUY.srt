1
00:00:00,000 --> 00:00:03,264
So we have this chicken and the egg
problem and those of you, I'm assuming,

2
00:00:03,264 --> 00:00:04,386
most of you are familiar,

3
00:00:04,386 --> 00:00:06,732
this is actually referred
to as k-means clustering.

4
00:00:06,732 --> 00:00:10,010
It solves the chicken and the egg
problem by smashing them together,

5
00:00:10,010 --> 00:00:12,781
which could be a pretty nasty picture,
but essentially,

6
00:00:12,781 --> 00:00:14,950
it's just an iterative algorithm.

7
00:00:14,950 --> 00:00:16,129
What you do is, just as it is says here.

8
00:00:16,129 --> 00:00:20,969
And we'll take an illustration in a
minute, you randomly, yes just randomly,

9
00:00:20,969 --> 00:00:22,689
pick some cluster centers.

10
00:00:22,689 --> 00:00:25,510
You might try to be a little smart
about how you do that or, or not.

11
00:00:25,510 --> 00:00:27,403
I find it easier to be not smart.

12
00:00:27,403 --> 00:00:29,360
And then, once you have the centers,

13
00:00:29,360 --> 00:00:32,429
we know which points should be
assigned to those clusters.

14
00:00:32,429 --> 00:00:35,493
Once we've assigned to the clusters,
that's step two,

15
00:00:35,493 --> 00:00:39,619
given the points in clusters, step
three, we can re-assign the center, and

16
00:00:39,619 --> 00:00:42,900
you set that, you set the new
cluster center to be that.

17
00:00:42,900 --> 00:00:45,980
And step four is, well if any
of the cluster centers move, and

18
00:00:45,980 --> 00:00:49,655
the only reason they would have moved
is because you changed the assignments,

19
00:00:49,655 --> 00:00:50,856
you go back to step two.

20
00:00:50,856 --> 00:00:54,880
And this is classical k-means and
I assume most of you have seen that.

21
00:00:54,880 --> 00:00:56,993
But, just showing a few
in the contest of,

22
00:00:56,993 --> 00:01:00,931
context of segmentation here's a nice
little example from Andrew Moore,

23
00:01:00,931 --> 00:01:03,980
recently appointed the Dean
at Carnegie Melon.

24
00:01:03,980 --> 00:01:05,180
Congratulations Andrew.

25
00:01:05,180 --> 00:01:07,212
So here we have k-means, right?

26
00:01:07,212 --> 00:01:09,720
So we've got a bunch of points.

27
00:01:09,720 --> 00:01:12,770
Now we know where we think
the cluster should be, but

28
00:01:12,770 --> 00:01:13,790
that's because we're smart.

29
00:01:13,790 --> 00:01:15,360
A computer's not so smart.

30
00:01:15,360 --> 00:01:16,890
We picked some random centers,
so there they are.

31
00:01:16,890 --> 00:01:21,000
Not a particularly good
choice of initial collection.

32
00:01:21,000 --> 00:01:24,090
We then partitioned the space up, right?

33
00:01:24,090 --> 00:01:27,383
So those of you from computer
science recognize this diagram.

34
00:01:27,383 --> 00:01:32,469
This is the, the fracturing of the,
of the space such that this line says

35
00:01:32,469 --> 00:01:38,220
that everything here is closest to
that point, more than any other point.

36
00:01:38,220 --> 00:01:41,820
And so once we have those assignments,
what we can do is,

37
00:01:41,820 --> 00:01:45,220
we move the centers to be at the center

38
00:01:45,220 --> 00:01:47,870
of the points that were assigned to
that cluster, and then we iterate.

39
00:01:47,870 --> 00:01:49,425
All right?

40
00:01:49,425 --> 00:01:51,780
And we repeat that until it terminates.
