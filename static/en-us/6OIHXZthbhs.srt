1
00:00:00,000 --> 00:00:03,000
Now, let's say we've done all this learning,

2
00:00:03,000 --> 00:00:06,000
we've applied our agent, and we've come up with a utility model;

3
00:00:06,000 --> 00:00:10,000
and we have the estimates for the utility for every state.

4
00:00:10,000 --> 00:00:13,000
Now what do we do when we want to act in the world?

5
00:00:13,000 --> 00:00:17,000
Well, we now have out policy for the state,

6
00:00:17,000 --> 00:00:20,000
which is determined by the expected value

7
00:00:20,000 --> 00:00:22,000
and we compute the expected value

8
00:00:22,000 --> 00:00:25,000
of each state by looking at the utility,

9
00:00:25,000 --> 00:00:27,000
which we just learned.

10
00:00:27,000 --> 00:00:31,000
But then, we have to multiply by the transition possibilities.

11
00:00:31,000 --> 00:00:37,000
What's the probability of each resulting state that we have to look up the utility of?

12
00:00:37,000 --> 00:00:39,000
And so, we need to know that--

13
00:00:39,000 --> 00:00:44,000
and in some cases, we're given the transition model, and so we know all these probabilities.

14
00:00:44,000 --> 00:00:46,000
But in other cases, we don't have it;

15
00:00:46,000 --> 00:00:48,000
and so if we haven't learned it, we can'tapply

16
00:00:48,000 --> 00:00:51,000
our policy, even though we know the utilities.

17
00:00:51,000 --> 00:00:54,000
I want to talk, briefly, about this alternative method

18
00:00:54,000 --> 00:00:57,000
called Q Learning, that I mentioned before.

19
00:00:57,000 --> 00:01:01,000
Where in Q Learning, we don't learn U direclty,

20
00:01:01,000 --> 00:01:03,000
and we don't need the transition model.

21
00:01:03,000 --> 00:01:08,000
Instead, what we learned is a direct mapping,

22
00:01:08,000 --> 00:01:11,000
Q, from states and actions

23
00:01:11,000 --> 00:01:15,000
to utilities and so then, once we've learned Q,

24
00:01:15,000 --> 00:01:18,000
we can determine the optimal policy of he state,

25
00:01:18,000 --> 99:59:59,999
just by taking the maximum overall possible actions of this Q of S, A values.
