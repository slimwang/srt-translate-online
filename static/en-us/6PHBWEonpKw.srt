1
00:00:00,000 --> 00:00:04,421
So when I run all of these, this one is slightly the fastest of all of these on the diagonal--

2
00:00:04,421 --> 00:00:09,965
64 threads per block operating on 16 items each.

3
00:00:09,965 --> 00:00:13,889
And so the thing to notice is that throughput is generally increasing

4
00:00:13,889 --> 00:00:17,159
as the granularity per thread increases, right?

5
00:00:17,159 --> 00:00:21,758
So having threads do more and more serial work is generally a good thing.

6
00:00:21,758 --> 00:00:24,168
On the other hand, if you've got a fixed tile size--

7
00:00:24,168 --> 00:00:28,763
in other words, the tile here is the total number of items that we're scanning over--

8
00:00:28,763 --> 00:00:33,366
for a fixed tile size there's diminishing returns.

9
00:00:33,366 --> 00:00:37,780
The improvements get smaller and smaller as you approach this sweet spot

10
00:00:37,780 --> 00:00:43,226
because you're trading off increased granularity per thread for decreased parallelilsm.

11
00:00:43,226 --> 00:00:46,104
And then it starts to go up again, and in my measurements on Fermi,

12
00:00:46,104 --> 00:00:50,151
which is the same GPU that you'll be using on the Udacity IDE,

13
00:00:50,151 --> 00:00:56,349
I get 32x32 being slightly slower than 64 threads with 16 items each.

14
00:00:56,349 --> 00:01:02,821
And at some point, you get to the point where you can no longer fit a problem size

15
00:01:02,821 --> 00:01:07,235
in a single thread's registers, and then performance falls of a cliff again.

16
00:01:07,235 --> 00:01:12,749
And for me on Fermi, 64 items per thread running at only 16 threads

17
00:01:12,749 --> 00:01:16,390
really starts to get pretty bad performance again.

18
00:01:16,390 --> 00:01:19,545
I encourage you to play around, experiment a little bit more

19
00:01:19,545 --> 00:01:22,222
to see what the rest of this matrix looks like.

20
00:01:22,222 --> 00:01:29,634
And if you're interested, go check out the CUB homepage and see what else you can do.

21
00:01:29,634 --> 00:01:32,320
There's different varieties of block scan, for example.

22
00:01:32,320 --> 00:01:36,507
There's work-efficient and step-efficient varieties of block scan,

23
00:01:36,507 --> 00:01:41,525
and so experiment a little bit and see how fast you can get this scan throughput,

24
00:01:41,525 --> 00:01:45,187
how few clocks you can spend per element scanned.

25
00:01:45,187 --> 00:01:47,220
And what I've found is that with the right balance,

26
00:01:47,220 --> 00:01:51,056
you can have a computational overhead of less than 1 clock cycle per item scanned.

27
00:01:51,056 --> 00:01:53,118
That is really pretty remarkable.

28
00:01:53,118 --> 00:01:55,568
Think about running this code on a CPU.

29
00:01:55,568 --> 00:01:59,228
You couldn't really achieve less than 1 clock per item scanned.
