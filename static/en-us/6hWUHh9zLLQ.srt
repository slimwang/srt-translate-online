1
00:00:00,025 --> 00:00:04,917
Finally, we need to make some assumptions, in order to use one way ANOVA. The

2
00:00:04,917 --> 00:00:08,870
first is normality. All the populations from which the samples are from are

3
00:00:08,870 --> 00:00:14,695
normally distributed. Another is homogeneity of variance. The data come from

4
00:00:14,695 --> 00:00:19,852
populations that have equal amounts of variability. And finally independence of

5
00:00:19,852 --> 00:00:25,115
observations. The results found from one sample won't affect the others.

6
00:00:25,115 --> 00:00:31,240
However, we can violate these assumptions under certain conditions. We can

7
00:00:31,240 --> 00:00:36,282
violate the normality assumption if the sample size is large. We can violate

8
00:00:36,282 --> 00:00:40,506
the homogeneity of variance assumption if all the samples have nearly equal

9
00:00:40,506 --> 00:00:46,534
sample sizes, and the ratio of any two variances does not exceed four. We have

10
00:00:46,534 --> 00:00:50,758
to maintain independence of observations, but we can use random assignment to

11
00:00:50,758 --> 00:00:56,154
conditions to help us meet this assumption. Let's do a quick summary of ANOVA

12
00:00:56,154 --> 00:01:01,296
to wrap up this lesson. I'm not actually going to rap this time though. We're

13
00:01:01,296 --> 00:01:06,032
just going to wrap up the lesson. If we have three or more samples, and we want

14
00:01:06,032 --> 00:01:10,119
to know if any two of them are significantly different, we look at both the

15
00:01:10,119 --> 00:01:16,783
between-group variability, and the within-group variability. Between-group

16
00:01:16,783 --> 00:01:21,327
variability is a measure of how spaced apart these sample means are from each

17
00:01:21,327 --> 00:01:28,049
other. And we do that by finding the grand mean and each squared deviation from

18
00:01:28,049 --> 00:01:34,578
the grand mean, for each sample mean. We multiply each sample size by the

19
00:01:34,578 --> 00:01:41,621
squared deviation of each sample mean from the grand mean. Then we add them up.

20
00:01:41,621 --> 00:01:47,099
Then we have to look at the within-group variability, which is essentially the

21
00:01:47,099 --> 00:01:52,328
squared deviation of each value in each sample, from the respective sample

22
00:01:52,328 --> 00:02:01,136
mean. So we add up all sums of squares from their respective sample mean. And

23
00:02:01,136 --> 00:02:04,918
then we have to find the average sum of squares for each, by dividing by the

24
00:02:04,918 --> 00:02:10,338
degrees of freedom. In the case of the between-groups, this is the number of

25
00:02:10,338 --> 00:02:16,322
samples minus 1. And for within-groups, this is the total number of values

26
00:02:16,322 --> 00:02:21,660
minus the number of groups. This is the same as adding the degrees of freedom

27
00:02:21,660 --> 00:02:27,022
for each group. There we have our F statistic. And if it falls out here in the

28
00:02:27,022 --> 00:02:32,502
critical region, past the F critical value, we'll reject the null. After making

29
00:02:32,502 --> 00:02:37,182
a statistical decision, we can use the multiple comparison test, one of which

30
00:02:37,182 --> 00:02:41,790
is Tukey's honestly significant difference, which is a value that if any two

31
00:02:41,790 --> 00:02:46,398
sample means have a difference greater than that value, they're considered

32
00:02:46,398 --> 00:02:53,750
honestly significantly different. You've also learned how to determine what

33
00:02:53,750 --> 00:02:58,910
proportion of the difference between means is due to the independent variable.

34
00:02:58,910 --> 00:03:01,600
That's eta squared. And that's a wrap.
