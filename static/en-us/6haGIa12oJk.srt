1
00:00:00,310 --> 00:00:02,690
So we can take all of that
in just turn it into an MDP.

2
00:00:02,690 --> 00:00:06,010
I think it's pretty straightforward
to think how you'd turn this kind of

3
00:00:06,010 --> 00:00:08,790
story management system into
a market decision process, right.

4
00:00:08,790 --> 00:00:09,675
So what are states?

5
00:00:09,675 --> 00:00:10,970
Well, we said that last time.

6
00:00:10,970 --> 00:00:14,820
States are just partial trajectories or
partial plot sequences.

7
00:00:14,820 --> 00:00:17,500
Actions, well, they're the story
actions that the system can take.

8
00:00:17,500 --> 00:00:21,080
Your model is still the same model
that we're using before except it's

9
00:00:21,080 --> 00:00:22,995
a player model that means you know.

10
00:00:22,995 --> 00:00:24,850
We've seen a sequence of states.

11
00:00:24,850 --> 00:00:27,190
Or we've seen these trajectories.

12
00:00:27,190 --> 00:00:30,270
Some action is being taken, and
I want to know what plot state

13
00:00:30,270 --> 00:00:33,660
we're likely to be in, given that
the player is doing something.

14
00:00:33,660 --> 00:00:35,320
So it still looks just
like a transition model.

15
00:00:35,320 --> 00:00:37,790
And then the rewards mean
exactly what they meant before,

16
00:00:37,790 --> 00:00:39,360
except here they're
actually standing in for

17
00:00:39,360 --> 00:00:43,640
some notion of what some writer of
a story thinks of as a good story.

18
00:00:43,640 --> 00:00:47,770
>> And you can somehow formalize that so
that you can plan?

19
00:00:47,770 --> 00:00:49,400
>> Yeah, well, you could, but

20
00:00:49,400 --> 00:00:52,330
that gets us to a whole other discussion
about where rewards come from,

21
00:00:52,330 --> 00:00:54,710
which is probably a discussion
that we should have someday.

22
00:00:54,710 --> 00:00:56,530
So maybe we'll do that
towards the end of the class.

23
00:00:56,530 --> 00:00:59,380
But just imagine the right now you
have the reward function the same way

24
00:00:59,380 --> 00:01:01,550
that you would with any other MDP and

25
00:01:01,550 --> 00:01:03,470
you're really worry about where
it comes from it's just there.

26
00:01:03,470 --> 00:01:07,240
>> Okay and
given that these trajectories, or sorry,

27
00:01:07,240 --> 00:01:09,490
the states are now these
full trajectories.

28
00:01:09,490 --> 00:01:11,980
It seems like that makes some things
harder because now there could be

29
00:01:11,980 --> 00:01:15,900
a lot of them but on the other hand you
can never go back to a state you've

30
00:01:15,900 --> 00:01:18,310
been in before so it's acyclic.

31
00:01:18,310 --> 00:01:20,460
So you can actually solve it
more efficiently because you

32
00:01:20,460 --> 00:01:23,920
don't have to run, I mean basically
one value iteration and you're done.

33
00:01:23,920 --> 00:01:26,620
>> Well, that's actually kind
of neat that you said that.

34
00:01:26,620 --> 00:01:30,490
Because I was about to say that
there are two problems with this.

35
00:01:30,490 --> 00:01:33,900
The first problem is, well,
now that you don't have states, but

36
00:01:33,900 --> 00:01:37,450
you have sequences of states,
that's a lot of new states.

37
00:01:37,450 --> 00:01:38,650
How big do you think that is?

38
00:01:38,650 --> 00:01:40,160
>> I'm going to say very.

39
00:01:40,160 --> 00:01:41,470
>> It is very, very big.

40
00:01:41,470 --> 00:01:44,340
In fact, we can be even
more quantitative in that.

41
00:01:44,340 --> 00:01:46,680
It's actually, hyper exponential.

42
00:01:46,680 --> 00:01:48,090
It's like 2 to the 2 to the f.

43
00:01:48,090 --> 00:01:50,800
>> Where,
in terms of the length of the sequence?

44
00:01:50,800 --> 00:01:53,540
>> Yes, of all possible sequences
you can have the number of states.

45
00:01:53,540 --> 00:01:54,890
>> Even.
Okay, yes, yes.

46
00:01:54,890 --> 00:01:55,570
Okay, all right.

47
00:01:55,570 --> 00:01:58,200
>> Because the number of states
is sort of in factorial and

48
00:01:58,200 --> 00:02:00,040
all the different kind of that
we can have but we don't.

49
00:02:00,040 --> 00:02:02,890
All the states don't have
to actually be there and so

50
00:02:02,890 --> 00:02:06,020
at the end of the day it ends up looking
something like 2 to the 2, you know.

51
00:02:06,020 --> 00:02:07,240
And be really really really big.

52
00:02:07,240 --> 00:02:11,580
So the space that we're looking over of
all possible trajectories where things

53
00:02:11,580 --> 00:02:14,730
might be there things might not be there
gets really really big very quickly.

54
00:02:14,730 --> 00:02:16,060
So that's one point that you said.

55
00:02:16,060 --> 00:02:18,660
But on the other hand it turns out that
there's structure there that we're going

56
00:02:18,660 --> 00:02:19,970
to be able to take advantage of.

57
00:02:19,970 --> 00:02:22,140
And, in fact, that's what we do and

58
00:02:22,140 --> 00:02:26,350
why we're bothering to introduce this
formalism that I mentioned earlier.

59
00:02:26,350 --> 00:02:27,040
So let's do that.

60
00:02:27,040 --> 00:02:29,880
Actually before we do that,
let me mention one other

61
00:02:29,880 --> 00:02:33,260
problem with thinking about things
as Markov Decision Processes.

62
00:02:33,260 --> 00:02:36,920
And it's going to be a problem with the
strength of a Markov Decision Process.

63
00:02:36,920 --> 00:02:43,220
So what is the goal of reinforcement
was the goal of solving an MDP?

64
00:02:43,220 --> 00:02:44,130
>> Maximize reward.

65
00:02:44,130 --> 00:02:45,470
>> You maximize reward.

66
00:02:45,470 --> 00:02:49,800
So, in fact, it turns out that if you
take this sort of notion of storytelling

67
00:02:49,800 --> 00:02:53,840
and how I can get someone to do a choose
your own adventure and I've got rewards.

68
00:02:53,840 --> 00:02:57,360
I've got some evaluation
of what best stories are.

69
00:02:57,360 --> 00:02:58,400
What's going to happen?

70
00:02:58,400 --> 00:03:00,090
What is the system going
to learn how to do?

71
00:03:00,090 --> 00:03:01,520
>> Make the author happy?

72
00:03:01,520 --> 00:03:02,800
>> Make the author happy.

73
00:03:02,800 --> 00:03:05,440
That's exactly right,
but not necessarily.

74
00:03:05,440 --> 00:03:06,850
Make the player happy.

75
00:03:06,850 --> 00:03:09,380
>> So what ends up happening
with Markov Decision Processes,

76
00:03:09,380 --> 00:03:12,910
using sort of modeling as Markov
Decision Processes in the obvious way,

77
00:03:12,910 --> 00:03:18,060
is that you force the player
to experience the best story.

78
00:03:18,060 --> 00:03:19,810
No matter what the player does.

79
00:03:19,810 --> 00:03:23,380
No matter whatever choices
you're trying to make.

80
00:03:23,380 --> 00:03:25,400
Nope, this system is
going to make certain

81
00:03:25,400 --> 00:03:26,730
that you're going to enjoy yourself.

82
00:03:26,730 --> 00:03:27,860
Dammit, and

83
00:03:27,860 --> 00:03:30,640
enjoying yourself means whatever
the author thinks is enjoying yourself.

84
00:03:30,640 --> 00:03:31,510
>> So like grad school.

85
00:03:31,510 --> 00:03:32,790
>> It's just like grad school.

86
00:03:32,790 --> 00:03:34,760
>> No, no, no, that's not right.

87
00:03:34,760 --> 00:03:39,140
Why is that a problem
because the the author.

88
00:03:39,140 --> 00:03:41,180
I mean in a book that is
certainly what happens.

89
00:03:41,180 --> 00:03:43,860
The author gets to choose
what you can experience.

90
00:03:43,860 --> 00:03:45,040
>> And that's a good argument, but

91
00:03:45,040 --> 00:03:48,050
if we're bothered with this whole
kind of interactive entertainment,

92
00:03:48,050 --> 00:03:52,510
choose your own adventure story, then if
you don't really have any choice in what

93
00:03:52,510 --> 00:03:54,590
you do, then it's not really
a choose your own adventure story.

94
00:03:54,590 --> 00:03:55,870
>> Got it.
>> The basic idea here,

95
00:03:55,870 --> 00:03:58,910
is because there is a reward, and
because all the algorithms we've ever

96
00:03:58,910 --> 00:04:01,570
thought about with
Markov Decision Processes are about

97
00:04:01,570 --> 00:04:04,500
maximizing long term award
that it's finding an answer.

98
00:04:04,500 --> 00:04:06,650
It's going to find an answer
that's going to force you

99
00:04:06,650 --> 00:04:07,720
down a particular path.

100
00:04:07,720 --> 00:04:12,250
But it turns out we can relax that
need to find the best answer and

101
00:04:12,250 --> 00:04:14,950
actually end up solving hyperextend
is a problem at the same time.

102
00:04:14,950 --> 00:04:15,450
So, let's do that.
