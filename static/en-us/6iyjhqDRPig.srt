1
00:00:00,210 --> 00:00:01,600
>> So, the second alternative I just wanted

2
00:00:01,600 --> 00:00:05,560
to mention very briefly is something called LDA, which

3
00:00:05,560 --> 00:00:09,350
is short for linear discriminant analysis. So, any

4
00:00:09,350 --> 00:00:11,870
guess on what linear discriminant analysis might do, Michael?

5
00:00:11,870 --> 00:00:14,970
>> It'll make, well, linear. So it's got a

6
00:00:14,970 --> 00:00:19,870
linear in it. Discriminant reminds me of, like, classification lines.

7
00:00:19,870 --> 00:00:22,870
>> Yes. So, what linear discriminant analysis

8
00:00:22,870 --> 00:00:25,690
does is, it finds a projection that

9
00:00:25,690 --> 00:00:29,690
discriminates based on the label. This actually will feel, this

10
00:00:29,690 --> 00:00:32,790
feels a lot like supervised learning, right? You take advantage

11
00:00:32,790 --> 00:00:35,490
of the label, and you come up with a transformation,

12
00:00:35,490 --> 00:00:39,400
such that you will, in fact, project things into different

13
00:00:39,400 --> 00:00:42,560
clumps or clusters, or otherwise separate them based upon their

14
00:00:42,560 --> 00:00:45,500
label. So, you can imagine using a lot of algorithms,

15
00:00:45,500 --> 00:00:47,120
as we've used in the past. In some sense, what

16
00:00:47,120 --> 00:00:51,050
they're doing is, linear discriminant analysis. They're finding lines or

17
00:00:51,050 --> 00:00:55,340
finding linear separators between different clumps of points. And

18
00:00:55,340 --> 00:00:56,870
if you think about the binary case, for example,

19
00:00:56,870 --> 00:00:59,050
you could think of SVM as doing something like

20
00:00:59,050 --> 00:01:03,850
this, where what you've done is, you transform your points

21
00:01:03,850 --> 00:01:07,112
not into a specific label plus or minus, one

22
00:01:07,112 --> 00:01:10,640
or zero, yes or no, but you instead project it

23
00:01:10,640 --> 00:01:12,680
onto the line such that it would clump things

24
00:01:12,680 --> 00:01:16,460
accordingly. And you use the value of that projection as

25
00:01:16,460 --> 00:01:19,840
a way of re-representing the data. Does that make sense?

26
00:01:19,840 --> 00:01:21,780
>> Yeah, and this, this approach seems a

27
00:01:21,780 --> 00:01:23,120
little different from all the other ones, in

28
00:01:23,120 --> 00:01:27,520
that it's actually paying attention to how the

29
00:01:27,520 --> 00:01:29,990
resulting components are going to get used. Right.

30
00:01:29,990 --> 00:01:31,170
>> That's exactly right.

31
00:01:31,170 --> 00:01:31,880
>> It actually is going to try

32
00:01:31,880 --> 00:01:34,690
to help you, specifically, with the classification.

33
00:01:34,690 --> 00:01:36,410
>> Alright, that's exactly right. All three

34
00:01:36,410 --> 00:01:37,860
of the examples that we gave before,

35
00:01:37,860 --> 00:01:39,700
feel almost like filter methods, right? In

36
00:01:39,700 --> 00:01:41,520
that they have some criterion they're trying

37
00:01:41,520 --> 00:01:45,100
to maximize. Even though randomized projections is, who knows

38
00:01:45,100 --> 00:01:48,070
what it's trying to maximize besides randomness. But they

39
00:01:48,070 --> 00:01:50,410
don't care about the ultimate learner, or the ultimate

40
00:01:50,410 --> 00:01:55,110
label that's associated with them. Whereas LDA does care explicitly

41
00:01:55,110 --> 00:01:57,080
about the labels and wants to find ways to

42
00:01:57,080 --> 00:02:00,750
discriminate, finds sort of projections or features that make it

43
00:02:00,750 --> 00:02:03,040
easier for you to do that discrimination. Now it

44
00:02:03,040 --> 00:02:06,580
also does not care about what learner's going to happen next,

45
00:02:06,580 --> 00:02:08,508
but it does care about the label. So

46
00:02:08,508 --> 00:02:11,090
it does end up doing something slightly different, and

47
00:02:11,090 --> 00:02:13,220
works pretty well in a world where you have

48
00:02:13,220 --> 00:02:15,960
very simple things that can be lineraly discriminated. Okay?

49
00:02:15,960 --> 00:02:16,890
>> Yeah.

50
00:02:16,890 --> 00:02:19,440
>> So that's it for the alternatives. I actually think that's pretty

51
00:02:19,440 --> 00:02:23,807
much it for, feature transformation. So why don't we wrap up. [CROSSTALK]

52
00:02:23,807 --> 00:02:25,690
>> Just a quick, just a quick question, though. [CROSSTALK] So, LDA, I've

53
00:02:25,690 --> 00:02:27,930
heard of LDA in this context before

54
00:02:27,930 --> 00:02:31,860
meaning something else. Like, latent Dirichlet allocation?

55
00:02:31,860 --> 00:02:33,450
Yeah, but that happened long after linear discriminant

56
00:02:33,450 --> 00:02:35,780
analysis and we haven't moved past the 90s.

57
00:02:35,780 --> 00:02:37,290
>> Well, I'm just saying that it does seem.

58
00:02:37,290 --> 00:02:40,230
And I think it is actually an unsupervised approach.

59
00:02:40,230 --> 00:02:42,850
>> Oh latent, oh no, absolutely. But it is well

60
00:02:42,850 --> 00:02:44,870
beyond the scope of the discussion that we're going to have here.

61
00:02:44,870 --> 00:02:46,010
>> All right. As you wish.

62
00:02:46,010 --> 00:02:49,620
>> But it is in fact. Every time you see a D you can think Dirichlet.

63
00:02:49,620 --> 00:02:50,700
>> Which is the computer science

64
00:02:50,700 --> 00:02:53,080
pronunciation. Other people pronounce it differently.

65
00:02:53,080 --> 00:02:55,400
>> Yes, because the correct way of pronouncing

66
00:02:55,400 --> 00:02:57,090
it is sort of beyond my. It has another

67
00:02:57,090 --> 00:02:59,020
syllable in there. Is it German or something?

68
00:02:59,020 --> 00:02:59,650
>> Yeah.

69
00:02:59,650 --> 00:03:03,414
>> Who is Dirichlet. Is it Dirichlet? [LAUGH] That

70
00:03:03,414 --> 00:03:06,220
sounded German. [LAUGH] Okay, so let's wrap up Michael.

71
00:03:06,220 --> 00:03:06,870
>> Alright.

72
00:03:06,870 --> 00:03:07,610
>> Okay.
