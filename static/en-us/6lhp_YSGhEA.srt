1
00:00:00,230 --> 00:00:01,270
Okay, Michael what's the answer?

2
00:00:01,270 --> 00:00:05,060
>> Well, it's still a half. But I guess we, we really should

3
00:00:05,060 --> 00:00:07,240
take into consideration those probability. So

4
00:00:07,240 --> 00:00:08,460
the number of mismatches they have, but

5
00:00:08,460 --> 00:00:12,600
the actual number of errors, the expected number of errors is like well,

6
00:00:12,600 --> 00:00:17,956
a 20th plus 20th, so like a 10th. So it's 90% correct, 10% error.

7
00:00:17,956 --> 00:00:21,600
>> Right. That's exactly right, so, what's important to see here

8
00:00:21,600 --> 00:00:25,270
is that even though you may get many examples wrong, in some

9
00:00:25,270 --> 00:00:28,610
sense some examples are more important than others. Because

10
00:00:28,610 --> 00:00:31,240
some are very rare. And if you think of error,

11
00:00:31,240 --> 00:00:33,570
or the sort of mistakes that you're making, not as

12
00:00:33,570 --> 00:00:37,000
the number of distinct mistakes you can make, but rather

13
00:00:37,000 --> 00:00:40,010
the amount of time you will be wrong, or the

14
00:00:40,010 --> 00:00:42,270
amount of time you'll make a mistake, then you can

15
00:00:42,270 --> 00:00:44,150
begin to see that it's important to think about the

16
00:00:44,150 --> 00:00:47,340
underlying distribution of examples that. You see. You buy that?

17
00:00:47,340 --> 00:00:47,710
>> Yeah.

18
00:00:47,710 --> 00:00:50,440
>> Okay, so, that notion of error

19
00:00:50,440 --> 00:00:53,990
turns out to be very important for boositng because in the

20
00:00:53,990 --> 00:00:57,360
end, boosting is going to use this trick of distributions in order

21
00:00:57,360 --> 00:01:00,220
to define what hardest is. Since we are going to have learning

22
00:01:00,220 --> 00:01:02,840
algorithms that do a pretty god job of learning on a bunch

23
00:01:02,840 --> 00:01:06,150
of examples. We're going to pass along to them a distribution

24
00:01:06,150 --> 00:01:09,430
over the examples, which is another way of saying, which examples are

25
00:01:09,430 --> 00:01:12,350
important to learn, versus which examples are not as important to

26
00:01:12,350 --> 00:01:15,920
learn. And that's where the hardest notion is going to come in.

27
00:01:15,920 --> 00:01:18,370
So, every time we see a bunch of examples, we're

28
00:01:18,370 --> 00:01:20,670
going to try to make the harder ones more important to get

29
00:01:20,670 --> 00:01:23,090
right. Than the ones that we already know how to solve.

30
00:01:23,090 --> 00:01:25,550
And I'll, I'll describe in a minute exactly how that's done.

31
00:01:25,550 --> 00:01:27,630
>> But isn't it the case that this distribution

32
00:01:27,630 --> 00:01:29,750
doesn't really matter? You should just get them all right.

33
00:01:29,750 --> 00:01:31,300
>> Sure. But now it's a question of

34
00:01:31,300 --> 00:01:33,050
how you're going to get them all right. Which

35
00:01:33,050 --> 00:01:35,580
brings me to my second definition I want to

36
00:01:35,580 --> 00:01:41,120
make. And that second definition is a weak learner.

37
00:01:41,120 --> 00:01:43,840
So there's this idea of a learning algorithm, which is

38
00:01:43,840 --> 00:01:46,940
what we mean by a learner here. As being weak. And

39
00:01:46,940 --> 00:01:49,200
that definition's actually fairly straightforward

40
00:01:49,200 --> 00:01:50,500
so straightforward in fact that you

41
00:01:50,500 --> 00:01:53,070
can sort of forget that it's really important. And all a

42
00:01:53,070 --> 00:01:56,510
weak learners is, is a learner that no matter what

43
00:01:56,510 --> 00:02:00,250
the distribution is over your data, will do better than chance

44
00:02:00,250 --> 00:02:02,770
when it tries to learn labels on that data. So what

45
00:02:02,770 --> 00:02:06,120
does does better than chance actually mean? Well what it means

46
00:02:06,120 --> 00:02:08,979
is, that no matter what the distribution over the

47
00:02:08,979 --> 00:02:12,320
data is, you're always going to have an error rate that's

48
00:02:12,320 --> 00:02:14,740
less than a half. So that means sort of

49
00:02:14,740 --> 00:02:17,580
as a formalism, is written down here below. That for

50
00:02:17,580 --> 00:02:20,040
all D, that is to say no matter what

51
00:02:20,040 --> 00:02:24,260
the distribution is, your learning algorithm We'll have an expected

52
00:02:24,260 --> 00:02:27,470
error. That is the probability that it will disagree with

53
00:02:27,470 --> 00:02:31,560
it through actual concept if you draw a single sample

54
00:02:31,560 --> 00:02:33,360
that is less than or equal to one half

55
00:02:33,360 --> 00:02:36,070
minus Epsilon. Now epsilon is a term that you end

56
00:02:36,070 --> 00:02:39,290
up seeing a lot in mathematical proofs and particularly ones

57
00:02:39,290 --> 00:02:42,520
involving machine learning. And epsilon just means a really, really

58
00:02:42,520 --> 00:02:47,020
small number somewhere between a little bigger than 0 and

59
00:02:47,020 --> 00:02:50,240
certainly much smaller than 1. So, here what this means

60
00:02:50,240 --> 00:02:53,980
technically is that you're bounded away from 1 1/2. Which

61
00:02:53,980 --> 00:02:56,560
another way of thinking about that is you always get

62
00:02:56,560 --> 00:02:58,240
some information from the learner. The

63
00:02:58,240 --> 00:03:00,830
learner's always able to learn something. Chance

64
00:03:00,830 --> 00:03:03,790
would be the case where your probability is 1/2 and you actually learn

65
00:03:03,790 --> 00:03:06,960
nothing at all which kind of ties us back into the notion of information

66
00:03:06,960 --> 00:03:10,700
gained way back when with decision trees. So does that all make sense Michael?

67
00:03:10,700 --> 00:03:14,180
>> I'm not sure that I get this right. Let's, maybe we can do

68
00:03:14,180 --> 00:03:17,910
a quiz and just kind of nail down some of the questions that I've got.

69
00:03:17,910 --> 00:03:19,820
>> Okay, sure. You got an idea for a quiz?

70
00:03:19,820 --> 00:03:20,430
>> Sure.
