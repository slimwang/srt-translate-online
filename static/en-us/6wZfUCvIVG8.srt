1
00:00:00,330 --> 00:00:03,240
So, in the first case, we will need ten bits

2
00:00:03,240 --> 00:00:07,290
for each of those ten flips. But in the second

3
00:00:07,290 --> 00:00:11,350
case, we don't need any bits because the result of

4
00:00:11,350 --> 00:00:13,660
the flip is always going to be the same. You

5
00:00:13,660 --> 00:00:16,290
don't even have to send this message. The folks at

6
00:00:16,290 --> 00:00:18,910
San Francisco will already know what is the result of

7
00:00:18,910 --> 00:00:22,030
those ten flips. So realize what we've discovered here, if

8
00:00:22,030 --> 00:00:25,330
the output of this coin is predictable, you don't need

9
00:00:25,330 --> 00:00:27,950
to communicate anything. But if the output is

10
00:00:27,950 --> 00:00:30,700
random, you need the communicate the result of each

11
00:00:30,700 --> 00:00:33,330
and every flip. So more information has to

12
00:00:33,330 --> 00:00:37,130
be translated. If the sequence is predictable or it

13
00:00:37,130 --> 00:00:41,560
has less uncertainty, then it has less information.

14
00:00:41,560 --> 00:00:46,090
Shannon described this measure as entropy. He said, if

15
00:00:46,090 --> 00:00:48,050
you had to predict the next symbol in a

16
00:00:48,050 --> 00:00:50,450
sequence, what is the minimum number of yes or

17
00:00:50,450 --> 00:00:53,640
no questions you would expect to ask. In the

18
00:00:53,640 --> 00:00:55,670
first example, you have to ask a yes or

19
00:00:55,670 --> 00:00:58,040
no question for every coin flip. So you have

20
00:00:58,040 --> 00:01:01,435
to ask at least one question for every flip.

21
00:01:01,435 --> 00:01:03,470
In the unfair coin, you don't have to ask

22
00:01:03,470 --> 00:01:07,450
any questions. So the information in the second case

23
00:01:07,450 --> 00:01:10,120
is zero, while the information in the first case

24
00:01:10,120 --> 00:01:13,400
is one. Let's consider another example to understand this better.
