1
00:00:00,580 --> 00:00:03,230
Decision Trees are powerful
modelling technique.

2
00:00:03,230 --> 00:00:06,540
But like all techniques
the algorithm can have its issues.

3
00:00:06,540 --> 00:00:09,760
Decision Trees are prone to
an error called overfitting

4
00:00:09,760 --> 00:00:12,750
where the model fits the sample
data a little too well and

5
00:00:12,750 --> 00:00:16,810
as a result does not predict future
results as well so it should.

6
00:00:16,810 --> 00:00:21,060
A technique that helps eliminate
this issue is a Random Forest Model.

7
00:00:21,060 --> 00:00:24,910
Let's go over a little background
of what a forest model actually is,

8
00:00:24,910 --> 00:00:28,870
to build out a forest model, let us
start by building a decision tree.

9
00:00:28,870 --> 00:00:32,630
If you recall, the decision tree
is constructed by looking for

10
00:00:32,630 --> 00:00:34,900
groups to split, or Data.

11
00:00:34,900 --> 00:00:38,670
Splits are chosen at places to produce
the largest difference and for this

12
00:00:38,670 --> 00:00:43,600
example, the largest difference in the
percent of the mode of transportation.

13
00:00:43,600 --> 00:00:46,930
We will continue down this path
creating different splits of the data

14
00:00:46,930 --> 00:00:50,210
until adding more splits
no longer adds value

15
00:00:50,210 --> 00:00:53,580
in predicting the correct
mode of transportation.

16
00:00:53,580 --> 00:00:57,170
Now what if we made the same tree
with slightly different data,

17
00:00:57,170 --> 00:00:58,360
would it be exactly the same?

18
00:00:59,490 --> 00:01:04,470
No, changes in the data could cause the
tree to split at different points, or

19
00:01:04,470 --> 00:01:05,720
in a different order.

20
00:01:05,720 --> 00:01:08,300
A forest model creates
hundreds of trees.

21
00:01:08,300 --> 00:01:12,680
This is call an assemble of decision
trees, where each tree is created by

22
00:01:12,680 --> 00:01:16,580
different randomly generated
chunks of the original data.

23
00:01:16,580 --> 00:01:19,980
Then it looks at the results as
a whole to make a prediction.

24
00:01:19,980 --> 00:01:24,790
So, how does this fix the overfitting
problem we mentioned before?

25
00:01:24,790 --> 00:01:29,390
Well each individual tree created
still has the overfitting issues, but

26
00:01:29,390 --> 00:01:31,360
when you look at the result as a whole.

27
00:01:31,360 --> 00:01:35,570
The overfitting gets averaged
out by all the other trees.

28
00:01:35,570 --> 00:01:39,820
The first decision tree will be created
with a subset of the original data.

29
00:01:39,820 --> 00:01:44,140
We'll also use a different combination
of predictor variables to assist with

30
00:01:44,140 --> 00:01:46,350
the splits along the trees.

31
00:01:46,350 --> 00:01:50,350
The second decision tree will
then do the exact same thing

32
00:01:50,350 --> 00:01:54,410
with a different subset of data
with different predictor variables

33
00:01:54,410 --> 00:01:57,070
being used to help with
the splits as well.

34
00:01:57,070 --> 00:02:01,040
This will continue to occur until the
number of decision trees specified has

35
00:02:01,040 --> 00:02:02,360
been created.

36
00:02:02,360 --> 00:02:04,750
By default this value
is 500 decision trees.

37
00:02:06,370 --> 00:02:09,750
Now each employee will
traverse through each tree

38
00:02:09,750 --> 00:02:11,920
until they reaches a terminal node.

39
00:02:11,920 --> 00:02:15,480
At each of the terminal nodes,
the decision tree has a vote.

40
00:02:15,480 --> 00:02:17,950
Car, public transit or bike.

41
00:02:17,950 --> 00:02:21,410
The terminal node that
is seem most often

42
00:02:21,410 --> 00:02:24,940
is then classified as the group
that employee falls within.
