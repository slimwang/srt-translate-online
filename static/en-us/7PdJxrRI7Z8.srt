1
00:00:00,000 --> 00:00:04,014
We are using CUDA to run a blurring kernel on a gray scale image

2
00:00:04,037 --> 00:00:10,180
where each pixel is a 32-bit single precision floating point number.

3
00:00:10,203 --> 00:00:16,642
Let's consider that the kernel is a 5 by 5 kernel, and we're not going to worry about where it's stored.

4
00:00:16,665 --> 00:00:24,582
Then our operation will be 25 multiplies and 24 adds to compute each pixel output.

5
00:00:24,620 --> 00:00:29,716
Naturally, we want to use shared memory to store the image so that

6
00:00:29,716 --> 00:00:33,763
we save memory bandwidth by reading each pixel as few times as possible.

7
00:00:33,763 --> 00:00:35,844
Our code will look something like this.

8
00:00:35,844 --> 00:00:40,784
You can choose how to launch this kernel in terms of threads per block configuration.

9
00:00:40,784 --> 00:00:50,301
You have 2 choices, 1024 by 1 or 32 by 32. Both choices will feature 1,024 threads per block.

10
00:00:50,301 --> 00:00:55,782
So the first question is, which 1 of these 2 configurations

11
00:00:55,782 --> 00:00:58,794
will require less global memory bandwidth?

12
00:00:58,794 --> 00:01:01,115
Please choose here.

13
00:01:01,126 --> 00:01:05,197
And the second question is, how much less was the ratio

14
00:01:05,197 --> 00:01:11,183
of global memory bandwidth between the kernel that uses more memory bandwidth and the kernel with less?

15
00:01:11,183 --> 00:01:15,953
In your calculation, please ignore the bandwidth required for the blurKernel

16
00:01:15,953 --> 00:01:19,953
and the bandwidth required for right back of v new, and please enter your answer here.
