1
00:00:00,450 --> 00:00:03,904
The reason I ask this question
is because of something called

2
00:00:03,904 --> 00:00:04,928
Occam's Razor.

3
00:00:04,928 --> 00:00:07,730
Occam can be spelled in
many different ways.

4
00:00:08,760 --> 00:00:12,960
And what Occam says is that
everything else being equal,

5
00:00:12,960 --> 00:00:15,120
choose the less complex hypothesis.

6
00:00:16,960 --> 00:00:22,690
Now in practice there's actually
a trade-off between really good data fit

7
00:00:22,690 --> 00:00:23,580
and low complexity.

8
00:00:25,590 --> 00:00:28,490
Let me illustrate this to you
by a hypothetical example.

9
00:00:29,880 --> 00:00:35,030
Consider the following graph where
the horizontal axis graphs complexity

10
00:00:35,030 --> 00:00:35,938
of the solution.

11
00:00:35,938 --> 00:00:36,799
For example,

12
00:00:36,799 --> 00:00:41,750
if you've used polynomials this might be
a high degree polynomial over here, and

13
00:00:41,750 --> 00:00:45,932
maybe a linear function over here,
which is low degree polynomial.

14
00:00:45,932 --> 00:00:50,080
The training data error
tends to go like this.

15
00:00:51,790 --> 00:00:57,630
The more complex a hypothesis you allow,
the more you can just fit your data.

16
00:00:59,200 --> 00:01:03,970
However, in reality your
generalization error on unknown data

17
00:01:03,970 --> 00:01:04,940
tends to go like this.

18
00:01:06,430 --> 00:01:10,660
It is the sum of the training
data error in another function,

19
00:01:12,010 --> 00:01:13,430
which is called the overfitting error.

20
00:01:15,340 --> 00:01:19,915
Not surprisingly the best complexity is
attained where the generalization error

21
00:01:19,915 --> 00:01:21,460
is minimum.

22
00:01:21,460 --> 00:01:24,998
And there are some of those methods
to calculate the overfitting error.

23
00:01:24,998 --> 00:01:30,210
They go into a statistical field under
the name, bias variance methods.

24
00:01:30,210 --> 00:01:33,065
However in practice, you're often
just given the training data error.

25
00:01:33,065 --> 00:01:40,006
You find, if you don't find the model
that minimizes the training data error,

26
00:01:40,006 --> 00:01:46,958
but instead pushed back on complexity
your algorithm tends to perform better.

27
00:01:46,958 --> 00:01:51,367
And that is something we will
studying a little bit in this class.

28
00:01:51,367 --> 00:01:54,704
However, this slide this
is really important for

29
00:01:54,704 --> 00:01:57,809
anybody doing machine
learning in practice.

30
00:01:57,809 --> 00:02:04,182
If you deal with data and have ways to
fit your data, be aware that overfitting

31
00:02:04,182 --> 00:02:10,110
is a major source of poor performance
of machine learning algorithms.

32
00:02:10,110 --> 00:02:12,000
And I'll give you examples
in just one second.
