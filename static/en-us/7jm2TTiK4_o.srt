1
00:00:00,630 --> 00:00:03,980
Most of the time we don't have
this transition function,

2
00:00:03,980 --> 00:00:07,280
and we don't have
the reward function either.

3
00:00:07,280 --> 00:00:08,990
So the robot, or

4
00:00:08,990 --> 00:00:14,430
the trader, whatever environment we're
in, has to interact with the world,

5
00:00:14,430 --> 00:00:19,260
observe what happens, and work with
that data to try to build a policy.

6
00:00:19,260 --> 00:00:21,490
So let me give you an example here.

7
00:00:21,490 --> 00:00:24,030
Let's say we were in state S1.

8
00:00:24,030 --> 00:00:25,710
So, that's what we observed there.

9
00:00:25,710 --> 00:00:29,579
Our robot took action, A1.

10
00:00:29,579 --> 00:00:34,430
I'm making this little subscript
to indicate which step in

11
00:00:34,430 --> 00:00:37,500
this series of steps it's at.

12
00:00:37,500 --> 00:00:40,900
We then find our self in S'.

13
00:00:40,900 --> 00:00:43,500
And we get reward R.

14
00:00:43,500 --> 00:00:47,560
Now this is an experience tubal.

15
00:00:47,560 --> 00:00:51,720
This is very similar to experience
tubal in regression learning

16
00:00:51,720 --> 00:00:55,780
where we have an X and
a Y paired together.

17
00:00:55,780 --> 00:01:00,520
That's an experience tubal of you know,
when you observe this X you see this Y.

18
00:01:00,520 --> 00:01:05,340
Here we're saying when you observe
the state, S1, you take action, A1,

19
00:01:05,340 --> 00:01:09,180
you end up in this new state, at least
it's an example of you ending up in this

20
00:01:09,180 --> 00:01:13,730
new state S1', and reward, R1.

21
00:01:13,730 --> 00:01:15,870
Now we find ourselves in a new state S2,

22
00:01:15,870 --> 00:01:20,490
but that's really,
this state is where we found our self.

23
00:01:20,490 --> 00:01:25,180
We take some new action,
A2, we end up in some

24
00:01:25,180 --> 00:01:30,050
new state, S2', and
we get a new reward, R2.

25
00:01:30,050 --> 00:01:33,260
When we do this over and
over and over and over and

26
00:01:33,260 --> 00:01:37,000
over again, gathering experience
tuples all along the way.

27
00:01:38,390 --> 00:01:42,960
Now, if we have this trail
of experience tuples,

28
00:01:42,960 --> 00:01:46,800
there's two things we can do with
them in order to find that policy pi.

29
00:01:48,070 --> 00:01:52,540
The first set of approaches is called
model based reinforcement learning.

30
00:01:52,540 --> 00:01:55,660
What we do is we look at
this data over time and

31
00:01:55,660 --> 00:02:01,320
we build a model of T just by looking
statistically at these transitions.

32
00:02:01,320 --> 00:02:05,380
In other words we can look a every
time we were in a particular state and

33
00:02:05,380 --> 00:02:07,180
took a particular action.

34
00:02:07,180 --> 00:02:09,900
And see which new states we ended up in.

35
00:02:09,900 --> 00:02:12,810
And just build a tabular
representation of that.

36
00:02:12,810 --> 00:02:14,190
It's not hard.

37
00:02:14,190 --> 00:02:17,930
Similarly, we can build a model of R.

38
00:02:17,930 --> 00:02:21,620
We just look statistically when
we're in a particular state, and

39
00:02:21,620 --> 00:02:24,500
we take an action, what's the reward?

40
00:02:24,500 --> 00:02:28,600
We can just average that
over all these instances.

41
00:02:28,600 --> 00:02:33,340
Once we have these models,
we can then use value iteration or

42
00:02:33,340 --> 00:02:36,080
policy iteration to solve the problem.

43
00:02:36,080 --> 00:02:38,750
There's another set of
approaches called model-free.

44
00:02:38,750 --> 00:02:41,720
And that's the type
we're going to focus on.

45
00:02:41,720 --> 00:02:45,510
In particular we're going to
learn about Q-learning.

46
00:02:45,510 --> 00:02:52,690
And model-free methods develop a policy
just directly by looking at the data.

47
00:02:52,690 --> 00:02:54,600
And of course we'll
talk about those soon.
