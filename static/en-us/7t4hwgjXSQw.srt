1
00:00:00,080 --> 00:00:04,220
Now, this gives us exactly what we wanted to describe quantitative at

2
00:00:04,220 --> 00:00:05,270
the beginning.

3
00:00:05,270 --> 00:00:10,490
Recall the problem, with the random variable X that takes on value X1 and X2,

4
00:00:10,490 --> 00:00:16,239
and we ask the question, what's the probability distribution between X1 and X2?

5
00:00:16,239 --> 00:00:19,280
And further, does this probability distribution,

6
00:00:19,280 --> 00:00:24,020
become more skewed to one side or the other, when we know that Y1 happened?

7
00:00:24,020 --> 00:00:28,840
So now we know, that this schemeness, is precisely captured by the entropy,

8
00:00:28,840 --> 00:00:32,020
before and after conditioning on Y1.

9
00:00:32,020 --> 00:00:38,920
Specifically, we want to look at H of X and H of X given that Y equals Y1.

10
00:00:38,920 --> 00:00:43,864
If the entropy, went down as a result of Y1 happening,

11
00:00:43,864 --> 00:00:46,080
then we know that Y has an impact on X.

12
00:00:47,130 --> 00:00:51,013
Now, the amount that H went down due to Y

13
00:00:51,013 --> 00:00:56,000
being Y1 is H of X minus H of X given Y equals Y1).

14
00:00:56,000 --> 00:00:59,561
Similarly, the amount that H went down,

15
00:00:59,561 --> 00:01:04,269
due to Y being Y2 is H of X minus H of X given Y equals Y2.

16
00:01:04,269 --> 00:01:06,670
The impact, from Y1 and

17
00:01:06,670 --> 00:01:11,540
Y2 are averaged to form what's called Conditional entropy.

18
00:01:11,540 --> 00:01:13,370
As you can see, Conditional entropy,

19
00:01:13,370 --> 00:01:18,350
is simply the expected value of H given the value of Y.

20
00:01:18,350 --> 00:01:23,494
And thus, the expected value of reduction to entropy on X

21
00:01:23,494 --> 00:01:27,390
due to knowing Y is H of X minus H of X given Y.

22
00:01:27,390 --> 00:01:32,310
Finally, this tells us what we need to know about selecting features.

23
00:01:32,310 --> 00:01:35,290
It we're interested in the random variable X.

24
00:01:35,290 --> 00:01:40,160
And we've been collecting other variables, A, B, C, and so on.

25
00:01:40,160 --> 00:01:44,230
The variable with the maximum influence over X, is the one for

26
00:01:44,230 --> 00:01:47,090
which the reduction and entropy is the largest.

27
00:01:48,380 --> 00:01:53,390
More precisely, the variable which has the biggest impact on X,

28
00:01:53,390 --> 00:01:55,910
is the arg min of this expression.

29
00:01:55,910 --> 00:01:59,280
In the next video we will apply this to identify important

30
00:01:59,280 --> 00:02:01,540
features influencing enter tweet time.
