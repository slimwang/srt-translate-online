1
00:00:00,190 --> 00:00:03,700
So let us look now at
the overall performance and

2
00:00:03,700 --> 00:00:06,590
reliability of a Raid 4 array.

3
00:00:06,590 --> 00:00:12,660
For reads, we are getting,
on average the throughput of N-1 disks.

4
00:00:12,660 --> 00:00:16,200
We can read data from all
of the data disks, but

5
00:00:16,200 --> 00:00:21,260
the parity disk has no useful data on
it, so for reads, we don't use it.

6
00:00:21,260 --> 00:00:25,150
This is significantly
improving RAID performance

7
00:00:25,150 --> 00:00:28,730
because usually we have more
than just two disks in an array.

8
00:00:28,730 --> 00:00:31,200
So for example with four disks
we are really getting for

9
00:00:31,200 --> 00:00:34,940
reads the throughput of three disks,
all of the data disks.

10
00:00:34,940 --> 00:00:40,750
For writes however, the data accesses
are distributed among the data disks.

11
00:00:40,750 --> 00:00:47,320
However, every write requires
the parity disk to read and then write.

12
00:00:47,320 --> 00:00:51,140
We need to read the old parity,
update it and then write it back.

13
00:00:52,140 --> 00:00:56,510
So we are getting only one half of
the throughput of a single disk,

14
00:00:56,510 --> 00:01:01,450
because the parity disk, again,
needs two accesses for every write.

15
00:01:01,450 --> 00:01:03,860
This is a significant disadvantage, and

16
00:01:03,860 --> 00:01:07,550
we will see that this is really the
primary reason why we need raid five.

17
00:01:07,550 --> 00:01:09,040
Now let's look at the reliability.

18
00:01:09,040 --> 00:01:12,150
What is the entity F of a raid for this?

19
00:01:12,150 --> 00:01:14,580
Again, we have two options.

20
00:01:14,580 --> 00:01:15,820
Just like raid one,

21
00:01:15,820 --> 00:01:20,270
the first period we get is while
all of the disks are working.

22
00:01:20,270 --> 00:01:24,200
The time you have during that period
is on average MTTF of the single disk

23
00:01:24,200 --> 00:01:30,630
divided by N, because that's how long we
have for the first of N disks to fail.

24
00:01:30,630 --> 00:01:35,630
Just like grade 1, after this first
failure, we did not lose the data yet.

25
00:01:35,630 --> 00:01:37,650
And now we have two options.

26
00:01:37,650 --> 00:01:40,982
If we do no repair of
the failed disk then,

27
00:01:40,982 --> 00:01:46,313
now we are operating with N minus 1
good disks, if we do no repair or

28
00:01:46,313 --> 00:01:51,931
the failed disks, what we get now is
at the point of the first failure,

29
00:01:51,931 --> 00:01:55,549
we are left with the N
minus 1 disk array, and

30
00:01:55,549 --> 00:02:01,210
any disk failure in that array
is going to result in data loss.

31
00:02:01,210 --> 00:02:06,006
So, if we don't do repair,
then to this, we add the expected time

32
00:02:06,006 --> 00:02:11,790
until the first failure among the N
minus 1 disks that we still have.

33
00:02:11,790 --> 00:02:15,930
This plus this, if N is, let's say, 4,

34
00:02:15,930 --> 00:02:20,400
it's not going to be better
than the MTTF of a single disk.

35
00:02:20,400 --> 00:02:25,360
So you don't want to even consider
using RAID 4 unless you're

36
00:02:25,360 --> 00:02:27,300
going to do repairs.

37
00:02:27,300 --> 00:02:31,340
So pretty much you don't do
this because it's a bad idea.

38
00:02:31,340 --> 00:02:33,960
You're not getting
an increased reliability.

39
00:02:33,960 --> 00:02:37,590
What you want to do is
repair the failed disk,

40
00:02:37,590 --> 00:02:43,190
in which case the chances
of a disk failing during

41
00:02:43,190 --> 00:02:48,010
the repair period is the MTTF
of what's left of our array.

42
00:02:48,010 --> 00:02:52,360
With RAID 1 we had one disk left
Here we have N-1 disk left, so

43
00:02:52,360 --> 00:02:56,930
the MTTF for the remaining disks
is lower than for one disk,

44
00:02:56,930 --> 00:03:01,600
because we are talking about
the first failure among the N disks.

45
00:03:01,600 --> 00:03:06,030
So for example if we have three disks,
the first failure among three

46
00:03:06,030 --> 00:03:10,120
is going to happen sooner than the first
failure among just the one disk.

47
00:03:10,120 --> 00:03:13,630
And then we divide that with
the mean time to repair.

48
00:03:13,630 --> 00:03:17,580
So this is the factor with which
we now multiply our, this is for

49
00:03:17,580 --> 00:03:21,710
how long we get a RAID 4 to
work until we lose a disk.

50
00:03:21,710 --> 00:03:26,920
So overall MTTF of our
RAID 4 array is going

51
00:03:26,920 --> 00:03:31,551
to be the entity of one disk times again

52
00:03:31,551 --> 00:03:36,326
the entity of one disk
divided by N time N

53
00:03:36,326 --> 00:03:40,845
minus 1 times the MTTR for the disk.

54
00:03:40,845 --> 00:03:44,445
So note that again this is
very small compared to this.

55
00:03:44,445 --> 00:03:47,745
So this particular ratio
is extremely large.

56
00:03:48,790 --> 00:03:51,320
The number of disks is, let's say, four,

57
00:03:51,320 --> 00:03:54,020
you don't want to have too
many disks in a RAID 4 array.

58
00:03:54,020 --> 00:03:58,970
If that is so, then what we get is
the MTTF of a single disk that is fairly

59
00:03:58,970 --> 00:04:03,040
large multiplied by something that
is usually in the thousands or

60
00:04:03,040 --> 00:04:07,360
more and divided by something
that is usually something like,

61
00:04:07,360 --> 00:04:10,380
if it's four disks, it's going to be 12.

62
00:04:10,380 --> 00:04:15,730
So this MTTF or RAID 4, if we repair
the failed disk every time it fails,

63
00:04:15,730 --> 00:04:18,890
we get an MTTF that is
still very very large.
