1
00:00:00,000 --> 00:00:03,000
So let's move on to Active Reinforcement Learning

2
00:00:03,000 --> 00:00:06,000
and, in particular, let's examine a simple

3
00:00:06,000 --> 00:00:10,000
approach called a Greedy Reinforcement Learner.

4
00:00:10,000 --> 00:00:13,000
And the way that works is it uses the same

5
00:00:13,000 --> 00:00:16,000
passive TD learning algorithm that we talked about,

6
00:00:16,000 --> 00:00:20,000
but, after each time we update the utilities

7
00:00:20,000 --> 00:00:23,000
or maybe after a couple of updates--you can decide how often you want to do it--

8
00:00:23,000 --> 00:00:25,000
after the change to the utilities,

9
00:00:25,000 --> 00:00:28,000
we recompute the new optimal policy, pi.

10
00:00:28,000 --> 00:00:32,000
So we throw away our old pi, pi1,

11
00:00:32,000 --> 00:00:35,000
and replace it with a new pi, pi2--

12
00:00:35,000 --> 00:00:41,000
which is a result of solving the MDP described by our new estimates of the utiliities.

13
00:00:41,000 --> 00:00:43,000
Now we have a new policy,

14
00:00:43,000 --> 00:00:45,000
and we continue learning with that new policy.

15
00:00:45,000 --> 00:00:49,000
And so, if the initial policy was flawed,

16
00:00:49,000 --> 00:00:52,000
the Greedy algorithm would tend to move away from the initial policy,

17
00:00:52,000 --> 99:59:59,999
towards a better policy--and we can show how well that works.
