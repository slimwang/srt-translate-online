1
00:00:00,310 --> 00:00:02,910
So why might we care to do this, Michael? So,

2
00:00:02,910 --> 00:00:05,130
there's a bunch of reasons but really it boils down

3
00:00:05,130 --> 00:00:08,360
to two. One is really about human beings and the

4
00:00:08,360 --> 00:00:11,660
other is really about machines and machine learning algorithms. And the

5
00:00:11,660 --> 00:00:14,330
first one really boils down to what I'll just call

6
00:00:14,330 --> 00:00:19,000
knowledge discovery, alright. It is often useful when you're thinking about

7
00:00:19,000 --> 00:00:21,000
a set of data in a problem, to be able

8
00:00:21,000 --> 00:00:25,590
to interpret the features. To figure out well, which subset features

9
00:00:25,590 --> 00:00:27,560
that I've been keeping track of say, when then

10
00:00:27,560 --> 00:00:31,220
I'm using on my sensors actually matter. And so really

11
00:00:31,220 --> 00:00:34,320
the feature selection problem for what other reasons you might

12
00:00:34,320 --> 00:00:37,070
think of doing it or often for interpretability and for

13
00:00:37,070 --> 00:00:40,970
insight. So it turns out that of the 1,000

14
00:00:40,970 --> 00:00:44,820
features that I keep track of only a few matter.

15
00:00:44,820 --> 00:00:47,650
Maybe only 10 of them matter for doing solving some

16
00:00:47,650 --> 00:00:50,850
problem of prediction. So let's think about our spam case

17
00:00:50,850 --> 00:00:53,640
for example. There are lots and lots and lots of features that

18
00:00:53,640 --> 00:00:56,430
we might care about in spam case, but it may turn out

19
00:00:56,430 --> 00:00:59,120
that after we do some kind of analysis that it really all

20
00:00:59,120 --> 00:01:03,160
boils down to something like whether you're getting mail from a Nigerian

21
00:01:03,160 --> 00:01:07,200
prince, for example. And some of these things that you thought mattered.

22
00:01:07,200 --> 00:01:11,120
Don't seem to matter. For for the purposes of solving some problem

23
00:01:11,120 --> 00:01:13,450
or, or for doing prediction. And, so, often, this is a very,

24
00:01:13,450 --> 00:01:15,850
very useful thing to do. You do this kind of thing all

25
00:01:15,850 --> 00:01:17,300
the time, Michael. Actually, if you go back

26
00:01:17,300 --> 00:01:18,940
and you look at some of the things that

27
00:01:18,940 --> 00:01:21,080
we have been giving for examples. You come up

28
00:01:21,080 --> 00:01:23,840
with problems and features taht are, for example, visible

29
00:01:23,840 --> 00:01:25,540
to two dimensions as opposed to three or

30
00:01:25,540 --> 00:01:28,200
four or five becdause it's easy for you to

31
00:01:28,200 --> 00:01:31,120
visualize them and to understand them. And for students

32
00:01:31,120 --> 00:01:33,450
to understand how the algorithms work in two d.

33
00:01:33,450 --> 00:01:34,520
>> Hm that makes sense.

34
00:01:34,520 --> 00:01:36,430
>> Does that all make sense? Okay. So that's one thing and there's, and

35
00:01:36,430 --> 00:01:38,540
this really should not be underestimated. People

36
00:01:38,540 --> 00:01:40,890
tend to ignore this a lot in the,

37
00:01:40,890 --> 00:01:43,880
in the, at least in the machine learning field, much less in the data

38
00:01:43,880 --> 00:01:47,910
mining field but this really is a sort of key and an important issue.

39
00:01:47,910 --> 00:01:51,390
But there's another reason we might care about if it's algorithmic and that is

40
00:01:51,390 --> 00:01:53,650
the curse of dimensionality. So do you

41
00:01:53,650 --> 00:01:55,160
remember what the Curse of Dimensionality is Michael?

42
00:01:55,160 --> 00:01:59,080
>> Yeah, as you add more features you may need

43
00:01:59,080 --> 00:02:01,550
exponentially need more data to kind of fill out the space.

44
00:02:01,550 --> 00:02:06,340
>> The Curse of Dimensionality says that the amount of data

45
00:02:06,340 --> 00:02:08,190
that you need grows exponentially in the

46
00:02:08,190 --> 00:02:10,970
number of features that you have. So it'd

47
00:02:10,970 --> 00:02:12,410
be really nice if you could have

48
00:02:12,410 --> 00:02:14,910
fewer features. So the problem of feature selection

49
00:02:14,910 --> 00:02:18,930
helps us, hopefully at least in a, in a nice world. To go from a

50
00:02:18,930 --> 00:02:21,550
whole bunch of features to just a few

51
00:02:21,550 --> 00:02:24,340
features, thus making the learning problem actually easier.

52
00:02:24,340 --> 00:02:27,750
>> [LAUGH] Looks a little bit like a smile, an alien smiley face now.

53
00:02:27,750 --> 00:02:30,140
>> That's right. So we came with an alien frowny face

54
00:02:30,140 --> 00:02:31,590
in the beginning and [LAUGH] then we [INAUDIBLE] with an alien

55
00:02:31,590 --> 00:02:34,190
smiley face. Okay, so you buy these reasons Michael?

56
00:02:34,190 --> 00:02:36,740
>> Yeah, I like how you managed to, you know if X

57
00:02:36,740 --> 00:02:39,790
is the input space here, you've managed to map X to Y.

58
00:02:39,790 --> 00:02:44,140
>> Michael uses the feature transformation function known as the pun.

59
00:02:44,140 --> 00:02:44,190
>> Mmm.

60
00:02:44,190 --> 00:02:49,750
>> That's really good, I like that. Okay. Anyway. Okay,

61
00:02:49,750 --> 00:02:52,200
so let's keep all this in mind when we're talking

62
00:02:52,200 --> 00:02:53,800
about all the algorithms that we're talking about in this

63
00:02:53,800 --> 00:02:56,910
lesson, and even in the future transformation lesson. The goal

64
00:02:56,910 --> 00:03:01,690
is to take, be able to use, in general, a bunch of features, because

65
00:03:01,690 --> 00:03:04,770
you think they might all be important, and then apply some kind of algorithm to

66
00:03:04,770 --> 00:03:08,320
get to just the important features. And if you can do that well, then not

67
00:03:08,320 --> 00:03:10,370
only will you understand your data better,

68
00:03:10,370 --> 00:03:12,330
But you'll also have an easier learning problem.

69
00:03:12,330 --> 00:03:13,090
>> Got it.
