1
00:00:00,160 --> 00:00:02,160
So this idea of Naive Bayes, where you have

2
00:00:02,160 --> 00:00:06,710
a network that has a label producing or, or

3
00:00:06,710 --> 00:00:10,880
conditionally producing a bunch of attribute values, is just

4
00:00:10,880 --> 00:00:13,330
a really cool and powerful idea. So one of the,

5
00:00:13,330 --> 00:00:15,544
one of the issues is that, even though inference

6
00:00:15,544 --> 00:00:19,346
in general is, is is a very difficult problem it's

7
00:00:19,346 --> 00:00:22,740
NP hard. To work out what these probabilities are,

8
00:00:22,740 --> 00:00:25,070
when you have a naive Bayes structure, it's cheap. It's,

9
00:00:25,070 --> 00:00:28,440
it's the formula that we had on the previous slide. The

10
00:00:28,440 --> 00:00:30,570
number of parameters that you need to write down, again even if

11
00:00:30,570 --> 00:00:33,970
you have a very large number of variables, it's not exponential

12
00:00:33,970 --> 00:00:38,450
in the number of variables, it's just linear. There's, two probabilities for

13
00:00:38,450 --> 00:00:42,050
each of the attributes and one probability for the class. We

14
00:00:42,050 --> 00:00:45,270
can actually estimate these probabilities. So so far, we've only been talking

15
00:00:45,270 --> 00:00:47,770
about Bayes Nets in, in not in a learning setting, but in

16
00:00:47,770 --> 00:00:50,700
a setting where we just write down what all the numbers are.

17
00:00:50,700 --> 00:00:54,780
We can actually very easily estimate these parameters. How would we

18
00:00:54,780 --> 00:00:56,660
do that? Well the odd, the easy way to do it, is

19
00:00:56,660 --> 00:01:01,220
you count. When you're trying to estimate the probability of a particular

20
00:01:01,220 --> 00:01:04,670
attribute value given a class, it's really just in your, in your

21
00:01:04,670 --> 00:01:08,170
labeled data. How often do you have an example that has an

22
00:01:08,170 --> 00:01:10,920
attribute value in that class, and then divide by the number of

23
00:01:10,920 --> 00:01:12,830
times you had that class at all, and that gives you the

24
00:01:12,830 --> 00:01:15,720
conditional probability. So this is, you know in, in the case of

25
00:01:15,720 --> 00:01:17,940
infinite data this is actually going to give you exactly the right

26
00:01:17,940 --> 00:01:20,870
number. It also connects this notion of inference that we've been

27
00:01:20,870 --> 00:01:23,670
talking about with classification. Which is mostly what this, this mini

28
00:01:23,670 --> 00:01:28,260
course has been about. So, that's really great to have a connection,

29
00:01:28,260 --> 00:01:30,450
it actually allows us to do all kinds of interesting things

30
00:01:30,450 --> 00:01:35,010
like instead of only generating what the labels are, we can actually

31
00:01:35,010 --> 00:01:37,810
generate what attributes are. We can do inference on, in, in

32
00:01:37,810 --> 00:01:41,780
any of these directions. And it turns out it's wildly successful empirically.

33
00:01:41,780 --> 00:01:45,680
So, my understanding is that Google uses a tremendous amount of Naive Bayes

34
00:01:45,680 --> 00:01:49,530
classification in what they do. If you have enough data you can estimate

35
00:01:49,530 --> 00:01:54,950
these values really well, and Naive Bayes is just remarkably good. So yeah

36
00:01:54,950 --> 00:01:59,030
so it's like unclear why we'd even have any other algorithms, right Charles?

37
00:01:59,030 --> 00:02:03,110
>> Well, there's no free lunch. But I, I gotta say I, I you know

38
00:02:03,110 --> 00:02:06,970
there's this as a famous man once said it works in practice but doesn't work

39
00:02:06,970 --> 00:02:11,800
in theory. And I'm trying to figure out how this can possibly work.

40
00:02:11,800 --> 00:02:15,680
So I noticed it's called Naive Bayes. And, I think I know why now.

41
00:02:15,680 --> 00:02:17,120
>> Alright.

42
00:02:17,120 --> 00:02:19,580
>> One is that it's well it's naive and

43
00:02:19,580 --> 00:02:23,560
in fact painfully ridiculous to believe that the bayesian

44
00:02:23,560 --> 00:02:24,854
net that you wrote up there in the upper

45
00:02:24,854 --> 00:02:27,910
right-hand corner represents the real world most of the time.

46
00:02:27,910 --> 00:02:31,540
>> Hm, I see, and why is that?

47
00:02:31,540 --> 00:02:32,270
>> Well because

48
00:02:32,270 --> 00:02:34,270
what the, what the network says is that all

49
00:02:34,270 --> 00:02:37,720
of the attributes are conditionally independent giving that you know

50
00:02:37,720 --> 00:02:41,250
the label, that just can't be true. We talked

51
00:02:41,250 --> 00:02:44,750
about this before where we were using Bayesian inference to,

52
00:02:44,750 --> 00:02:48,510
to derive the sum of squared errors that it

53
00:02:48,510 --> 00:02:51,240
makes a very strong assumption about where your errors come

54
00:02:51,240 --> 00:02:54,360
from and an even stronger assumption about where your errors

55
00:02:54,360 --> 00:02:57,270
don't come from. So you're not modeling any of the

56
00:02:57,270 --> 00:03:00,960
interrelationships, between, the different attributes and

57
00:03:00,960 --> 00:03:03,280
that just doesn't seem right. So, one

58
00:03:03,280 --> 00:03:06,600
question I have. I have two, we'll save the second one though. One question

59
00:03:06,600 --> 00:03:10,660
I have is, how in the world can it possibly be the case

60
00:03:10,660 --> 00:03:15,724
that this works in practice? Hm, that's a good question. It does. Moving on.

61
00:03:15,724 --> 00:03:18,330
>> [LAUGH] No, that's not satisfying.

62
00:03:18,330 --> 00:03:18,840
>> No?

63
00:03:18,840 --> 00:03:21,360
>> How about, how about I give it a guess? Okay?

64
00:03:21,360 --> 00:03:21,880
>> Alright.

65
00:03:21,880 --> 00:03:22,350
>> Now,

66
00:03:22,350 --> 00:03:24,660
now that I yelled at you, why don't I, why don't I give it a guess.

67
00:03:24,660 --> 00:03:24,829
>> [LAUGH]

68
00:03:24,829 --> 00:03:27,050
>> I think it comes back to one of

69
00:03:27,050 --> 00:03:29,500
the conversation we had in the previous slide. When

70
00:03:29,500 --> 00:03:31,330
I was saying well we don't have to care.

71
00:03:31,330 --> 00:03:33,390
We don't care about probabilities. And you said we

72
00:03:33,390 --> 00:03:35,390
do care about probabilities because of the question your

73
00:03:35,390 --> 00:03:38,380
asking and that was fair. But once were down

74
00:03:38,380 --> 00:03:43,070
to classification. The probabilities really don't matter. Right all

75
00:03:43,070 --> 00:03:47,570
that matters is that you get the right answers.

76
00:03:47,570 --> 00:03:50,810
So its okay I guess if the probabilities you

77
00:03:50,810 --> 00:03:53,568
get are long. So long as they're sort, sort

78
00:03:53,568 --> 00:03:55,370
of in the right direction right. That you end

79
00:03:55,370 --> 00:03:58,940
up getting the, the right label as a result.

80
00:03:58,940 --> 00:04:01,410
>> Yeah, that's a good point. That in fact

81
00:04:01,410 --> 00:04:03,861
we're introducing this idea in the context of, of

82
00:04:03,861 --> 00:04:06,312
Bayesian Inference it might actually not be so good

83
00:04:06,312 --> 00:04:09,250
at that even if it is particularly good at classification.

84
00:04:10,300 --> 00:04:12,540
>> Oh, oh actually I think I have a good example so,

85
00:04:12,540 --> 00:04:15,434
so here, here write this down. So let's imagine there are four

86
00:04:15,434 --> 00:04:18,796
actually you can use the network that you have up there okay

87
00:04:18,796 --> 00:04:19,216
>> Good.

88
00:04:19,216 --> 00:04:22,511
>> So let's say that the first attribute, I'm just going to call it A

89
00:04:22,511 --> 00:04:26,842
and the second attribute I'm going to call B, and let's say we're really, we're

90
00:04:26,842 --> 00:04:28,916
really lucky and our naÃ¯ve assumption is

91
00:04:28,916 --> 00:04:32,910
right and they really are conditionally independent. But

92
00:04:32,910 --> 00:04:36,090
let's say the third attribute, is actually

93
00:04:36,090 --> 00:04:38,200
just another way of writing down A, and

94
00:04:38,200 --> 00:04:40,550
the fourth attribute is just another way of writing down

95
00:04:40,550 --> 00:04:45,419
B. So, clearly there are interrelationships between the attributes, right?

96
00:04:45,419 --> 00:04:48,090
>> The third attiribute is the first one, the fourth attribute is

97
00:04:48,090 --> 00:04:51,032
the second one. There's not way around that. And so you'd think

98
00:04:51,032 --> 00:04:55,590
Naive Bayes would fail. But, actually, looking at your equation right below

99
00:04:55,590 --> 00:04:59,800
there where you're doing counting, I actually think, it'll work just fine.

100
00:04:59,800 --> 00:05:01,560
>> Why?

101
00:05:01,560 --> 00:05:03,380
>> Because all you're really doing

102
00:05:03,380 --> 00:05:06,080
is double counting the sort of weight of

103
00:05:06,080 --> 00:05:08,540
attribute A, but you're also double counting the

104
00:05:08,540 --> 00:05:10,980
weight of attribute B and they'll cancel each

105
00:05:10,980 --> 00:05:12,650
other out. And you'll get the right answer.

106
00:05:12,650 --> 00:05:15,060
>> When you do the arg max, but these

107
00:05:15,060 --> 00:05:15,820
>> When you do the arg max

108
00:05:15,820 --> 00:05:17,410
>> You get bad probabilities. The probabilities

109
00:05:17,410 --> 00:05:18,890
end up being kind of squared of what

110
00:05:18,890 --> 00:05:20,520
they should, what they're supposed to be.

111
00:05:20,520 --> 00:05:22,060
But that's okay because the ordering is preserved.

112
00:05:22,060 --> 00:05:25,780
>> Right, exactly. And so, even if you're unlucky and

113
00:05:25,780 --> 00:05:29,170
the fourth attribute wasn't B but it was something else, C.

114
00:05:29,170 --> 00:05:31,200
It doesn't matter if you double count A as

115
00:05:31,200 --> 00:05:32,700
long as it still gives you the right label.

116
00:05:32,700 --> 00:05:34,826
And you can imagine that if you have weak

117
00:05:34,826 --> 00:05:39,630
inner relationships or, you know, you have enough attributes and,

118
00:05:39,630 --> 00:05:41,370
and so on that you would still get the

119
00:05:41,370 --> 00:05:44,780
right, you know, yes this is the correct label, even

120
00:05:44,780 --> 00:05:46,780
if you've got the probabilities wildly wrong. Okay, so

121
00:05:46,780 --> 00:05:49,710
I'm willing to believe that that could happen in practice.

122
00:05:49,710 --> 00:05:49,930
>> Okay.

123
00:05:49,930 --> 00:05:54,870
>> So in fact, my guess is that Naive Bayes believes

124
00:05:54,870 --> 00:05:58,160
it's answer too much. But it doesn't matter if it happens to be right.

125
00:05:58,160 --> 00:06:00,465
>> All right and did you have other issues with it?

126
00:06:00,465 --> 00:06:03,270
>> So the second problem I have actually boils down to that

127
00:06:03,270 --> 00:06:06,320
equation you wrote there. So it's really nice and neat that you

128
00:06:06,320 --> 00:06:10,490
can compute the probabilities of seeing an attribute, given a value by

129
00:06:10,490 --> 00:06:14,740
just doing counting. But, I don't have an infinite amount of data, right?

130
00:06:14,740 --> 00:06:15,980
>> Not on a bad day, no.

131
00:06:15,980 --> 00:06:17,530
>> No. Or even on a good day I usually

132
00:06:17,530 --> 00:06:19,870
don't have an infinite amount of data. So what if

133
00:06:19,870 --> 00:06:25,040
I'm unlucky enough that for some particular attribute value,

134
00:06:25,040 --> 00:06:28,410
I have never seen it paired with that label, V.

135
00:06:28,410 --> 00:06:32,650
>> Right. So then, that means this numerator will be zero

136
00:06:32,650 --> 00:06:34,050
>> Right.

137
00:06:34,050 --> 00:06:34,960
>> So.

138
00:06:34,960 --> 00:06:36,810
>> Well that numerator is zero, but since

139
00:06:36,810 --> 00:06:40,120
the computation involves a product by just having

140
00:06:40,120 --> 00:06:43,170
one attribute value that I've never seen before.

141
00:06:43,170 --> 00:06:46,020
I'm going to end up saying well the probability

142
00:06:46,020 --> 00:06:50,170
of that entire product of seeing that value given

143
00:06:50,170 --> 00:06:52,180
a set of attributes is also going to be zero. So

144
00:06:52,180 --> 00:06:55,310
one unseen attribute, basically says it doesn't matter what else

145
00:06:55,310 --> 00:06:57,620
is going on. Which seems a little weird, right? You,

146
00:06:57,620 --> 00:07:00,080
you, you'd think that you, if all the other

147
00:07:00,080 --> 00:07:03,930
attributes are screaming yes, yes, yes, yes, it should be

148
00:07:03,930 --> 00:07:07,770
positive. But just because you haven't happened to have seen

149
00:07:07,770 --> 00:07:11,100
any examples of some other one single attribute, that shouldn't

150
00:07:11,100 --> 00:07:12,430
be enough to do veto.

151
00:07:12,430 --> 00:07:14,090
>> Good point, so in fact that's not what

152
00:07:14,090 --> 00:07:18,380
people often do. People will often, what they call smooth

153
00:07:18,380 --> 00:07:22,340
the probabilities, by essentially initializing the count, so that

154
00:07:22,340 --> 00:07:25,480
nothing is zero, everything has a tiny little non-zero value

155
00:07:25,480 --> 00:07:28,230
in it. And there's, there's smarter and less smart

156
00:07:28,230 --> 00:07:30,150
ways of doing that, but no, you're absolutely right. That,

157
00:07:30,150 --> 00:07:32,590
that is, that zeroing out problem is a real

158
00:07:32,590 --> 00:07:34,530
thing and you have to be a little bit careful.

159
00:07:34,530 --> 00:07:36,090
>> Hey, hey I just had a thought. So,

160
00:07:36,090 --> 00:07:38,800
if you, you have to do that, because if you don't do

161
00:07:38,800 --> 00:07:43,260
that, then you're believing your data too much. You're kind of over fitting.

162
00:07:43,260 --> 00:07:45,930
>> Ooh. Over fitting comes up again.

163
00:07:45,930 --> 00:07:49,310
>> Oh, oh, it's okay, okay so, so, so, so, so bear with

164
00:07:49,310 --> 00:07:52,200
me on this Michael. So if you're over fitting by believing the data,

165
00:07:52,200 --> 00:07:56,270
and you're fixing it by smooth, I usually spell it with a V,

166
00:07:56,270 --> 00:08:01,100
but whatever. If you, you'd think that by being smooth, then you're making

167
00:08:01,100 --> 00:08:04,200
an assumption. There's a kind of inductive

168
00:08:04,200 --> 00:08:07,690
bias, right? Your'e, you're saying that I go

169
00:08:07,690 --> 00:08:10,920
in with the assumption that they're sort

170
00:08:10,920 --> 00:08:13,020
of all things are at least mildly possible.

171
00:08:13,020 --> 00:08:13,324
>> Good.

172
00:08:13,324 --> 00:08:13,824
>> Huh.

173
00:08:14,960 --> 00:08:16,290
>> Yea, that's, that's right.

174
00:08:16,290 --> 00:08:19,470
>> Okay, Naive Bayes is cool, you've convinced me.

175
00:08:19,470 --> 00:08:20,220
>> Nice.
