1
00:00:00,610 --> 00:00:03,520
Probably the most important consideration in cache design is the

2
00:00:03,520 --> 00:00:07,440
trade off between speed and capacity. To help understand the impact

3
00:00:07,440 --> 00:00:09,940
of this trade off, I'd like to visualize requests from the

4
00:00:09,940 --> 00:00:13,390
CPU to Main Memory like this, where the orange lines represent

5
00:00:13,390 --> 00:00:16,260
the requests to the CPU. We would like the cache hit

6
00:00:16,260 --> 00:00:19,250
rate to be high and the speed to be fast. These

7
00:00:19,250 --> 00:00:22,880
goals, however, are somewhat contradictory. We can have a small but

8
00:00:22,880 --> 00:00:25,710
fast cache and just accept that this would have a low

9
00:00:25,710 --> 00:00:28,600
hit rate, being only able to intercept quickly a few of

10
00:00:28,600 --> 00:00:31,200
the requests from the CPU. Or we could have a large

11
00:00:31,200 --> 00:00:34,230
cache, but this would be slow there. Either because the physical

12
00:00:34,230 --> 00:00:37,610
limitations like the speed of light, or because of the high cost

13
00:00:37,610 --> 00:00:42,450
of fast hardware. So we intercept more requests from the CPU.

14
00:00:42,450 --> 00:00:45,050
But, we aren't able to return them as quickly. What is

15
00:00:45,050 --> 00:00:47,740
the right trade off? Well, it's hard to say. But, fortunately

16
00:00:47,740 --> 00:00:50,800
we don't have to make just one choice. We can trade off

17
00:00:50,800 --> 00:00:54,320
side versus speed multiple times at various levels within

18
00:00:54,320 --> 00:00:57,410
something called the cache hierarchy. What is not in

19
00:00:57,410 --> 00:01:00,500
the CPU registers, we look for in an L1

20
00:01:00,500 --> 00:01:03,450
cache. What's not in an L1 cache, we look

21
00:01:03,450 --> 00:01:07,576
in the L2 cache. A multicore processors, there might

22
00:01:07,576 --> 00:01:10,420
be even a third level of cache. What's not

23
00:01:10,420 --> 00:01:13,940
there, finally we look for in main memory. Even

24
00:01:13,940 --> 00:01:15,800
main memory, however, we can think of as a

25
00:01:15,800 --> 00:01:19,050
cache for what is on disk. Or we can also think

26
00:01:19,050 --> 00:01:22,350
about maybe main memory as a cache for what's on the file

27
00:01:22,350 --> 00:01:26,330
system. Notice here that the top is smaller, faster and costlier

28
00:01:26,330 --> 00:01:29,360
per byte. And as we go down the hierarchy, we get things

29
00:01:29,360 --> 00:01:32,370
that are bigger, slower and cheaper per byte. And if we

30
00:01:32,370 --> 00:01:34,320
do a good job of keeping the data that we're going to

31
00:01:34,320 --> 00:01:37,860
access next in the higher levels of the hierarchy. That is, if

32
00:01:37,860 --> 00:01:40,830
we can keep our hit rates high, then making at the speed

33
00:01:40,830 --> 00:01:43,940
of the top part of the pyramid that has the capacity in

34
00:01:43,940 --> 00:01:46,760
the overall cost per byte of the lower end of the pyramid.
