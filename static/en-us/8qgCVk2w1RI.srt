1
00:00:00,116 --> 00:00:03,036
We compile it and run it.

2
00:00:03,036 --> 00:00:09,203
I made a really small matrix to begin with so we can just see that it's acting correctly.

3
00:00:09,203 --> 00:00:17,869
So as you see our input matrix goes from 0 to 63, and the transposed version does the same thing down the columns instead of the rows.

4
00:00:17,869 --> 00:00:22,236
So we've written correct code, as we expected, and we're just going

5
00:00:22,236 --> 00:00:25,961
to store this routine for generating this transpose in the gold matrix,

6
00:00:25,961 --> 00:00:29,127
and we'll use that to compare so we don't have to keep printing out these matrices.

7
00:00:29,127 --> 00:00:35,200
For the rest of the lecture, I'll leave a smaller window so that we can see the results

8
00:00:35,200 --> 00:00:37,197
without actually printing out the matrices.

9
00:00:37,197 --> 00:00:42,628
Ok, so here's how you code transpose on a serial machine like a CPU, and here's how you call it.

10
00:00:42,628 --> 00:00:56,796
So let's do our first CUDA version. We'll copy this, paste it, make it a kernel, rename it.

11
00:00:56,796 --> 00:01:00,394
So now we've got a kernel that will operate on the GPU and call it.

12
00:01:00,394 --> 00:01:08,866
We need to do a few things. We need to allocate space for the input and output matrix on the device,

13
00:01:08,866 --> 00:01:16,595
so allocate those, and we need to copy the input matrix onto the device,

14
00:01:16,595 --> 00:01:20,828
call our kernel, so what you see here is I've taken that same kernel that we wrote,

15
00:01:20,828 --> 00:01:23,570
a serial kernel, I've called it on a single thread,

16
00:01:23,570 --> 00:01:29,801
running at a single block, passed in d_in and d_out and then I copy my result and we're done.

17
00:01:29,801 --> 00:01:34,812
See, that was quick. GPU programming is easy. I'm actually making an important point here.

18
00:01:34,812 --> 00:01:38,652
Okay, this code is going to run correctly, and it was really easy to write.

19
00:01:38,652 --> 00:01:41,815
And the problem, of course, is that it won't perform very well, right?

20
00:01:41,815 --> 00:01:45,481
It's using a single thread, which is a really small fraction of the horsepower of a GPU,

21
00:01:45,481 --> 00:01:49,550
especially a high-end GPU which can run tens of thousands of threads concurrently.

22
00:01:49,550 --> 00:01:54,018
So let's go ahead and time this. Okay, so I've added some timer code.

23
00:01:54,018 --> 00:01:56,760
Okay, we're going to start the timer, measure it,

24
00:01:56,760 --> 00:02:01,786
and I also added--I'm using a helper function here called compare matrices,

25
00:02:01,786 --> 00:02:08,816
which just compares the output to the golden matrix that we calculated to make sure that we did the transpose correctly.

26
00:02:08,816 --> 00:02:12,717
Lets compile again, run again.

27
00:02:12,717 --> 00:02:21,314
Great, so, yes, we performed the transpose successfully, and it took some very small fraction of a millisecond,

28
00:02:21,314 --> 00:02:25,483
but this was a really small problem. Lets make this problem bigger again,

29
00:02:25,483 --> 00:02:32,050
say 1024 by 1024 matrix. So now we've got roughly a million elements to touch,

30
00:02:32,050 --> 00:02:34,212
and we're going to do this in a single thread.

31
00:02:34,212 --> 00:02:38,212
Okay, now this is taking almost half a second; that's a long time.
