1
00:00:00,260 --> 00:00:02,600
Welcome back to Computer Vision.

2
00:00:02,600 --> 00:00:07,130
Today we're going to be learning
about a technique that is important,

3
00:00:07,130 --> 00:00:10,430
in general, but
we sort of slid it here and

4
00:00:10,430 --> 00:00:14,960
we're talking about recognition under
the generative model description.

5
00:00:14,960 --> 00:00:18,720
So, last time we introduced which
I hope for you is the last time.

6
00:00:18,720 --> 00:00:19,970
Certainly for me it was the last time.

7
00:00:21,020 --> 00:00:22,760
They say,
you never forget your first time.

8
00:00:22,760 --> 00:00:24,120
Me, I can barely remember the last time.

9
00:00:26,190 --> 00:00:29,310
Last time we introduced generative
models for classification.

10
00:00:29,310 --> 00:00:31,630
And there are a couple of
points about generative models.

11
00:00:31,630 --> 00:00:33,060
Meghan's still giggling
in the back there.

12
00:00:34,720 --> 00:00:38,870
One of the points was that in general
models you build the new model for

13
00:00:38,870 --> 00:00:41,240
every new object category.

14
00:00:41,240 --> 00:00:42,830
And you don't really
think too much about

15
00:00:42,830 --> 00:00:44,520
the other objects that you know about.

16
00:00:44,520 --> 00:00:46,140
And you just you,
you build the description.

17
00:00:46,140 --> 00:00:49,370
And then when new things come in you
see how likely is the new thing for

18
00:00:49,370 --> 00:00:51,860
each of the description,
each of the models that you have.

19
00:00:51,860 --> 00:00:52,840
But there was another point and

20
00:00:52,840 --> 00:00:56,160
that was perhaps a little more subtle
because it was kind of Flew by.

21
00:00:56,160 --> 00:01:01,320
And that is that generative models
work best in lower dimensional spaces.

22
00:01:01,320 --> 00:01:05,290
And what I mean by that is since we're
going to build these probabilistic

23
00:01:05,290 --> 00:01:09,910
descriptions usually some sort of
probability density function, it's only

24
00:01:09,910 --> 00:01:14,670
plausible to estimate those, and you've
got a modest number of dimensions.

25
00:01:14,670 --> 00:01:16,550
If you have very high dimen,
number of dimensions, or

26
00:01:16,550 --> 00:01:19,910
even, even, even just a moderate
number of dimensions.

27
00:01:19,910 --> 00:01:24,860
To estimate a real probability density
requires lots and lots and lots of data.

28
00:01:24,860 --> 00:01:27,660
So today what we're going to do
is we're going to learn about one

29
00:01:27,660 --> 00:01:30,760
of the most common and, and, perhaps, I
don't want to sa, call it important, but

30
00:01:30,760 --> 00:01:34,160
it, but, just everybody would expect
that you would understand it.

31
00:01:34,160 --> 00:01:36,230
Method of dimensionality reduction,

32
00:01:36,230 --> 00:01:39,300
which referred to as
principal component analysis.

33
00:01:39,300 --> 00:01:42,530
You may have seen this in other
signal stuff, and maybe even in one

34
00:01:42,530 --> 00:01:46,380
of your algebra, but it became,
pretty significant in Computer Vision

35
00:01:46,380 --> 00:01:49,340
because of this particular piece
of work that I'll tell you about.

36
00:01:49,340 --> 00:01:51,980
And it, because of that it
gets used all the time.
