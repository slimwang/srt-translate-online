1
00:00:00,122 --> 00:00:03,705
Let me come back to one piece of this code that needs a little more explanation,

2
00:00:03,705 --> 00:00:05,307
and that's this kernel call.

3
00:00:05,307 --> 00:00:07,580
Here's the name of the kernel call, square,

4
00:00:07,580 --> 00:00:11,215
and we call it with these launch parameters with these arguments.

5
00:00:11,215 --> 00:00:16,087
There's a lot going on in this call so I need to explain details that we didn't need in our example

6
00:00:16,087 --> 00:00:18,421
that are necessary as we move forward.

7
00:00:18,421 --> 00:00:24,461
What I told you is that we were launching one block of 64 threads, and that's absolutely true.

8
00:00:24,461 --> 00:00:27,797
But let me explain what's happening under the hood in a little more detail.

9
00:00:27,797 --> 00:00:33,415
When you launch a kernel, you specify both the number of blocks and the number of threads per block.

10
00:00:33,415 --> 00:00:37,609
In our example, we only had one block of 64 threads.

11
00:00:37,609 --> 00:00:39,912
But we want to run bigger problems than this.

12
00:00:39,912 --> 00:00:43,313
What you need to know about the hardware right now is two things.

13
00:00:43,313 --> 00:00:47,084
One, it is capable of running many blocks at the same time.

14
00:00:47,084 --> 00:00:50,922
Two, each block has a maximum number of threads that it can support.

15
00:00:50,922 --> 00:00:56,927
Newer GPUs can support 1024 threads per block; older GPUs can only support 512.

16
00:00:56,927 --> 00:01:01,604
When you have lots of work to do, you'll divide that work into any number of blocks

17
00:01:01,604 --> 00:01:06,435
each of which had no more than 512 or possibly 1,024 threads.

18
00:01:06,435 --> 00:01:12,806
If we wanted to launch 128 threads and square the values in each of them instead of 64 threads,

19
00:01:12,806 --> 00:01:17,313
we could change this call to square of 1,128.

20
00:01:17,313 --> 00:01:23,116
If we wanted to launch 1280 threads instead we could call square of 10,128,

21
00:01:23,116 --> 00:01:26,386
launching ten blocks of 128 threads each.

22
00:01:26,386 --> 00:01:31,894
Or square 5,256, we're launching five blocks of 256 threads each.

23
00:01:31,894 --> 00:01:37,533
But we can't call square 1,1280 because that's too many threads per block.

24
00:01:37,533 --> 00:01:42,469
You should pick the breakdown of threads and blocks that makes the most sense for your problem.

25
00:01:42,469 --> 00:01:46,833
As we saw before, each thread that we launch knows its index within that block.

26
00:01:46,833 --> 00:01:52,438
And you won't be surprised to hear that each thread also knows the index of its block as well.

27
00:01:52,438 --> 00:01:55,341
We'll see how we access this information in a moment.

28
00:01:55,341 --> 00:01:59,812
Now how do these kernels actually map into threads and blocks?

29
00:01:59,812 --> 00:02:06,321
Well, when we square 10,128, we're going to launch 10 thread blocks of 128 threads each.

30
00:02:06,321 --> 00:02:12,825
When we square 5,256, we'll launch 5 consecutive thread blocks of 256 threads each.

31
00:02:12,825 --> 00:02:18,302
These diagrams are one dimensional, they only progress in one dimension, the x dimension.

32
00:02:18,302 --> 00:02:20,838
That's fine if your problem is one dimensional.

33
00:02:20,838 --> 00:02:23,168
But many problems are 2 or 3 dimensional.

34
00:02:23,168 --> 00:02:25,479
You'll be doing image processing in the homeworks, for instance,

35
00:02:25,479 --> 00:02:28,014
and that's very definitely a two dimensional problem.

36
00:02:28,014 --> 00:02:33,522
It makes sense that CUDA would natively support not only one dimensional layouts of blocks and threads,

37
00:02:33,522 --> 00:02:37,194
like we're showing here, but also 2 and 3 dimension as well.

38
00:02:37,194 --> 00:02:43,664
For instance, perhaps we like to process this 128 x 128 pixel image.

39
00:02:43,664 --> 00:02:45,867
We'd like to launch one thread per pixel.

40
00:02:45,867 --> 00:02:50,539
We might choose, for instance, to launch these 128 x 128 threads

41
00:02:50,539 --> 00:02:55,770
as 128 blocks in the y-dimension where each one of those blocks

42
00:02:55,770 --> 00:03:00,699
is a 128 by 1 block of threads in the x-dimension.

43
00:03:00,699 --> 00:03:10,377
Or we might instead choose to launch an 8 x 8 grid of blocks where each block is 16 threads by 16 threads.

44
00:03:10,377 --> 00:03:16,046
So CUDA supports 1, 2, or 3 dimensional thread blocks.

45
00:03:16,046 --> 00:03:20,684
We can also arrange thread blocks into 1, 2, or 3 dimensional grids.

46
00:03:20,684 --> 00:03:23,355
Now, let's return to how we launch kernels.

47
00:03:23,355 --> 00:03:26,391
We put two parameters in the triple chevrons.

48
00:03:26,391 --> 00:03:29,676
The first is the dimensionality of the grid of blocks,

49
00:03:29,676 --> 00:03:33,094
and the second is the dimensionality of the threads within a block.

50
00:03:33,094 --> 00:03:35,775
We can specify up to three dimensions for each.

51
00:03:35,775 --> 00:03:38,843
But if we don't specify a dimension, it defaults to one.

52
00:03:38,843 --> 00:03:43,582
More generally, you can specify a 3 dimensional configuration for grid of blocks

53
00:03:43,582 --> 00:03:49,920
or block of threads with this dim3 struct which you initialize as dim3 x,y,z.

54
00:03:49,920 --> 00:03:54,527
Recall that, again, if we don't specify y or z, they default to 1.

55
00:03:54,527 --> 00:04:01,532
When we say dim3 of w, that means the same thing as dim3 of w,1,1,

56
00:04:01,532 --> 00:04:05,503
and you can actually abbreviate this with simply the integer w.

57
00:04:05,503 --> 00:04:17,478
When we specified square of 1,64, that was equivalent to square of dim3 1,1,1, dim3 64,1,1.
