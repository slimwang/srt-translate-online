1
00:00:00,326 --> 00:00:02,160
Alright, Charles [LAUGH], let's, let's get back to

2
00:00:02,160 --> 00:00:04,540
the task at hand, and you thought maybe you

3
00:00:04,540 --> 00:00:08,300
had some ideas. So, tell, tell me, how can we define U and pi in terms of Q?

4
00:00:08,300 --> 00:00:11,283
>> Okay. Well, so I guess the first thing

5
00:00:11,283 --> 00:00:14,000
to observe is that U is a value. Right?

6
00:00:14,000 --> 00:00:18,510
>> U are a value. Okay. Yes. U is a value. That's correct.

7
00:00:18,510 --> 00:00:20,030
>> [LAUGH] U returns a number. A

8
00:00:20,030 --> 00:00:23,610
scalar, in particular, where pi returns an action.

9
00:00:23,610 --> 00:00:25,260
>> Yeah, U of s returns a scalar,

10
00:00:25,260 --> 00:00:27,060
that's right, and pi of s returns an action, very good.

11
00:00:27,060 --> 00:00:29,400
>> Right, okay. And, you know, we have the

12
00:00:29,400 --> 00:00:31,790
definition for U up there near the top, and it's

13
00:00:31,790 --> 00:00:34,100
just the reward and then sort of behaving optimally

14
00:00:34,100 --> 00:00:37,820
thereafter, where Q is a reward and then taking a

15
00:00:37,820 --> 00:00:42,350
specific action, and and behaving optimally there after. So,

16
00:00:42,350 --> 00:00:44,550
we can sort of turn Q into U, if we

17
00:00:44,550 --> 00:00:47,250
always pick the best action. And we know the best

18
00:00:47,250 --> 00:00:51,140
action is, it's the one that, you know maximizes the

19
00:00:51,140 --> 00:00:54,300
value that you're going to get from that point on. So, I would say that

20
00:00:54,300 --> 00:01:00,350
U of s could be defined as Max over a of Q, s comma a.

21
00:01:00,350 --> 00:01:01,900
>> Simplicity itself.

22
00:01:01,900 --> 00:01:02,772
>> Mm-hm.

23
00:01:02,772 --> 00:01:04,780
>> Nicely done. I have no idea how we're going to grade

24
00:01:04,780 --> 00:01:08,240
that automatically, because it's hard to type a's under other a's. But,

25
00:01:08,240 --> 00:01:11,340
yeah, that's exactly right, that we have the maximization over all possible

26
00:01:11,340 --> 00:01:14,930
actions of the Q value in that state. Like, that's just great!

27
00:01:14,930 --> 00:01:15,483
>> Mm-hm

28
00:01:15,483 --> 00:01:16,370
>> How about

29
00:01:16,370 --> 00:01:16,960
the policy?

30
00:01:16,960 --> 00:01:20,051
>> So the policy will look exactly the same, the, the,

31
00:01:20,051 --> 00:01:25,150
the policy that you want to follow anyway is the one that...

32
00:01:25,150 --> 00:01:29,450
Maximizes your value going forward except the differences. It's returning an

33
00:01:29,450 --> 00:01:32,320
A and not an actual value, so it should be argmax.

34
00:01:32,320 --> 00:01:34,260
>> Oh, So it's not exactly the same.

35
00:01:34,260 --> 00:01:34,710
>> Right.

36
00:01:34,710 --> 00:01:36,090
>> It's almost exactly the same.

37
00:01:36,090 --> 00:01:37,838
>> Rrâ€¦ max

38
00:01:37,838 --> 00:01:39,238
>> Right, so this is, it's just so

39
00:01:39,238 --> 00:01:41,664
trivial, so, so Q gives us everything we need.

40
00:01:41,664 --> 00:01:44,460
If only we had a good way of finding Q.

41
00:01:44,460 --> 00:01:47,870
>> Yeah, if only, if only we could find a Q, a Q for you.

42
00:01:47,870 --> 00:01:52,040
>> [LAUGH] and that's going to be the essense of Q LEARNING.

43
00:01:52,040 --> 00:01:52,980
>> Oh, well done.
