1
00:00:00,440 --> 00:00:02,320
Let's formalize this a little bit.

2
00:00:02,320 --> 00:00:04,910
What we've been working with
is something called a Markov

3
00:00:04,910 --> 00:00:06,490
decision problem.

4
00:00:06,490 --> 00:00:09,050
And here's what makes up
a Markov decision problem.

5
00:00:10,120 --> 00:00:13,410
There are a set of states S.

6
00:00:13,410 --> 00:00:17,540
Those are all the values that this S
can take as it comes into the robot.

7
00:00:18,610 --> 00:00:20,970
There's a set of actions A,

8
00:00:20,970 --> 00:00:24,290
which is these potential actions we
can take to act on the environment.

9
00:00:25,680 --> 00:00:27,060
There's a transition function.

10
00:00:28,120 --> 00:00:31,740
This is the T within the environment.

11
00:00:31,740 --> 00:00:35,590
And this is a little bit complicated,
but let's just step through it.

12
00:00:35,590 --> 00:00:41,410
T is a three-dimensional object,
and it records in each of its cells

13
00:00:41,410 --> 00:00:45,560
the probability that if
we are in state S and

14
00:00:45,560 --> 00:00:50,900
we take action A,
we will end up in state S prime.

15
00:00:50,900 --> 00:00:56,730
Something to note about this transition
function is, suppose we're in state,

16
00:00:56,730 --> 00:01:00,170
a particular state S and
we take a particular action A.

17
00:01:01,430 --> 00:01:08,160
The sum of all the next states we
might end up in has to sum to one.

18
00:01:08,160 --> 00:01:10,880
In other words, with probability one,

19
00:01:10,880 --> 00:01:15,040
we're going to end up in some new state,
but the distribution of probabilities

20
00:01:15,040 --> 00:01:20,330
across these different states is what
makes this informative and revealing.

21
00:01:20,330 --> 00:01:24,040
Finally, an important component
of Markov decision problems

22
00:01:24,040 --> 00:01:25,690
is the reward function.

23
00:01:25,690 --> 00:01:27,220
And that's what gives us the reward.

24
00:01:27,220 --> 00:01:28,720
If we're in a particular state and

25
00:01:28,720 --> 00:01:32,570
we take an action A,
we get a particular reward.

26
00:01:32,570 --> 00:01:36,100
So if we have all of
these things defined,

27
00:01:36,100 --> 00:01:39,370
we have what's called
a Markov decision problem.

28
00:01:39,370 --> 00:01:43,250
Now, the problem for a reinforcement
learning algorithm is to find

29
00:01:43,250 --> 00:01:48,810
this policy pi that will
maximize reward over time.

30
00:01:48,810 --> 00:01:51,440
And, in fact,
if it finds the optimal policy,

31
00:01:51,440 --> 00:01:55,580
we give it a little symbol pi starred
to indicate that it's optimal.

32
00:01:56,770 --> 00:02:02,020
Now, if we have these, and,
in particular, if we have T and

33
00:02:02,020 --> 00:02:07,380
R, there are algorithms we can unleash
that will find this optimal policy.

34
00:02:07,380 --> 00:02:10,900
Two of them are policy iteration and
value iteration.

35
00:02:12,070 --> 00:02:17,700
Now, in this class, we don't
start off knowing T and R, and so

36
00:02:17,700 --> 00:02:22,110
we're not going to be able to use these
algorithms directly to find this policy.
