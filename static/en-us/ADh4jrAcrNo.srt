1
00:00:00,470 --> 00:00:03,080
That's sort of how to do
sampling efficiently.

2
00:00:03,080 --> 00:00:07,240
The other thing is,
how often do you have to resample?

3
00:00:07,240 --> 00:00:10,810
Well, in the algorithm that I showed
you, we resample all the time, and

4
00:00:10,810 --> 00:00:14,340
that's sort of we have a whole bunch of
samples that are about uniform weight,

5
00:00:14,340 --> 00:00:15,240
and we spread them out.

6
00:00:16,379 --> 00:00:20,370
If I don't have too many samples
that have very large weights,

7
00:00:20,370 --> 00:00:21,550
maybe I don't have to resample.

8
00:00:21,550 --> 00:00:25,490
Maybe I could just reuse the weights
the same samples over again.

9
00:00:25,490 --> 00:00:26,090
Right?
I'd still have to

10
00:00:26,090 --> 00:00:30,270
apply the deterministic part, and
I have to apply the expansion part, but

11
00:00:30,270 --> 00:00:32,320
I don't have to do that resampling.

12
00:00:32,320 --> 00:00:34,960
And you can do that by just saying,

13
00:00:34,960 --> 00:00:37,580
let me take a look at sort of
the variation in the weights.

14
00:00:37,580 --> 00:00:39,480
I want them as uniform as possible.

15
00:00:39,480 --> 00:00:42,010
And as long as they have a certain
amount of uniformity to them,

16
00:00:42,010 --> 00:00:42,910
I won't resample.

17
00:00:42,910 --> 00:00:45,640
So that reduces the number
of times you do resampling.

18
00:00:45,640 --> 00:00:49,990
A second problem is
what if p of z given x.

19
00:00:49,990 --> 00:00:53,180
What if your likelihood
is incredibly peaked.

20
00:00:53,180 --> 00:00:57,750
Remember, you multiply the likelihood
times the weights of the samples.

21
00:00:57,750 --> 00:00:58,990
If it's incredibly peaked,

22
00:00:58,990 --> 00:01:02,369
that means that p of z given x
is zero almost everywhere else.

23
00:01:03,500 --> 00:01:06,270
That's not very helpful,
because what that means is,

24
00:01:06,270 --> 00:01:12,360
you've wiped out all of your your,
your possible predictions.

25
00:01:12,360 --> 00:01:17,380
In general, you want to allow for
noise in your p of z given x.

26
00:01:17,380 --> 00:01:20,370
Don't be so systematic about it.

27
00:01:20,370 --> 00:01:23,380
And you also might want to

28
00:01:23,380 --> 00:01:27,710
be better at sort of how you do your
your proposals to begin with, right?

29
00:01:27,710 --> 00:01:30,760
So you want to make sure
that particle filter

30
00:01:30,760 --> 00:01:32,510
sort of spreads out to
points where to begin with.

31
00:01:32,510 --> 00:01:35,650
And one way of doing that actually
is if you think of a sample,

32
00:01:35,650 --> 00:01:39,080
each sample you can think of as having
its own little mini Kalman filter.

33
00:01:39,080 --> 00:01:43,240
So you can think of it as having a
Gaussian that sort of spreads itself out

34
00:01:43,240 --> 00:01:44,110
and goes forward.

35
00:01:44,110 --> 00:01:49,440
In general, you're much better off
overestimating noise and let your

36
00:01:49,440 --> 00:01:54,390
measurements sort of narrow you down,
than underestimating noise and have

37
00:01:54,390 --> 00:01:57,140
your measurements think that they're
much more accurate than they are.

38
00:01:57,140 --> 00:01:59,680
Or I should say much more
constraining than they are.

39
00:01:59,680 --> 00:02:01,650
Another issue is recovery from failure.

40
00:02:01,650 --> 00:02:05,440
Suppose an object really
did disappear and reappear.

41
00:02:05,440 --> 00:02:09,840
If I have no particles there, I don't
have any way of tracking it, ever.

42
00:02:09,840 --> 00:02:14,460
So, a standard thing to do is
each iteration, you randomly put

43
00:02:14,460 --> 00:02:18,210
out some particles everywhere, or
sort of uniformly distribute it.

44
00:02:18,210 --> 00:02:21,720
Because if my measurements
suddenly have moved over here,

45
00:02:21,720 --> 00:02:24,120
there'll be some particles over there,
and

46
00:02:24,120 --> 00:02:28,500
eventually that will be
the distribution that will be tracked.

47
00:02:28,500 --> 00:02:33,780
So those three issues of sort
of making sure you're smart and

48
00:02:33,780 --> 00:02:35,910
efficient about your resampling.

49
00:02:35,910 --> 00:02:39,760
Not letting your system think that
measurements are too precise, and making

50
00:02:39,760 --> 00:02:43,100
sure you have enough noise so that you
don't kill off samples too quickly.

51
00:02:43,100 --> 00:02:46,970
And this idea of having some
random samples distributed in a,

52
00:02:46,970 --> 00:02:51,720
in a useful way, so you can track things
if things change very unexpectedly.

53
00:02:51,720 --> 00:02:54,380
All these things are necessary
to make particle filtering work.
