1
00:00:00,347 --> 00:00:02,942
One of the cool recent announcements we've seen is the announcement

2
00:00:02,942 --> 00:00:06,477
of the Titan Supercomputer, which is now the fastest supercomputer in the world,

3
00:00:06,477 --> 00:00:09,448
and this has NVIDIA processors in its core.

4
00:00:09,448 --> 00:00:13,181
Can you talk a little bit about that process and how NVIDIA got to be involved

5
00:00:13,181 --> 00:00:15,986
and why that's such an exciting thing for GPU computing?

6
00:00:15,986 --> 00:00:17,617
Well, first of all, Titan is an awesome machine.

7
00:00:17,617 --> 00:00:31,089
It's 18,688 Kepler K20 GPUs and is the fastest computer in the world at running high performance winPAC.

8
00:00:31,089 --> 00:00:32,701
There's an interesting story there.

9
00:00:32,701 --> 00:00:36,994
The story of Titan actually starts with a meeting that I had with Steve Scott,

10
00:00:36,994 --> 00:00:43,700
who at the time was CTO of Cray at the Salishan Conference up on the Oregon Coast in 2009.

11
00:00:43,700 --> 00:00:48,413
I was talking to Steve and trying to see how can we work together.

12
00:00:48,413 --> 00:00:51,253
We really should get NVIDIA GPUs into Cray supercomputers

13
00:00:51,253 --> 00:00:55,587
because we have the best compute per dollar, compute per watt,

14
00:00:55,587 --> 00:00:57,957
which are the two things that matter in high performance computing,

15
00:00:57,957 --> 00:01:02,329
of anybody in the world, and they were actually going through a problem

16
00:01:02,329 --> 00:01:06,399
because they had bet on a different vendor who cancelled a project on them,

17
00:01:06,399 --> 00:01:12,872
and it left a hole in what they wanted to bid for this solicitation

18
00:01:12,872 --> 00:01:17,177
that was out from Oakridge to build what they call their leadership class computing facility--

19
00:01:17,177 --> 00:01:19,578
ultimately what turned in into Titan.

20
00:01:19,578 --> 00:01:23,983
It was just a nice sort of juxtaposition in time that I was having this conversation

21
00:01:23,983 --> 00:01:27,486
with him right at the point in time where there was a hole to be filled.

22
00:01:27,486 --> 00:01:30,090
It turns out Kepler filled that hole wonderfully.

23
00:01:30,090 --> 00:01:34,994
There was a lot of challenges along the way that, I think, really had to do with getting the people

24
00:01:34,994 --> 00:01:41,068
in the National Labs to embrace the model of parallelism that Cuda presents.

25
00:01:41,068 --> 00:01:43,602
I think that once they embraced it, they found it was actually easier

26
00:01:43,602 --> 00:01:47,072
to write their programs that way, and they actually ran better across the board

27
00:01:47,072 --> 00:01:55,114
once they were reorganized into that style of parallelism of watching CTAs and organizing things in that style.

28
00:01:55,114 --> 00:01:59,453
But they had a very large chunk of legacy code mostly in Fortran.

29
00:01:59,453 --> 00:02:03,985
It was coded sort of Fortran running on a single node with MPI used to communicate between the nodes,

30
00:02:03,985 --> 00:02:08,193
and it was a nontrivial exercise to really bring that software over

31
00:02:08,193 --> 00:02:11,698
and get it to run well on a GPU accelerated system.

32
00:02:11,698 --> 00:02:16,377
And I think beyond the LINPAC number, which is extra relatively easy number to get because it's one program,

33
00:02:16,377 --> 00:02:20,005
what really is a success of Titan is the very large number

34
00:02:20,005 --> 00:02:27,677
of basic energy science and defense codes that have been ported over very successfully

35
00:02:27,677 --> 00:02:31,598
and get just tremendous performance on the K20s.
