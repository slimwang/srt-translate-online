1
00:00:00,222 --> 00:00:02,469
All right, so that brings us to
temporal difference learning.

2
00:00:02,469 --> 00:00:05,766
So, temporal difference learning,
the algorithm we're going to look at

3
00:00:05,766 --> 00:00:07,920
originally by Sutton,
is called TD Lambda.

4
00:00:07,920 --> 00:00:09,620
This is a little Lambda symbol.

5
00:00:09,620 --> 00:00:12,310
So TD stands for temporal difference.

6
00:00:12,310 --> 00:00:13,970
And what really it's about,

7
00:00:13,970 --> 00:00:17,490
is learning to make predictions
that take place over time.

8
00:00:17,490 --> 00:00:20,280
And so as a concrete example,
a concrete way of looking at this,

9
00:00:20,280 --> 00:00:24,245
we're going to look at how we can
from a sequence of states, state

10
00:00:24,245 --> 00:00:28,470
zero goes to state one, goes to state
two, goes to let's say a final state.

11
00:00:28,470 --> 00:00:29,890
And each time there's a transition,

12
00:00:29,890 --> 00:00:32,200
there's some reward that's
associated with that.

13
00:00:32,200 --> 00:00:35,690
We're going to try to predict
the expected sum of discounted

14
00:00:35,690 --> 00:00:37,410
rewards from that.

15
00:00:37,410 --> 00:00:38,630
>> Okay.

16
00:00:38,630 --> 00:00:41,890
Which is what we're trying to do anyway.

17
00:00:41,890 --> 00:00:44,900
>> Yes, so this is going to be a really
important subroutine for trying to do

18
00:00:44,900 --> 00:00:48,050
reinforcement learning, because we're
going to use the notion that we can

19
00:00:48,050 --> 00:00:54,430
predict future rewards to try to better
choose actions to generate high rewards.

20
00:00:54,430 --> 00:00:54,950
>> Okay, that's fair.

21
00:00:54,950 --> 00:00:57,620
>> All right, so for the kind of
prediction problems that we're going to

22
00:00:57,620 --> 00:01:00,190
be learning with temporal difference
methods, it's helpful to have

23
00:01:00,190 --> 00:01:03,860
a concrete example to kind of
ground out some of these terms.

24
00:01:03,860 --> 00:01:06,530
So, let's take this as a Markov chain.

25
00:01:06,530 --> 00:01:10,200
With states S1, S2, S3, S4,
S5 and SF for the final state.

26
00:01:10,200 --> 00:01:10,700
>> Okay.

27
00:01:12,210 --> 00:01:15,040
>> I've labeled each of
the transitions with the reward

28
00:01:15,040 --> 00:01:18,240
that happens when you make
a transition from that to the next.

29
00:01:18,240 --> 00:01:23,060
And this particular transition out of
S3 actually is a stochastic transition,

30
00:01:23,060 --> 00:01:27,310
with 90% probability going to S4, and
ten percent probability going to S5.

31
00:01:27,310 --> 00:01:30,080
>> Okay, that makes sense.

32
00:01:30,080 --> 00:01:32,110
>> And what we'd like to learn
here is a value function,

33
00:01:32,110 --> 00:01:35,100
a function that maps
the state to some number,

34
00:01:35,100 --> 00:01:40,470
where that number is set to zero for the
final state and for every other state

35
00:01:40,470 --> 00:01:44,930
it's the expected value of the reward
plus the discounted value of the state

36
00:01:44,930 --> 00:01:48,880
that we end up in, averaged across all
the different states we might end up in.

37
00:01:48,880 --> 00:01:50,180
>> Okay that makes sense.

38
00:01:50,180 --> 00:01:53,070
>> Great, okay, good,
then let's have a quiz.

39
00:01:53,070 --> 00:01:55,514
>> [LAUGH]
