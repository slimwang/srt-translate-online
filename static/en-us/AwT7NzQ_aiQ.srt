1
00:00:00,300 --> 00:00:06,566
So there's an entire region over here where there's no example anywhere near B.

2
00:00:06,567 --> 00:00:17,099
This region is often called the margin. The margin of a linear separator is the distance of the separator to the closest training example.

3
00:00:17,100 --> 00:00:20,366
The margin is a really important concept in machine learning.

4
00:00:20,367 --> 00:00:31,899
There's an entire class of maximum margin linear rhythms, and the two most popular are support vector machines and boosting.

5
00:00:31,900 --> 00:00:35,632
If you're familiar with machine learning, you've come across these terms.

6
00:00:35,633 --> 00:00:41,566
These are very frequently used these days in actual discrimination learning tasks.

7
00:00:41,567 --> 00:00:49,666
I will not go into any details because it would go way beyond the scope of this introduction to artificial intelligence class.

8
00:00:49,667 --> 00:00:57,466
But I'd like to say a few abstract words specifically about support vector machines or SVMs.

9
00:00:57,467 --> 00:01:13,499
As I said before, a support vector machine derives a linear separator, and it picks the one that actually maximizes the margin, as shown over here.

10
00:01:13,500 --> 00:01:23,366
By doing so, it attains additional robustness over perceptron, which only picks a linear separator without consideration of the margin.

11
00:01:23,367 --> 00:01:31,566
Now, the problem of finding the margin maximizing linear separator can be solved by a quadratic program,

12
00:01:31,567 --> 00:01:38,599
which is an iterative method for finding the best linear separator that maximizes the margin.

13
00:01:38,600 --> 00:01:50,932
One of the nice things that support vector machines do in practice is they use linear techniques to solve nonlinear separation problems.

14
00:01:50,933 --> 00:01:56,566
And I'm just going to give you a glimpse of what's happening without going into any detail.

15
00:01:56,567 --> 00:02:05,032
Suppose your data looks as follows: We have a positive class, which is near the origin of a coordinate system,

16
00:02:05,033 --> 00:02:08,799
and a negative class that surrounds the positive class.

17
00:02:08,800 --> 00:02:18,632
Clearly, these two classes are not linearly separable because there's no line I can draw that separates the negative examples from the positive examples.

18
00:02:18,633 --> 00:02:27,832
An idea that underlies SVMs that will ultimately be known as the kernel trick is to augment the feature set by new features.

19
00:02:27,833 --> 00:02:35,166
Suppose this is x1 and this is x2, and normally x1 and x2 would be (unintelligible) features.

20
00:02:35,167 --> 00:02:41,499
In this example you might derive a third one. Let me pick a third one.

21
00:02:41,500 --> 00:02:50,699
Suppose x3 equals the square root of x1 square plus x2 square.

22
00:02:50,700 --> 00:02:58,899
In other words, x3 is the distance of any data point from the center of the coordinate system.

23
00:02:58,900 --> 00:03:02,832
Then things do become linearly separable.

24
00:03:02,833 --> 00:03:11,266
Plotted just along the third dimension, all the positive examples end up to be close to the origin.

25
00:03:11,267 --> 00:03:21,199
And all the negative examples are further away. And the line is orthogonal to the third input feature, solves the separation problem.

26
00:03:21,200 --> 00:03:34,032
Map back into the space over here, it's actually a circle, which is a set of all values of x3 that are equidistant to the center of the origin.

27
00:03:34,033 --> 00:03:39,666
Now, this trick could be done in any linear learning algorithm, and it's really an amazing trick.

28
00:03:39,667 --> 00:03:48,932
You can take any nonlinear problem, add features of this type or any other type, and use linear techniques and get better solutions.

29
00:03:48,933 --> 00:03:53,732
This is a very deep machine learning insight that you can extend your feature space in this way.

30
00:03:53,733 --> 00:03:57,532
And there's numerous papers written about this.

31
00:03:57,533 --> 00:04:04,899
In SVMs, the extension of the feature space is mathematically done by what's called kernel.

32
00:04:04,900 --> 00:04:12,266
I can't really tell you what it is in this class, but it makes it possible to derive very large new feature spaces,

33
00:04:12,267 --> 00:04:22,432
including infinity dimensional new feature spaces. This is a very powerful, it turns out you never really compute all those features.

34
00:04:22,433 --> 00:04:31,132
They're implicitly represented by so-called kernels, and if you care about this, I recommend you to dive deeper into the literature

35
00:04:31,133 --> 00:04:39,767
of support vector machines. This is meant to just give you an overview of the essence of what support vector machines are all about.
