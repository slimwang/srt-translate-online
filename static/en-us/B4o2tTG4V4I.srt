1
00:00:00,100 --> 00:00:01,680
So I was thinking of talking to you more

2
00:00:01,680 --> 00:00:04,720
about sampling, but it seems like it might work out

3
00:00:04,720 --> 00:00:07,480
best to just have some hands on experience with it

4
00:00:07,480 --> 00:00:09,730
so we're going to put those things on the homework. So

5
00:00:09,730 --> 00:00:11,960
given that we're actually in a position now to, to

6
00:00:11,960 --> 00:00:14,530
kind of wrap up the whole Bayes net inference piece

7
00:00:14,530 --> 00:00:16,500
that we were talking about. So do you want to help

8
00:00:16,500 --> 00:00:19,260
remind me, Charles, what were the things that we covered?

9
00:00:19,260 --> 00:00:24,105
>> Sure, I can help you with that. We covered Bayesian Inference [LAUGH]

10
00:00:24,105 --> 00:00:26,210
I'm sorry.

11
00:00:26,210 --> 00:00:27,660
I'm punch drunk.

12
00:00:27,660 --> 00:00:30,780
>> I'm going to choose not to pay attention to that. Instead, write

13
00:00:30,780 --> 00:00:32,280
Bayesian Networks. We talked about the

14
00:00:32,280 --> 00:00:35,880
Bayesian Network representation of joint probability distributions.

15
00:00:35,880 --> 00:00:42,900
>> Right. We did a lot of examples of how to actually do inference with networks.

16
00:00:42,900 --> 00:00:44,150
You know, exactly how do we, do we

17
00:00:44,150 --> 00:00:48,080
compute probabilities of particular values. We mentioned sampling.

18
00:00:48,080 --> 00:00:48,700
>> That's right.

19
00:00:48,700 --> 00:00:51,630
>> And then we did a Naive Bayes.

20
00:00:51,630 --> 00:00:54,850
>> Well first we did say that, that in general it's hard

21
00:00:54,850 --> 00:00:58,155
to do exact inference. It's actually hard to do even approximate inference.

22
00:00:58,155 --> 00:00:59,193
>> Mm-hm.

23
00:00:59,193 --> 00:01:00,430
>> But we talked about a special

24
00:01:00,430 --> 00:01:03,150
case of bayesian networks, that was called naive

25
00:01:03,150 --> 00:01:05,379
bayes with the naive part being, that we're

26
00:01:05,379 --> 00:01:08,630
assuming that attributes are independent of one another.

27
00:01:08,630 --> 00:01:10,710
>> Condition on the label.

28
00:01:10,710 --> 00:01:12,900
>> Right. And this was actually helping us make a

29
00:01:12,900 --> 00:01:16,700
link between all this bayesian stuff. The bayesian rabbit hole we

30
00:01:16,700 --> 00:01:21,350
went down. And classification, which is the core machine learning

31
00:01:21,350 --> 00:01:23,370
topic that we've been spending a lot of time on.

32
00:01:23,370 --> 00:01:24,350
>> So the other thing that I really

33
00:01:24,350 --> 00:01:27,160
liked about this notion, this link to classification, Michael,

34
00:01:27,160 --> 00:01:29,210
is that when I was talking about Bayesian

35
00:01:29,210 --> 00:01:31,730
learning, what we ended up with at the end

36
00:01:31,730 --> 00:01:36,360
is this nice idea that we had a gold standard, right? We had a sort of way

37
00:01:36,360 --> 00:01:39,260
of talking about what the right hypothesis was

38
00:01:39,260 --> 00:01:41,790
and, ultimately, what the right classification was by computing

39
00:01:41,790 --> 00:01:45,120
these probabilities. And sometimes, we couldn't do it because, typically,

40
00:01:45,120 --> 00:01:48,880
you can't actually do the for loop that requires you compute

41
00:01:48,880 --> 00:01:53,290
conditional probabilities of hypothesis given data. Over say an infinite number

42
00:01:53,290 --> 00:01:55,110
of hypothesis, but at least we kind of knew what the

43
00:01:55,110 --> 00:01:57,580
right thing was and we made right assumptions we could do

44
00:01:57,580 --> 00:02:01,190
things like derive, oh I don't know, a sum of squared

45
00:02:01,190 --> 00:02:04,120
errors or various other things that you might do and that

46
00:02:04,120 --> 00:02:06,790
was all very cool. But what you've done here when you

47
00:02:06,790 --> 00:02:10,300
do inference. Is at least with a Naive Bayes case,

48
00:02:10,300 --> 00:02:12,460
you've shown us a way that we can do classification

49
00:02:12,460 --> 00:02:15,090
using these things, that actually is tractable, and is the

50
00:02:15,090 --> 00:02:17,480
right thing to do under certain assumptions. I really like

51
00:02:17,480 --> 00:02:19,790
that. And the other thing that I think is worth

52
00:02:19,790 --> 00:02:22,930
mentioning is that not only does it link this Bayesian

53
00:02:22,930 --> 00:02:27,950
learning to classification. But it connects classification back to this

54
00:02:27,950 --> 00:02:31,810
general notion of Bayesian learning, Bayesian inference where, you don't

55
00:02:31,810 --> 00:02:36,220
have to worry about just figuring out the most likely label

56
00:02:36,220 --> 00:02:39,100
given a bunch of attributes. But because it's a Bayes network and

57
00:02:39,100 --> 00:02:41,720
you can compute anything from it, you could try to ask

58
00:02:41,720 --> 00:02:44,560
well what's the likelihood that I see some particular attribute or set

59
00:02:44,560 --> 00:02:48,050
of attributes, given a label or given a subset of attributes

60
00:02:48,050 --> 00:02:50,740
on all those kind of things that you could do. With the

61
00:02:50,740 --> 00:02:53,680
Bayesian learning. So inference gives us this power to not just

62
00:02:53,680 --> 00:02:57,110
do classification, but to do a larger set of things beyond classification.

63
00:02:57,110 --> 00:02:57,820
I think that's kind of cool.

64
00:02:57,820 --> 00:03:00,610
>> Cool. Yeah, well said. The, the For, and another thing,

65
00:03:00,610 --> 00:03:03,080
kind of in that same space is that it handles missing attributes

66
00:03:03,080 --> 00:03:07,260
really well. So whereas things like, oh. You know, decision trees

67
00:03:07,260 --> 00:03:09,610
and so forth, if you give me an example that doesn't have

68
00:03:09,610 --> 00:03:12,220
one of the attribute values and you've hit that part of

69
00:03:12,220 --> 00:03:14,830
the decision tree where you need to know that attribute value you're

70
00:03:14,830 --> 00:03:18,650
stuck. Whereas in this naive base setting, you can still do the

71
00:03:18,650 --> 00:03:20,840
probabalistic inference over the missing attributes

72
00:03:20,840 --> 00:03:22,190
because all the things are linked

73
00:03:22,190 --> 00:03:23,630
by probabilities.

74
00:03:23,630 --> 00:03:24,020
>> Nice.

75
00:03:24,020 --> 00:03:25,900
>> All right. So I think, you know, you'll, you'll get

76
00:03:25,900 --> 00:03:28,570
a much stronger handle of this when you go through the,

77
00:03:28,570 --> 00:03:32,670
the homework problems. But I think that's enough for Bayesian inference.

78
00:03:32,670 --> 00:03:36,490
And I think that actually wraps up classification and regression more generally.

79
00:03:36,490 --> 00:03:40,460
>> Right. So we're done with supervised learning. Well, one's never done with

80
00:03:40,460 --> 00:03:42,970
supervised learning. But we're at least done with this part of the course.

81
00:03:42,970 --> 00:03:44,850
>> Because there's always more to supervise learn.

82
00:03:44,850 --> 00:03:47,270
>> That's right. And in particular you'll get

83
00:03:47,270 --> 00:03:50,120
a nice example of this, because you'll be taking an exam.

84
00:03:50,120 --> 00:03:51,369
>> [LAUGH]

85
00:03:51,369 --> 00:03:54,750
>> And your input will be the exam, and then we'll give you a label back.

86
00:03:54,750 --> 00:03:57,120
>> [LAUGH] I guess that's one way to think about it.

87
00:03:57,120 --> 00:03:58,770
>> Well and then they'll get to generalize beyond

88
00:03:58,770 --> 00:04:00,200
that for the next time they take the exam.

89
00:04:00,200 --> 00:04:01,997
>> Very good! All right. Well, well thanks

90
00:04:01,997 --> 00:04:03,860
very much, this has been fun. Thanks Charles.

91
00:04:03,860 --> 00:04:06,810
>> This has been fun. I will see you in the second mini course.

92
00:04:06,810 --> 00:04:07,510
>> All right.

93
00:04:07,510 --> 00:04:08,840
>> Bye.
