1
00:00:00,000 --> 00:00:03,801
Now, some libraries are less about solving a particular domain of problems

2
00:00:03,801 --> 00:00:07,384
and more about enabling the programmers to design their own solutions.

3
00:00:07,384 --> 00:00:09,204
And I call these programming power tools,

4
00:00:09,204 --> 00:00:12,268
and the first example I'm going to highlight is called Thrust.

5
00:00:12,268 --> 00:00:17,879
Now, C++ programmers will know about the Standard Template Library, or STL.

6
00:00:17,879 --> 00:00:21,116
STL provides a set of incredibly useful abstractions

7
00:00:21,116 --> 00:00:25,067
that are organized into containers and iterators.

8
00:00:25,067 --> 00:00:29,303
Thrust provides a data parallel analog to the STL for CUDA

9
00:00:29,303 --> 00:00:32,869
and also borrows niceties from the popular Boost library.

10
00:00:32,869 --> 00:00:35,935
Thrust allows host code that runs on the CPU

11
00:00:35,935 --> 00:00:39,087
to very easily create and manipulate data on the GPU.

12
00:00:39,087 --> 00:00:43,571
The principal container abstraction in Thrust is device vector,

13
00:00:43,571 --> 00:00:46,759
and this is analogous to the vector container in STL,

14
00:00:46,759 --> 00:00:50,335
but as the name implies, it lives on the device, on the GPU.

15
00:00:50,335 --> 00:00:54,798
Like the STL vector container, Thrust device vectors are generic containers,

16
00:00:54,798 --> 00:00:58,773
which means they're able to hold any data type and they can be resized dynamically.

17
00:00:58,773 --> 00:01:02,645
So once your data is in a device vector, you can do a bunch of things trivially.

18
00:01:02,645 --> 00:01:07,341
You can do things like sort the device vector or perform a scan operation on it,

19
00:01:07,341 --> 00:01:10,009
you can do a reduction operation or a reduce-by-key operation

20
00:01:10,009 --> 00:01:15,513
where you actually take a vector and reduce it according to a key that's in another vector.

21
00:01:15,513 --> 00:01:19,511
So, for example, if you have a device_vector X--

22
00:01:19,511 --> 00:01:22,867
in this case it's got 3 elements and it's of type float--

23
00:01:22,867 --> 00:01:25,695
then I can do a reduction on those 3 elements

24
00:01:25,695 --> 00:01:30,173
by just saying thrust::reduce, pass in x.begin and x.end

25
00:01:30,173 --> 00:01:32,869
to indicate that I'm going to reduce over the entire range of the vector,

26
00:01:32,869 --> 00:01:34,471
and it will give me the result.

27
00:01:34,471 --> 00:01:37,312
Or you can do a reduction of a different sort.

28
00:01:37,312 --> 00:01:42,709
So rather than simply doing a sum reduce, we can pass in an operator to use or a functor.

29
00:01:42,709 --> 00:01:44,788
In this case we'll do thrust::maximum.

30
00:01:44,788 --> 00:01:48,158
So in this case, this reduction will give us the maximum number in this vector.

31
00:01:48,158 --> 00:01:51,538
You can also do more general transformations.

32
00:01:51,538 --> 00:01:54,859
You can transform 1 or more input vectors into an output vector.

33
00:01:54,859 --> 00:01:58,562
For instance, you could do a vector addition operation, taking 2 vectors

34
00:01:58,562 --> 00:02:02,273
and adding them into a third vector using the built-in Thrust plus operator.

35
00:02:02,273 --> 00:02:07,076
Or you can apply a user-defined transformation, using a functor,

36
00:02:07,076 --> 00:02:11,733
and this is a chunk of code that you give Thrust and tell it, run this on every element

37
00:02:11,733 --> 00:02:16,014
or on the collection of input and output elements that I'm interested in.

38
00:02:16,014 --> 00:02:21,041
And you can interoperate with CUDA codes, so you can mix up Thrust code

39
00:02:21,041 --> 00:02:23,660
which is running on the host and doing things on the device,

40
00:02:23,660 --> 00:02:27,558
but if you then need to do something that is not built in to Thrust or that's difficult to do with Thrust

41
00:02:27,558 --> 00:02:30,185
and you've got some raw CUDA kernels that you want to run,

42
00:02:30,185 --> 00:02:34,680
then you can simply get the pointer to the data in a device vector

43
00:02:34,680 --> 00:02:39,242
or hand a pointer to Thrust to wrap up as a device vector.

44
00:02:39,242 --> 00:02:43,913
In the end, using Thrust is a good idea because it helps you avoid

45
00:02:43,913 --> 00:02:47,313
writing a lot of error-prone sort of cut and paste boilerplate code,

46
00:02:47,313 --> 00:02:52,454
and you can exploit high-performance implementations of these data parallel primitives

47
00:02:52,454 --> 00:02:55,057
like sort and scan and reduce and reduce-by-key.

48
00:02:55,057 --> 00:03:00,031
These have all been implemented by CUDA Ninjas for you so that you can just reuse that code.

49
00:03:01,200 --> 00:03:02,440
So Thrust can be very handy.

50
00:03:02,440 --> 00:03:05,806
We're going to give you Thrust code that sorts 10^6 floats,

51
00:03:05,806 --> 00:03:08,064
and we're going to ask you to time it,

52
00:03:08,064 --> 00:03:12,887
and then we're going to ask you to also time sorting 10^5 and 10^7 floats

53
00:03:12,887 --> 00:03:18,543
and then enter here how long it took to sort 10^5 floats, 10^6 floats, 10^7 floats,

54
00:03:18,543 --> 00:03:22,346
and 10^5, 10^6, and 10^7 bytes.

55
00:03:22,346 --> 00:03:24,381
And enter your answer in milliseconds,

56
00:03:24,381 --> 00:03:27,757
and we'll just check to within a significant digit or so.
