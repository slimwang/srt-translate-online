1
00:00:00,500 --> 00:00:02,647
Two more technical details
about Word 2 Vec and

2
00:00:02,647 --> 00:00:05,270
about methods to learn
embeddings in general.

3
00:00:05,270 --> 00:00:09,320
First, because of the way embeddings are
trained, it's often better to measure

4
00:00:09,320 --> 00:00:13,430
the closeness using a cosine
distance instead of L2, for example.

5
00:00:13,430 --> 00:00:16,079
That's because the length
of the embedding vector

6
00:00:16,079 --> 00:00:18,340
is not relevant to the classification.

7
00:00:18,340 --> 00:00:21,910
In fact, it's often better to
normalize all embedding vectors

8
00:00:21,910 --> 00:00:23,930
to simply have unit norm.

9
00:00:23,930 --> 00:00:27,080
Second, we have the issue of
trying to predict words, and

10
00:00:27,080 --> 00:00:28,330
there are lots of them.

11
00:00:28,330 --> 00:00:32,450
So in Word 2 Vec we have this set up,
we have a word that we're going to embed

12
00:00:32,450 --> 00:00:37,070
into a small vector, then feed that into
a simple linear model with weights and

13
00:00:37,070 --> 00:00:41,360
biases and
that outputs a softmax probability.

14
00:00:41,360 --> 00:00:45,560
This is then compared to the target
which is another word in the context

15
00:00:45,560 --> 00:00:47,560
of the input word.

16
00:00:47,560 --> 00:00:49,550
The problem of course is
that there might be many,

17
00:00:49,550 --> 00:00:51,790
many words in our vocabulary.

18
00:00:51,790 --> 00:00:56,740
And computing the softmax function of
all those words can be very inefficient.

19
00:00:56,740 --> 00:00:58,190
But you can use a trick.

20
00:00:58,190 --> 00:01:02,120
Instead of treating the softmax as
if the label had probability of 1,

21
00:01:02,120 --> 00:01:04,730
and every other word
had probability of 0,

22
00:01:04,730 --> 00:01:07,960
you can sample the words
that are not the targets,

23
00:01:07,960 --> 00:01:12,880
pick only a handful of them and act
as if the other words were not there.

24
00:01:12,880 --> 00:01:15,430
This idea of sampling
the negative targets for

25
00:01:15,430 --> 00:01:18,770
each example is often
called sampled softmax.

26
00:01:18,770 --> 00:01:21,940
And it makes things faster
at no cost in performance.

27
00:01:21,940 --> 00:01:23,200
That's all there is to it.

28
00:01:23,200 --> 00:01:27,570
You take your large corpus of texts,
look at each word, embed it, and

29
00:01:27,570 --> 00:01:29,150
predict its neighbors.

30
00:01:29,150 --> 00:01:32,300
What you get out of that is
a mapping from words to vectors

31
00:01:32,300 --> 00:01:34,730
where similar words are going to
be close to each other.
