1
00:00:00,260 --> 00:00:03,120
So now that we have an idea
of what POMDPs are about.

2
00:00:03,120 --> 00:00:05,650
Maybe we should talk a little bit
about what it might mean to do

3
00:00:05,650 --> 00:00:08,730
reinforcement learning in them because
we basically talked about planning.

4
00:00:08,730 --> 00:00:12,649
If we have a model of the POMDP we can
run some calculations we can run value

5
00:00:12,649 --> 00:00:17,650
iteration to get a way of deciding
what to do actually did I say that?

6
00:00:17,650 --> 00:00:18,550
I think I did not say that.

7
00:00:18,550 --> 00:00:20,690
I think what I said is how
you get a value function.

8
00:00:20,690 --> 00:00:23,520
Is it clear how you would use
a value function to get a policy?

9
00:00:23,520 --> 00:00:26,230
>> In the same way you would for
an MDP right?

10
00:00:26,230 --> 00:00:27,357
Yes? No? That's not true?

11
00:00:27,357 --> 00:00:28,200
>> Yeah that's right.

12
00:00:28,200 --> 00:00:31,170
If you have the model, then you can
use one step look ahead with the value

13
00:00:31,170 --> 00:00:34,210
function to figure out what the optimal
action is in any given believe state.

14
00:00:34,210 --> 00:00:38,130
Once we've run valuation and gotten
an approximation of the optimal value

15
00:00:38,130 --> 00:00:42,708
function, and what about that valuation
doesn't actually necessarily converge.

16
00:00:42,708 --> 00:00:43,390
>> It doesn't?

17
00:00:43,390 --> 00:00:44,590
You didn't tell me that.

18
00:00:44,590 --> 00:00:47,185
>> I did actually when we
do in less than triple a.

19
00:00:47,185 --> 00:00:48,171
>> Lesson triple a?

20
00:00:48,171 --> 00:00:50,360
>> Yeah the advanced algorithm analysis.

21
00:00:50,360 --> 00:00:53,300
Well we said there is that value
iteration converges in the limit

22
00:00:53,300 --> 00:00:55,400
to the right value function but

23
00:00:55,400 --> 00:00:58,710
after any finite number of steps it need
not have the optimal value function.

24
00:00:58,710 --> 00:01:01,620
But it will after some finite number
of steps have the optimal policy

25
00:01:01,620 --> 00:01:04,560
that's not going to be true in the POMDP
case because there's an infinite number

26
00:01:04,560 --> 00:01:05,604
of states.

27
00:01:05,604 --> 00:01:06,290
>> I see.
>> But

28
00:01:06,290 --> 00:01:09,170
it is going to be the case that
we get an arbitrarily good

29
00:01:09,170 --> 00:01:12,550
approximation after some
finite number of iterations.

30
00:01:12,550 --> 00:01:14,700
>> Wait arbitrarily good
approximation of what?

31
00:01:14,700 --> 00:01:15,880
>> Of the optimal value function.

32
00:01:15,880 --> 00:01:17,500
>> Wait but
not necessarily optimal policy?

33
00:01:17,500 --> 00:01:20,720
>> Yeah if you do one step back ups or
one step look ahead

34
00:01:20,720 --> 00:01:24,440
with a near optimal value function
you get a near optimal policy?

35
00:01:24,440 --> 00:01:25,940
>> Okay so that's nearly good.

36
00:01:25,940 --> 00:01:29,500
>> [LAUGH] Now that we've
talked about planning in POMPDs

37
00:01:29,500 --> 00:01:31,100
we should talk about
reinforcement learning in POMPDs.

38
00:01:31,100 --> 00:01:32,940
>> Charles, what's the difference
between planning and

39
00:01:32,940 --> 00:01:33,830
reinforcement learning?

40
00:01:33,830 --> 00:01:35,960
>> Well, in planning you know everything
in a reinforcement learning you

41
00:01:35,960 --> 00:01:39,200
don't know everything and
you have to explore and exploit and

42
00:01:39,200 --> 00:01:41,340
do all that other stuff in
order to find things out like

43
00:01:41,340 --> 00:01:44,940
you don't know the model necessarily and
that makes the problem harder so

44
00:01:44,940 --> 00:01:49,600
what you're telling me is reinforcement
learning is harder then planning

45
00:01:49,600 --> 00:01:51,442
in the way you're using the words
>> And

46
00:01:51,442 --> 00:01:54,610
POMDPs are more difficult
to deal with than MDPs.

47
00:01:54,610 --> 00:01:58,800
So reinforcement learning in POMDPs
should be like, is that additive,

48
00:01:58,800 --> 00:02:00,070
multiplicative?

49
00:02:00,070 --> 00:02:02,270
>> So reinforcement
learning in POMDPs is hard.

50
00:02:02,270 --> 00:02:03,550
>> Hm.
>> Like, super hard.

51
00:02:03,550 --> 00:02:09,181
Actually planning in POMDPs is formally
undecidable in the sense that [LAUGH]

52
00:02:09,181 --> 00:02:10,086
>> Undecidable?

53
00:02:10,086 --> 00:02:12,490
>> Yeah,
[LAUGH] if I give you a POMDP and

54
00:02:12,490 --> 00:02:16,380
ask, is this the optimal first action
to take from this belief state?

55
00:02:16,380 --> 00:02:18,970
If you could solve that problem you
could solve the halting problem.

56
00:02:18,970 --> 00:02:19,790
>> Holy cow!

57
00:02:19,790 --> 00:02:21,890
>> Wait, wait a minute, wait a minute,
wait a minute, wait a minute.

58
00:02:21,890 --> 00:02:24,250
That has profound implications.

59
00:02:24,250 --> 00:02:25,490
>> No, it doesn't.
>> Yes it does,

60
00:02:25,490 --> 00:02:29,060
because we're human beings
running around in the world

61
00:02:29,060 --> 00:02:31,690
we're living in a POMDP because
we can't know everything.

62
00:02:31,690 --> 00:02:35,220
So you telling me that even
if I could relive life

63
00:02:35,220 --> 00:02:38,870
an infinite number of times I still
don't know what the right thing to do is

64
00:02:38,870 --> 00:02:40,500
you're saying that life is unknowable.

65
00:02:40,500 --> 00:02:42,390
It's undecidable you can
never know what to do.

66
00:02:42,390 --> 00:02:43,450
That's profound.

67
00:02:43,450 --> 00:02:44,510
Wow I need a moment.
