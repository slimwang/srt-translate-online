1
00:00:00,200 --> 00:00:02,510
>> So this is a remarkable fact about this Q-learning

2
00:00:02,510 --> 00:00:05,030
rule, and that is if we start Q hat off

3
00:00:05,030 --> 00:00:07,880
pretty much anywhere, and then we update it according to

4
00:00:07,880 --> 00:00:11,580
the rule that we talked about. Q for, for when

5
00:00:11,580 --> 00:00:15,910
we see a transition s,a, r, s prime, then we

6
00:00:15,910 --> 00:00:19,870
update (s,a), the Q value for (s,a), move it alpha

7
00:00:19,870 --> 00:00:22,290
of the way towards r plus gamma, max a of

8
00:00:22,290 --> 00:00:25,740
the, well basically the Q value of the state S prime.

9
00:00:25,740 --> 00:00:29,590
Then as long as we do that, then this estimate, this q hat S A

10
00:00:29,590 --> 00:00:33,080
goes to Q S A. The actual solution to the Bellman equation. And I write

11
00:00:33,080 --> 00:00:34,390
this with an exclamation mark, because it's

12
00:00:34,390 --> 00:00:36,350
like, it's one line of code! It's one

13
00:00:36,350 --> 00:00:39,900
line of code, like, how could you not just go out and write this right now?

14
00:00:39,900 --> 00:00:40,680
>> Hm.

15
00:00:40,680 --> 00:00:45,636
>> But the, the, the, let me just, to finish is, this is only true if we

16
00:00:45,636 --> 00:00:51,470
actually visit SA infinitely often. So you know, that's an important caveat.

17
00:00:51,470 --> 00:00:53,940
That for this to, to actually hold true, for you to really converge

18
00:00:53,940 --> 00:00:56,870
to the, the solution, it has to run for a long time. It

19
00:00:56,870 --> 00:00:59,760
has to visit all state action pairs. The learning rates have to be

20
00:00:59,760 --> 00:01:02,770
updated the way that we talked about before. The next states need to

21
00:01:02,770 --> 00:01:05,950
be drawn from the actual transition probabilities but that's, that's cool, if we

22
00:01:05,950 --> 00:01:10,320
actually are learning in some actual environment and the rewards need to be

23
00:01:10,320 --> 00:01:13,010
drawn from the rewards function. So, this isn't so problematic. This is a

24
00:01:13,010 --> 00:01:16,620
little bit problematic, but it is still very reassuring, this idea that we have

25
00:01:16,620 --> 00:01:19,320
the right form of an update rule, so that the thing

26
00:01:19,320 --> 00:01:22,330
that we converge to is the actual optimal solution to the MDP.

27
00:01:22,330 --> 00:01:23,640
>> Cool. And we just have to wait til the

28
00:01:23,640 --> 00:01:26,040
heat death of the universe, or infinity, and then we're done.

29
00:01:26,040 --> 00:01:27,080
>> Yeah.
