1
00:00:00,220 --> 00:00:03,560
What they showed in some of the recent
work is that you run into problems if

2
00:00:03,560 --> 00:00:06,880
you try to minimize the loss function
that we talked about which is

3
00:00:06,880 --> 00:00:09,280
trying to minimize the kind
of GTD error directly.

4
00:00:09,280 --> 00:00:13,550
And instead, you actually get something
that's much more well behaved if you

5
00:00:13,550 --> 00:00:17,170
incorporate the function approximation
itself into the last function.

6
00:00:17,170 --> 00:00:20,530
So take advantage of the fact that we
know that we're changing parameters

7
00:00:20,530 --> 00:00:24,110
that are alternately going to influence
a linear representation of the function.

8
00:00:24,110 --> 00:00:26,540
And then through that linear
representation the function that that's

9
00:00:26,540 --> 00:00:28,810
where we're going to
actually measure the loss.

10
00:00:28,810 --> 00:00:31,700
And so the projection with respect
to the function approximate or

11
00:00:31,700 --> 00:00:35,350
ends up showing up in the last function
and that makes things happier.

12
00:00:35,350 --> 00:00:36,320
>> Nice, well done.

13
00:00:36,320 --> 00:00:38,800
So we had successes we had problems and

14
00:00:38,800 --> 00:00:40,580
then we have successes
again it's pretty good.

15
00:00:40,580 --> 00:00:44,160
>> Yeah and one of the thing to
note on the successes side is that

16
00:00:44,160 --> 00:00:45,460
the current We can talk about this so

17
00:00:45,460 --> 00:00:49,920
much but the go to algorithm these
>> Is called fitted Q iteration,

18
00:00:49,920 --> 00:00:52,630
which is a way of mixing
function approximation and

19
00:00:52,630 --> 00:00:54,730
Q value learning together.

20
00:00:54,730 --> 00:00:58,590
Again, if [LAUGH] it has the same
kinds of problems, it need not work,

21
00:00:58,590 --> 00:01:01,310
it need not converge,
it can behave really badly, but

22
00:01:01,310 --> 00:01:04,290
this is something that a lot of
people have gotten to work decently.

23
00:01:04,290 --> 00:01:06,810
So if you're you know if you're trying
to solve these kinds of problems this is

24
00:01:06,810 --> 00:01:07,810
this is the go to.

25
00:01:07,810 --> 00:01:12,300
GTD is not quite there yet
it's more of theoretical interest but

26
00:01:12,300 --> 00:01:15,650
the hope is that these will actually
come more together in the future.

27
00:01:15,650 --> 00:01:18,580
And there's one other topic that we that
we spent time on that I think is worth

28
00:01:18,580 --> 00:01:21,397
mentioning which is
the notion of Averagers.

29
00:01:21,397 --> 00:01:22,310
>> Yes.
>> And

30
00:01:22,310 --> 00:01:25,530
the cool thing about an Averager was
not only where things well behaved

31
00:01:25,530 --> 00:01:28,260
you got convergence with
a function approximater,

32
00:01:28,260 --> 00:01:32,730
but it actually could be viewed
as a kind of MDP itself.

33
00:01:32,730 --> 00:01:36,270
The function approximater or could be
viewed as kind of MDP itself which,

34
00:01:36,270 --> 00:01:39,650
lets us get all kinds of nice
properties like that there's a unique.

35
00:01:39,650 --> 00:01:41,890
Value function that it
needs to converge to.

36
00:01:41,890 --> 00:01:45,960
One thing we didn't talk about
that's also really related to this

37
00:01:45,960 --> 00:01:50,520
this topic is in the Averagers case we
have a set of anchor points for that?

38
00:01:50,520 --> 00:01:51,230
>> Yes I do.

39
00:01:51,230 --> 00:01:54,760
>> As the number of anchor
points increases, the error

40
00:01:54,760 --> 00:01:58,560
in the value function approximation
that you get actually goes down.

41
00:01:58,560 --> 00:02:03,630
And so you can actually converge
over time on not only an answer but

42
00:02:03,630 --> 00:02:06,260
actually the right answer,
the right value function in the limit.

43
00:02:06,260 --> 00:02:10,080
>> Right because this is just K and
N, or was a lot like K and N.

44
00:02:10,080 --> 00:02:11,910
And that would be true for
canon as well.

45
00:02:11,910 --> 00:02:12,540
>> Yes.

46
00:02:12,540 --> 00:02:14,100
>> Alright if you had
an infinite number of points for

47
00:02:14,100 --> 00:02:16,110
example you could actually
represent the point.

48
00:02:16,110 --> 00:02:19,310
>> So in optional reading we'll actually
put a link to it to the first paper

49
00:02:19,310 --> 00:02:21,710
that really goes through that
argument and shows that.

50
00:02:21,710 --> 00:02:25,870
Yeah in fact as the amount of data
increases things like averages kernel

51
00:02:25,870 --> 00:02:29,438
methods converge to the optimal value
function the true value function.

52
00:02:29,438 --> 00:02:30,170
>> That's pretty cool, so

53
00:02:30,170 --> 00:02:32,400
that everything we learn
including stuff we did learn?

54
00:02:32,400 --> 00:02:33,710
>> I think that's
everything we learned and

55
00:02:33,710 --> 00:02:35,650
a bunch of things that we didn't learn,
and

56
00:02:35,650 --> 00:02:41,552
then I probably should also just
say LSPI and just leave it at that.

57
00:02:41,552 --> 00:02:44,260
>> [LAUGH] I think that's right we

58
00:02:44,260 --> 00:02:47,870
have to leave them to go learn something
on their own or those with a point B.

59
00:02:47,870 --> 00:02:48,910
>> Yes.

60
00:02:48,910 --> 00:02:50,120
Least squares policy iteration,

61
00:02:50,120 --> 00:02:53,230
which is which is a kind of linear
function approximation where

62
00:02:53,230 --> 00:02:56,260
policy improvement is actually
incorporated into the system itself, and

63
00:02:56,260 --> 00:03:01,100
it has some super nice properties and
people have gotten it to do good things.

64
00:03:01,100 --> 00:03:05,150
The tricky thing is always the feature
set, getting the right features so

65
00:03:05,150 --> 00:03:08,010
that linear function
approximation does an okay job.

66
00:03:08,010 --> 00:03:10,090
>> Well it's always true, isn't it?

67
00:03:10,090 --> 00:03:10,590
>> Exactly.

68
00:03:11,590 --> 00:03:14,810
>> If we could always have the right
features, then the problem becomes easy.

69
00:03:14,810 --> 00:03:15,870
>> So that's a lot to learn.

70
00:03:15,870 --> 00:03:16,980
>> Yeah, and to have learned.

71
00:03:16,980 --> 00:03:18,250
What are we going to next time, Michael?

72
00:03:18,250 --> 00:03:19,540
>> Here, I'll give you a hint.

73
00:03:19,540 --> 00:03:20,660
I'm hiding the slide.

74
00:03:20,660 --> 00:03:23,060
We're going to talk about
partially observable models.

75
00:03:23,060 --> 00:03:24,490
>> Hm, okay, that makes sense.

76
00:03:24,490 --> 00:03:26,605
That means that I got lots
of opportunities for puns.

77
00:03:26,605 --> 00:03:29,200
>> [LAUGH]
>> All right, we'll see you there then.

78
00:03:29,200 --> 00:03:30,741
Wouldn't want to miss that.

79
00:03:30,741 --> 00:03:32,340
>> [LAUGH] See it has already begun.

80
00:03:32,340 --> 00:03:33,940
All right well by, Michael.

81
00:03:33,940 --> 00:03:34,580
>> See you, Charles.
