1
00:00:00,200 --> 00:00:02,080
Ok, Michael, so I'm going to talk a little bit

2
00:00:02,080 --> 00:00:05,525
about bias. In particular, the preference bias for K

3
00:00:05,525 --> 00:00:08,950
[INAUDIBLE]. So, let me remind you what preference bias

4
00:00:08,950 --> 00:00:13,220
is. Preference bias is kind of our notion of

5
00:00:13,220 --> 00:00:16,700
why we would prefer one hypothesis over another. And

6
00:00:16,700 --> 00:00:19,000
they say all things, other things being equal. And

7
00:00:19,000 --> 00:00:21,950
what that really means is, it's the thing that

8
00:00:21,950 --> 00:00:26,250
encompasses our belief. About what makes a good hypothesis.

9
00:00:26,250 --> 00:00:27,570
So in some of the previous examples that

10
00:00:27,570 --> 00:00:30,790
we used it was things like shorter trees,

11
00:00:30,790 --> 00:00:36,518
smoother functions, simpler functions, Occam's Razor, those sorts of

12
00:00:36,518 --> 00:00:39,793
things were the ways that we expressed our preferences

13
00:00:39,793 --> 00:00:43,410
over various hypothesis. And kNN is no exception.

14
00:00:43,410 --> 00:00:45,070
It also has preference by its built in

15
00:00:45,070 --> 00:00:51,280
as does every algorithm of any note. So I just wanted to go through three that I

16
00:00:51,280 --> 00:00:54,770
thought of is as being indicitive of this bias. And they're,

17
00:00:54,770 --> 00:00:57,030
kind of all related to one another. So the first one

18
00:00:57,030 --> 00:01:01,460
is a notion of locality. Right? There's this idea that near

19
00:01:01,460 --> 00:01:04,650
points are similiar to one another. Does that make sense to you?

20
00:01:04,650 --> 00:01:07,150
>> Yeah. Yeah. That was really important. It came out

21
00:01:07,150 --> 00:01:09,950
nicely in the the real estate example. Right. So the

22
00:01:09,950 --> 00:01:12,610
whole idea. The whole thing we are using to generalize

23
00:01:12,610 --> 00:01:15,030
from one thing to another is this notion of nearness.

24
00:01:15,030 --> 00:01:16,580
>> Right. And exactly

25
00:01:16,580 --> 00:01:18,770
how this notion of nearness works out.

26
00:01:18,770 --> 00:01:20,910
Is embedded in whatever distance function we happen

27
00:01:20,910 --> 00:01:22,360
to be given. And so, there's further

28
00:01:22,360 --> 00:01:25,110
bias that might come out, based upon exactly

29
00:01:25,110 --> 00:01:26,780
the way we implement distances. So, in

30
00:01:26,780 --> 00:01:29,630
the example we just did, euclidian distance is

31
00:01:29,630 --> 00:01:31,820
making a different assumption about what nearness or

32
00:01:31,820 --> 00:01:34,730
similarity is, compared to Manhattan distance, for example.

33
00:01:34,730 --> 00:01:37,550
>> So is there like, a perfect distance function for a given problem?

34
00:01:37,550 --> 00:01:41,970
>> There's certainly a perfect distance function for any particular problem.

35
00:01:41,970 --> 00:01:44,250
>> Yeah, that's what I mean. Not one that works for the

36
00:01:44,250 --> 00:01:48,380
universe, but one, like, you know, if I give you a problem and

37
00:01:48,380 --> 00:01:50,600
you can work on it all day long. Can you find, is

38
00:01:50,600 --> 00:01:51,720
there a notion that there''s a

39
00:01:51,720 --> 00:01:54,720
distance function that would capture things perfectly?

40
00:01:54,720 --> 00:01:59,670
>> Well, it has to be the case for any given fixed problem. That there

41
00:01:59,670 --> 00:02:04,260
is some distance function that minimizes, say,

42
00:02:04,260 --> 00:02:07,580
sum of squared errors or something like that.

43
00:02:07,580 --> 00:02:09,680
First is some other distance function. Right?

44
00:02:09,680 --> 00:02:10,590
>> Okay.

45
00:02:10,590 --> 00:02:12,510
>> That has to be the case. So there, there always to

46
00:02:12,510 --> 00:02:16,590
be at least one best distance function given everything else is fixed.

47
00:02:16,590 --> 00:02:17,890
>> That makes sense.

48
00:02:17,890 --> 00:02:20,400
>> Right. Now, what that is, who

49
00:02:20,400 --> 00:02:24,690
knows. Maybe you finding it might be. Arbitrarily

50
00:02:24,690 --> 00:02:27,560
difficult. Because there's at least an infinite, there's

51
00:02:27,560 --> 00:02:29,915
at least an infinite number of distance functions.

52
00:02:29,915 --> 00:02:32,780
>> Well yeah, I was thinking that, that for latter to find distance functions

53
00:02:32,780 --> 00:02:36,350
to be anything we want. What about a distance function that said

54
00:02:36,350 --> 00:02:40,090
the distance between all the things that have the same answer, is zero.

55
00:02:40,090 --> 00:02:40,643
>> Mm-hm.

56
00:02:40,643 --> 00:02:42,180
>> And the distance between them and the ones that

57
00:02:42,180 --> 00:02:45,090
have different answers is you know, infinity or something big.

58
00:02:45,090 --> 00:02:45,310
>> Yeah.

59
00:02:45,310 --> 00:02:47,670
>> And then, then the distance function,

60
00:02:47,670 --> 00:02:49,680
like, somehow already has built in the solution

61
00:02:49,680 --> 00:02:51,350
to the problem because it's already put

62
00:02:51,350 --> 00:02:52,850
the things that have the same answers together.

63
00:02:52,850 --> 00:02:54,820
>> Right, you could do that, and of course, doing

64
00:02:54,820 --> 00:02:58,650
that would require again solving the original problem. But yeah, so.

65
00:02:58,650 --> 00:03:01,240
So, such a function has to exist, or, well, you

66
00:03:01,240 --> 00:03:03,650
know, there's always noise. What if there's noise in your data,

67
00:03:03,650 --> 00:03:05,870
you know? But some such function like that has to

68
00:03:05,870 --> 00:03:08,930
exist, the question is finding it. But I think the real

69
00:03:08,930 --> 00:03:11,150
point to take there is, there are some good distance

70
00:03:11,150 --> 00:03:13,460
functions for our problem and there are some bad distance functions

71
00:03:13,460 --> 00:03:17,010
for our problem. And how you pick those is really fundamental

72
00:03:17,010 --> 00:03:19,630
assumption your making about the domain. That's why it's domain knowledge.

73
00:03:19,630 --> 00:03:21,600
>> Yeah, that sounds right.

74
00:03:21,600 --> 00:03:23,780
>> Mm 'Kay. So, locality however it's expressed

75
00:03:23,780 --> 00:03:26,910
to the distance function, that is similarity. It's built

76
00:03:26,910 --> 00:03:29,244
in to kNN that we believe that near points

77
00:03:29,244 --> 00:03:32,290
are similar. Kind of by definition. That leads actually

78
00:03:32,290 --> 00:03:34,270
to the second preference bias which is this notion of

79
00:03:34,270 --> 00:03:39,870
smoothness. Alright. That we are by choosing to average.

80
00:03:39,870 --> 00:03:42,130
And by choosing to look at points that are similar

81
00:03:42,130 --> 00:03:45,070
to one another. We are expecting functions to behave,

82
00:03:45,070 --> 00:03:48,730
smoothly. Alright, so, you know, in the 2D case.

83
00:03:48,730 --> 00:03:52,530
It's, it's kind of easy to see, right? You, you, you, you have these,

84
00:03:52,530 --> 00:03:55,070
these sort of points and you're basically

85
00:03:55,070 --> 00:03:57,470
saying, look, these two points should somehow be

86
00:03:57,470 --> 00:04:02,600
related to one another more than this point and this point. And that sort

87
00:04:02,600 --> 00:04:06,370
of assumes kind of smoothly changing behavior

88
00:04:06,370 --> 00:04:08,560
as you move from one neighborhood to another.

89
00:04:11,390 --> 00:04:11,790
Does that make sense?

90
00:04:11,790 --> 00:04:15,240
>> I mean, it seems like we're defining to be pretty similar to locality.

91
00:04:15,240 --> 00:04:19,640
>> In this case. So I'm, I'm drawing an example, such that, you know,

92
00:04:19,640 --> 00:04:22,750
whatever we meant by locality has already been kind of expressed in the graph.

93
00:04:22,750 --> 00:04:23,450
>> Okay.

94
00:04:23,450 --> 00:04:25,940
>> And you know, by picking, you know, this is really for pedagogical

95
00:04:25,940 --> 00:04:28,725
reasons. You know can imagine, this you know, these are points that live in

96
00:04:28,725 --> 00:04:31,480
77,000 dimensions, and it's impossible to actually

97
00:04:31,480 --> 00:04:33,090
visualize them much less draw them. And

98
00:04:33,090 --> 00:04:34,510
I could try. [LAUGH] But here's, here's

99
00:04:34,510 --> 00:04:36,910
three dimensions and here's the fourth dimension.

100
00:04:37,920 --> 00:04:40,250
I think I'm going to get tired before I hit seven and seven thousand

101
00:04:40,250 --> 00:04:43,270
but, you know, you kind of, you kind of get the idea, right? That, if. In,

102
00:04:43,270 --> 00:04:45,450
you know, if you can imagine in your head points that are really

103
00:04:45,450 --> 00:04:47,590
near one another in some space, you

104
00:04:47,590 --> 00:04:50,270
kind of hope that they behave similarly. Right.

105
00:04:50,270 --> 00:04:53,350
>> Right. Okay, so locality and smoothness. And I think these make sense. I

106
00:04:53,350 --> 00:04:54,460
mean, these, this is hardly the only

107
00:04:54,460 --> 00:04:56,100
algorithm that makes these kind of assumptions.

108
00:04:56,100 --> 00:04:59,810
But there is another assumption which is a bit more subtle I think. Which

109
00:04:59,810 --> 00:05:03,370
is worth spending a second talking about, which is, for at least the distance

110
00:05:03,370 --> 00:05:07,520
functions we've looked at before. The Euclidian distance and

111
00:05:07,520 --> 00:05:11,500
the Manhattan distance. They all kind of looked at each

112
00:05:11,500 --> 00:05:14,600
of the dimensions, sort of, and subtracted them, and

113
00:05:14,600 --> 00:05:16,780
squared them, or didn't, or took their absolute value and

114
00:05:16,780 --> 00:05:19,810
added them all together. What that means is, we

115
00:05:19,810 --> 00:05:22,060
were treating, at least in those cases, that all the

116
00:05:22,060 --> 00:05:24,320
features mattered. And not only did they matter, they

117
00:05:24,320 --> 00:05:28,530
mattered equally. Right. So think about the the last quiz

118
00:05:28,530 --> 00:05:34,500
I gave you. Right. It said y equals x 1 squared plus x 2. And you noticed we

119
00:05:34,500 --> 00:05:36,320
got answers that were wildly off from what the

120
00:05:36,320 --> 00:05:38,470
actual answer was. Well if I know that the

121
00:05:38,470 --> 00:05:41,890
first dimension. The first feature is going to be squared

122
00:05:41,890 --> 00:05:43,950
and the second one is not going to be

123
00:05:43,950 --> 00:05:47,060
squared. Do you think either one of these features

124
00:05:47,060 --> 00:05:48,970
is more important or more important to get right?

125
00:05:50,190 --> 00:05:52,460
>> Okay. Right. Trying to think about what that might mean. So,

126
00:05:53,800 --> 00:05:57,380
if, yea its definitely the case that when you look

127
00:05:57,380 --> 00:06:00,110
for similar examples in the database you want to care

128
00:06:00,110 --> 00:06:03,020
more about X1 because a little bit of a difference

129
00:06:03,020 --> 00:06:06,580
in X1 gets squared out. Right? It can lead to a

130
00:06:06,580 --> 00:06:10,910
very large difference in the corresponding Y value. Whereas in

131
00:06:10,910 --> 00:06:13,790
the x2's, it's not quite as crucial. Th, th, the,

132
00:06:13,790 --> 00:06:15,830
if you're off a little bit more, then you're off

133
00:06:15,830 --> 00:06:18,900
a little bit more, it's just a linear relationship. So yeah,

134
00:06:18,900 --> 00:06:21,360
it does seems like that first dimension needs to be a lot

135
00:06:21,360 --> 00:06:25,150
more important, I guess, when you're doing the matching. Then the second one.

136
00:06:25,150 --> 00:06:26,670
>> Right so, we probably would have gotten

137
00:06:26,670 --> 00:06:27,860
different, I'm not going to go through this but,

138
00:06:27,860 --> 00:06:31,250
we probably would have gotten different answers if, in

139
00:06:31,250 --> 00:06:34,370
the Euclidian or Manhattan case we had instead of

140
00:06:34,370 --> 00:06:37,330
just taking the difference between the first two The

141
00:06:37,330 --> 00:06:40,000
first dimensions, we had taken that difference and squared

142
00:06:40,000 --> 00:06:41,810
it. And then in the case, including this and

143
00:06:41,810 --> 00:06:44,380
squaring it again, and then some of those things

144
00:06:44,380 --> 00:06:46,020
that were closer in the first dimension instead

145
00:06:46,020 --> 00:06:48,300
of the second dimension would've looked more similar and

146
00:06:48,300 --> 00:06:49,980
we might've gotten better answer. That's probably a good

147
00:06:49,980 --> 00:06:52,265
exercise to go back and do for someone else.

148
00:06:52,265 --> 00:06:54,150
>> [LAUGH] Yeah, I was thinking of doing it right

149
00:06:54,150 --> 00:06:56,160
now, but yeah, probably should leave it for other people.

150
00:06:56,160 --> 00:06:59,500
>> Well you can do it if you want to. So did you do it Michael?

151
00:06:59,500 --> 00:07:01,096
>> I did.

152
00:07:01,096 --> 00:07:02,289
>> And?

153
00:07:02,289 --> 00:07:05,140
>> So it's a kind of now a mix between the Manhattan distance

154
00:07:05,140 --> 00:07:06,700
and the Euclidian distance. So, I'm taking

155
00:07:06,700 --> 00:07:09,700
the first component, take the difference, square

156
00:07:09,700 --> 00:07:09,840
it.

157
00:07:09,840 --> 00:07:10,255
>> Mm-hm.

158
00:07:10,255 --> 00:07:12,350
>> Take the second component, take the difference,

159
00:07:12,350 --> 00:07:14,800
absolute value it. And add those two things together.

160
00:07:14,800 --> 00:07:15,870
>> Sure.

161
00:07:15,870 --> 00:07:18,440
>> All right. So if I do that, with one nearest neighbor,

162
00:07:18,440 --> 00:07:21,280
I still get that tie, but the output answer ends up being 12.

163
00:07:21,280 --> 00:07:21,780
>> Hm. Which

164
00:07:23,980 --> 00:07:25,199
is better than 24.7.

165
00:07:25,199 --> 00:07:27,490
>> And that's better than eight, which is what

166
00:07:27,490 --> 00:07:29,260
it was before. So the eight has gone up to

167
00:07:29,260 --> 00:07:32,040
12, which is better than the other one, which I

168
00:07:32,040 --> 00:07:34,600
think was 35.5, comes down to 29.5 Close here again

169
00:07:34,600 --> 00:07:37,120
to the correct answer which is eighteen. So in

170
00:07:37,120 --> 00:07:39,510
both cases it kind of pushed in the right direction,

171
00:07:39,510 --> 00:07:42,640
it was using more, of the, the answers that were

172
00:07:42,640 --> 00:07:44,680
relevant and fewer of the answers that were not relevant.

173
00:07:44,680 --> 00:07:49,140
>> Right. There you go. So the notion of relevance by the way,

174
00:07:49,140 --> 00:07:54,100
turns out to be very, very important. And highlights a weakness of

175
00:07:54,100 --> 00:07:58,220
kNN. So this brings me to a kind of theorem or fundamental

176
00:07:58,220 --> 00:08:03,320
results of a machine learning that is particularly relevant to kNN but

177
00:08:03,320 --> 00:08:07,060
its actually relevant everywhere. Do you think its worth while to mention it?

178
00:08:07,060 --> 00:08:08,130
>> Sure it sounds very relevant.

179
00:08:08,130 --> 00:08:08,790
>> Alright let's do it.
