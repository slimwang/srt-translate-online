1
00:00:00,067 --> 00:00:06,932
And the answer is that difference in values can, at most, be X. So if the difference in value between B and A is larger than X,

2
00:00:06,933 --> 00:00:11,799
value of A divided by X and the value of B divided by X, would round down to different numbers.

3
00:00:11,800 --> 00:00:17,966
So that's why we know the amount of value lost in each mistake that we make is actually smaller than X.

4
00:00:17,967 --> 00:00:24,532
And that is a very cool thing to know, because with these three things, knowing how many objects can be, at most,

5
00:00:24,533 --> 00:00:30,599
in an optimum solution, how many mistakes we can have made. And having quantified how much value

6
00:00:30,600 --> 00:00:37,766
we have lost in each mistake allows us to quantify the overall mistake that we can have made, at most.

7
00:00:37,767 --> 00:00:41,032
And I call this the maximum value lost, as opposed to an optimum solution.

8
00:00:41,033 --> 00:00:45,666
And that maximum value is, of course, well, there's, at most, N objects in an optimum solution.

9
00:00:45,667 --> 00:00:51,766
That means we can make, at most, N mistakes, and each of those mistakes costs us less than X.

10
00:00:51,767 --> 00:00:58,499
So the maximum value that we can lose overall by rounding down is smaller than N times X.

11
00:00:58,500 --> 00:01:03,432
And that, of course, is the absolute error that the algorithm can make by this rounding down technique.

12
00:01:03,433 --> 00:01:09,166
So now let's figure out the relative error here. And the relative error, since KNAPSACK is a maximization problem,

13
00:01:09,167 --> 00:01:14,832
is of course the value of an optimum solution divided by the value of an approximate solution.

14
00:01:14,833 --> 00:01:19,932
So let's just call the value of the optimum solution V opt, and opt stands for optimal here.

15
00:01:19,933 --> 00:01:25,932
And in that case we can write the approximate solution as V opt minus N times X.

16
00:01:25,933 --> 00:01:30,899
Now, this is not the precise value of the approximate solution, but it's a bound for the approximate solution.

17
00:01:30,900 --> 00:01:37,132
So we know it cannot get any worse than that. So, but to be precise here, of course then we'll have to write it like this here.

18
00:01:37,133 --> 00:01:41,199
Because we know that the approximate solution will have at least this value.

19
00:01:41,200 --> 00:01:46,866
Actually, it will always be bigger, because we have a strict smaller than here, so we should more precisely even write it like this.

20
00:01:46,867 --> 00:01:50,532
So it's smaller than V opt divided by V opt minus N times X.

21
00:01:50,533 --> 00:01:55,866
Now, we can simply write this as one plus N times X over V opt minus N times X.

22
00:01:55,867 --> 00:02:03,466
We know something about V opt because the value of an optimum solution can, at most, be the value of all of all objects combined,

23
00:02:03,467 --> 00:02:09,632
and we call that number V. So we can rewrite this as one plus N times X over V minus N times X.

24
00:02:09,633 --> 00:02:17,232
Now, let's have a closer look at X. How did we define X? Well, we said that X was equal to V over N

25
00:02:17,233 --> 00:02:20,932
times one minus C, where C was some number between zero and one.

26
00:02:20,933 --> 00:02:30,466
So if we plug the value of X into this equation here, what we get is one plus V times one minus C over V minus

27
00:02:30,467 --> 00:02:37,866
V times one minus C. Which means that Vs here simply cancel out, and you get one plus one minus C

28
00:02:37,867 --> 00:02:40,932
over one minus one plus C. Which is the same as one over C.

29
00:02:40,933 --> 00:02:47,766
So I didn't tell you about C earlier, but what we just found out is that the approximation factor of our algorithm

30
00:02:47,767 --> 00:02:52,632
is actually one over C. And you'll remember that the running time was also dependent on C.

31
00:02:52,633 --> 00:02:57,699
So we already figured this one out, right? It was O of N squared over one minus C.

32
00:02:57,700 --> 00:03:05,666
So a good approximation would be to set C close to one. Because then the relative error becomes almost one,

33
00:03:05,667 --> 00:03:10,966
but in return--and you see that in the running time here--the running time actually becomes very large,

34
00:03:10,967 --> 00:03:17,666
because if C is close to one, then you get a very small denominator here, which means that the overall running time,

35
00:03:17,667 --> 00:03:21,499
since it's a fraction depending on this, gets very large.

36
00:03:21,500 --> 00:03:28,799
But if you're happy with a--and I'm going to just call it bad approximation, then C can actually be smaller than one.

37
00:03:28,800 --> 00:03:34,999
C will always be larger than zero, so it'll always be a positive number, but it could be 0.5, 0.2 or even smaller,

38
00:03:35,000 --> 00:03:40,032
what will then happen is, if C is smaller than one--so it's this very tiny fraction,

39
00:03:40,033 --> 00:03:45,132
well, then actually this part here in the denominator will be very close to one.

40
00:03:45,133 --> 00:03:51,899
So the running time is very good, but this fraction here will actually increase.

41
00:03:51,900 --> 00:03:57,299
So in return, forgetting about the running time, you get a relative error that is much worse.

42
00:03:57,300 --> 00:04:03,399
And this is exactly the principle behind the PTAS. So we're trading on the quality of the approximation factor,

43
00:04:03,400 --> 00:04:10,132
which is this fraction here--one over C--against running time. More running time means we can get a better approximation factor.

44
00:04:10,133 --> 00:04:13,966
A worse approximation factor, in return, will mean that the algorithm runs faster.

45
00:04:13,967 --> 00:04:20,565
So this is already pretty amazing. Now, one last detail here: As you'll notice, the running time of the approximation algorithm

46
00:04:20,567 --> 00:04:28,332
is actually polynomially dependent on C, which is why this PTAS is sometimes also referred to as a fully polynomial

47
00:04:28,333 --> 00:04:34,566
time approximation scheme, or F PTAS. For the majority of polynomial time approximation schemes, the running time

48
00:04:34,567 --> 00:04:39,332
actually depends exponentially on the approximation factor that you want to achieve.

49
00:04:39,333 --> 00:04:44,299
But this is something for a more advanced course. I still sometimes find it mind-boggling that KNAPSACK

50
00:04:44,300 --> 00:04:49,899
is NP-complete, meaning that, in terms of intractability, KNAPSACK is likely to be just as hard to solve

51
00:04:49,900 --> 00:04:54,299
as nastier problems such as SAT or CLIQUE. So approximation, I think, shows very well

52
00:04:54,300 --> 00:04:58,966
that even within NP-completeness, there's a broad spectrum of difficulty, if you will.

53
00:04:58,967 --> 00:05:04,399
So, some of the most difficult problems to approximate appear to be CLIQUE and INDEPENDENT set.

54
00:05:04,400 --> 00:05:07,699
And then in the middle of the spectrum, we have problems like VERTEX COVER.

55
00:05:07,700 --> 00:05:12,532
And then we have problems which have a PTAS or even a fully polynomial time approximation scheme,

56
00:05:12,533 --> 00:05:16,599
such as KNAPSACK. All of these problems here are NP-complete.

57
00:05:16,600 --> 00:05:20,532
But some of them can be approximated with no constant factor at all.

58
00:05:20,533 --> 00:05:25,999
Others have algorithms that, if you don't look closely, could almost be mistaken for being polynomial.

59
00:05:26,000 --> 00:05:30,032
Of course, all of this is still under the assumption that P does not equal NP.

60
00:05:30,033 --> 00:05:34,899
If P equals NP, then all of these problems here would be polynomial time solvable, anyway.

61
00:05:34,900 --> 00:05:41,366
Now that we have looked at approximation, what else is there that you could try to tackle NP-complete problems?

62
00:05:41,367 --> 00:05:49,532
Next time we'll be basically poking around, meaning that we'll be looking at how randomization and random algorithms

63
00:05:49,533 --> 00:05:54,033
could maybe help us tackle these problems here as well. So, see you next time.
