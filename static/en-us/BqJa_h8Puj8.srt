1
00:00:00,140 --> 00:00:04,450
All right, so 1,
keep track of the MDP we have so far.

2
00:00:04,450 --> 00:00:10,190
We're saying that any unknown state
action pair is an arm act self loop.

3
00:00:10,190 --> 00:00:14,860
We can get the maximum amount of reward
and we can get back to that state and

4
00:00:14,860 --> 00:00:16,379
get it again and again.

5
00:00:16,379 --> 00:00:17,040
So clearly,

6
00:00:17,040 --> 00:00:19,900
there's no policy that's going to be
better than that from that state.

7
00:00:19,900 --> 00:00:20,720
>> Right.

8
00:00:20,720 --> 00:00:23,560
>> So then we're going to solve the MDP,
again,

9
00:00:23,560 --> 00:00:26,840
with the assumption that
anything we don't know is Rmax.

10
00:00:26,840 --> 00:00:28,690
>> So solve the MDP assuming 1 and 2.

11
00:00:28,690 --> 00:00:30,560
>> And then we go back to 1,
essentially, right?

12
00:00:30,560 --> 00:00:32,100
>> Well,
you just execute the optimal policy.

13
00:00:32,100 --> 00:00:33,205
>> Like that.
Yeah, and

14
00:00:33,205 --> 00:00:36,863
then as we take those action we
may discover that some of the 8

15
00:00:36,863 --> 00:00:40,690
action pairs were wrong, but
we'll just fix that by doing 1.

16
00:00:40,690 --> 00:00:41,898
And we'll just solve the new MDP.

17
00:00:41,898 --> 00:00:42,942
>> Good, all right, so

18
00:00:42,942 --> 00:00:46,377
there's a couple of things we need
to convince ourselves about this.

19
00:00:46,377 --> 00:00:49,860
One is that we get
Epsilon optimal reward.

20
00:00:49,860 --> 00:00:50,620
>> Yeah.

21
00:00:50,620 --> 00:00:52,930
>> Another one is that the number of
mistakes that it makes is going to be

22
00:00:52,930 --> 00:00:54,050
relatively small.

23
00:00:54,050 --> 00:00:57,370
>> I think the second one's
probably pretty easy, right?

24
00:00:57,370 --> 00:00:58,739
>> Well, let's go through it.
