1
00:00:00,237 --> 00:00:01,905
Now let's look at the kernel itself.

2
00:00:01,905 --> 00:00:05,409
Recall that this will look like a serial program that will run on one thread,

3
00:00:05,409 --> 00:00:09,723
and the CPU is responsible for launching that program on many parallel threads.

4
00:00:09,723 --> 00:00:12,983
This kernel indeed looks exactly like a serial program.

5
00:00:12,983 --> 00:00:16,417
So here's our kernel right here, this global void square.

6
00:00:16,417 --> 00:00:22,659
Here's the interesting things about this short kernel program. First global.

7
00:00:22,659 --> 00:00:26,295
This is underscore, underscore, global, underscore, underscore.

8
00:00:26,295 --> 00:00:32,926
That's a C language construct called a "declaration specifier"--for short, deckle speck.

9
00:00:32,926 --> 00:00:33,966
Don't worry about the name.

10
00:00:33,966 --> 00:00:39,399
Just know that this is the way that cuda knows this code is a kernel as opposed to CPU code.

11
00:00:39,399 --> 00:00:41,235
Next we have void.

12
00:00:41,235 --> 00:00:43,835
Void just means the kernel doesn't return a value.

13
00:00:43,835 --> 00:00:47,838
Instead it writes the output into the pointer specified in its argument list.

14
00:00:47,838 --> 00:00:49,877
This kernel takes two arguments.

15
00:00:49,877 --> 00:00:53,613
These are pointers to the output and the input arrays.

16
00:00:53,613 --> 00:00:59,752
Recall that both these pointers need to be allocated on the GPU, or else your program will crash spectacularly.

17
00:00:59,752 --> 00:01:04,524
Now note that I name them with d out and d in.

18
00:01:04,524 --> 00:01:07,600
That's certainly not foolproof, but if I called it d_

19
00:01:07,600 --> 00:01:10,269
and I'm consistent about the way I allocate my variables,

20
00:01:10,269 --> 00:01:14,453
I know that d_ variables are allocated on the GPU.

21
00:01:14,453 --> 00:01:16,713
Let's walk through the body of the kernel.

22
00:01:16,713 --> 00:01:18,577
So the first line of the body here.

23
00:01:18,577 --> 00:01:21,913
Remember how I said that each thread know its own index?

24
00:01:21,913 --> 00:01:23,616
Here's how we get that index.

25
00:01:23,616 --> 00:01:27,985
CUDA has a built in variable called thread index, threadIDX,

26
00:01:27,985 --> 00:01:31,458
and that's going to tell each thread its index within a block.

27
00:01:31,458 --> 00:01:39,398
ThreadIDX is actually a c struct with 3 members. .x, .y, and .z. The c struct is called a dim3.

28
00:01:39,398 --> 00:01:43,349
We're just using .x in this code, but we'll explain the others a little bit later.

29
00:01:43,349 --> 00:01:45,846
Now, we'll launch 64 threads.

30
00:01:45,846 --> 00:01:50,766
For the first instance of those threads, threadIDX.x will return zero,

31
00:01:50,766 --> 00:01:55,270
for the second instance 1, and so on, up to 63 for the last element.

32
00:01:55,270 --> 00:01:59,157
Everything else in this kernel just looks like straightforward C.

33
00:01:59,157 --> 00:02:01,174
It looks just like a serial program.

34
00:02:01,174 --> 00:02:02,842
What are we actually doing in this kernel?

35
00:02:02,842 --> 00:02:07,386
Well, for each thread we're going to first read the array element corresponding

36
00:02:07,386 --> 00:02:10,018
to this thread index from global memory.

37
00:02:10,018 --> 00:02:12,819
We're going to store it in this float variable f.

38
00:02:12,819 --> 00:02:18,141
We're then going to square f, and we're going to write that value back to global memory

39
00:02:18,141 --> 00:02:21,623
in the output array element that corresponds to our thread index.
