1
00:00:00,460 --> 00:00:04,310
So how do we find the correct values
of theta to minimize our cost function

2
00:00:04,310 --> 00:00:05,500
J of big theta?

3
00:00:05,500 --> 00:00:09,240
We'll use a search algorithm that
takes some initial guess for theta and

4
00:00:09,240 --> 00:00:12,990
iteratively changes theta, so
that J of theta keeps on getting smaller

5
00:00:12,990 --> 00:00:14,860
until it converges on
some minimum value.

6
00:00:15,940 --> 00:00:19,160
The algorithm we're going to
discuss is called gradient descent.

7
00:00:19,160 --> 00:00:21,780
Here's a one dimensional depiction of
what gradient descent might look like.

8
00:00:22,810 --> 00:00:25,870
We have some starting point
where J of theta is large, and

9
00:00:25,870 --> 00:00:28,320
we continue to try new values of theta.

10
00:00:28,320 --> 00:00:31,234
And J of theta keeps on
getting smaller and smaller,

11
00:00:31,234 --> 00:00:33,496
until we arrive here, the final value.

12
00:00:33,496 --> 00:00:36,483
This theta value gives us our
smallest value of J(theta).

13
00:00:36,483 --> 00:00:39,450
And so
we know that we've minimized J(theta).

14
00:00:39,450 --> 00:00:41,970
This is what gradient descent
might look like in one dimension.

15
00:00:41,970 --> 00:00:43,955
Let's see what it looks
like in two dimensions.

16
00:00:43,955 --> 00:00:47,133
Here again we see that we start
at some high value of J(theta).

17
00:00:47,133 --> 00:00:49,886
And we continue to iteratively
update theta 1 and

18
00:00:49,886 --> 00:00:53,633
theta 2 until we arrive here,
the global minimum of J(big theta)
