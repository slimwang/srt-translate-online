1
00:00:00,000 --> 00:00:02,850
Okay. Well, this first code snippet doesn't assign streams at all.

2
00:00:02,850 --> 00:00:07,715
So none of these operations declare a stream, and therefore they're all in the default stream.

3
00:00:07,715 --> 00:00:10,578
And since they are in the same stream, each one must complete

4
00:00:10,578 --> 00:00:12,950
before the next one in that stream is allowed to run.

5
00:00:12,950 --> 00:00:17,697
So cudaMemcpy will run for 1 second, kernel A will launch and run for 1 second,

6
00:00:17,697 --> 00:00:21,590
and cudaMemcpy will run from another second, will copy the memory down,

7
00:00:21,590 --> 00:00:24,095
do some operation on it, copy the memory back up.

8
00:00:24,095 --> 00:00:27,539
And after 3 seconds the results will be ready for the host.

9
00:00:27,539 --> 00:00:32,435
This next code snippet does declare a stream. It puts them all in s1.

10
00:00:32,435 --> 00:00:34,849
And so since all 3 operations are in the same stream,

11
00:00:34,849 --> 00:00:38,517
the previous operation must complete before the next one can start.

12
00:00:38,517 --> 00:00:40,919
So once again, this will take a second, this will take a second,

13
00:00:40,919 --> 00:00:44,777
this will take a second, and the final result will be ready in 3 seconds.

14
00:00:44,777 --> 00:00:46,561
This one's a bit more complicated.

15
00:00:46,561 --> 00:00:49,223
Here I'm doing 2 operations on different chunks of memory,

16
00:00:49,223 --> 00:00:56,607
so I'm copying h array 1 into d array 1, calling kernel A on d array 1,

17
00:00:56,607 --> 00:00:59,978
and copying d array 1 back to h array 1.

18
00:00:59,978 --> 00:01:07,531
And then I'm copying h array 2 to d array 2, doing some operation on d array 2 by launching kernel B,

19
00:01:07,531 --> 00:01:10,524
and copying d array 2 back to h array 2.

20
00:01:10,524 --> 00:01:13,218
So the first 3 operations are in stream 1.

21
00:01:13,218 --> 00:01:15,820
So cudaMemcpyAsync will be called,

22
00:01:15,820 --> 00:01:18,907
and it'll start a memory copy to happen on the GPU

23
00:01:18,907 --> 00:01:21,960
and then CPU execution will continue; the next statement will happen.

24
00:01:21,960 --> 00:01:25,872
We don't stop and wait for Memcpy to finish before we execute the next statement,

25
00:01:25,872 --> 00:01:27,895
which is the launch of kernel A.

26
00:01:27,895 --> 00:01:30,001
Kernel launches are always asynchronous.

27
00:01:30,001 --> 00:01:33,653
So, in effect, this queues up the launch of kernel A,

28
00:01:33,653 --> 00:01:36,610
which will be run on the GPU after this Memcpy has finished,

29
00:01:36,610 --> 00:01:40,454
and it proceeds to the next statement, which is another cudaMemcpyAsync,

30
00:01:40,454 --> 00:01:44,199
to copy the results of kernel A back onto the host.

31
00:01:44,199 --> 00:01:48,188
All 3 of these are in stream 1, so they will all wait for each other on the GPU,

32
00:01:48,188 --> 00:01:51,959
but the host just issues a command, issues a command, issues a command.

33
00:01:51,959 --> 00:01:57,396
It doesn't stop. It proceeds to cudaMemcpyAsync in s2.

34
00:01:57,396 --> 00:02:00,840
And so now it starts another asynchronous memory transfer.

35
00:02:00,840 --> 00:02:05,708
This one, because it's in s2, doesn't have to wait for any of these operations to complete,

36
00:02:05,708 --> 00:02:08,685
so it gets started right away. It'll complete in 1 second.

37
00:02:08,685 --> 00:02:13,645
It calls kernel B in 1 block of 192 threads, also in s2,

38
00:02:13,645 --> 00:02:19,265
and then calls cudaMemcpyAsync, again in s2, to copy the results back.

39
00:02:19,265 --> 00:02:25,013
So the first memory copy will happen, these next 2 operations in s1 will get queued up,

40
00:02:25,013 --> 00:02:28,954
and the next memory copy will happen essentially immediately.

41
00:02:28,954 --> 00:02:32,161
So in the first second this Memcpy and this Memcpy will be running,

42
00:02:32,161 --> 00:02:35,441
in the next second kernel A will be running and kernel B will be running,

43
00:02:35,441 --> 00:02:39,170
and in the final second the return Memcpy will be running in s1

44
00:02:39,170 --> 00:02:41,986
and the return Memcpy will be running in s2.

45
00:02:41,986 --> 00:02:44,043
And so the minimum amount of time this can take,

46
00:02:44,043 --> 00:02:49,384
if all of the operations that can run concurrently do run concurrently, will again be 3 seconds.

47
00:02:49,384 --> 00:02:52,956
But in this 3 seconds we'll have actually gotten twice as much work done.

48
00:02:52,956 --> 00:02:54,927
We've overlapped the execution here.

49
00:02:54,927 --> 00:02:59,591
And I phrased this very carefully when I talked about the minimum time this is going to take.

50
00:02:59,591 --> 00:03:05,496
It's worth noting that there's a lot of reasons why it might not be able to overlap all of these operations.

51
00:03:05,496 --> 00:03:10,035
Some older GPUs can't do this many asynchronous Memcpys at the same time.

52
00:03:10,035 --> 00:03:16,721
And I was pretty careful to launch a single block, so this probably will not fill the GPU.

53
00:03:16,721 --> 00:03:20,952
On the other hand, if kernel A had run tens of thousands of blocks,

54
00:03:20,952 --> 00:03:24,014
it might well fill up the GPU with those blocks

55
00:03:24,014 --> 00:03:29,026
before kernel B got a chance to start filling the GPU with any of its blocks.

56
00:03:29,026 --> 00:03:33,959
And so kernel B might end up waiting on kernel A simply because resources are not available to run it.

57
00:03:33,959 --> 00:03:38,195
The final example is the same operations just reorganized.

58
00:03:38,195 --> 00:03:41,765
So in this case we issue both Memcpys in stream 1 and 2,

59
00:03:41,765 --> 00:03:47,364
we launch both kernels in stream 1 and 2, and we issue both Memcpys back in stream 1 and 2.

60
00:03:47,364 --> 00:03:49,645
So rearranging these things shouldn't make a difference.

61
00:03:49,645 --> 00:03:53,110
The minimum time that this could take is still 3 seconds.
