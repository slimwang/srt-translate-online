1
00:00:00,370 --> 00:00:04,650
Consider our robot here that's
interacting with the world.

2
00:00:04,650 --> 00:00:10,920
The first thing it sees is some state,
and as we've talked about already,

3
00:00:10,920 --> 00:00:15,120
the thing to do now is to take that
state, go look in the Q table and

4
00:00:15,120 --> 00:00:19,710
find the action that corresponds
to the maximum Q value.

5
00:00:19,710 --> 00:00:21,860
And then take that action.

6
00:00:21,860 --> 00:00:25,130
Once that action is taken,
that results in two new things.

7
00:00:25,130 --> 00:00:32,530
One is a new state,
we call that S prime, and a reward.

8
00:00:32,530 --> 00:00:34,970
All that information
comes into the robot, and

9
00:00:34,970 --> 00:00:38,900
it needs to use that information
to update its Q table.

10
00:00:38,900 --> 00:00:43,770
So as a consequence of this interaction
with the world, it's got an s,

11
00:00:43,770 --> 00:00:46,840
an a, an s prime, and an r.

12
00:00:47,840 --> 00:00:52,060
How does it take that information
to improve this Q table?

13
00:00:52,060 --> 00:00:54,770
There are two main parts
to the update rule.

14
00:00:54,770 --> 00:00:59,612
The first is, what is the old
value that we used to have?

15
00:00:59,612 --> 00:01:01,692
And that's Q [s, a].

16
00:01:02,780 --> 00:01:04,920
And what is our improved estimate?

17
00:01:04,920 --> 00:01:07,320
And we want to blend them together.

18
00:01:07,320 --> 00:01:10,740
And to combine them,
we introduce this new variable, alpha.

19
00:01:10,740 --> 00:01:13,110
Alpha is the learning rate.

20
00:01:13,110 --> 00:01:16,890
Alpha can take on any value from 0 to 1.

21
00:01:16,890 --> 00:01:21,430
Usually, we use about 0.2.

22
00:01:21,430 --> 00:01:27,300
And what that means is,
in our new improved version of Q,

23
00:01:27,300 --> 00:01:32,410
which I indicate over here as Q prime,
it's a blend of alpha

24
00:01:32,410 --> 00:01:38,140
times the improved estimate,
plus 1 minus alpha of the old value.

25
00:01:38,140 --> 00:01:41,920
So larger values of alpha cause
us to learn more quickly,

26
00:01:41,920 --> 00:01:46,610
lower values of alpha cause
the learning to be more slow.

27
00:01:46,610 --> 00:01:50,460
So a low value of alpha, for instance,
means that in this update rule,

28
00:01:50,460 --> 00:01:55,250
the previous value for
Q of sa is more strongly preserved.

29
00:01:56,370 --> 00:01:58,840
So stretching it out
a little bit more detail.

30
00:01:58,840 --> 00:02:04,820
Again we have here,
our current value for the Q at s,a,

31
00:02:04,820 --> 00:02:12,830
plus alpha times the immediate reward,
plus gamma times later rewards.

32
00:02:12,830 --> 00:02:17,019
Now we're introducing gamma here, so I
promise there's only two new parameters

33
00:02:17,019 --> 00:02:19,330
here that you have to worry about.

34
00:02:19,330 --> 00:02:21,360
What gamma is is the discount rate.

35
00:02:22,640 --> 00:02:27,590
And similar to alpha,
gamma usually ranges from 0 to 1.

36
00:02:27,590 --> 00:02:35,060
A low value of gamma means that
we value later rewards less.

37
00:02:35,060 --> 00:02:38,108
Remember the discount rate when
we were talking about bonds?

38
00:02:38,108 --> 00:02:40,683
Same thing.

39
00:02:40,683 --> 00:02:48,660
A low value of gamma equates to
essentially a high discount rate.

40
00:02:48,660 --> 00:02:51,950
The high value of gamma,
in other words a gamma near 1,

41
00:02:51,950 --> 00:02:57,440
means that we value later
rewards very significantly.

42
00:02:57,440 --> 00:03:03,130
If we were to use 1.0, that means
a reward 20 steps in the future

43
00:03:03,130 --> 00:03:06,170
is worth just as much
as a reward right now.

44
00:03:06,170 --> 00:03:10,100
Now, we have to expand this component
here in a little bit more detail.

45
00:03:11,130 --> 00:03:15,020
This next part here is a little
bit tricky, but don't worry,

46
00:03:15,020 --> 00:03:17,710
we're going to step
through it step by step.

47
00:03:17,710 --> 00:03:22,550
This component represents our
future discounted rewards.

48
00:03:22,550 --> 00:03:26,744
In other words, we end up in state
s prime, and from then on out,

49
00:03:26,744 --> 00:03:31,439
we're going to act optimally, or
at least, the best that we know how to.

50
00:03:31,439 --> 00:03:36,076
And the question is,
what is the value of those future

51
00:03:36,076 --> 00:03:41,550
rewards if we reach state S prime and
we act appropriately?

52
00:03:41,550 --> 00:03:47,290
Well, it is simply that Q value, but
we have to find out what the action

53
00:03:47,290 --> 00:03:51,700
is that we would have taken, so that
we can reference the Q table properly.

54
00:03:51,700 --> 00:03:56,760
So, If we're in state s prime,
the action that

55
00:03:56,760 --> 00:04:01,760
we would take that would maximize
our future reward is argmax a prime,

56
00:04:01,760 --> 00:04:05,440
so a prime is the next action
that we're going to take.

57
00:04:05,440 --> 00:04:08,500
With regard to Q, s prime, a prime.

58
00:04:08,500 --> 00:04:11,540
So we're going to find
that best a prime,

59
00:04:11,540 --> 00:04:16,507
that best action, that maximizes
the value when we're in that state.

60
00:04:16,507 --> 00:04:21,500
So this collapses just to a prime, and

61
00:04:21,500 --> 00:04:25,960
then we look up the Q table value for
Q s prime, a prime.

62
00:04:27,050 --> 00:04:29,970
So that allows us to bring
it all together now.

63
00:04:29,970 --> 00:04:32,560
So our update rule is the following.

64
00:04:32,560 --> 00:04:37,334
Our new Q value in state s,
action a is that

65
00:04:37,334 --> 00:04:42,440
old value multiplied by 1 minus alpha.

66
00:04:42,440 --> 00:04:46,630
So depending on how large alpha is,
we valued that old value more or

67
00:04:46,630 --> 00:04:51,970
less, plus alpha times
our new best estimate.

68
00:04:53,360 --> 00:04:57,480
And our new best estimate is,
again, our immediate reward,

69
00:04:57,480 --> 00:05:02,350
plus the discounted reward for
all of our future actions.

70
00:05:02,350 --> 00:05:03,520
And that's it.

71
00:05:03,520 --> 00:05:06,150
This is the equation you need
to know to implement Q learning.
