1
00:00:00,370 --> 00:00:03,500
Alright, Michael, so, here's what you have before you. You have the same

2
00:00:03,500 --> 00:00:07,260
housing data that we've looked at a couple of times before. I've, for the

3
00:00:07,260 --> 00:00:10,500
sake of readability, I've drawn over, some of the data points so that

4
00:00:10,500 --> 00:00:15,000
they're easier to see. This is exactly the data, that we've always had. Okay?

5
00:00:15,000 --> 00:00:15,680
>> Okay.

6
00:00:15,680 --> 00:00:18,830
>> Now, you'll notice that I marked one of

7
00:00:18,830 --> 00:00:21,860
them as green, because here's what we're going to do.

8
00:00:21,860 --> 00:00:23,430
I'm going to take the housing data you've got, I'm

9
00:00:23,430 --> 00:00:25,470
going to do some ensemble learning on it. And I'm

10
00:00:25,470 --> 00:00:28,050
going to hold out the green data point. Okay? So of

11
00:00:28,050 --> 00:00:30,110
the nine data points, you're only going to learn on 8

12
00:00:30,110 --> 00:00:32,180
of them. And I'm going to add that green data point

13
00:00:32,180 --> 00:00:35,460
as my test example and see how it does. Okay?

14
00:00:35,460 --> 00:00:38,130
>> Okay. So that sounds like, cross validation.

15
00:00:38,130 --> 00:00:41,790
>> It does. This is a cross validation. Or you could just say,

16
00:00:41,790 --> 00:00:46,080
I just put my training set and my test set on the same slide.

17
00:00:46,080 --> 00:00:46,230
>> Okay.

18
00:00:46,230 --> 00:00:50,620
>> Okay, Michael, so the first thing I'm going to do is I'm going to pick

19
00:00:50,620 --> 00:00:54,620
a random subset of these points. And just for the

20
00:00:54,620 --> 00:00:59,560
sake of the example, I'm going to pick five points

21
00:00:59,560 --> 00:01:01,530
randomly. And I'm going to do that five times. So

22
00:01:01,530 --> 00:01:05,840
I'm going to have five subsets of five examples. And by the

23
00:01:05,840 --> 00:01:08,610
way, I'm going to choose those randomly, and I'm going to choose

24
00:01:08,610 --> 00:01:10,550
them with replacement. So we're not going to end up in the

25
00:01:10,550 --> 00:01:13,310
situation we ended up just a couple of minutes ago

26
00:01:13,310 --> 00:01:15,640
where we never go to see the same data point twice.

27
00:01:15,640 --> 00:01:15,850
Okay?

28
00:01:15,850 --> 00:01:16,940
>> Yeah.

29
00:01:16,940 --> 00:01:21,590
>> Alright. So 5 subsets of 5 examples, and then I'm

30
00:01:21,590 --> 00:01:26,320
going to learn a third order polynomial. And I'm going to take those

31
00:01:26,320 --> 00:01:28,870
3rd order polynomials. I'm just going to learn on that subset, and then

32
00:01:28,870 --> 00:01:31,570
I'm going to combine them by averaging. Want to see what we get?

33
00:01:31,570 --> 00:01:33,010
>> Oh, yeah, sure.

34
00:01:33,010 --> 00:01:35,800
>> So here's what you get, Michael. Here I'm showing you a plot over

35
00:01:35,800 --> 00:01:38,400
those same points, with the five different

36
00:01:38,400 --> 00:01:41,000
3rd order polynomials. Can you see them?

37
00:01:41,000 --> 00:01:44,680
>> Yeah. They're, right. There's like a bunch of wispy hairs.

38
00:01:44,680 --> 00:01:47,320
>> Just like most third order polynomials. And as

39
00:01:47,320 --> 00:01:49,020
you can see they're, they're kind of you stare at

40
00:01:49,020 --> 00:01:51,690
them and you see their kind of similar. But

41
00:01:51,690 --> 00:01:54,300
some of them they veer off a little bit because

42
00:01:54,300 --> 00:01:56,910
they're looking at different data points. One of them

43
00:01:56,910 --> 00:01:58,980
actually very hard to see because it's only one like

44
00:01:58,980 --> 00:02:02,920
this. Actually veers off like this because just, purely

45
00:02:02,920 --> 00:02:05,850
randomly, it never got to see the two final points.

46
00:02:05,850 --> 00:02:06,450
>> I see.

47
00:02:06,450 --> 00:02:08,740
But they all, but they all seem to be pretty much in

48
00:02:08,740 --> 00:02:10,460
agreement, like between points three and

49
00:02:10,460 --> 00:02:12,360
four. There's a lot of consistency there.

50
00:02:12,360 --> 00:02:14,140
>> Right. Because just picking five of the

51
00:02:14,140 --> 00:02:16,280
subsets you seem to be able to either get

52
00:02:16,280 --> 00:02:17,780
things on the end, or you get things in

53
00:02:17,780 --> 00:02:19,950
the middle. And maybe one or two things on

54
00:02:19,950 --> 00:02:21,190
the end it sort of works out. Even the

55
00:02:21,190 --> 00:02:24,890
one that doesn't see the, the last two points

56
00:02:24,890 --> 00:02:26,580
still got to see a bunch of first ones

57
00:02:26,580 --> 00:02:29,760
and get this part of this space fairly right.

58
00:02:29,760 --> 00:02:30,590
>> Cool.

59
00:02:30,590 --> 00:02:31,310
>> Okay.

60
00:02:31,310 --> 00:02:34,670
So the question now becomes how good is

61
00:02:34,670 --> 00:02:36,810
the average of these compared to something we might

62
00:02:36,810 --> 00:02:39,760
have learned over the entire data set? And here's

63
00:02:39,760 --> 00:02:42,961
what we get when do that. So what you're

64
00:02:42,961 --> 00:02:46,354
looking at now Michael, is the red line,

65
00:02:46,354 --> 00:02:49,573
is the average of all of those five third

66
00:02:49,573 --> 00:02:53,342
order polynomials. And the blue line, is the fourth

67
00:02:53,342 --> 00:02:56,540
order polynomial that we learned when we did this

68
00:02:56,540 --> 00:03:00,295
with simple regression, a couple of lessons back.

69
00:03:00,295 --> 00:03:00,315
>> Okay.

70
00:03:00,315 --> 00:03:02,380
>> And you actually see them pretty close.

71
00:03:02,380 --> 00:03:05,260
>> Why is one of them a fourth order, and one a third order?

72
00:03:05,260 --> 00:03:09,140
>> Well what I wanted to do is try a simpler set of hypothesis,

73
00:03:09,140 --> 00:03:12,050
than we were doing, than when we were doing full blown regression. So third

74
00:03:12,050 --> 00:03:16,500
order simpler than fourth order. So, I thought we'd combine a bunch of simpler

75
00:03:16,500 --> 00:03:19,940
rules. Then the one we had used before and see how well it does.

76
00:03:19,940 --> 00:03:20,850
>> You want to know how well it does?

77
00:03:20,850 --> 00:03:21,700
>> I would!

78
00:03:21,700 --> 00:03:24,680
>> Well it turns out that on this data

79
00:03:24,680 --> 00:03:26,630
set and I did this many, many, many times

80
00:03:26,630 --> 00:03:28,470
just to see what would happen with many different

81
00:03:28,470 --> 00:03:31,550
random subsets. It typically is the case that the

82
00:03:31,550 --> 00:03:34,240
blue line always does better on the training set,

83
00:03:34,240 --> 00:03:37,320
the red points, than the red line does. But

84
00:03:37,320 --> 00:03:39,790
the red line almost always does better on the

85
00:03:39,790 --> 00:03:42,490
green point on the test set or the validation set.

86
00:03:42,490 --> 00:03:43,510
>> Interesting.

87
00:03:43,510 --> 00:03:46,940
>> That is kind of interesting. So wait, so let me get this straight.

88
00:03:46,940 --> 00:03:50,690
It seems sort of magical. So, so it learns an average

89
00:03:50,690 --> 00:03:54,710
of third degree polynomials, third order polynomials, which is itself a

90
00:03:54,710 --> 00:03:58,460
third order polynomial. But you're saying it does better by doing

91
00:03:58,460 --> 00:04:03,320
this kind of trick than just learning a third order polynomial directly.

92
00:04:03,320 --> 00:04:05,180
>> Yeah, why might you think that might be?

93
00:04:06,470 --> 00:04:08,420
I have a guess, you tell me what you think.

94
00:04:08,420 --> 00:04:12,050
>> Wow, so well, I mean, you know, the danger is often

95
00:04:12,050 --> 00:04:16,829
over fitting, over fitting is like the scary possibility. And so maybe

96
00:04:16,829 --> 00:04:20,930
by, by kind of mixing the data up in this way and focusing

97
00:04:20,930 --> 00:04:24,330
on different subsets of it. I don't know. Somehow manages to find the

98
00:04:24,330 --> 00:04:26,170
important structure as opposed to getting

99
00:04:26,170 --> 00:04:28,020
misled by any of the individual datapoints.

100
00:04:28,020 --> 00:04:29,640
>> Yeah. That's the basic idea. It's kind of the

101
00:04:29,640 --> 00:04:31,880
same thing, at least that's what I, I think that's

102
00:04:31,880 --> 00:04:34,650
a good answer. It's basically the same kind of argument

103
00:04:34,650 --> 00:04:37,080
you make for cross validation. Alright. You take a random

104
00:04:37,080 --> 00:04:42,440
bunch of subsets. You don't get trapped by one or two points that happen to

105
00:04:42,440 --> 00:04:44,290
be wrong because they happen to be wrong

106
00:04:44,290 --> 00:04:46,090
because of noise or whatever. And you sort

107
00:04:46,090 --> 00:04:48,180
of average out all of the variances and

108
00:04:48,180 --> 00:04:51,140
the differences. Hm. And often times it works.

109
00:04:51,140 --> 00:04:55,180
And in practice this particular technique of ensemble learning

110
00:04:55,180 --> 00:04:58,150
does quite well in getting rid of overfitting.

111
00:04:58,150 --> 00:04:59,680
>> And what is this called?

112
00:04:59,680 --> 00:05:02,230
>> So, this particular version, where you take

113
00:05:02,230 --> 00:05:06,320
a random subset and you combine by the mean, it's called bagging.

114
00:05:06,320 --> 00:05:09,350
>> And I guess the bags are the random subsets?

115
00:05:09,350 --> 00:05:09,930
>> Sure.

116
00:05:09,930 --> 00:05:12,630
>> [LAUGH]

117
00:05:12,630 --> 00:05:13,610
That's how I'm going to think of it.

118
00:05:13,610 --> 00:05:15,940
>> That's how I'm going to think of it. It also has another name

119
00:05:15,940 --> 00:05:19,880
which is called bootstrap aggregation. So I

120
00:05:19,880 --> 00:05:21,900
guess the different subsets are the boots.

121
00:05:21,900 --> 00:05:24,520
>> [LAUGH] No,no, no, no bootstrap usually

122
00:05:24,520 --> 00:05:27,070
refers to pulling yourself up by your bootstraps.

123
00:05:27,070 --> 00:05:27,900
>> Yeah, I

124
00:05:27,900 --> 00:05:30,730
like my, I like my answer better. So, each of the subsets

125
00:05:30,730 --> 00:05:34,120
are the boots and the averaging is the strap. And there you go.

126
00:05:34,120 --> 00:05:37,250
So, regardless of whether you call it bootstrap aggregation or you call it

127
00:05:37,250 --> 00:05:40,890
bagging, you'll notice it's not what I said we were going to talk about

128
00:05:40,890 --> 00:05:45,230
during today's discussion. I said we were going to talk about boosting. So we're

129
00:05:45,230 --> 00:05:46,380
talking about bagging but we're going to

130
00:05:46,380 --> 00:05:48,020
talk about boosting. The reason I wanted

131
00:05:48,020 --> 00:05:50,460
to talk about bagging is because it's really the simplest thing you can

132
00:05:50,460 --> 00:05:53,010
think of and it actually works remarkably well. But there are a couple

133
00:05:53,010 --> 00:05:54,490
of things that are wrong with it, or a couple

134
00:05:54,490 --> 00:05:58,070
of things you might imagine you might do better. That

135
00:05:58,070 --> 00:06:00,800
might address some of the issues and we're going to see

136
00:06:00,800 --> 00:06:03,630
all of those when we talk about boosting right now.
