1
00:00:00,510 --> 00:00:03,660
So, like we said earlier in this lesson, we've actually been talking about kinds

2
00:00:03,660 --> 00:00:06,300
of meta-cognition throughout this course, even if we didn't call it that at

3
00:00:06,300 --> 00:00:09,570
the time. We were talking about agents reflecting on their own knowledge, and

4
00:00:09,570 --> 00:00:13,370
correcting it when they were introduced to a mistake. Earlier in this lesson,

5
00:00:13,370 --> 00:00:16,340
we also talked about the possibility that an agent would reflect on the learning

6
00:00:16,340 --> 00:00:18,860
process that led it to the incorrect knowledge, and

7
00:00:18,860 --> 00:00:22,560
correct that learning process, as well. Back during partial order planning,

8
00:00:22,560 --> 00:00:25,150
we talked about agents that could balance multiple plans and

9
00:00:25,150 --> 00:00:28,646
resolve conflicts between those plans. This could be seen as a form of

10
00:00:28,646 --> 00:00:32,366
meta-cognition as well. The agent plans out a plan for achieving one goal,

11
00:00:32,366 --> 00:00:36,187
a plan for achieving the other goal, and then thinks about its own plans for

12
00:00:36,187 --> 00:00:39,477
those two goals. Then it detects the conflict between those two plans and

13
00:00:39,477 --> 00:00:43,022
it resolves that conflict accordingly. Then it detects the conflict between

14
00:00:43,022 --> 00:00:46,540
those two plans and creates a new plan to avoid that conflict.

15
00:00:46,540 --> 00:00:50,520
Here the agent is reasoning over its own planning process. We saw this in

16
00:00:50,520 --> 00:00:54,270
production systems as well. We had an agent that reached an impasse, it had two

17
00:00:54,270 --> 00:00:57,100
different pitches which is suggested and it couldn't decide between the two.

18
00:00:57,100 --> 00:01:01,180
Let's find a new learning goal to find a rule to choose between those pitches.

19
00:01:01,180 --> 00:01:05,300
It then selected a learning strategy, chunking, went into its memory, found

20
00:01:05,300 --> 00:01:10,310
a case, and chunked a rule that would it resolve that impasse. In this case,

21
00:01:10,310 --> 00:01:14,000
the agent used that impasse to set up a new learning goal. It didn't select

22
00:01:14,000 --> 00:01:18,690
the strategy, strategy selection, to achieve that learning goal. We can also see

23
00:01:18,690 --> 00:01:22,860
medicognition in version spaces. Our agent has the notion of specific and

24
00:01:22,860 --> 00:01:26,470
general models, and it also has the notion of convergence. The agent is

25
00:01:26,470 --> 00:01:30,040
consistently thinking about it's own specific and general model, and looking for

26
00:01:30,040 --> 00:01:34,020
opportunities to converge them down into one model of the concept. And finally,

27
00:01:34,020 --> 00:01:37,720
we can very clearly see metacognition in our lesson on diagnosis.

28
00:01:37,720 --> 00:01:40,710
We talked about how all the results for our treatment become new data for our

29
00:01:40,710 --> 00:01:45,170
iterative process of diagnosis. If our treatment didn't spond desirable results,

30
00:01:45,170 --> 00:01:48,860
it also sponds data for the metal layer. Not only do we still want to diagnose

31
00:01:48,860 --> 00:01:52,820
the current malfunction,. But we also want to diagnose, why we weren't able to

32
00:01:52,820 --> 00:01:56,360
diagnose it correctly in the first place. So, now we're diagnosing the problem

33
00:01:56,360 --> 00:02:00,140
with our diagnosing process. So as we can see, meta cognition's actually been

34
00:02:00,140 --> 00:02:02,830
implicit in several of the topics we've talked about in this course.
