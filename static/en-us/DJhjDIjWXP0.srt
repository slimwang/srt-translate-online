1
00:00:00,260 --> 00:00:02,860
To try to motivate this idea
let's look at a little example.

2
00:00:02,860 --> 00:00:05,566
So let's imagine we've got two agents,
agent 1 and agent 2.

3
00:00:05,566 --> 00:00:07,886
And agent 1 has kind of a goal or

4
00:00:07,886 --> 00:00:11,648
some kind of reward
function that it's running.

5
00:00:11,648 --> 00:00:16,810
And to make that reward function have
high reward, agent 1 needs an apple.

6
00:00:16,810 --> 00:00:20,160
Now the apple is not far away but
it is very close to agent two.

7
00:00:20,160 --> 00:00:25,600
So what are some things that agent
one could do to satisfy its goal.

8
00:00:25,600 --> 00:00:27,350
>> It could walk over to the apple.

9
00:00:27,350 --> 00:00:28,277
>> Good, anything else?

10
00:00:28,277 --> 00:00:32,119
Well, once it gets there can say I
thought you were giving me an iPad and

11
00:00:32,119 --> 00:00:33,655
then kick agent 2 and run.

12
00:00:33,655 --> 00:00:36,710
>> [LAUGH]
I see, wrong apple.

13
00:00:36,710 --> 00:00:38,030
This is the food kind of apple.

14
00:00:38,030 --> 00:00:40,530
But, you're right that probably
this particular agent 1 would

15
00:00:40,530 --> 00:00:42,230
would like an Apple watch.

16
00:00:42,230 --> 00:00:45,490
>> Yes, well actually this particular
agent 1 has an Apple watch.

17
00:00:45,490 --> 00:00:48,620
Then let's see,
the other thing that could happen is

18
00:00:48,620 --> 00:00:52,310
agent 1 could ask agent 2
To bring him the apple.

19
00:00:52,310 --> 00:00:56,240
>> Good and I guess presumably each one
could just also just do nothing and

20
00:00:56,240 --> 00:01:00,410
just wait for agent two to for
the apple to just show up using artwork.

21
00:01:00,410 --> 00:01:04,959
Like maybe agent two would bring it by-
>> I guess it maybe not apples but like

22
00:01:04,959 --> 00:01:09,390
all men's there's certainly agents that
bring each one home mints and ice cream.

23
00:01:09,390 --> 00:01:09,990
>> Yeah.

24
00:01:09,990 --> 00:01:11,270
Have you got ice cream in while?

25
00:01:12,340 --> 00:01:14,130
>> Yeah.
Okay, and then yes.

26
00:01:14,130 --> 00:01:18,410
That's too bad looks like your looks
like your Apple Watch exploded.

27
00:01:18,410 --> 00:01:20,270
>> That's okay,
I'll just get another one.

28
00:01:20,270 --> 00:01:21,720
>> Yeah, because this one's broken.

29
00:01:21,720 --> 00:01:23,310
Anyway, when we're doing planning or

30
00:01:23,310 --> 00:01:26,390
sequential decision making we need
to decide amongst these different

31
00:01:26,390 --> 00:01:30,580
options if you will or different
courses of action that we could take.

32
00:01:30,580 --> 00:01:33,660
How do we usually do that in the
framework that we've been talking about?

33
00:01:33,660 --> 00:01:37,360
>> Well, if we're living in an MDP like
world we actually define the MDP and

34
00:01:37,360 --> 00:01:40,470
I think the secret sauce there is to
figure out what the right rewards are.

35
00:01:40,470 --> 00:01:42,300
>> Right.
>> And the reason I said it that way is

36
00:01:42,300 --> 00:01:46,410
because in the game theory case we
usually call those utilities and

37
00:01:46,410 --> 00:01:49,410
not rewards, and I just want to
differentiate between the two of them.

38
00:01:49,410 --> 00:01:50,960
Okay and then what we do with those?

39
00:01:50,960 --> 00:01:52,290
>> Well once we have rewards or

40
00:01:52,290 --> 00:01:55,870
utilities we then find
a policy that maximizes them.

41
00:01:55,870 --> 00:01:59,380
We maximize our reward actually
as we maximize our utility.

42
00:01:59,380 --> 00:02:03,080
>> It seems like we could do this sort
of thing in the scenario where we've got

43
00:02:03,080 --> 00:02:04,650
multiple agents and
they need to coordinate and

44
00:02:04,650 --> 00:02:05,820
communicate with each other.

45
00:02:05,820 --> 00:02:09,030
But it turns out it starts to get
a little bit hard to define exactly what

46
00:02:09,030 --> 00:02:13,960
the costs are, and was the likelihood
that this is actually going to work and

47
00:02:13,960 --> 00:02:14,680
so forth.

48
00:02:14,680 --> 00:02:17,990
So what we're going to do
is actually think about

49
00:02:17,990 --> 00:02:20,170
just like we did when
we talked about POMDPs.

50
00:02:20,170 --> 00:02:26,120
We said that they allow us to strike a
balance between actions to gain reward.

51
00:02:26,120 --> 00:02:30,100
And actions to gain information and we
don't have special like a special other

52
00:02:30,100 --> 00:02:33,320
way of reasoning about that all of
that being folded into one model.

53
00:02:33,320 --> 00:02:34,820
Where actions are being taken and

54
00:02:34,820 --> 00:02:38,540
they might have informational impact and
they might have reward impact.

55
00:02:38,540 --> 00:02:42,780
The informational impact is subservient
to the reward in a sense right so

56
00:02:42,780 --> 00:02:44,440
the agent just tries to figure out.

57
00:02:44,440 --> 00:02:47,350
I will take an action to gain
information if it helps me get more

58
00:02:47,350 --> 00:02:48,760
reward in the long run.

59
00:02:48,760 --> 00:02:52,640
And so,
the DEC-POMDPs is a similar sort of idea

60
00:02:52,640 --> 00:02:56,340
that's going to let us put coordinating
and communicating along with

61
00:02:56,340 --> 00:02:59,630
actions that they may become actions
just like everything else like this.

62
00:02:59,630 --> 00:03:03,600
Ask for it becomes a kind of action and
walk to Apple becomes a kind of action,

63
00:03:03,600 --> 00:03:06,060
and the agents can reason about
whether that's a good idea or

64
00:03:06,060 --> 00:03:09,151
not depending on how to
maximize their utility.

65
00:03:09,151 --> 00:03:10,391
>> Sounds promising.
