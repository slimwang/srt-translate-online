1
00:00:00,340 --> 00:00:03,420
Consider the problem of
the matrix-matrix multiply operation,

2
00:00:03,420 --> 00:00:05,860
C gets C+A*B.

3
00:00:05,860 --> 00:00:08,500
Suppose we run this operation
on an ideal cash machine.

4
00:00:08,500 --> 00:00:11,750
And let's make our usual
simplifying assumptions.

5
00:00:11,750 --> 00:00:16,100
Namely, that the matrix is square, and
stuff conveniently divides other stuff.

6
00:00:16,100 --> 00:00:19,310
Now if you want to minimize data
transfers in a memory hierarchy

7
00:00:19,310 --> 00:00:24,310
one way is to reorganize the computation
to multiply b by b sub-blocks.

8
00:00:24,310 --> 00:00:27,520
Again for simplicity let's
assume that b divides n.

9
00:00:27,520 --> 00:00:31,060
The corresponding pseudocode algorithm
would look something like this.

10
00:00:31,060 --> 00:00:34,720
Now to make this algorithm cache
efficient, you would just choose b so

11
00:00:34,720 --> 00:00:38,340
that these three b by
b blocks fit in cache.

12
00:00:38,340 --> 00:00:42,250
In other words, b ought to be
proportional to the square root of z.

13
00:00:42,250 --> 00:00:46,270
Now there's a subtle detail here
about what fitting in cache means.

14
00:00:46,270 --> 00:00:50,250
To see it, suppose the matrices
are stored in column major order.

15
00:00:50,250 --> 00:00:53,330
That means the elements of each
column are laid out consecutively

16
00:00:53,330 --> 00:00:56,810
in linear memory addresses,
with one column following the other.

17
00:00:56,810 --> 00:00:59,290
This layout is common in
linear algebra packages,

18
00:00:59,290 --> 00:01:01,970
though note that languages like C and
C++ assume,

19
00:01:01,970 --> 00:01:06,370
by default, that a two dimensional
array uses row major order.

20
00:01:06,370 --> 00:01:10,860
A little more formally,
the ij element maps to a linear address.

21
00:01:10,860 --> 00:01:17,300
The offset is i + j * s, where s is the
stride or sometimes leading dimension.

22
00:01:17,300 --> 00:01:19,270
It has to be at least n.

23
00:01:19,270 --> 00:01:22,620
Now let's take any b by b
block of one of the matrices.

24
00:01:22,620 --> 00:01:25,570
Let's say we just want to
fit this block into cache.

25
00:01:25,570 --> 00:01:29,670
Then it has to be the case
that z is at least L squared.

26
00:01:29,670 --> 00:01:33,110
In literature, this is sometimes
written as the following lower bound.

27
00:01:33,110 --> 00:01:36,423
This condition is called
the tall-cache assumption.

28
00:01:36,423 --> 00:01:40,120
The tall-cache assumption says
the cache should be taller in terms of

29
00:01:40,120 --> 00:01:44,940
the number of lines than it is wide in
terms of the number of words per line.

30
00:01:44,940 --> 00:01:46,640
Let's take a simple example.

31
00:01:46,640 --> 00:01:50,140
Suppose our matrix block is
exactly the size of the cache z

32
00:01:50,140 --> 00:01:52,400
which holds 16 words.

33
00:01:52,400 --> 00:01:55,370
Now suppose the line size is four words.

34
00:01:55,370 --> 00:01:59,480
In this case, the Tall-Cache Assumption
holds assuming the matrix is

35
00:01:59,480 --> 00:02:04,090
stored in column major layout
columns would map to complete lines.

36
00:02:04,090 --> 00:02:07,520
Now suppose instead that L
is equal to eight words,

37
00:02:07,520 --> 00:02:10,354
in this case the Tall-Cache Assumption
does not hold.

38
00:02:10,354 --> 00:02:15,175
Columns no longer fill full lines,
thus the four by four block

39
00:02:15,175 --> 00:02:19,315
doesn't actually fit in cache even
though the cache has enough capacity.

40
00:02:19,315 --> 00:02:22,480
Now the Tall-Cache Assumption is
actually a really interesting one.

41
00:02:22,480 --> 00:02:26,580
It's basically an artifact of how
we chose to lay out the matrices.

42
00:02:26,580 --> 00:02:30,890
Put differently, in a memory hierarchy
model and efficient algorithm might be

43
00:02:30,890 --> 00:02:34,990
linked to your choice of data structure
and how you choose to lay out the data.

44
00:02:34,990 --> 00:02:36,640
We'll see more examples
of this soon enough.
