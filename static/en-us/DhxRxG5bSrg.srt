1
00:00:00,210 --> 00:00:04,080
Before ending this lesson, I want
to talk about a couple of practical

2
00:00:04,080 --> 00:00:09,410
considerations of particle filters,
and the first one involves sampling.

3
00:00:09,410 --> 00:00:12,380
As you can imagine,
sampling is really important.

4
00:00:12,380 --> 00:00:13,140
Okay?

5
00:00:13,140 --> 00:00:15,060
Because I have this set of samples, and

6
00:00:15,060 --> 00:00:18,330
every step I have to generate
a new set of samples.

7
00:00:18,330 --> 00:00:19,440
So I have to re-sample it.

8
00:00:19,440 --> 00:00:24,370
I go, grab a sample, where the
probability of pulling out that sample,

9
00:00:24,370 --> 00:00:28,600
for its state x, is proportional
to the weight of that sample.

10
00:00:28,600 --> 00:00:32,210
And typically, I'm doing this a constant
amount of times, so n times, so

11
00:00:32,210 --> 00:00:35,500
I'm using 1,000 samples, 10,000, however
many samples, I'm doing that every time.

12
00:00:36,590 --> 00:00:40,440
We'll talk actually not today but next
time, that number can get kind of big if

13
00:00:40,440 --> 00:00:43,940
your state space gets large, so
being able to sample effectively or

14
00:00:43,940 --> 00:00:46,980
efficiently, or less often,
is kind of important.

15
00:00:46,980 --> 00:00:49,030
So I'll talk about
resampling a little bit.

16
00:00:49,030 --> 00:00:53,500
What we have here is a,
an outer ring, okay,

17
00:00:53,500 --> 00:00:58,090
where the idea is that the arc length of
that ring is proportional to the weight.

18
00:00:58,090 --> 00:01:03,190
Okay, so, you know, w1 is small, w2 is
bigger, w3 is huge, etc., whatever.

19
00:01:03,190 --> 00:01:05,160
And these are all the weights, right?

20
00:01:05,160 --> 00:01:08,620
So you can think of sampling as
the following, I just randomly pick

21
00:01:08,620 --> 00:01:13,830
a direction, and if I hit w3,
I'm going to take sample number three.

22
00:01:13,830 --> 00:01:15,590
So that'd be my first sample.

23
00:01:15,590 --> 00:01:16,860
All right, do it again!

24
00:01:16,860 --> 00:01:20,360
If I hit wn,
I pull out the nth sample, and

25
00:01:20,360 --> 00:01:23,759
basically I have to keep
doing that n times, right?

26
00:01:23,759 --> 00:01:26,845
So, it's kind of,
it's like a roulette wheel.

27
00:01:26,845 --> 00:01:31,665
I spin the roulette wheel n times, and
I have to figure out where I land, and

28
00:01:31,665 --> 00:01:33,835
that figuring out is
actually a binary search, so

29
00:01:33,835 --> 00:01:36,085
that's log n, but I do that n times.

30
00:01:36,085 --> 00:01:38,815
So it's an n log n operation.

31
00:01:38,815 --> 00:01:43,480
So that's kind of it's not awful,
but it doesn't

32
00:01:43,480 --> 00:01:48,320
scale linearly with the number of, of
samples you use, it goes up by n log n.

33
00:01:48,320 --> 00:01:51,790
It turns out there's a cute little
trick that you can use that is,

34
00:01:51,790 --> 00:01:55,280
instead of like a roulette wheel,
is more like a wagon wheel, and

35
00:01:55,280 --> 00:01:57,110
the way it works is this.

36
00:01:57,110 --> 00:01:59,650
So let's suppose I have n samples.

37
00:01:59,650 --> 00:02:04,408
I can make a set of spokes that
are 1 over n full rotation around.

38
00:02:04,408 --> 00:02:07,844
So here I've got 1, 2,
3, 4, 5, 6, 7, 8, right?

39
00:02:07,844 --> 00:02:09,526
So each one is an eighth of an arc.

40
00:02:09,526 --> 00:02:14,161
And what I can do is,
I can randomly put it down someplace,

41
00:02:14,161 --> 00:02:18,889
and if I've randomly done it,
then I can just read off, for

42
00:02:18,889 --> 00:02:22,504
each one of these spokes,
which w did it hit?

43
00:02:22,504 --> 00:02:25,640
Okay?
And in fact, the starting way, right,

44
00:02:25,640 --> 00:02:29,900
I can just start at some random one
here, and I see what w am I at, and

45
00:02:29,900 --> 00:02:32,830
I go to the next one and
I see, did I pass a boundary?

46
00:02:32,830 --> 00:02:35,990
If so, then I'm at the next w,
and likewise.

47
00:02:35,990 --> 00:02:40,780
So, what's nice about that is
that's done in constant n time.

48
00:02:40,780 --> 00:02:43,650
And it's sometimes called
stochastic universal sampling,

49
00:02:43,650 --> 00:02:45,320
systematic resampling.

50
00:02:45,320 --> 00:02:49,090
The nice thing about it is that
it's a linear time complexity.

51
00:02:49,090 --> 00:02:52,270
You know if you do 10,000 points,
it takes you so long.

52
00:02:52,270 --> 00:02:55,510
If you do 20,000 points, it takes
you just twice as long, all right?

53
00:02:55,510 --> 00:02:56,790
And it's very easy to implement.

54
00:02:56,790 --> 00:02:59,000
In fact so easy to implement,
let me show it to you.
