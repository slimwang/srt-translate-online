1
00:00:00,220 --> 00:00:01,819
So as I know from the last slide, right?

2
00:00:01,819 --> 00:00:07,150
Once we start thinking about goal
abstraction and sort of all of these

3
00:00:07,150 --> 00:00:11,800
functions or modules or
procedures running at the same time,

4
00:00:11,800 --> 00:00:15,520
what you really have done is turn
this into a problem of arbitration.

5
00:00:15,520 --> 00:00:17,170
By the way this does have a name.

6
00:00:17,170 --> 00:00:20,540
I really like the name, it's called
modular reinforcement learning.

7
00:00:20,540 --> 00:00:24,420
And that just sort of captures the idea
that we divided the world into a bunch

8
00:00:24,420 --> 00:00:27,790
of modules or
what we might call options or

9
00:00:27,790 --> 00:00:30,390
what we might call
temporally extended actions.

10
00:00:30,390 --> 00:00:32,560
And these things always have
associated with them goals.

11
00:00:32,560 --> 00:00:36,090
They're always trying to do something or
to accomplish something.

12
00:00:36,090 --> 00:00:38,740
They're different models and
with operating system, and

13
00:00:38,740 --> 00:00:42,160
we need to decide which program
we're going to let run next.

14
00:00:42,160 --> 00:00:44,490
And there are lots of ways
you might imagine doing this.

15
00:00:44,490 --> 00:00:49,180
Here are three of the more popular
ones each of which has strings and

16
00:00:49,180 --> 00:00:52,890
drawbacks and they're kind of the things
that you would come up with if you were

17
00:00:52,890 --> 00:00:55,230
given five minutes on a test
to come up with something.

18
00:00:55,230 --> 00:00:57,320
So here are the various
arbitration techniques.

19
00:00:57,320 --> 00:01:01,080
So the simplest one is greatest
mass Q learning which well,

20
00:01:01,080 --> 00:01:02,860
Michael what do you think that means?

21
00:01:02,860 --> 00:01:06,350
>> It means Greatest Mass Q learning.

22
00:01:07,600 --> 00:01:11,270
>> So we have Q functions maybe for
each of the individual subtasks and

23
00:01:11,270 --> 00:01:15,680
we say well whoever's got the highest
Q value gets to take the next action.

24
00:01:15,680 --> 00:01:16,540
>> That's close.

25
00:01:16,540 --> 00:01:18,260
So the first thing you said is right.

26
00:01:18,260 --> 00:01:21,130
Since each of these things are their own
little goals running around they're like

27
00:01:21,130 --> 00:01:25,830
their own little RL bots or agents,
then each of them has a Q function.

28
00:01:25,830 --> 00:01:31,030
And what greatest mass Q function does
is it adds up all of the Q functions.

29
00:01:31,030 --> 00:01:34,450
So for the state that we happen
to be in, add up the Q value for

30
00:01:34,450 --> 00:01:35,570
every state action pair.

31
00:01:35,570 --> 00:01:39,039
And whichever one is the largest
that's the one that we execute.

32
00:01:40,750 --> 00:01:45,770
So I was saying whichever Q function
has the largest value you're actually

33
00:01:45,770 --> 00:01:49,360
saying which action after combining
the Q functions has the largest rate.

34
00:01:49,360 --> 00:01:50,080
>> Right.

35
00:01:50,080 --> 00:01:52,260
>> So it's sort of like they're voting.

36
00:01:52,260 --> 00:01:56,910
>> That's exactly right it's exactly
like they're voting so I say that

37
00:01:56,910 --> 00:02:01,010
the Q value of taking a particular
action in any state is actually the sum

38
00:02:02,410 --> 00:02:07,130
of all the Q values for
each of the agents indexed here by i.

39
00:02:07,130 --> 00:02:09,169
>> And then we take, we go greedy.

40
00:02:09,169 --> 00:02:14,320
>> I picked the action that has
the most mass across all the agents.

41
00:02:14,320 --> 00:02:15,760
>> Okay, so then why is it Q then?

42
00:02:15,760 --> 00:02:19,130
Why isn't it just greatest
action mass or something?.

43
00:02:19,130 --> 00:02:21,890
>> Well the reason you have it as
Q-learning is because you assume there

44
00:02:21,890 --> 00:02:26,090
was Q-learning going on and you're using
the Q-function to determine how to vote.

45
00:02:26,090 --> 00:02:27,799
So that's exactly why
it's called Q-learning

46
00:02:27,799 --> 00:02:29,850
because there's a Q-function involved.

47
00:02:29,850 --> 00:02:31,540
So this is adding everything up and

48
00:02:31,540 --> 00:02:34,240
it does sort of exactly
what you would expect.

49
00:02:34,240 --> 00:02:38,910
Basically everyone votes and whichever
wins has the biggest vote wins.

50
00:02:38,910 --> 00:02:41,940
>> So I want to say that
the the modules are voting but

51
00:02:41,940 --> 00:02:43,170
they're voting on different actions.

52
00:02:43,170 --> 00:02:46,280
And whichever action has
the largest number of votes wins.

53
00:02:46,280 --> 00:02:47,580
>> Right, I'm sorry,
that's what I meant to say.

54
00:02:47,580 --> 00:02:48,310
What did I say?

55
00:02:48,310 --> 00:02:48,940
>> Okay.

56
00:02:48,940 --> 00:02:53,885
>> Well it sounded like,
it was interpretable as the whichever

57
00:02:53,885 --> 00:02:58,840
Q-module gets the most
votes gets to decide.

58
00:02:58,840 --> 00:02:59,490
>> I'm sorry.

59
00:02:59,490 --> 00:03:03,110
Yeah, so that's actually closer
to what you were describing.

60
00:03:03,110 --> 00:03:03,910
>> Yeah I know.
>> But yeah so

61
00:03:03,910 --> 00:03:06,090
here all you're doing is
you're voting for each action.

62
00:03:06,090 --> 00:03:09,190
That's the greatest mass Q-learning and
you just go from there.

63
00:03:09,190 --> 00:03:13,920
And that's neat and
kind of makes sense if you assume

64
00:03:13,920 --> 00:03:18,060
sort of they're all care about
some ultimate goal together.

65
00:03:18,060 --> 00:03:23,840
But people have observed that that could
be bad because it might not be good for

66
00:03:23,840 --> 00:03:26,490
any particular agent, right?

67
00:03:26,490 --> 00:03:31,020
So imagine that I have a couple of,
I have ten agents, they're ten actions,

68
00:03:31,020 --> 00:03:35,190
and each one strongly believes
in a subset of those actions.

69
00:03:35,190 --> 00:03:36,250
But they're different ones.

70
00:03:36,250 --> 00:03:38,320
So each one being the different action.

71
00:03:38,320 --> 00:03:41,890
And so, or better yet, there are five,

72
00:03:41,890 --> 00:03:44,370
there are ten actions and
there's five agents.

73
00:03:44,370 --> 00:03:48,510
And there's an action that is the fifth
choice of every one of those agents.

74
00:03:48,510 --> 00:03:51,800
But because the fifth choice of every
one of those agents it's the one

75
00:03:51,800 --> 00:03:54,450
that ends up getting the most mass and
so

76
00:03:54,450 --> 00:03:56,860
you haven't really satisfied
any of the subgoals.

77
00:03:56,860 --> 00:04:00,110
>> Yeah I mean I could see that as being
a bad thing or I could see that as being

78
00:04:00,110 --> 00:04:03,210
good thing right because it is choosing
that action not just because it's

79
00:04:03,210 --> 00:04:08,050
bad it's just that actually because
it makes everybody a little happy.

80
00:04:08,050 --> 00:04:10,440
Instead of making some
people really unhappy.

81
00:04:10,440 --> 00:04:13,790
Right but you can construct examples
where sort of everyone's least favorite

82
00:04:13,790 --> 00:04:16,810
choice will still end up being the one
with the most votes if nobody agrees

83
00:04:16,810 --> 00:04:18,000
on anything else.

84
00:04:18,000 --> 00:04:20,459
But that's okay I mean it's still as you
say it's a reasonable thing to do under

85
00:04:20,459 --> 00:04:24,130
lots of circumstances or just happen to
be cases where doesn't always work which

86
00:04:24,130 --> 00:04:27,180
would there's no free lunch so it's
not surprising that that's the case.
