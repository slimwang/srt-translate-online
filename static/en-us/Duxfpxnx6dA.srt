1
00:00:00,190 --> 00:00:02,850
In this lesson I've covered distributed shared memory

2
00:00:02,850 --> 00:00:06,200
and particularly I've given you a specific example

3
00:00:06,200 --> 00:00:09,640
of a distributed shared memory system called Treadmarks

4
00:00:09,640 --> 00:00:13,420
that uses lazy release consistency and multiple writer

5
00:00:13,420 --> 00:00:15,580
coherence. I just want to leave you with

6
00:00:15,580 --> 00:00:19,550
some thoughts about non-page based DSM systems before

7
00:00:19,550 --> 00:00:22,742
concluding this lesson. There have been systems that

8
00:00:22,742 --> 00:00:25,370
have been built that do not use granularity

9
00:00:25,370 --> 00:00:28,120
of a page for coherence maintenance. I

10
00:00:28,120 --> 00:00:31,040
mentioned earlier that if you want to maintain

11
00:00:31,040 --> 00:00:34,920
granularity not at the page level, then you

12
00:00:34,920 --> 00:00:39,280
have to track individual reads and writes that

13
00:00:39,280 --> 00:00:43,750
is happening on a thread. So one approach is what is called a library based

14
00:00:43,750 --> 00:00:46,880
approach. Here the idea is that, the programming

15
00:00:46,880 --> 00:00:50,400
framework, the programming library, is going to give

16
00:00:50,400 --> 00:00:54,190
you a way by which you can annotate

17
00:00:54,190 --> 00:00:56,130
shared variables that you're going to use in your

18
00:00:56,130 --> 00:01:01,000
program. Whenever you touch a shared variable part of

19
00:01:01,000 --> 00:01:04,470
creating the executable is to cause a trap at

20
00:01:04,470 --> 00:01:07,120
the point of access to the shared variable

21
00:01:07,120 --> 00:01:10,480
so that the DSM software will be contacted, and

22
00:01:10,480 --> 00:01:13,620
the DSM software can then take the coherence action

23
00:01:13,620 --> 00:01:16,210
at the point of access to that shared variable.

24
00:01:16,210 --> 00:01:18,320
So, in this case, there is no operating

25
00:01:18,320 --> 00:01:22,690
system support needed because in the binary itself

26
00:01:22,690 --> 00:01:28,760
we are making sure that at the point of access we're going to result in a trap

27
00:01:28,760 --> 00:01:33,420
that will get us into the trap handler that is part of DSM software so that it

28
00:01:33,420 --> 00:01:36,510
can take the coherence actions. And examples of

29
00:01:36,510 --> 00:01:41,320
systems that use this mechanism include Shasta, that was

30
00:01:41,320 --> 00:01:45,490
done at Digital Equipment Corporation, and Beehive which

31
00:01:45,490 --> 00:01:48,170
was done at Georgia Tech. And because we

32
00:01:48,170 --> 00:01:51,300
are doing this sharing at the level of

33
00:01:51,300 --> 00:01:54,440
variables, you don't have any fault sharing which

34
00:01:54,440 --> 00:01:57,396
is possible with page based systems and single

35
00:01:57,396 --> 00:02:00,490
write or cache coherence protocol. So once the

36
00:02:00,490 --> 00:02:03,190
DSM software takes the coherence action, which might

37
00:02:03,190 --> 00:02:06,350
include fetching the data that is associated with the

38
00:02:06,350 --> 00:02:08,610
variable you are trying to access, then the

39
00:02:08,610 --> 00:02:12,120
DSM software can resume this thread that caused

40
00:02:12,120 --> 00:02:14,770
this trap in the first place. Another approach

41
00:02:14,770 --> 00:02:19,820
to providing shared abstractions is not at the level

42
00:02:19,820 --> 00:02:23,110
of memory locations, but at the level of

43
00:02:23,110 --> 00:02:26,560
structures that are meaningful for an application. And,

44
00:02:26,560 --> 00:02:28,750
this is what is called structured DSM. So,

45
00:02:28,750 --> 00:02:31,910
the idea is that there is a programming library

46
00:02:31,910 --> 00:02:35,240
which actually provides abstractions that can be

47
00:02:35,240 --> 00:02:38,200
manipulated in an application program. And the

48
00:02:38,200 --> 00:02:41,840
abstractions can be manipulated using API calls

49
00:02:41,840 --> 00:02:44,140
that are part of the language runtime. So

50
00:02:44,140 --> 00:02:47,960
when the application makes the API call,

51
00:02:47,960 --> 00:02:51,040
at that point, when an application makes those

52
00:02:51,040 --> 00:02:53,890
API calls that point, the language runtime

53
00:02:53,890 --> 00:02:57,120
gets into gear and says what coherence actions

54
00:02:57,120 --> 00:03:00,540
do I need to make in order to satisfy

55
00:03:00,540 --> 00:03:04,150
this API call. All of those coherence actions are going

56
00:03:04,150 --> 00:03:06,450
to be taken at the point of that API

57
00:03:06,450 --> 00:03:09,530
call, and that might include fetching data from a remote

58
00:03:09,530 --> 00:03:12,553
node in the cluster. And once the semantics of

59
00:03:12,553 --> 00:03:16,130
that API call have been executed by the language runtime,

60
00:03:16,130 --> 00:03:19,050
then it's going to resume this thread that made

61
00:03:19,050 --> 00:03:22,180
the call in the first place. Again, there is no

62
00:03:22,180 --> 00:03:28,175
OS support needed for this. And the structured DSM is a very popular approach

63
00:03:28,175 --> 00:03:33,310
that has been used in systems such as Linda, Orca, and Stampede that was done at

64
00:03:33,310 --> 00:03:36,410
Georgia Tech, and successors to Stampede called

65
00:03:36,410 --> 00:03:40,420
Stampede RT and, and PTS. And in this

66
00:03:40,420 --> 00:03:43,280
course later on, we're going to see PTS

67
00:03:43,280 --> 00:03:46,120
as an example of a structured DSM system.
