1
00:00:00,390 --> 00:00:02,480
A common request pattern in data

2
00:00:02,480 --> 00:00:06,040
centers today is something called barrier synchronization

3
00:00:06,040 --> 00:00:12,950
whereby a client or an application might have many parallel threads. And no

4
00:00:12,950 --> 00:00:17,420
forward progress can be made until all the responses for those threads are

5
00:00:17,420 --> 00:00:21,690
satisfied. For example, a client might send

6
00:00:21,690 --> 00:00:26,240
a synchronized read with four parallel requests,

7
00:00:26,240 --> 00:00:31,200
but suppose, that the fourth is dropped. At this point, we have a request,

8
00:00:31,200 --> 00:00:36,030
sent at time zero, then we see a response less than a millisecond later,

9
00:00:36,030 --> 00:00:41,150
and at this point, threads one to three complete but TCP may time out on

10
00:00:41,150 --> 00:00:46,270
the fourth. In this case, the link is idle for a very long

11
00:00:46,270 --> 00:00:51,650
time while that fourth connection is timed out. The addition of

12
00:00:51,650 --> 00:00:57,410
more servers in the network induces an overflow of the switch buffer,

13
00:00:57,410 --> 00:01:00,522
causing severe packet loss, and inducing

14
00:01:00,522 --> 00:01:04,440
throughput collapse. One solution to this problem

15
00:01:04,440 --> 00:01:10,160
is to use fine grained TCP retransmission timers, on the order of

16
00:01:10,160 --> 00:01:12,610
microseconds, rather than on the order

17
00:01:12,610 --> 00:01:16,750
of milliseconds. Reducing the retransmission timeout for

18
00:01:16,750 --> 00:01:20,020
TCP, thus improves system throughput. Another way to

19
00:01:20,020 --> 00:01:22,520
reduce the network load is to have the client

20
00:01:22,520 --> 00:01:26,080
acknowledge every other packet rather then every packet. Thus

21
00:01:26,080 --> 00:01:29,850
reducing the overall network load. The basic idea here,

22
00:01:29,850 --> 00:01:31,770
and the premise, is that the timers need

23
00:01:31,770 --> 00:01:34,145
to operate on a granularity that's close to the

24
00:01:34,145 --> 00:01:36,500
round-trip time of the network. In the case of

25
00:01:36,500 --> 00:01:39,000
a data center that's hundreds of microseconds or less.
