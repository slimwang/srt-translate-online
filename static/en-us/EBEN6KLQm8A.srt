1
00:00:00,420 --> 00:00:04,599
The training phase essentially
consists of extracting features for

2
00:00:04,599 --> 00:00:08,529
each sample in the training set,
and supplying these feature

3
00:00:08,529 --> 00:00:13,189
vectors to the training algorithm,
along with corresponding labels.

4
00:00:13,189 --> 00:00:16,179
The training algorithm
initializes a model, and

5
00:00:16,179 --> 00:00:20,230
then tweaks its parameters using
the feature vectors and labels.

6
00:00:20,230 --> 00:00:24,530
Typically, this involves an iterative
procedure where one or more samples

7
00:00:24,530 --> 00:00:28,950
are presented to the classifier at a
time, which then predicts their labels.

8
00:00:28,949 --> 00:00:31,515
The error between these
predicted labels and

9
00:00:31,515 --> 00:00:35,590
ground-truth is used as a signal
to modify the parameters.

10
00:00:35,590 --> 00:00:39,900
When this error falls below a certain
threshold, or when enough iterations

11
00:00:39,899 --> 00:00:44,739
have passed, you can consider the model
to have been sufficiently trained.

12
00:00:44,740 --> 00:00:49,210
Now, you can verify how it performs
on previously unseen examples

13
00:00:49,210 --> 00:00:50,939
using the test set.

14
00:00:50,939 --> 00:00:55,039
The error on the test set is typically
larger than that on the training set,

15
00:00:55,039 --> 00:00:56,409
which is expected.

16
00:00:56,409 --> 00:01:01,579
Also, both errors typically decrease
the more you train your model.

17
00:01:01,579 --> 00:01:04,310
However, you have to be
careful about one thing.

18
00:01:04,310 --> 00:01:08,090
If you keep training beyond a certain
point, your training error may

19
00:01:08,090 --> 00:01:12,890
keep decreasing, but your test
error will begin to increase again.

20
00:01:12,890 --> 00:01:14,780
This is known as overfitting.

21
00:01:14,780 --> 00:01:17,710
Your model fits the training
data very well, but

22
00:01:17,709 --> 00:01:21,099
is unable to generalize
to unseen examples.

23
00:01:21,099 --> 00:01:25,689
One thing we haven't talked about yet
is the choice of what classifier to use.

24
00:01:25,689 --> 00:01:28,980
That's because it might require some
experimentation to figure out what

25
00:01:28,980 --> 00:01:31,900
classifier works best for
a given problem.

26
00:01:31,900 --> 00:01:35,621
In this case, we're going to start
with support vector machines,

27
00:01:35,621 --> 00:01:38,314
which have been shown to
work with HoG features.

28
00:01:38,314 --> 00:01:42,051
But you're free to choose what
classifier you ultimately use.

29
00:01:42,052 --> 00:01:45,830
It could even be a combination or
ensemble of multiple classifiers.

