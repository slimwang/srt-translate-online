1
00:00:00,270 --> 00:00:02,960
So now that I have that,
I backed everything up and

2
00:00:02,960 --> 00:00:04,300
I can just do selection again.

3
00:00:04,300 --> 00:00:07,110
Now this time around because I've
changed sort of my estimates

4
00:00:07,110 --> 00:00:09,240
of the Q functions along the way.

5
00:00:09,240 --> 00:00:11,260
I might want to make
a different selection.

6
00:00:11,260 --> 00:00:15,650
And so that actually tells us where
this original policy comes from.

7
00:00:15,650 --> 00:00:20,750
This original policy is just
an estimate of two values

8
00:00:20,750 --> 00:00:24,860
that I have where I happen to feel
very comfortable about those Q values.

9
00:00:24,860 --> 00:00:26,900
I've visited these states many times,

10
00:00:26,900 --> 00:00:30,340
I've backed up a lot of information from
them so I have a decent estimate and I'm

11
00:00:30,340 --> 00:00:36,550
willing to do the kind of greedy policy
that is implied by these Q functions.

12
00:00:36,550 --> 00:00:41,090
>> I see, so we're taking greedy
steps down the black part of the tree

13
00:00:41,090 --> 00:00:45,490
when we hit a leaf them we
do the roll out policy pi r

14
00:00:45,490 --> 00:00:47,920
to get estimates at
those different values.

15
00:00:47,920 --> 00:00:51,660
Do we now know anything
new about the tree?

16
00:00:51,660 --> 00:00:55,160
>> Yes, and in fact,
if we did this 100 bazillion times,

17
00:00:55,160 --> 00:01:00,010
or however many number of times we sort
of wanted to, we just can say well,

18
00:01:00,010 --> 00:01:03,390
I know the actions that I
can take from this node.

19
00:01:03,390 --> 00:01:08,560
And so I feel comfortable in expanding
my policy all the way down to here for

20
00:01:08,560 --> 00:01:10,120
this particular node as well.

21
00:01:10,120 --> 00:01:13,840
And when if I did this again, and let's
say the selection algorithm had me go

22
00:01:13,840 --> 00:01:16,740
here, here, here and
I got back to here again.

23
00:01:16,740 --> 00:01:19,400
I would actually know which
action I wanted to take,

24
00:01:19,400 --> 00:01:21,820
say it ended up with me being here.

25
00:01:21,820 --> 00:01:24,920
But then once again I've gotten to
a place where I don't feel comfortable

26
00:01:24,920 --> 00:01:27,310
about what to do next, and
I would do the expansion and

27
00:01:27,310 --> 00:01:30,000
the rollout again, over and
over and over again.

28
00:01:30,000 --> 00:01:32,310
So I just keep doing this again and
again and again and again and again and

29
00:01:32,310 --> 00:01:33,830
again and again and again.

30
00:01:33,830 --> 00:01:38,910
And what this gives me is an ever
expanding tree where I feel more and

31
00:01:38,910 --> 00:01:41,480
more comfortable about my cue
values and, therefore more and

32
00:01:41,480 --> 00:01:44,240
more comfortable about
a policy that I should take.

33
00:01:44,240 --> 00:01:46,850
>> And it seems like it's
getting more accurate over time.

34
00:01:46,850 --> 00:01:49,280
>> Yeah, and in fact if you're
at each of these nodes, right?

35
00:01:49,280 --> 00:01:51,400
and you took those actions and
you did the expansion and

36
00:01:51,400 --> 00:01:54,320
you did that well let's say
an infinite number of times, and

37
00:01:54,320 --> 00:01:57,000
then you do an infinite number of
roll outs over and over again.

38
00:01:57,000 --> 00:02:00,080
Eventually you would converge
to a property two value.

39
00:02:00,080 --> 00:02:03,550
>> Cool that's right so this is
a planning algorithm we need to know

40
00:02:03,550 --> 00:02:06,530
the transition model to be
able to do those roll outs.

41
00:02:06,530 --> 00:02:09,770
>> Right, or you have to have
some way of doing the simulation.

42
00:02:09,770 --> 00:02:12,390
So when you say you need to
know the transition model,

43
00:02:12,390 --> 00:02:14,370
you could mean a couple of things.

44
00:02:14,370 --> 00:02:16,130
You could mean I actually
have the transition model and

45
00:02:16,130 --> 00:02:17,920
I can do calculations with it.

46
00:02:17,920 --> 00:02:22,200
Or you could just mean I have some
way of simulating the world and

47
00:02:22,200 --> 00:02:24,120
I can just sample from it.

48
00:02:24,120 --> 00:02:25,180
So this is pretty neat, right?

49
00:02:25,180 --> 00:02:27,410
I think it has some neat
little features to it.

50
00:02:27,410 --> 00:02:29,590
It allows me to kind of
build up a model the world.

51
00:02:29,590 --> 00:02:31,270
Do a bunch of simulation and
figure out what to do.

52
00:02:31,270 --> 00:02:34,620
But there are a couple of questions
that should pop out to you even

53
00:02:34,620 --> 00:02:37,150
though I kind of ran
over them fairly quickly.

54
00:02:37,150 --> 00:02:39,070
I think the first one is,
well, when do you stop?

55
00:02:39,070 --> 00:02:40,480
What does this actually, I mean,

56
00:02:40,480 --> 00:02:42,440
this is kind of an inner
loop of a process, right?

57
00:02:42,440 --> 00:02:45,480
I'm at this state I
have an estimate of Q

58
00:02:45,480 --> 00:02:48,770
values that I'm comfortable
enough in that I'm going to take

59
00:02:48,770 --> 00:02:51,900
actual actions following
the policy that's implied by them.

60
00:02:51,900 --> 00:02:54,700
And then I'm going to keep expanding and
learning more and more about it.

61
00:02:54,700 --> 00:02:57,070
Well in principle I could
just do that for eternity.

62
00:02:57,070 --> 00:02:58,670
But at some point you have to stop.

63
00:02:58,670 --> 00:02:59,730
And when do you stop?

64
00:02:59,730 --> 00:03:01,040
Well you stop when you get bored.

65
00:03:01,040 --> 00:03:02,840
>> And what do you do when you stop?

66
00:03:02,840 --> 00:03:06,520
>> Well, once you stop so next
question well what I do after I stop?

67
00:03:06,520 --> 00:03:07,460
Well, you execute.

68
00:03:07,460 --> 00:03:11,420
You do a sort of one step policy
based upon what you've learned and

69
00:03:11,420 --> 00:03:13,210
then you throw it all away and
you do it all over again.

70
00:03:13,210 --> 00:03:15,900
>> Wait, so
you take that step in the world

71
00:03:15,900 --> 00:03:19,070
using policy the whole
chain that you figured out?

72
00:03:19,070 --> 00:03:22,350
>> Well, you could but and
in fact sometimes I mean,

73
00:03:22,350 --> 00:03:24,370
well you can do actually
any number of things there.

74
00:03:24,370 --> 00:03:27,790
But sort of the simplest version of this
is I started out with an empty tree.

75
00:03:27,790 --> 00:03:28,880
I'm in some particular state,

76
00:03:28,880 --> 00:03:32,140
I'm going to hallucinate all
the things that I might do from there.

77
00:03:32,140 --> 00:03:34,810
As I do that I'll be learning more and
more about the world.

78
00:03:34,810 --> 00:03:37,560
This will allow me to do better and
better exploration

79
00:03:37,560 --> 00:03:41,840
because I do this kind of Monte Carlo
search and simulation and hallucination.

80
00:03:41,840 --> 00:03:44,770
And then eventually I learn enough that
I know what the right thing to do is

81
00:03:44,770 --> 00:03:46,980
from this state and then I do it.

82
00:03:46,980 --> 00:03:49,960
And then I end up wherever it is
I end up in the real world and

83
00:03:49,960 --> 00:03:52,770
when I'm asked to do something
again I just do it all over again.

84
00:03:52,770 --> 00:03:56,200
>> Wow, so it does a lot of
thinking each time it takes a step.

85
00:03:56,200 --> 00:03:58,020
>> Yeah but
it's very fast thinking because

86
00:03:58,020 --> 00:03:59,290
you know I'm just flipping coins.

87
00:03:59,290 --> 00:04:03,690
If I have a fast simulator and I'm
able to do addition pretty quickly and

88
00:04:03,690 --> 00:04:08,490
maybe some averaging I can actually
do this estimate fairly well.

89
00:04:08,490 --> 00:04:13,570
What's going to impact this a lot
is sort of how far down I can go

90
00:04:13,570 --> 00:04:15,590
before I run out of computational power?

91
00:04:15,590 --> 00:04:18,649
Typically that's what board
really means board means okay

92
00:04:18,649 --> 00:04:22,760
I've taken 15 seconds that's too much
time or maybe I've taken three seconds.

93
00:04:22,760 --> 00:04:25,640
That's too much time it's time to do
the best thing that I can based upon

94
00:04:25,640 --> 00:04:26,970
what I know.

95
00:04:26,970 --> 00:04:29,910
But if I can do that every single time,
if I can expand this tree pretty

96
00:04:29,910 --> 00:04:33,730
deeply at every time step,
then I lose 50 milliseconds.

97
00:04:33,730 --> 00:04:37,340
But 50 milliseconds is a long time for
a computer and not any time at all for

98
00:04:37,340 --> 00:04:38,180
human beings.

99
00:04:38,180 --> 00:04:41,580
And the alternative would be to
actually explore the entire space and

100
00:04:41,580 --> 00:04:42,850
try to solve the underlying MDP.

101
00:04:42,850 --> 00:04:45,170
And if you have a whole
lot of states and

102
00:04:45,170 --> 00:04:50,170
a whole lot of actions then that
can take a very, very long time.

103
00:04:50,170 --> 00:04:54,710
So do you take 50 or 200 milliseconds
every time you take an action and

104
00:04:54,710 --> 00:04:55,450
do it online?

105
00:04:55,450 --> 00:04:58,880
Or do you take three and a half months
of super computing power to figure out

106
00:04:58,880 --> 00:05:00,770
the optimal policy and then use that?

107
00:05:00,770 --> 00:05:01,850
And the answer is, it depends.
