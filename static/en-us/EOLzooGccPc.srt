1
00:00:00,670 --> 00:00:04,480
As it turns out, when you compute the accuracy you can actually see that,

2
00:00:04,480 --> 00:00:07,400
that the simpler decision boundary gives you a better result.

3
00:00:07,400 --> 00:00:15,040
You have a 91.2% accuracy with the min sample split set to 50.

4
00:00:15,040 --> 00:00:19,020
And then when you allow it to go all the way down to two,

5
00:00:19,020 --> 00:00:20,360
you only get 90.8% accuracy.

6
00:00:20,360 --> 00:00:25,190
So, you can see that it does make a difference,

7
00:00:25,190 --> 00:00:27,370
not just in terms of how the decision boundary looks, but

8
00:00:27,370 --> 00:00:30,170
actually in terms of the performance of your classifier.

9
00:00:30,170 --> 00:00:32,729
And, this might not look like a huge difference.

10
00:00:32,729 --> 00:00:36,130
But, that's something that we got for free just by tuning a parameter.

11
00:00:36,130 --> 00:00:38,970
And, if you recall, there were a lot of parameters that we had to play around

12
00:00:38,970 --> 00:00:40,640
with in decision trees.

13
00:00:40,640 --> 00:00:43,920
So, I would encourage you right now or at some point in the future to sort of

14
00:00:43,920 --> 00:00:46,230
go in and play with, around with a lot of those.

15
00:00:46,230 --> 00:00:48,860
Get some intuition for what they're controlling and

16
00:00:48,860 --> 00:00:52,070
how they actually change the shape of the decision boundary.

17
00:00:52,070 --> 00:00:55,710
Because in the future, those are going to be the parameters that you're going to

18
00:00:55,710 --> 00:00:58,110
be tuning to get the best performance out of your algorithm
