1
00:00:00,125 --> 00:00:01,425
All right.
So that's supervised learning and

2
00:00:01,425 --> 00:00:02,087
unsupervised learning.

3
00:00:02,087 --> 00:00:02,921
That's pretty good.

4
00:00:02,921 --> 00:00:04,640
The last one is reinforcement learning.

5
00:00:04,640 --> 00:00:05,442
>> [SOUND].

6
00:00:05,442 --> 00:00:07,587
>> Now reinforcement learning
is what we both do, so

7
00:00:07,587 --> 00:00:10,561
Michael does a little bit of
reinforcement learning here and there.

8
00:00:10,561 --> 00:00:12,906
You've got how many papers published
in reinforcement learning?

9
00:00:12,906 --> 00:00:13,545
>> All of them.

10
00:00:13,545 --> 00:00:14,902
[LAUGH] Several.

11
00:00:14,902 --> 00:00:15,704
I have several.

12
00:00:15,704 --> 00:00:19,455
>> The man has like a hundred papers of
reinforcement learnings and in fact he

13
00:00:19,455 --> 00:00:24,299
wrote with his colleagues the great
summary journal article bringing every

14
00:00:24,299 --> 00:00:28,012
one up to date on what reinforcement
learning was like back in 1990.

15
00:00:28,012 --> 00:00:29,718
>> Yeah like 112 years ago.

16
00:00:29,718 --> 00:00:30,417
>> 1992.

17
00:00:30,417 --> 00:00:33,386
>> People are saying yeah we should
probably somebody should write a new one

18
00:00:33,386 --> 00:00:35,733
because the other ones getting
a little long in the tooth.

19
00:00:35,733 --> 00:00:37,520
>> But there's been books written
on machine learning system.

20
00:00:37,520 --> 00:00:38,055
>> That's right.

21
00:00:38,055 --> 00:00:38,659
>> It's a very popular field.

22
00:00:38,659 --> 00:00:39,760
That's why we're both in it.

23
00:00:39,760 --> 00:00:41,573
Michael tends to prove a lot of things,.

24
00:00:41,573 --> 00:00:43,521
>> It is not, that is not why I'm in it.

25
00:00:43,521 --> 00:00:45,391
>> What, I didn't, wait, what?

26
00:00:45,391 --> 00:00:47,701
>> You said it's a very popular
field and that's why we're in it.

27
00:00:47,701 --> 00:00:48,561
>> No, no, no, no, no.

28
00:00:48,561 --> 00:00:49,212
Did I say that?

29
00:00:49,212 --> 00:00:49,908
>> That's what I heard.

30
00:00:49,908 --> 00:00:50,976
>> I didn't mean to say that.

31
00:00:50,976 --> 00:00:52,323
>> [SOUND] Let's run it back and see.

32
00:00:52,323 --> 00:00:52,903
>> It's a very popular, yeah,

33
00:00:52,903 --> 00:00:53,981
let's do that again because
I did not mean to say that.

34
00:00:53,981 --> 00:00:55,096
It is a very popular field.

35
00:00:55,096 --> 00:00:56,535
Perhaps because you're in it Michael.

36
00:00:56,535 --> 00:00:57,107
>> I don't think that's it.

37
00:00:57,107 --> 00:00:58,518
When I was an undergraduate,

38
00:00:58,518 --> 00:01:01,008
I thought the thing that I
really want to understand.

39
00:01:01,008 --> 00:01:03,040
I liked AI,
I liked the whole idea of AI.

40
00:01:03,040 --> 00:01:06,457
But what I really want to understand
is how can you learn to be

41
00:01:06,457 --> 00:01:08,009
better from experience?

42
00:01:08,009 --> 00:01:10,595
And like I, I built a tic-tac-toe
playing program, and like,

43
00:01:10,595 --> 00:01:13,802
I want this tic-tac-toe playing program
to get really good at tic-tac-toe.

44
00:01:13,802 --> 00:01:17,438
because I was always interested
in the most practical society

45
00:01:17,438 --> 00:01:18,885
impacting problems.

46
00:01:18,885 --> 00:01:22,323
>> I think that generalized
pretty well to world hunger.

47
00:01:22,323 --> 00:01:22,939
>> Eventually.

48
00:01:22,939 --> 00:01:24,831
So so that is what got
me interested in it, and

49
00:01:24,831 --> 00:01:27,144
I was, I didn't even know what
it was called for a long time.

50
00:01:27,144 --> 00:01:29,391
So I started doing
reinforcement learning, and

51
00:01:29,391 --> 00:01:31,927
then discovered that it was
interesting and popular.

52
00:01:31,927 --> 00:01:33,111
>> Right.

53
00:01:33,111 --> 00:01:35,799
Well, I certainly wouldn't suggest that
we're doing the science that we're doing

54
00:01:35,799 --> 00:01:36,592
because it's popular.

55
00:01:36,592 --> 00:01:37,951
We're doing it because
we're interested in it.

56
00:01:37,951 --> 00:01:38,508
>> Yes.

57
00:01:38,508 --> 00:01:41,792
>> And I'm interested in reinforcement
learning because in some sense,

58
00:01:41,792 --> 00:01:44,592
it kind of encapsulates all
the things I happen to care about.

59
00:01:44,592 --> 00:01:48,587
I come from a sort of general AI
background, and I care modeling people.

60
00:01:48,587 --> 00:01:51,567
I care about building smart agents
that have to live in in world that

61
00:01:51,567 --> 00:01:54,813
other smart agents, thousands of them,
hundreds of thousand of them,

62
00:01:54,813 --> 00:01:55,737
thousands of them.

63
00:01:55,737 --> 00:01:57,362
Some of them might be human and

64
00:01:57,362 --> 00:02:00,230
have to feel some way to
predict what to do over time.

65
00:02:00,230 --> 00:02:03,386
So, from a sort a technical point
of view, if we can think of re,

66
00:02:03,386 --> 00:02:06,545
in, supervised learning as
function approximation and

67
00:02:06,545 --> 00:02:08,485
unsupervised learning as, you know.

68
00:02:08,485 --> 00:02:09,362
>> Concise-
>> Concise,

69
00:02:09,362 --> 00:02:12,076
impact description, what's
the difference between something like

70
00:02:12,076 --> 00:02:13,640
reinforcement learning and those two?

71
00:02:13,640 --> 00:02:14,464
Supervised learning.

72
00:02:14,464 --> 00:02:17,284
>> So often the way that
supervised learning oh, sorry,

73
00:02:17,284 --> 00:02:21,290
reinforcement learning is described is,
is learning from delayed reward.

74
00:02:21,290 --> 00:02:22,969
>> Mm-hm.
So instead of the feedback that you get

75
00:02:22,969 --> 00:02:25,249
in supervised learning which
is here's what you should do.

76
00:02:25,249 --> 00:02:28,364
And the feedback that you get in
unsupervised learning which is

77
00:02:28,364 --> 00:02:31,481
the feedback in reinforcement
learning may come several steps

78
00:02:31,481 --> 00:02:33,968
after the decisions that
you've actually made.

79
00:02:33,968 --> 00:02:36,257
>> So a good example of that, or
the easy example of that would be,

80
00:02:36,257 --> 00:02:38,044
actually your tic-tac-toe program,
right?

81
00:02:38,044 --> 00:02:42,452
So, you do something in tic-tac-toe,
you put an X in the center and

82
00:02:42,452 --> 00:02:45,266
then you put a, let's say,
an O over here.

83
00:02:45,266 --> 00:02:45,766
>> Oh.

84
00:02:45,766 --> 00:02:47,762
>> And then I put an X right here.

85
00:02:47,762 --> 00:02:48,401
>> Nice.

86
00:02:48,401 --> 00:02:51,327
>> And then you ridiculously
put an O in the center.

87
00:02:51,327 --> 00:02:53,728
>> Which allows me to put
an X over here and I win.

88
00:02:53,728 --> 00:02:54,309
>> All right.

89
00:02:54,309 --> 00:02:56,219
>> Now what's interesting about that is,

90
00:02:56,219 --> 00:02:59,487
I didn't tell you what happened until
the very end when I said X wins.

91
00:02:59,487 --> 00:03:00,509
>> Right.

92
00:03:00,509 --> 00:03:03,161
And now I know I made a mistake
somewhere along the way but

93
00:03:03,161 --> 00:03:04,584
I don't know exactly where.

94
00:03:04,584 --> 00:03:07,650
I may have to kind of roll back the game
in my mind and eventually figure out

95
00:03:07,650 --> 00:03:10,389
where it is that I went off track,
and what it is that I did wrong.

96
00:03:10,389 --> 00:03:12,339
>> And in the full generality
of reinforcement learning,

97
00:03:12,339 --> 00:03:13,547
you may have never made a mistake.

98
00:03:13,547 --> 00:03:15,650
It may simply be that's
the way games go but

99
00:03:15,650 --> 00:03:18,584
you would like to know which of
the moves you made mattered.

100
00:03:18,584 --> 00:03:21,307
Now, if it were a supevised learning
problem, I would have put the X here,

101
00:03:21,307 --> 00:03:24,004
he would have put the O there, and
it would have been called that's Good.

102
00:03:24,004 --> 00:03:25,727
I would have put the X here, and

103
00:03:25,727 --> 00:03:29,781
when he put the O there, it would have
been that's Bad, the O goes here.

104
00:03:29,781 --> 00:03:30,339
>> Mm-hm.

105
00:03:30,339 --> 00:03:31,382
Right.
>> Or something like that.

106
00:03:31,382 --> 00:03:33,777
It would have told you where
he should have put the O.

107
00:03:33,777 --> 00:03:36,059
But here, all he gets is eventually
some kind of signal saying,

108
00:03:36,059 --> 00:03:36,962
you did something well.

109
00:03:36,962 --> 00:03:39,955
You did something poorly and even
then it's only relative to the other

110
00:03:39,955 --> 00:03:41,536
signals that you might have gotten.

111
00:03:41,536 --> 00:03:44,428
>> Right, so then reinforcement
learning is in some sense harder

112
00:03:44,428 --> 00:03:46,400
because nobody's telling you what to do.

113
00:03:46,400 --> 00:03:47,782
You have to work it out on your own.

114
00:03:47,782 --> 00:03:51,624
>> Yeah it's like playing a game
without knowing any of the rules.

115
00:03:51,624 --> 00:03:56,478
Or at least knowing how you win or lose.

116
00:03:56,478 --> 00:04:00,342
But being told every once in awhile that
you've won or you've lost, okay, now-

117
00:04:00,342 --> 00:04:01,283
>> Sometimes I feel like that.

118
00:04:01,283 --> 00:04:03,130
>> I know man.

