1
00:00:00,230 --> 00:00:01,630
Alright, so what I'd like to do is work up

2
00:00:01,630 --> 00:00:06,735
to a proof that K-means does something good. [LAUGH] And to

3
00:00:06,735 --> 00:00:09,272
do that I think it's helpful to have little bit more

4
00:00:09,272 --> 00:00:12,190
precise notation than what I was doing before. So here we

5
00:00:12,190 --> 00:00:14,210
go, let's, we're going to work up to doing K-means in

6
00:00:14,210 --> 00:00:18,081
a Euclidean space. And the notation that I'm going to use is,

7
00:00:18,081 --> 00:00:20,364
we've got at any given moment in time in the algorithm,

8
00:00:20,364 --> 00:00:25,310
there's some partition, p super t of x. And these clusters

9
00:00:25,310 --> 00:00:28,140
are defined the same way that we did before, which

10
00:00:28,140 --> 00:00:30,660
it just returns the, you know, some kind of identifier

11
00:00:30,660 --> 00:00:33,480
for cluster x is in and this is at iteration

12
00:00:33,480 --> 00:00:37,670
t of K-means. We also have another kind of more convenient

13
00:00:37,670 --> 00:00:41,110
way of writing that which is C sub i of

14
00:00:41,110 --> 00:00:43,730
t, which is the set of all points in cluster

15
00:00:43,730 --> 00:00:46,070
i. It's really just the same as the set x,

16
00:00:46,070 --> 00:00:49,340
such that p of x equals i. Does that make sense?

17
00:00:49,340 --> 00:00:50,655
>> Yeah, okay that makes perfect sense.

18
00:00:50,655 --> 00:00:53,510
You've got some kind of partition, and everything in the

19
00:00:53,510 --> 00:00:57,630
same partition belongs together in something you're calling C for cluster.

20
00:00:59,475 --> 00:01:01,870
>> Good, and we're also going to need to be able to refer to the

21
00:01:01,870 --> 00:01:03,670
center of a cluster because we're in

22
00:01:03,670 --> 00:01:06,600
a Euclidean space, it's meaningful to add the

23
00:01:06,600 --> 00:01:09,220
objects together. So, take all the objects

24
00:01:09,220 --> 00:01:11,580
that are in some cluster, Ci at iteration

25
00:01:11,580 --> 00:01:15,680
t and divide by the number of objects in that cluster. So just summing those

26
00:01:15,680 --> 00:01:17,840
points together. This is also sometimes called the centroid, right?

27
00:01:17,840 --> 00:01:19,730
>> Right, so it's the mean or the centroid.

28
00:01:19,730 --> 00:01:22,630
>> Yeah it's like. That's right. I mean in one dimension it's

29
00:01:22,630 --> 00:01:26,385
definitely the mean. In higher dimensions it's like a per dimension mean.

30
00:01:26,385 --> 00:01:29,690
>> Oh, so in fact you're going to end up with K means.

31
00:01:30,790 --> 00:01:33,230
>> Oh, I see what you did there.

32
00:01:33,230 --> 00:01:33,660
>> Mm.

33
00:01:33,660 --> 00:01:36,920
>> So now, here's an equation arrow version of the algorithm,

34
00:01:36,920 --> 00:01:40,580
that we start off picking centers in some way for iteration zero,

35
00:01:40,580 --> 00:01:45,680
and we hand it over to this process that's going to assign the partitions. In

36
00:01:45,680 --> 00:01:52,130
particular, the partition of point x is going to be the minimum over

37
00:01:52,130 --> 00:01:56,610
all the clusters of the distance between x and the center of that cluster.

38
00:01:56,610 --> 00:01:58,700
Alright, so it's just assigning each point

39
00:01:58,700 --> 00:02:01,083
to its closest center. Does that make sense?

40
00:02:01,083 --> 00:02:02,850
>> Mm-hm, and that all those bars

41
00:02:02,850 --> 00:02:05,310
with the sub two just mean Euclidean distance.

42
00:02:05,310 --> 00:02:05,540
>> Yeah,

43
00:02:05,540 --> 00:02:07,690
we're just, that's right, exactly. We just computing the distance

44
00:02:07,690 --> 00:02:13,530
between those two things. And we hand that partition over

45
00:02:13,530 --> 00:02:16,550
to the process that computes the center. So it's, it's

46
00:02:16,550 --> 00:02:20,305
now using this other representation of the clustering Ci, it's

47
00:02:20,305 --> 00:02:22,600
adding the points together, divided by the number of points

48
00:02:22,600 --> 00:02:25,200
in that cluster, and it hands those centers back to

49
00:02:25,200 --> 00:02:27,560
this first process to recompute the cluster. So we're just

50
00:02:27,560 --> 00:02:30,700
going to be ping-ponging back and forth between these two processes.

51
00:02:30,700 --> 00:02:31,200
>> Okay.

52
00:02:31,200 --> 00:02:31,350
>> Good?

53
00:02:31,350 --> 00:02:34,160
>> Sure, and t keeps getting updated after every cycle.

54
00:02:34,160 --> 00:02:39,930
>> Right, yes. So, this is where t gets incremented, t plus plus. So this is

55
00:02:39,930 --> 00:02:42,030
the algorithm, and an interesting fact about it

56
00:02:42,030 --> 00:02:44,580
is: Do you remember when we talked about optimization?

57
00:02:44,580 --> 00:02:47,400
>> I do remember when we talked about optimization.

58
00:02:47,400 --> 00:02:48,990
>> And optimization is good because it's finding

59
00:02:48,990 --> 00:02:52,090
better and better answers. So if we can think

60
00:02:52,090 --> 00:02:53,640
of this as being kind of an optimization

61
00:02:53,640 --> 00:02:55,750
algorithm then that could be good. It could be

62
00:02:55,750 --> 00:02:57,750
that that what we're doing, what we're going around here

63
00:02:57,750 --> 00:03:01,500
is not just randomly reassigning things, but actually improving things.

64
00:03:01,500 --> 00:03:03,610
>> Okay. Well how are you going to convince me of that?

65
00:03:03,610 --> 00:03:05,770
>> Alright, let's do that.
