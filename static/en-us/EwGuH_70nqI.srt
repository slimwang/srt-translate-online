1
00:00:00,170 --> 00:00:05,410
Okay Michael, so we've gone on a journey of discovery. [LAUGH] Through

2
00:00:05,410 --> 00:00:09,560
unsupervised learning, and so my question to you is, what have we learned?

3
00:00:09,560 --> 00:00:12,090
>> I think we learned a little bit about ourselves.

4
00:00:12,090 --> 00:00:14,110
>> And a little bit about America.

5
00:00:14,110 --> 00:00:15,640
So what A's have we learned today, Michael?

6
00:00:15,640 --> 00:00:18,300
>> So there was PCA.

7
00:00:18,300 --> 00:00:20,167
>> Mm-hmm.

8
00:00:20,167 --> 00:00:20,970
>> ICA.

9
00:00:20,970 --> 00:00:22,837
>> Mm-hmm.

10
00:00:22,837 --> 00:00:23,640
>> LDA.

11
00:00:23,640 --> 00:00:23,640
>>

12
00:00:23,640 --> 00:00:27,339
Mm-hmm. RCA and USA.

13
00:00:27,339 --> 00:00:29,950
>> [INAUDIBLE]

14
00:00:29,950 --> 00:00:33,010
USA, we're number one! Whoo!

15
00:00:33,010 --> 00:00:33,062
>> [LAUGH]

16
00:00:33,062 --> 00:00:38,170
>> Okay, we're just going to erase that little bit. [LAUGH]

17
00:00:38,170 --> 00:00:39,210
>> [LAUGH]

18
00:00:39,210 --> 00:00:41,660
>> Okay, yeah, okay, so we learned about a lot of A's today.

19
00:00:41,660 --> 00:00:42,413
>> Uh-huh.

20
00:00:42,413 --> 00:00:44,130
>> Which is the same grade that all our students

21
00:00:44,130 --> 00:00:47,490
are going to get, I am sure. That would be great.

22
00:00:47,490 --> 00:00:49,330
>> Or

23
00:00:49,330 --> 00:00:51,630
that's if they're truly independent. If they aren't independent, then the

24
00:00:51,630 --> 00:00:56,110
central limit theorem says, there will be a normal distribution across grades.

25
00:00:56,110 --> 00:00:56,152
>> Mm-mmm.

26
00:00:56,152 --> 00:00:56,200
>> Mm-mmm.

27
00:00:56,200 --> 00:00:57,080
>> Ring that bell curve.

28
00:00:57,080 --> 00:00:59,510
>> Aw, yeah, baby. Okay. So we learned about PCA,

29
00:00:59,510 --> 00:01:01,500
ICA, LDA and RCA. What else did we learn about?

30
00:01:01,500 --> 00:01:04,769
>> Well, I think that was it. But we talked about specifically.

31
00:01:05,980 --> 00:01:08,910
we, we talked in detail about the relationships between some of these.

32
00:01:08,910 --> 00:01:11,410
>> Mm-hm.

33
00:01:11,410 --> 00:01:14,396
>> In particular, these are all examples of

34
00:01:14,396 --> 00:01:17,468
feature transformation.

35
00:01:17,468 --> 00:01:20,713
>> That's right. Okay, so we found out about the relationships

36
00:01:20,713 --> 00:01:23,140
between different transformation analysis. Oh,

37
00:01:23,140 --> 00:01:24,570
here's something we learned. We learned

38
00:01:24,570 --> 00:01:28,530
that the A doesn't just stand for analysis. In the algorithms,

39
00:01:28,530 --> 00:01:30,900
but it actually does stand for the analysis of the data.

40
00:01:30,900 --> 00:01:34,350
>> Because that's unsupervised learning.

41
00:01:34,350 --> 00:01:37,290
>> That's right. And that in particular I

42
00:01:37,290 --> 00:01:39,430
gave some examples where ICA tells you what the

43
00:01:39,430 --> 00:01:43,310
underlying structure of the data is. You can use it to find structure.

44
00:01:45,480 --> 00:01:50,290
So that, for example, the independent components of natural scenes are edges.

45
00:01:50,290 --> 00:01:54,020
So it's interesting, because I feel like the other time that you

46
00:01:54,020 --> 00:01:58,990
emphasized structure was when you were talking about, mimic which was a

47
00:01:58,990 --> 00:02:01,710
piece of work that you did when you were a graduate student.

48
00:02:01,710 --> 00:02:02,440
>> Yep

49
00:02:02,440 --> 00:02:05,490
>> One would almost want to guess that maybe

50
00:02:05,490 --> 00:02:07,690
you worked on ICA when you were a graduate student.

51
00:02:07,690 --> 00:02:11,080
>> I actually did. My very first paper

52
00:02:11,080 --> 00:02:14,880
as a young graduate student was on mimic and my very last paper

53
00:02:14,880 --> 00:02:16,510
as a young graduate student. My

54
00:02:16,510 --> 00:02:18,940
actual dissertation, was on independent components analysis.

55
00:02:18,940 --> 00:02:21,460
>> I had that sense from the number of strong

56
00:02:21,460 --> 00:02:25,173
points you felt the need to make [LAUGH] about ICA.

57
00:02:25,173 --> 00:02:27,380
>> Well listen man, really, structure runs my life.

58
00:02:27,380 --> 00:02:29,830
As you know, everything about my life is well-structured.

59
00:02:29,830 --> 00:02:30,394
>> Yeah, sure.

60
00:02:30,394 --> 00:02:32,980
>> [LAUGH] Okay, did we learn anything else?

61
00:02:32,980 --> 00:02:36,230
>> So, yeah, so I mean I feel like we spent a lot of time talking about

62
00:02:36,230 --> 00:02:40,990
so P, ICA is a more probabilistic kind of modeling, method

63
00:02:40,990 --> 00:02:45,660
and PCA is a more I want to say linear algebraic, modeling model.

64
00:02:45,660 --> 00:02:49,960
>> That's a really good point Michael. So, we didn't say it explicitly this

65
00:02:49,960 --> 00:02:53,880
way, but actually, even in our own work it often comes up that sometimes,

66
00:02:53,880 --> 00:02:56,150
you want to think about, information theory. You

67
00:02:56,150 --> 00:02:58,360
want to think about probability. And sometimes, you

68
00:02:58,360 --> 00:03:01,760
really just want to think about linear algebra. And you could see PCA as being

69
00:03:01,760 --> 00:03:04,210
really about linear algebra. And sometimes only

70
00:03:04,210 --> 00:03:06,820
coincidentally being about probability. Where as ICA

71
00:03:06,820 --> 00:03:09,690
is all about probability and information theory,

72
00:03:09,690 --> 00:03:11,800
and only coincidentally ever about linear algebra.

73
00:03:11,800 --> 00:03:13,810
>> Yeah, that's helpful. That does seem to be a fundamental

74
00:03:13,810 --> 00:03:15,840
split in a lot of work that happens in machine learning.

75
00:03:15,840 --> 00:03:18,130
>> Yeah and it, and it makes some sense. I mean, we, we

76
00:03:18,130 --> 00:03:20,680
know what the right answer is in probability, but we know what the

77
00:03:20,680 --> 00:03:23,150
right answer is in linear algebra. And I guess it's, it's often the

78
00:03:23,150 --> 00:03:27,870
case Michael, would you agree that. That the linear algebra approach is often

79
00:03:27,870 --> 00:03:31,640
easier to think about, or easier to do in

80
00:03:31,640 --> 00:03:34,720
practice and sometimes, it can be interpreted as if

81
00:03:34,720 --> 00:03:37,560
it's probability. And that typically breaks down on the

82
00:03:37,560 --> 00:03:39,680
edge cases, but you know, you can kind of

83
00:03:39,680 --> 00:03:43,200
work around it for sort of common cases. Yeah,

84
00:03:43,200 --> 00:03:45,780
that, that the linear algebra algorithms are often cheaper

85
00:03:45,780 --> 00:03:49,380
to implement, cheaper to execute less prone to local

86
00:03:49,380 --> 00:03:53,060
minima issues. There's sort of a well defined answer

87
00:03:53,060 --> 00:03:55,950
that they're, that they're finding. But it's often not quite the

88
00:03:55,950 --> 00:03:58,600
answer that you want, and the probability methods give you the

89
00:03:58,600 --> 00:04:02,080
answer that you want, but can be very hard to find.

90
00:04:02,080 --> 00:04:04,410
Right. And in fact you can see that in ICA and PCA,

91
00:04:04,410 --> 00:04:07,080
in that PCA is very well understood. There are lots of

92
00:04:07,080 --> 00:04:10,230
fast algorithms for it. And you know that the principle components

93
00:04:10,230 --> 00:04:13,810
always exist. Interestingly, we didn't talk about this but by contrast,

94
00:04:13,810 --> 00:04:18,110
ICA with its more information, theoretic and probabilistic roots, has a very

95
00:04:18,110 --> 00:04:20,190
specific model. And it isn't always the case

96
00:04:20,190 --> 00:04:22,530
that that model fits, and so in fact, sometimes

97
00:04:22,530 --> 00:04:25,863
you can't find independent components. [INAUDIBLE] Because they don't

98
00:04:25,863 --> 00:04:29,050
actually exist, except in the most trivial sense. So

99
00:04:29,050 --> 00:04:31,330
it's both more expensive, because of the way

100
00:04:31,330 --> 00:04:33,330
you end up searching the space. And it doesn't

101
00:04:33,330 --> 00:04:35,390
always produce an answer. But when it does produce

102
00:04:35,390 --> 00:04:37,770
an answer, it tends to produce very satisfying ones.

103
00:04:37,770 --> 00:04:39,700
>> Well, I think that's a good place to stop, in the

104
00:04:39,700 --> 00:04:43,050
sense that I wanted to know just one more interesting fact about ICA.

105
00:04:43,050 --> 00:04:45,110
And now that I've got that, I feel

106
00:04:45,110 --> 00:04:47,670
fully satisfied. Well, there's another fact I can

107
00:04:47,670 --> 00:04:48,790
tell you which is that it's the only

108
00:04:48,790 --> 00:04:51,380
one of these algorithms that start with a vowel.

109
00:04:51,380 --> 00:04:53,460
>> And now I'm more than satisfied.

110
00:04:53,460 --> 00:04:56,170
>> It is always my goal to leave you more than satisifed.

111
00:04:56,170 --> 00:04:59,570
>> [LAUGH]. Alright, then!

112
00:04:59,570 --> 00:05:01,320
>> OK. Well I think we are done

113
00:05:01,320 --> 00:05:06,600
with this entire sub lesson mini course thingy.

114
00:05:06,600 --> 00:05:08,000
>> About unsupervised learning.

115
00:05:08,000 --> 00:05:08,570
Well that, well that's great!

116
00:05:08,570 --> 00:05:09,370
>> About unsupervised learning.

117
00:05:09,370 --> 00:05:12,310
>> What does that, what does that leave us to do? Doesn't lead us to do

118
00:05:12,310 --> 00:05:16,110
anything, at least not with this particular, mini-course.

119
00:05:16,110 --> 00:05:17,890
I think actually the description we had here, what

120
00:05:17,890 --> 00:05:21,570
we've learned for this particular, lesson actually applies,

121
00:05:21,570 --> 00:05:23,060
even going backwards to some of our other

122
00:05:23,060 --> 00:05:25,430
lessons. And what we're going to get to do

123
00:05:25,430 --> 00:05:30,040
next is, decision problems and reinforcement learning. Ooo, exciting.

124
00:05:30,040 --> 00:05:31,170
>> It is exciting.

125
00:05:31,170 --> 00:05:33,960
>> But I think first people probably have some

126
00:05:33,960 --> 00:05:37,980
homeworky stuff to do, projects to do in the context of this minicourse.

127
00:05:37,980 --> 00:05:39,820
>> Yes, and probably an exam of some sort.

128
00:05:39,820 --> 00:05:40,606
>> [LAUGH]

129
00:05:40,606 --> 00:05:41,392
>> [LAUGH]

130
00:05:41,392 --> 00:05:42,700
>> Good luck to everyone on that.

131
00:05:42,700 --> 00:05:45,160
>> Yes, we're absolutely sure you'll do fine.

132
00:05:45,160 --> 00:05:46,770
Be sure to go over these lessons and be

133
00:05:46,770 --> 00:05:48,550
sure to read all of the material, because there's

134
00:05:48,550 --> 00:05:51,250
a lot of detail in the material that wouldn't

135
00:05:51,250 --> 00:05:54,660
make a lot of sense for us to cover in this format. But do give it a

136
00:05:54,660 --> 00:05:57,090
read, come back, look at the stuff that we've

137
00:05:57,090 --> 00:05:59,370
talked about, it should help you understand the intuition

138
00:05:59,370 --> 00:06:01,600
behind whats really happening there. Excellent. Well this

139
00:06:01,600 --> 00:06:03,310
is great, thanks, thanks Charles, i learned a lot,

140
00:06:03,310 --> 00:06:08,790
>> I did too. Bye Michael i will hear from you soon. Alright, awesome.

141
00:06:08,790 --> 00:06:09,560
>> Bye.
