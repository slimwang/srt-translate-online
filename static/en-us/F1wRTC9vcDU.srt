1
00:00:00,420 --> 00:00:03,370
Okay Michael, so that was pretty good with the quiz. I want to

2
00:00:03,370 --> 00:00:05,430
do another derivation and I want you to help me with it, okay?

3
00:00:05,430 --> 00:00:06,100
>> Hm. Cool.

4
00:00:06,100 --> 00:00:07,740
>> Okay, so Michael, we have a similar

5
00:00:07,740 --> 00:00:09,680
setup to what we've had before. We're given a

6
00:00:09,680 --> 00:00:12,470
bunch of training data, XI inputs and DI

7
00:00:12,470 --> 00:00:15,760
outputs. But this time we're dealing with real valued

8
00:00:15,760 --> 00:00:18,140
functions. So the way DIs are constructed is

9
00:00:18,140 --> 00:00:21,590
there's some Deterministic function f, that we pass the

10
00:00:21,590 --> 00:00:23,880
fs through. And that gives us some value.

11
00:00:23,880 --> 00:00:25,690
And that's really what we're trying to figure out.

12
00:00:25,690 --> 00:00:28,610
What is the underlying f? But to make our job

13
00:00:28,610 --> 00:00:31,990
a little bit harder, we have noisy outputs. So, for every

14
00:00:31,990 --> 00:00:36,820
single DI that is generated, there's some small error, epsilon

15
00:00:36,820 --> 00:00:40,530
that is added to it. Now, this particular error term, is

16
00:00:40,530 --> 00:00:43,640
in fact drawn from a normal distribution with zero mean

17
00:00:43,640 --> 00:00:46,080
and some variance. We don't know what the variance is. It's

18
00:00:46,080 --> 00:00:48,830
going to turn out. It doesn't actually matter. There's some variance

19
00:00:48,830 --> 00:00:51,330
going on here. The important thing is that there's zero mean.

20
00:00:51,330 --> 00:00:51,870
So, you got it?

21
00:00:51,870 --> 00:00:55,720
>> And it's important that it's probably the same variance for all the data.

22
00:00:55,720 --> 00:01:00,110
>> That's right, in fact, each of these epsilon sub i's are drawn iid.

23
00:01:00,110 --> 00:01:02,280
>> And is that f, are we assuming it's linear?

24
00:01:02,280 --> 00:01:03,700
>> Nope, we're not assuming that it's linear.

25
00:01:03,700 --> 00:01:04,019
>> Okay.

26
00:01:04,019 --> 00:01:04,879
>> It's just some function.

27
00:01:04,879 --> 00:01:05,740
>> All right, I'm with you.

28
00:01:05,740 --> 00:01:07,050
>> Okay, so you got it?

29
00:01:07,050 --> 00:01:07,480
>> Yep.

30
00:01:07,480 --> 00:01:10,110
>> All right. So, here's my question

31
00:01:10,110 --> 00:01:14,050
to you. What is The maximum likelihood hypothesis.

32
00:01:14,050 --> 00:01:16,210
>> Do we know f? Can I just say f?

33
00:01:16,210 --> 00:01:16,690
>> No,

34
00:01:16,690 --> 00:01:20,230
we don't know f. All we see are x sub i's and d sub i's. But we

35
00:01:20,230 --> 00:01:22,660
know there is some underlying f. And we

36
00:01:22,660 --> 00:01:25,100
know that it's noisy, according to some normal distribution.

37
00:01:25,100 --> 00:01:27,210
>> I don't know how I would find that.

38
00:01:27,210 --> 00:01:29,850
>> Well let's try to walk it through. So.

39
00:01:29,850 --> 00:01:32,140
We know how to find the maximum likelihood hypothesis, at

40
00:01:32,140 --> 00:01:34,010
least we know an equation for it. The maximum

41
00:01:34,010 --> 00:01:39,070
likelihood hypothesis is simply the one that maximizes this expression.

42
00:01:39,070 --> 00:01:41,710
>> Right. That was when we assumed a uniform

43
00:01:41,710 --> 00:01:43,300
prior on the hypotheses.

44
00:01:43,300 --> 00:01:45,480
>> Exactly. And so we, this is sort of the easiest

45
00:01:45,480 --> 00:01:47,900
case to think about Where it turns out that finding the

46
00:01:47,900 --> 00:01:51,550
hypothesis that best fits the data is the same as finding

47
00:01:51,550 --> 00:01:54,470
a hypothesis that describes the data the best. If you make an

48
00:01:54,470 --> 00:01:58,400
assumption about a uniform distribution, or a uniform prior. Okay, so.

49
00:01:58,400 --> 00:02:00,130
This is all we have to do now is figure out

50
00:02:00,130 --> 00:02:04,720
what we're going to do to expand this expression. So what do

51
00:02:04,720 --> 00:02:06,910
you think we should do first? The probability of the data given

52
00:02:06,910 --> 00:02:10,889
the hypothesis. Right. So each we assumed IID.

53
00:02:10,889 --> 00:02:11,473
>> Mm-hm.

54
00:02:11,473 --> 00:02:13,660
>> You actually helpfully even wrote that down.

55
00:02:13,660 --> 00:02:15,820
So we can expand that into the product

56
00:02:15,820 --> 00:02:18,990
over all the data elements of the probablity

57
00:02:18,990 --> 00:02:22,700
of that data element given the hypothesis. And x.

58
00:02:22,700 --> 00:02:23,930
>> Okay, so let's do that, Michael.

59
00:02:23,930 --> 00:02:27,390
Let's write that out. So, finding the hypothesis

60
00:02:27,390 --> 00:02:32,470
that maximizes the data that we see, as you point out, is just a product

61
00:02:32,470 --> 00:02:35,330
over each of the independent data that we

62
00:02:35,330 --> 00:02:39,330
see. Or datums. So that's good. That's one

63
00:02:39,330 --> 00:02:41,170
nice step. So we've gone from talking about

64
00:02:41,170 --> 00:02:43,130
all of the data together to each of the

65
00:02:43,130 --> 00:02:45,390
individual training data that we see. So what

66
00:02:45,390 --> 00:02:47,310
do we do next? What is the probability

67
00:02:47,310 --> 00:02:52,350
of seeing one particular P sub i, given that we're in a world where H is true.

68
00:02:52,350 --> 00:02:57,850
>> So okay, given that H is true that means whatever the corresponding xi

69
00:02:57,850 --> 00:03:02,500
is, if we push that through the f function, then the di is going to

70
00:03:02,500 --> 00:03:08,280
be F of XI plus some error term so I guess if we took di minus

71
00:03:08,280 --> 00:03:10,900
F X I, that would tell us what the error term is and the we

72
00:03:10,900 --> 00:03:14,110
just need an expression for saying how likely it is that we get that much error.

73
00:03:14,110 --> 00:03:16,340
>> Right, so, what is the expression that tells us that?

74
00:03:16,340 --> 00:03:18,540
>> I'm guessing it's something that uses the

75
00:03:18,540 --> 00:03:20,600
normal distribution, it probably has an E in it.

76
00:03:20,600 --> 00:03:23,120
>> [LAUGH] I think that' s absolutely right.

77
00:03:23,120 --> 00:03:25,770
So, let's be particular about what you said. So, when

78
00:03:25,770 --> 00:03:27,930
you say that we should push it through F of X,

79
00:03:27,930 --> 00:03:30,600
let's be clear that that's basically what H is supposed

80
00:03:30,600 --> 00:03:33,250
to be. Our goal here is, given all of this training

81
00:03:33,250 --> 00:03:37,330
data, lets recover what the true f of x is.

82
00:03:37,330 --> 00:03:40,630
And that's what our H is. Each of our hypotheses a

83
00:03:40,630 --> 00:03:44,660
guess about what the true underlying deterministic function F is.

84
00:03:44,660 --> 00:03:48,210
So, if we have some particular labels, some particular value D

85
00:03:48,210 --> 00:03:51,340
sub I that is at variance with that. What's the probability of us

86
00:03:51,340 --> 00:03:53,440
seeing something that far away from the

87
00:03:53,440 --> 00:03:57,150
true underlying F. Well, it's completely determined

88
00:03:57,150 --> 00:04:00,091
by, the noise model. And the noise is a Gaussian. So, we can

89
00:04:00,091 --> 00:04:01,749
actually write down Gaussian. Do you remember

90
00:04:01,749 --> 00:04:03,985
what the equation for a Gaussian is?

91
00:04:03,985 --> 00:04:06,190
>> Yes. It's exactly something that has an E in it.

92
00:04:06,190 --> 00:04:09,630
>> That's right. So I'll see, I'm going to start writing it

93
00:04:09,630 --> 00:04:12,470
and you see if you remember any of what I'm writing down.

94
00:04:12,470 --> 00:04:13,720
>> E to the...

95
00:04:13,720 --> 00:04:14,190
>> No.

96
00:04:14,190 --> 00:04:14,772
>> Okay, good.

97
00:04:14,772 --> 00:04:16,612
>> It's 1 over.

98
00:04:16,612 --> 00:04:17,498
>> E to the.

99
00:04:17,498 --> 00:04:18,116
>> No.

100
00:04:18,116 --> 00:04:19,069
>> Okay.

101
00:04:19,069 --> 00:04:26,393
>> Square root of, it's, it's coming back to you now. 2 pi sigma squared.

102
00:04:26,393 --> 00:04:27,350
>> Okay.

103
00:04:27,350 --> 00:04:28,330
>> Times...

104
00:04:28,330 --> 00:04:29,830
>> I was going to put that in after.

105
00:04:29,830 --> 00:04:33,260
>> Oh, okay. So now you get your E, so E to the what?

106
00:04:33,260 --> 00:04:38,730
>> It's going to be the value, which, in our case, is, like, H of XI

107
00:04:38,730 --> 00:04:39,610
minus DI.

108
00:04:39,610 --> 00:04:44,870
>> Yeah. And then I feel like, we probably square that?

109
00:04:44,870 --> 00:04:45,570
>> Yep.

110
00:04:45,570 --> 00:04:47,810
>> And then we divide by sigma squared?

111
00:04:49,200 --> 00:04:49,490
>> right.

112
00:04:49,490 --> 00:04:50,170
>> Really?

113
00:04:50,170 --> 00:04:50,770
>> Yeah.

114
00:04:50,770 --> 00:04:51,480
>> Sweet!

115
00:04:51,480 --> 00:04:53,780
>> And your missing one tiny thing.

116
00:04:53,780 --> 00:04:55,060
>> There needs to be another two.

117
00:04:55,060 --> 00:04:57,500
>> Yes. And in fact it's minus one half.

118
00:04:57,500 --> 00:04:59,200
>> Got it.

119
00:04:59,200 --> 00:05:01,450
>> So, this is exactly the form of the Gaussian

120
00:05:01,450 --> 00:05:03,791
in the normal distribution. And what it basically says is the

121
00:05:03,791 --> 00:05:07,561
probability me seeing some particular point, in this case D of I. Given

122
00:05:07,561 --> 00:05:11,225
that the mean is H of X. Which is to say

123
00:05:11,225 --> 00:05:14,310
that's the underlying the function. Is exactly this expression. e to

124
00:05:14,310 --> 00:05:17,535
the minus one half, of the distance from the mean, squared,

125
00:05:17,535 --> 00:05:20,620
divided by the merits. Okay. And that's just, you either remember

126
00:05:20,620 --> 00:05:23,050
that or you don't. But that's just the definition of a

127
00:05:23,050 --> 00:05:26,590
Gaussian. So that means the probability of us seeing the data

128
00:05:26,590 --> 00:05:28,800
is the product of the probability of us seeing each of

129
00:05:28,800 --> 00:05:32,450
the data items. And that's just the product of this expression

130
00:05:32,450 --> 00:05:36,430
here. Good. Now, we need to simplify this. We could stop here

131
00:05:36,430 --> 00:05:38,910
because this is true, but we really need to simplify this and

132
00:05:38,910 --> 00:05:42,250
I think it's pretty... Not to hard to do. It's pretty easy.

133
00:05:42,250 --> 00:05:43,050
>> Mm..hm.

134
00:05:43,050 --> 00:05:45,880
>> What kind of trick do you think we would do here to simplify this?

135
00:05:45,880 --> 00:05:49,050
>> So, first thing I would do is, noticed that the 1 over square root 2 pi

136
00:05:49,050 --> 00:05:51,610
sigma square doesn't depend on i at all, and

137
00:05:51,610 --> 00:05:53,990
maybe move it outside the pi but then realize,

138
00:05:53,990 --> 00:05:57,530
well, actually since we're doing an argmax anyway, it's not going to have

139
00:05:57,530 --> 00:06:00,270
any impact at all. [CROSSTALK] I would just like cross that baby out.

140
00:06:00,270 --> 00:06:03,680
>> I like that. No point in keeping it. All right, now I'm

141
00:06:03,680 --> 00:06:07,120
hoping that the other sigma squared we can make that go away too.

142
00:06:07,120 --> 00:06:10,000
So I'm tempted to just cross it out, but I'd rather, I'd be

143
00:06:10,000 --> 00:06:13,450
much more happy if I had a good explanation for why that's okay.

144
00:06:13,450 --> 00:06:14,460
>> Well, so what's the normal trick,

145
00:06:14,460 --> 00:06:15,710
so we're trying to maximize the function,

146
00:06:15,710 --> 00:06:19,270
right? What you just said is we can get rid of this particular constant

147
00:06:19,270 --> 00:06:21,335
expression because it doesn't affect the max.

148
00:06:21,335 --> 00:06:22,550
What's making it hard for you to get

149
00:06:22,550 --> 00:06:24,466
rid of the sigma squared here is that

150
00:06:24,466 --> 00:06:26,470
it's being passed through some exponential and you

151
00:06:26,470 --> 00:06:27,790
can't remember off the top of your head

152
00:06:27,790 --> 00:06:29,750
what clever work you can do with constants

153
00:06:29,750 --> 00:06:31,180
inside of exponentials. So it would be nice

154
00:06:31,180 --> 00:06:32,580
if we could get rid of the exponential.

155
00:06:34,790 --> 00:06:39,170
>> Very good. So because log is concave.

156
00:06:39,170 --> 00:06:40,790
>> No, because it's monotonic.

157
00:06:40,790 --> 00:06:45,370
>> um-hm. We can take the log of the whole shabang. So this is going to

158
00:06:45,370 --> 00:06:50,390
be equal to the argmax of the sum of the log of that expression, which

159
00:06:50,390 --> 00:06:53,490
is going to move the thing to the outside and the log of E, so that's

160
00:06:53,490 --> 00:06:54,660
going to be good, so it's going to be

161
00:06:54,660 --> 00:06:57,439
the sum of the superscript thing, the power.

162
00:06:57,439 --> 00:07:00,240
>> Right. So let's write that down. Okay, so just

163
00:07:00,240 --> 00:07:02,290
to be sure that that was clear to everybody, let's just

164
00:07:02,290 --> 00:07:04,720
point out that we basically took the log of both, the

165
00:07:04,720 --> 00:07:07,020
natural log of both sides, and so we said, instead of

166
00:07:07,020 --> 00:07:10,260
trying to find the maximum hypothesis or the maximum likelihood hypothesis

167
00:07:10,260 --> 00:07:14,530
by evaluating this expression directly, we instead evaluated the log of

168
00:07:14,530 --> 00:07:19,160
that expression. And as you'll recall from intermediate algebra, the log

169
00:07:19,160 --> 00:07:21,190
of a product is the same as the sum of the

170
00:07:21,190 --> 00:07:25,480
logs, and the log of E to something is just that thing.

171
00:07:25,480 --> 00:07:26,940
>> As long as we do natural log.

172
00:07:26,940 --> 00:07:30,230
>> As long as we do natural log when we have E. If we were doing

173
00:07:30,230 --> 00:07:34,640
something to the, 2 to the power of something, we'd want to do log base 2. Okay.

174
00:07:34,640 --> 00:07:37,940
>> Got it. And you said to do it to both sides but we really didn't need to

175
00:07:37,940 --> 00:07:39,270
do it to both sides we just needed to

176
00:07:39,270 --> 00:07:41,314
do it inside the things we taking the argmax.

177
00:07:41,314 --> 00:07:43,340
>> That's correct. Okay, so we've got here. So,

178
00:07:43,340 --> 00:07:45,430
is there any other simplifying that we can do.

179
00:07:45,430 --> 00:07:46,950
>> Yeah, yeah now it seems much clearer

180
00:07:46,950 --> 00:07:50,550
so the. The negative one half divided by sigma

181
00:07:50,550 --> 00:07:54,410
squared all can move outside the sum cause it doesn't depend on I at all.

182
00:07:54,410 --> 00:07:56,270
>> Right. And then the sigma squared you

183
00:07:56,270 --> 00:07:58,402
said that before you said that that wasn't going

184
00:07:58,402 --> 00:08:00,815
to turn out to matter. Both sigma squares ended

185
00:08:00,815 --> 00:08:03,873
up, you know, getting tossed into the rubbish heap.

186
00:08:03,873 --> 00:08:04,356
>> That's right.

187
00:08:04,356 --> 00:08:06,721
>> And I want to be careful with the negative sign. Like I feel like

188
00:08:06,721 --> 00:08:09,345
the half can go and the sigma square can go but the negative has to stay.

189
00:08:09,345 --> 00:08:12,077
>> You're right. The half can go. And the sigma squared

190
00:08:12,077 --> 00:08:15,497
can go. So that leaves us with this expression. So I've taken,

191
00:08:15,497 --> 00:08:17,462
gotten rid of the one half, like you

192
00:08:17,462 --> 00:08:20,112
suggested. Got rid of the sigma squared like you

193
00:08:20,112 --> 00:08:22,617
suggested, and I moved the minus sign outside

194
00:08:22,617 --> 00:08:25,532
of the summation. And I'm left with this expression.

195
00:08:25,532 --> 00:08:28,497
>> I have a thought about getting rid of that minus sign.

196
00:08:28,497 --> 00:08:30,837
>> Well how would you get rid of a minus sign?

197
00:08:30,837 --> 00:08:34,238
>> So the max of a negative is the min. Right, so we can

198
00:08:34,238 --> 00:08:36,086
get rid of the minus sign by

199
00:08:36,086 --> 00:08:40,756
just simply minimizing instead of maximizing that expression.

200
00:08:40,756 --> 00:08:41,905
We end up with this expression.

201
00:08:41,905 --> 00:08:44,599
>> Nice. That's much simpler than where we started. The e is gone.

202
00:08:44,599 --> 00:08:46,338
>> It's much simpler. We got rid of a bunch

203
00:08:46,338 --> 00:08:47,896
of e's. We got rid of a bunch of turns

204
00:08:47,896 --> 00:08:50,966
out extraneous constants. We got rid of multiplication. We did

205
00:08:50,966 --> 00:08:53,226
a bunch of stuff, and we ended up with this.

206
00:08:53,226 --> 00:08:57,146
>> You know, we got rid of two pis. It's kind of sad I would like some pie.

207
00:08:57,146 --> 00:08:59,157
>> Mm, I wonder what kind of pie it was?

208
00:08:59,157 --> 00:08:59,788
>> Pecan pie?

209
00:08:59,788 --> 00:09:00,251
>> [LAUGH]

210
00:09:00,251 --> 00:09:03,054
>> Okay, so we got this expression, and that's kind of

211
00:09:03,054 --> 00:09:05,950
nice on your own you say, but actually it's even nicer

212
00:09:05,950 --> 00:09:06,556
than that.

213
00:09:06,556 --> 00:09:06,926
>> What?

214
00:09:06,926 --> 00:09:09,595
>> What does this expression remind you of Michael?

215
00:09:09,595 --> 00:09:10,689
>> The Sum of Squares.

216
00:09:10,689 --> 00:09:12,414
>> This is exactly it. This is, in

217
00:09:12,414 --> 00:09:15,352
fact. The sum of squared error, which is awesome.

218
00:09:15,352 --> 00:09:17,784
>> Yeah, whoever decided it would be a good idea

219
00:09:17,784 --> 00:09:20,882
to model noise as a Gaussian was really on to something.

220
00:09:20,882 --> 00:09:22,752
>> Mm-hm. Now, think about what this

221
00:09:22,752 --> 00:09:25,827
means, Michael. We just took, using Bayesian Learning,

222
00:09:25,827 --> 00:09:28,452
a very simple idea of maximizing a likelihood.

223
00:09:28,452 --> 00:09:30,793
We did nothing but substitution, here and there.

224
00:09:30,793 --> 00:09:33,260
With the noise model. We got rid of a bunch of things

225
00:09:33,260 --> 00:09:36,206
that we didn't have to get rid of. We cleverly used the natural

226
00:09:36,206 --> 00:09:38,709
log. Notice that the minus sign can be taken away with the

227
00:09:38,709 --> 00:09:42,062
min. And, we ended up with sum of squared error. Which suggests that

228
00:09:42,062 --> 00:09:45,194
all that stuff we were doing with back propagation. And, all these

229
00:09:45,194 --> 00:09:48,374
other kinds of things we're doing with receptrons is the right thing to

230
00:09:48,374 --> 00:09:51,512
do. Minimizing the sum of square error, which we've just been doing

231
00:09:51,512 --> 00:09:55,232
before. Is in fact the right thing to do according to Bayesian learning.

232
00:09:55,232 --> 00:09:55,882
>> Right in this

233
00:09:55,882 --> 00:09:58,107
case meaning meaning what a Bayesian would say.

234
00:09:58,107 --> 00:10:00,407
>> Meaning what a Bayesian would say which I believe

235
00:10:00,407 --> 00:10:02,117
is sort of right by definition. More importantly here it is.

236
00:10:02,117 --> 00:10:02,927
>> They certainly believe it.

237
00:10:02,927 --> 00:10:04,184
>> Well, they, they do frequently.

238
00:10:04,184 --> 00:10:07,432
>> Oh! I see what you did there.

239
00:10:07,432 --> 00:10:09,471
>> No one will get that but, but us. Anyway,

240
00:10:09,471 --> 00:10:11,965
the thing is this is the thing you should minimize if

241
00:10:11,965 --> 00:10:15,590
you're trying to find the maximum likelihood hypothesis. Now, I just

242
00:10:15,590 --> 00:10:21,029
want to say something. That is beautiful. Absolutely beautiful. That you

243
00:10:21,029 --> 00:10:24,263
do something very simple like finding the maximum likelihood

244
00:10:24,263 --> 00:10:27,585
hypothesis and you end up deriving sum of squared errors.

245
00:10:27,585 --> 00:10:28,421
>> So, just to make sure that

246
00:10:28,421 --> 00:10:30,339
I'm understanding. because I see some beauty here,

247
00:10:30,339 --> 00:10:34,393
but maybe not all of it. We didn't talk about what the hypothesis class here

248
00:10:34,393 --> 00:10:36,129
was. Right, so, if you don't know

249
00:10:36,129 --> 00:10:38,832
what the hypothesis class is... You're, you're kind

250
00:10:38,832 --> 00:10:40,457
of stalled at this point, but if we

251
00:10:40,457 --> 00:10:42,782
say the hypothesis class is say linear functions.

252
00:10:42,782 --> 00:10:43,157
>> Mm-hm.

253
00:10:43,157 --> 00:10:45,782
>> Then, what we're saying is we can do linear regression,

254
00:10:45,782 --> 00:10:48,657
because linear regression is exactly about minimizing the sum

255
00:10:48,657 --> 00:10:51,935
of the squares, right? So linear regression comes popping out

256
00:10:51,935 --> 00:10:55,037
of this kind of Bayesian perspective just like that, so

257
00:10:55,037 --> 00:10:57,413
is, is that part of what makes it so cool?

258
00:10:57,413 --> 00:10:58,568
>> That is part of what makes it cool, but I

259
00:10:58,568 --> 00:11:01,390
just think more generally about gradient descent right? The way gradient descend

260
00:11:01,390 --> 00:11:03,730
works is you take a derivative by stepping in this, in

261
00:11:03,730 --> 00:11:06,586
this space of the error function, which is sum of squared error.

262
00:11:06,586 --> 00:11:08,269
>> I see, so you get gradient descend too.

263
00:11:08,269 --> 00:11:10,712
>> Yes, you get all of the stuff that people have been doing.

264
00:11:10,712 --> 00:11:13,306
Now, there's a piece of beauty there, which is that we derived

265
00:11:13,306 --> 00:11:16,271
things like gradient descend and linear regression, all of the stuff we

266
00:11:16,271 --> 00:11:19,137
were talking about before and we have an argument. For why it's

267
00:11:19,137 --> 00:11:21,926
the right thing to do at least in a Bayesian sense. But

268
00:11:21,926 --> 00:11:25,412
there's an even deeper beauty here, which is tied in with ugliness,

269
00:11:25,412 --> 00:11:28,089
which is the reason this is the right thing to do, is

270
00:11:28,089 --> 00:11:31,662
because of the specific assumptions that we've made. So what were the

271
00:11:31,662 --> 00:11:35,750
assumptions that we made? We assumed that there was some True deterministic

272
00:11:35,750 --> 00:11:38,054
function that was mapping our x's to our in

273
00:11:38,054 --> 00:11:40,550
this case our d's and that they were corrupted

274
00:11:40,550 --> 00:11:43,302
say transmission error or line noise or however you

275
00:11:43,302 --> 00:11:46,356
want to think about it. They are corrupted by some

276
00:11:46,356 --> 00:11:50,930
noise that has a very particular form. Uncorrelated, independently

277
00:11:50,930 --> 00:11:54,730
drawn, Gaussian noise, with mean zero. So the less pretty

278
00:11:54,730 --> 00:11:58,113
way of thinking about it is. Whenever you're trying

279
00:11:58,113 --> 00:12:00,873
to minimize the sum of squared error, you are in

280
00:12:00,873 --> 00:12:04,461
fact assuming that the data that you have has been corrupted by

281
00:12:04,461 --> 00:12:07,798
Gaussian noise. And if it's corrupted by some other noise, or you're

282
00:12:07,798 --> 00:12:11,860
actually not trying to model determinance function, of this sort. And then

283
00:12:11,860 --> 00:12:15,250
you are in fact, possibly, in fact most likely doing the wrong thing.

284
00:12:15,250 --> 00:12:18,820
>> I mean are there other noise models

285
00:12:18,820 --> 00:12:21,440
that lead to some other kinds of learning.

286
00:12:21,440 --> 00:12:23,755
>> Sure, pick any other model in here that does't look

287
00:12:23,755 --> 00:12:26,420
Gaussian at all, and you would end up with something else.

288
00:12:26,420 --> 00:12:28,329
I don't know what you would end up with because. You

289
00:12:28,329 --> 00:12:30,720
know, you couldn't do all these cute tricks with natural logs but

290
00:12:30,720 --> 00:12:33,594
yes, you would end up with something different. And one question

291
00:12:33,594 --> 00:12:36,810
you might ask yourself is well, if I try to do minimizing

292
00:12:36,810 --> 00:12:39,480
the sum of the squared errors, or something for which this

293
00:12:39,480 --> 00:12:43,330
model was not the right one, what sort of bad things might

294
00:12:43,330 --> 00:12:48,040
happen? Here let me give you an example, let's imagine that we're

295
00:12:48,040 --> 00:12:52,615
looking at this here, and our X's are, I don't know measurements

296
00:12:52,615 --> 00:12:56,970
of people. Okay? So height and weight. Something like that.

297
00:12:56,970 --> 00:12:58,003
>> Mm-hm.

298
00:12:58,003 --> 00:13:00,000
>> And in fact let's make it, let's make it

299
00:13:00,000 --> 00:13:01,750
let's make it even simpler than that. Let's imagine that our

300
00:13:01,750 --> 00:13:05,490
x is our height. And our outputs, our d's, are

301
00:13:05,490 --> 00:13:07,790
say weight. And what we're trying to learn is some kind of

302
00:13:07,790 --> 00:13:09,976
function from height to weight. Now, this doesn't make a

303
00:13:09,976 --> 00:13:11,686
lot of sense to have a true [INAUDIBLE], but I'm trying

304
00:13:11,686 --> 00:13:14,300
to make a point here. So what we're saying here is

305
00:13:14,300 --> 00:13:18,430
that we, we measure our height and then we measure weight.

306
00:13:19,620 --> 00:13:21,670
That there's some simple relationship between

307
00:13:21,670 --> 00:13:24,620
them that's captured by f. But, when

308
00:13:24,620 --> 00:13:28,110
we measure the weight, we get a sort of noisy version of that

309
00:13:28,110 --> 00:13:31,630
weight. Okay? That seems reasonable. But

310
00:13:31,630 --> 00:13:33,730
what's not reasonable is we're saying. Our

311
00:13:33,730 --> 00:13:36,820
measurement of the weight is noisy, but our measurement of height is not.

312
00:13:38,110 --> 00:13:42,120
>> Because if the x's are noisy, then this is not a valid assumption.

313
00:13:42,120 --> 00:13:42,800
>> I see.

314
00:13:42,800 --> 00:13:45,110
>> So, it seems to work

315
00:13:45,110 --> 00:13:47,330
a lot of the time and we have an argument for

316
00:13:47,330 --> 00:13:50,640
when it will work, but it's not clear that this particular assumption

317
00:13:50,640 --> 00:13:53,210
actually makes a lot of sense in the real world. Even

318
00:13:53,210 --> 00:13:56,490
though in practice it seems to do just fine. Okay, got it?

319
00:13:56,490 --> 00:13:58,520
>> I think so though I feel like if the error if you put

320
00:13:58,520 --> 00:14:02,830
an error term inside the f along with the x and f is say linear.

321
00:14:02,830 --> 00:14:03,376
>> Mm-hm.

322
00:14:03,376 --> 00:14:06,451
>> Then maybe it pops out and it just becomes another

323
00:14:06,451 --> 00:14:10,200
part of the noise term and, and it all still goes through.

324
00:14:10,200 --> 00:14:12,670
Like I feel lines are still pretty happy even with that.

325
00:14:12,670 --> 00:14:14,803
>> No I think you're right. Lines would be happy here because

326
00:14:14,803 --> 00:14:17,410
linear, I mean linear functions are very nicely behaved in that way.

327
00:14:17,410 --> 00:14:19,500
But of course, they'd have to be the same noise model in

328
00:14:19,500 --> 00:14:20,850
order for it to work the way you want it to work.

329
00:14:20,850 --> 00:14:20,940
>> Yeah.

330
00:14:20,940 --> 00:14:23,390
>> They'd have to both be Gaussian. They have to both have

331
00:14:23,390 --> 00:14:26,360
zero mean, right? And they'd have to be independent of one another.

332
00:14:26,360 --> 00:14:30,910
So your measuring device that gives you an error for your height

333
00:14:30,910 --> 00:14:35,010
would also have to give you an independent normal error for the weight.

334
00:14:35,010 --> 00:14:35,010
>>

335
00:14:35,010 --> 00:14:36,960
Yeah. Though I feel like my scale and my

336
00:14:36,960 --> 00:14:41,410
yardstick actually are fairly independent. And they're Gaussian? .

337
00:14:41,410 --> 00:14:42,490
>> Oh mine is clearly Gaussian.

338
00:14:42,490 --> 00:14:42,850
>> Yeah.

339
00:14:42,850 --> 00:14:44,140
>> Yeah. Well at least they're normal.

340
00:14:44,140 --> 00:14:44,880
>> They're normally are.

341
00:14:44,880 --> 00:14:45,865
>> Mm-hm.

342
00:14:45,865 --> 00:14:49,310
>> Okay good. So let's move on to the next thing Michael. Let's try one

343
00:14:49,310 --> 00:14:52,830
more example of this and, and then I hope that means you got it, okay?

344
00:14:52,830 --> 00:14:53,670
>> Sure.

345
00:14:53,670 --> 00:14:54,100
>> Beautiful.
