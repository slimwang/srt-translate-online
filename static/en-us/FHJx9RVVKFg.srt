1
00:00:00,000 --> 00:00:04,571
And I hope you guessed function A.

2
00:00:04,571 --> 00:00:08,342
Even though both perfectly describe the data

3
00:00:08,342 --> 00:00:10,911
B is much more complex than A.

4
00:00:10,911 --> 00:00:12,957
In fact, outside the data

5
00:00:12,957 --> 00:00:16,315
B seems to go to a minus infinity much faster

6
00:00:16,315 --> 00:00:17,518
than these data points

7
00:00:17,518 --> 00:00:19,653
and to plus infinity much faster

8
00:00:19,653 --> 00:00:21,362
with these data points over here.

9
00:00:21,362 --> 00:00:22,291
And in between

10
00:00:22,291 --> 00:00:23,963
we have wide oscillations

11
00:00:23,963 --> 00:00:25,959
that don't correspond to any data.

12
00:00:25,959 --> 00:00:27,361
So I would argue

13
00:00:27,361 --> 00:00:31,003
A is preferable.

14
00:00:31,003 --> 00:00:32,933
The reason why I asked this question

15
00:00:32,933 --> 00:00:35,068
is because of something called Occam's Razor.

16
00:00:35,068 --> 00:00:38,939
Occam can be spelled in many different ways.

17
00:00:38,939 --> 00:00:41,775
And what Occam says is that

18
00:00:41,775 --> 00:00:43,071
everything else being equal

19
00:00:43,071 --> 00:00:46,649
chose the less complex hypothesis.

20
00:00:46,649 --> 00:00:48,849
Now in practice

21
00:00:48,849 --> 00:00:50,852
there's actually a trade-off

22
00:00:50,852 --> 00:00:53,153
between a really good data fit

23
00:00:53,153 --> 00:00:55,656
and low complexity.

24
00:00:55,656 --> 00:00:58,166
Let me illustrate this to you

25
00:00:58,166 --> 00:00:59,629
by a hypothetical example.

26
00:00:59,629 --> 00:01:02,095
Consider the following graph

27
00:01:02,095 --> 00:01:04,498
where the horizontal axis graphs

28
00:01:04,498 --> 00:01:07,734
complexity of the solution.

29
00:01:07,734 --> 00:01:10,103
For example, if you use polynomials

30
00:01:10,103 --> 00:01:12,272
this might be a high-degree polynomial over here

31
00:01:12,272 --> 00:01:14,443
and maybe a linear function over here

32
00:01:14,443 --> 00:01:16,051
which is a low-degree polynomial

33
00:01:16,051 --> 00:01:19,068
your training data error

34
00:01:19,068 --> 00:01:22,749
tends to go like this.

35
00:01:22,749 --> 00:01:25,719
The more complex the hypothesis you allow

36
00:01:25,719 --> 00:01:29,141
the more you can just fit your data.

37
00:01:29,141 --> 00:01:31,959
However, in reality

38
00:01:31,959 --> 00:01:33,994
your generalization error on unknown data

39
00:01:33,994 --> 00:01:37,364
tends to go like this.

40
00:01:37,364 --> 00:01:40,167
It is the sum of the training data error

41
00:01:40,167 --> 00:01:42,836
and another function

42
00:01:42,836 --> 00:01:46,507
which is called the overfitting error.

43
00:01:46,507 --> 00:01:47,508
Not surprisingly

44
00:01:47,508 --> 00:01:49,843
the best complexity is obtained

45
00:01:49,843 --> 00:01:52,246
where the generalization error is minimum.

46
00:01:52,246 --> 00:01:53,647
There are methods

47
00:01:53,647 --> 00:01:55,549
to calculate the overfitting error.

48
00:01:55,549 --> 00:01:57,818
They go into a statistical field

49
00:01:57,818 --> 00:02:01,139
under the name Bayes variance methods.

50
00:02:01,139 --> 00:02:02,155
However, in practice

51
00:02:02,155 --> 00:02:04,491
you're often just given the training data error.

52
00:02:04,491 --> 00:02:08,562
You find if you don't find the model

53
00:02:08,562 --> 00:02:11,164
that minimizes the training data error

54
00:02:11,164 --> 00:02:14,304
but instead pushes back the complexity

55
00:02:14,304 --> 00:02:17,638
your algorithm tends to perform better

56
00:02:17,638 --> 00:02:20,974
and that is something we will study a little bit

57
00:02:20,974 --> 00:02:22,809
in this class.

58
00:02:22,809 --> 00:02:26,079
However, this slide is really important

59
00:02:26,079 --> 00:02:29,116
for anybody doing machine learning in practice.

60
00:02:29,116 --> 00:02:31,151
If you deal with data

61
00:02:31,151 --> 00:02:33,187
and you have ways to fit your data

62
00:02:33,187 --> 00:02:36,029
be aware that overfitting

63
00:02:36,029 --> 00:02:39,219
is a major source of poor performance

64
00:02:39,219 --> 00:02:41,146
of a machine learning algorithm.

65
00:02:41,146 --> 99:59:59,999
And I give you examples in just one second.
