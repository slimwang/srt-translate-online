1
00:00:00,160 --> 00:00:04,350
So let's look more closely at what
is called cache-to-cache transfers,

2
00:00:04,350 --> 00:00:10,360
which is what happens when core one
has block B in a modified state,

3
00:00:10,360 --> 00:00:15,230
and another core puts a read
request on the bus, so

4
00:00:15,230 --> 00:00:18,228
that it wants to basically bring
the block in its own cache.

5
00:00:18,228 --> 00:00:23,050
At this point, C1 has to react.

6
00:00:23,050 --> 00:00:26,370
Because C1 is the one that
has to provide the data,

7
00:00:26,370 --> 00:00:28,110
if we have the block in the M state,

8
00:00:28,110 --> 00:00:33,420
that means that the only up to date
copy in the system is our copy.

9
00:00:33,420 --> 00:00:36,430
Even the memory doesn't
have an up to date copy.

10
00:00:36,430 --> 00:00:42,370
So if the data is going to be sent to
C2, it had better be our data, but how,

11
00:00:42,370 --> 00:00:46,450
and we have briefly touched on this when
we were discussing the inside protocol.

12
00:00:46,450 --> 00:00:48,520
There are really two solutions.

13
00:00:48,520 --> 00:00:51,300
The first solution is abort and retry.

14
00:00:51,300 --> 00:00:56,680
The idea here is that C1
somehow cancels C2's request,

15
00:00:56,680 --> 00:00:59,710
and for that, we need some
sort of an abort bus signal,

16
00:00:59,710 --> 00:01:04,510
that when our request is placed on the
bus, another core can assert the abort

17
00:01:04,510 --> 00:01:09,820
signal that tells the requesting core to
kind of back off and try again later.

18
00:01:09,820 --> 00:01:17,410
Now that C2's request has been aborted,
C1 can do a normal write back to memory.

19
00:01:17,410 --> 00:01:21,680
At this point,
the memory has the up to date data, and

20
00:01:21,680 --> 00:01:26,790
when C2 retries its read request,
it will get the data from memory.

21
00:01:26,790 --> 00:01:29,710
The problem with this
approach is that really,

22
00:01:29,710 --> 00:01:32,557
from the time C2 makes
the request If this data

23
00:01:32,557 --> 00:01:36,572
was normally coming from memory,
we will have a memory latency,

24
00:01:36,572 --> 00:01:40,700
so if this is just a read miss,
we will have a memory latency.

25
00:01:40,700 --> 00:01:45,170
But if it is a read miss where
another core has the data,

26
00:01:45,170 --> 00:01:50,110
we really have to have two memory
latency's before C2 can get the data.

27
00:01:50,110 --> 00:01:54,980
One for the write-back to memory
to happen on C1, and then for

28
00:01:54,980 --> 00:01:57,320
us to actually get the data from memory.

29
00:01:57,320 --> 00:02:02,240
So the read miss from C1 has a latency
that is twice the memory latency,

30
00:02:02,240 --> 00:02:03,290
plus some.

31
00:02:03,290 --> 00:02:06,020
So this is why this
approach is not ideal.

32
00:02:06,020 --> 00:02:10,110
The second solution is
called intervention.

33
00:02:10,110 --> 00:02:14,740
In this case, C1C is the read
request is being made, and

34
00:02:14,740 --> 00:02:19,650
it tells the memory somehow that C1 will
supply the data instead of the memory.

35
00:02:19,650 --> 00:02:23,440
So normally the memory would
respond to a read miss request,

36
00:02:23,440 --> 00:02:26,890
now C1 kind of tells memory
that I will do that, and for

37
00:02:26,890 --> 00:02:31,170
that we need an intervention
bus signal through which C1 by

38
00:02:32,270 --> 00:02:36,890
asserting the interventions signal,
signals to the memory not to respond.

39
00:02:36,890 --> 00:02:40,970
So the memory now doesn't
respond with the data, but

40
00:02:40,970 --> 00:02:45,700
C1 does, and now the memory must
pick up the data that C1 was

41
00:02:45,700 --> 00:02:50,940
responding with in order to put it
in the appropriate memory block.

42
00:02:50,940 --> 00:02:55,990
The reason the memory has to pick up
the data is that once C1 responds

43
00:02:55,990 --> 00:03:01,160
with the data, both C1 and C2 will
have the block in the shared state.

44
00:03:01,160 --> 00:03:05,580
So both of them will think that the
block is not dirty, and that means that

45
00:03:05,580 --> 00:03:09,780
if the memory doesn't get the data now,
it will never get the fresh data.

46
00:03:09,780 --> 00:03:11,230
So by picking up the data,

47
00:03:11,230 --> 00:03:16,160
the memory ensures that it gets the data
that will not be written back anymore.

48
00:03:16,160 --> 00:03:20,580
So the disadvantage of this approach,
is that it needs more complex hardware.

49
00:03:20,580 --> 00:03:24,990
Now our cache needs to be able to kind
of insert its data, into what would

50
00:03:24,990 --> 00:03:29,480
normally be the memory sending data,
and also the memory has to have enough

51
00:03:29,480 --> 00:03:34,160
intelligence to pick up the data when
a core is responding to an intervention

52
00:03:34,160 --> 00:03:39,820
request, instead of just picking up the
data when there is a write back request.

53
00:03:39,820 --> 00:03:42,850
In modern processors
we nowadays mostly use

54
00:03:42,850 --> 00:03:45,530
a variant of an intervention approach.

55
00:03:45,530 --> 00:03:49,870
And the reason is that, yes the hardware
is slightly more complex, but

56
00:03:49,870 --> 00:03:54,020
fancier snooping protocols actually
eliminate the problem of memory picking

57
00:03:54,020 --> 00:03:58,070
up the data, and the complexity to
respond with the data is not too large.

58
00:03:58,070 --> 00:04:00,810
In contrast,
here the performance suffers.

59
00:04:00,810 --> 00:04:05,390
So pretty much, between having
a fancier protocol and having enough

60
00:04:05,390 --> 00:04:10,120
transistors from Moore's law to
actually add this capability to caches,

61
00:04:10,120 --> 00:04:14,280
we pretty much can do this and not have
to suffer in performance like this.
