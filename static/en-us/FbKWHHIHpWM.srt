1
00:00:00,000 --> 00:00:03,906
So, if we go through this analysis for all of these kernels, we'll see that our

2
00:00:03,906 --> 00:00:07,808
parallel per-element version of the code achieves 12.5 gigabytes per second,

3
00:00:07,808 --> 00:00:13,122
our parallel per-row version of the code gets about 1.8 gigabytes per second,

4
00:00:13,122 --> 00:00:18,725
and our serial version of the code gets an abysmal 0.018 gigabytes per second.

5
00:00:18,725 --> 00:00:23,816
This is roughly the speed of a carrier pigeon. And a better way to think about this, perhaps,

6
00:00:23,816 --> 00:00:30,621
is not in absolute numbers but as a percentage of what the particular GPU we're using can achieve.

7
00:00:30,621 --> 00:00:33,717
So, if we were to work out the percentages we're achieving,

8
00:00:33,717 --> 00:00:39,051
it's something like 31% of theoretical peak bandwidth with our highest performing kernel,

9
00:00:39,051 --> 00:00:47,341
4.5% peak bandwidth with our per-row kernel, and less than a 10th of a percent with our serial kernel.

10
00:00:47,341 --> 00:00:49,965
So, back to the question. Why is this number so low?

11
00:00:49,965 --> 00:00:56,705
Well, we can take a pretty shrewd guess that whenever you see really low DRAM utilization,

12
00:00:56,705 --> 00:01:00,278
really low percentage bandwidth, your first guess is always coalescing.

13
00:01:00,278 --> 00:01:04,834
A way to think about coalescing is that the GPU is always accessing global memory,

14
00:01:04,834 --> 00:01:10,667
accessing the DRAM in pretty large chunks, 32 or 128 bytes at a time.

15
00:01:10,667 --> 00:01:14,120
And this means that we are going to need the fewest

16
00:01:14,120 --> 00:01:21,414
total memory transactions when the threads in a warp access contiguous adjacent memory locations.

17
00:01:21,414 --> 00:01:23,828
So, this is an example of good coalescing.

18
00:01:23,828 --> 00:01:27,004
Every thread is either reading or writing an adjacent memory location.

19
00:01:27,004 --> 00:01:32,608
And clearly, if the threads in a warp are reading and writing completely random locations and memory,

20
00:01:32,608 --> 00:01:35,879
then you're going to get poor coalescing, right?

21
00:01:35,879 --> 00:01:40,716
So, if these accesses are spread out all over the memory,

22
00:01:40,716 --> 00:01:47,990
then the total number of chunks of memory that we have to read could be as large as the number of threads in the warp.

23
00:01:47,990 --> 00:01:51,742
So, a random access pattern clearly leads to bad coalescing.

24
00:01:51,742 --> 00:01:57,348
So, a much more common access pattern is what's called strided, and this is where threads access

25
00:01:57,348 --> 00:02:02,620
a location memory that's a function of their thread ID times some stride.

26
00:02:02,620 --> 00:02:12,596
So, for example, thread 0 might access location 0, thread 1 location 2, thread 2 location 4, thread 3 location 6, and so on.

27
00:02:12,596 --> 00:02:17,284
In that case, that would be a stride of 2 because there's two elements between thread accesses,

28
00:02:17,284 --> 00:02:25,150
and strided accesses range from, okay, like in this case where with the stride of 2 elements,

29
00:02:25,150 --> 00:02:28,283
I'm really only doubling the number of memory transactions.

30
00:02:28,283 --> 00:02:34,230
So, I'm sort of halving the quality of my coalescing all the way to really, really bad, right?

31
00:02:34,230 --> 00:02:37,924
So, you can imagine that if the stride between elements is large enough,

32
00:02:37,924 --> 00:02:43,996
then every thread in the warp is accessing a completely different 32- or 128-byte chunk of memory,

33
00:02:43,996 --> 00:02:47,703
and then you're guaranteed to get bad behavior.

34
00:02:47,703 --> 00:02:51,941
Guaranteed to be maximizing the number of memory transactions that you have to do.

35
00:02:51,941 --> 00:02:56,212
So, let's look at the code for our kernels. Here's where we're reading from the input matrix,

36
00:02:56,212 --> 00:02:58,478
and this actually works out pretty well.

37
00:02:58,478 --> 00:03:04,885
Every thread is reading a value in memory which is equal to some large offset; J times N plus I.

38
00:03:04,885 --> 00:03:09,103
And if you look at I, I is really the thread index plus some offset.

39
00:03:09,103 --> 00:03:13,058
So, adjacent threads, you know, threads with adjacent thread into season X

40
00:03:13,058 --> 00:03:17,397
are reading adjacent values of the input matrix. That's exactly what we want,

41
00:03:17,397 --> 00:03:21,068
so this is good coalescing. On the other hand, when we write the output matrix,

42
00:03:21,068 --> 00:03:28,578
adjacent threads, threads with adjacent values of I, are riding to places separated in memory by N, right?

43
00:03:28,578 --> 00:03:36,217
And N was like 1024. So, adjacent threads are running memory locations that are 1024 elements away from each other.

44
00:03:36,217 --> 00:03:40,664
This is clearly bad, this is bad coalescing. Bad, bad, bad, bad.

45
00:03:40,664 --> 00:03:42,723
This is, in fact, the root of our problem.
