1
00:00:00,380 --> 00:00:03,440
One cool thing about a higher
dimensional network is figuring out how

2
00:00:03,440 --> 00:00:06,290
to exploit it both for fun and profit.

3
00:00:06,290 --> 00:00:08,780
Let's start with a low
dimensional example.

4
00:00:08,780 --> 00:00:11,990
Suppose we're doing an all-gather
on a linear network.

5
00:00:11,990 --> 00:00:14,450
Here's the before and the after picture.

6
00:00:14,450 --> 00:00:16,920
This is a lower bound
on the execution time.

7
00:00:16,920 --> 00:00:20,290
N, in this case,
is the length of each output vector.

8
00:00:20,290 --> 00:00:23,650
Now, if you did the all-gather using
a bucketing scheme, then you would find

9
00:00:23,650 --> 00:00:28,265
its execution time to be
approximately alpha times P + beta n.

10
00:00:28,265 --> 00:00:31,995
So this matches the lower bound beta,
but not in alpha.

11
00:00:31,995 --> 00:00:33,335
Here's a question.

12
00:00:33,335 --> 00:00:37,015
If I gave you a higher dimensional mesh,
could you do better?

13
00:00:37,015 --> 00:00:39,732
Let's take a 2D mesh as an example.

14
00:00:39,732 --> 00:00:42,722
Initially, suppose there
are m words per node.

15
00:00:43,752 --> 00:00:45,802
So, since this is
an all-gather on every node,

16
00:00:45,802 --> 00:00:49,982
we have the capacity to store all of the
words from all of the processes, but,

17
00:00:49,982 --> 00:00:52,332
initially, we only
have one little piece.

18
00:00:52,332 --> 00:00:55,082
Now, if you do an all-gather,
here's the output.

19
00:00:55,082 --> 00:00:58,842
That is, when it completes,
all nodes have a copy of all the data.

20
00:00:58,842 --> 00:01:01,220
So how do you implement this operation?

21
00:01:01,220 --> 00:01:04,220
You could certainly run
the 1D linear algorithm.

22
00:01:04,220 --> 00:01:04,840
The question is,

23
00:01:04,840 --> 00:01:08,820
can you exploit the extra capacity
that comes from extra links?

24
00:01:08,820 --> 00:01:10,130
Here's an idea.

25
00:01:10,130 --> 00:01:15,920
Why don't you start by doing a 1D
all-gather within each process row?

26
00:01:15,920 --> 00:01:19,950
When that completes, each node
will have a complete row of data.

27
00:01:19,950 --> 00:01:23,250
Now do an all-gather, but
this time within each column.

28
00:01:23,250 --> 00:01:26,360
When it completes,
you will have the final output.

29
00:01:26,360 --> 00:01:28,630
So how much time does this take?

30
00:01:28,630 --> 00:01:31,550
The row and
the column gathers are one dimensional.

31
00:01:31,550 --> 00:01:35,050
Let's suppose we use the one
dimensional bucketing algorithm.

32
00:01:35,050 --> 00:01:37,290
Remember the cost of that algorithm.

33
00:01:37,290 --> 00:01:40,530
Let's consider the row
all-gathers first.

34
00:01:40,530 --> 00:01:44,670
Within each row, there are square
root of P processes participating.

35
00:01:44,670 --> 00:01:49,210
The total output size is m
times the square root of P.

36
00:01:49,210 --> 00:01:52,570
Therefore, this is
the cost of the row part.

37
00:01:52,570 --> 00:01:55,067
Notice that relative to
the one D algorithm alone,

38
00:01:55,067 --> 00:01:56,992
we've already improved the latency.

39
00:01:56,992 --> 00:02:00,092
It scales like square root of P,
rather than P.

40
00:02:00,092 --> 00:02:02,250
What about the column communication?

41
00:02:02,250 --> 00:02:04,890
The data volume is now N, but

42
00:02:04,890 --> 00:02:08,699
you still only have square root
of P processes participating.

43
00:02:08,699 --> 00:02:12,336
Therefore, the execution time to run
the column part of the algorithm is

44
00:02:12,336 --> 00:02:17,675
the 1D algorithm on an input of size,
n, over square root of P processes.

45
00:02:17,675 --> 00:02:20,585
Let's sum up the row and
the column times.

46
00:02:20,585 --> 00:02:24,595
Asymptotically, the total time is now
proportional to just square root of P

47
00:02:24,595 --> 00:02:26,290
in the alpha term.

48
00:02:26,290 --> 00:02:29,480
With respect to the beta term,
it's basically optimal.

49
00:02:29,480 --> 00:02:30,140
In other words,

50
00:02:30,140 --> 00:02:33,560
taking advantage of a higher dimensional
network gets us a little bit closer to

51
00:02:33,560 --> 00:02:36,360
the ultimate lower bound,
at least on the number of messages.
