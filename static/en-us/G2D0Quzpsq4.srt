1
00:00:00,012 --> 00:00:02,819
All right, so here is a rule we're
going to call the TD (0) rule,

2
00:00:02,819 --> 00:00:04,875
which gives it a different
name from TD (1).

3
00:00:04,875 --> 00:00:07,970
But eventually we're going to connect
the zero and the one together.

4
00:00:07,970 --> 00:00:11,380
And the rule looks like this,
not so unfamiliar looking.

5
00:00:11,380 --> 00:00:14,210
That the way we're going to
compute our value estimate for

6
00:00:14,210 --> 00:00:20,630
the state that we just left,
when we make a transition at epoch T for

7
00:00:20,630 --> 00:00:25,040
trajectory T, big T,
is what the previous value was.

8
00:00:25,040 --> 00:00:27,940
Plus we're going to move a little bit
in the direction with our learning rate

9
00:00:27,940 --> 00:00:28,780
toward what?

10
00:00:28,780 --> 00:00:31,410
Toward the reward that we observed,

11
00:00:31,410 --> 00:00:35,810
plus the discounted estimated value
of the state that we ended up in,

12
00:00:35,810 --> 00:00:40,300
minus the estimated value of
this state that we just left.

13
00:00:40,300 --> 00:00:42,790
So does this look familiar?

14
00:00:42,790 --> 00:00:45,134
>> It's got Vs, and rs,
and Alphas is in it.

15
00:00:45,134 --> 00:00:46,489
And so- [CROSSTALK]
>> Well sure, all right, but

16
00:00:46,489 --> 00:00:48,090
let's maybe we can be a little
more precise about this.

17
00:00:48,090 --> 00:00:52,580
So what would we expect this outcome
to look like on average, right?

18
00:00:52,580 --> 00:00:56,076
So the thing that's random here,
at least the way we've been talking

19
00:00:56,076 --> 00:00:59,509
about it is, if we were in some
state St-1 and we make a transition,

20
00:00:59,509 --> 00:01:01,960
we don't know what state
we're going to end up in.

21
00:01:01,960 --> 00:01:04,239
But there's some probabilities of those.

22
00:01:04,239 --> 00:01:09,080
So basically this update is being done
more often in the states that we end up

23
00:01:09,080 --> 00:01:11,850
in more often, the more common Sts, and

24
00:01:11,850 --> 00:01:15,235
less often in the Sts that
are less common, or less likely.

25
00:01:15,235 --> 00:01:20,130
And so really if we take the expectation
of this, what is it going to look like?

26
00:01:20,130 --> 00:01:21,860
If we repeat this update over and
over again,

27
00:01:21,860 --> 00:01:25,810
what we're really doing is sampling
different possible St values, right?

28
00:01:25,810 --> 00:01:31,130
So really we're taking an expectation
over what we get as the next state

29
00:01:31,130 --> 00:01:34,650
of the reward plus the discounted
estimated value of that next state.

30
00:01:35,760 --> 00:01:38,140
>> That seems right,
that's kind of what we want.

31
00:01:38,140 --> 00:01:40,820
>> Yeah, so this is exactly what
the maximum likelihood estimate is

32
00:01:40,820 --> 00:01:41,950
supposed to be.

33
00:01:41,950 --> 00:01:45,450
As long as these probabilities for

34
00:01:45,450 --> 00:01:50,260
what the next state is going to be
match what the data has shown so

35
00:01:50,260 --> 00:01:54,180
far as the transition to St.
>> Is that what that blue

36
00:01:54,180 --> 00:01:56,890
stuff above the equations is about?

37
00:01:56,890 --> 00:02:00,890
>> Yeah, so here's the idea, is that if
we repeat this update rule on the finite

38
00:02:00,890 --> 00:02:04,400
data that we've got over and over and
over again, then we're actually taking

39
00:02:04,400 --> 00:02:08,590
an average with respect to how often
we've seen each of those transitions.

40
00:02:08,590 --> 00:02:11,290
So it really is computing
the maximum likelihood estimate.

41
00:02:11,290 --> 00:02:12,730
So this does the right thing.

42
00:02:12,730 --> 00:02:13,500
>> Oh, okay.

43
00:02:13,500 --> 00:02:17,362
So that finite data thing's important
because if we had infinite data,

44
00:02:17,362 --> 00:02:19,660
then everything would work.

45
00:02:19,660 --> 00:02:22,307
>> So right, in the infinite
data case this is also true, but

46
00:02:22,307 --> 00:02:24,750
also TD(1) does the right
thing in infinite data.

47
00:02:24,750 --> 00:02:27,130
>> Kind of everything does
the right thing in infinite data.

48
00:02:27,130 --> 00:02:29,100
>> Yeah, everything's
getting all averaged out, and

49
00:02:29,100 --> 00:02:30,115
it'll do the right thing.

50
00:02:30,115 --> 00:02:33,465
But here we've got a finite
set of data so far.

51
00:02:33,465 --> 00:02:37,497
And the issue is that if we run our
update rule over that data over and

52
00:02:37,497 --> 00:02:38,793
over and over again,

53
00:02:38,793 --> 00:02:43,138
then we're going to get the effect of
having a maximum likelihood model.

54
00:02:43,138 --> 00:02:45,590
And that's not true of
the outcome based model.

55
00:02:45,590 --> 00:02:49,612
Because in the data that we just saw
where we were only saw one transition

56
00:02:49,612 --> 00:02:53,920
from S2 to anything else, we can run
over that over and over and over again.

57
00:02:53,920 --> 00:02:57,820
But the estimate is not going to change,
it's always going to be exactly that.

58
00:02:57,820 --> 00:02:59,020
>> Okay, that makes sense.

59
00:02:59,020 --> 00:03:00,930
Okay, sure, sure, I'll buy that.

60
00:03:00,930 --> 00:03:04,037
>> We can contrast this with the outcome
based idea where we're not doing this

61
00:03:04,037 --> 00:03:05,080
sort of bootstrapping.

62
00:03:05,080 --> 00:03:08,340
We're not using the estimate that
we've gotten at some other state.

63
00:03:08,340 --> 00:03:11,720
We actually use literally
the reward sequence that we saw.

64
00:03:12,760 --> 00:03:17,690
And so as a result of that, if we've
only seen a reward sequence once,

65
00:03:17,690 --> 00:03:21,190
like in the case of S2,
repeating that update over and

66
00:03:21,190 --> 00:03:22,345
over again doesn't change anything.

67
00:03:22,345 --> 00:03:27,770
>> Right, sure, sure, right because
the expectation is the expectation.

68
00:03:27,770 --> 00:03:31,360
>> Right, whereas in here,
we're actually using the intermediate

69
00:03:31,360 --> 00:03:34,620
estimates that we've computed and
refined on all the intermediate nodes.

70
00:03:34,620 --> 00:03:37,890
All the states that we've encountered
along the way to improve our estimate

71
00:03:37,890 --> 00:03:40,040
of the value of every other state,
right?

72
00:03:40,040 --> 00:03:42,710
So this kind of is more self consistent,
right?

73
00:03:42,710 --> 00:03:45,684
It's actually kind of connecting the
values of states to the other values of

74
00:03:45,684 --> 00:03:47,710
the states which is what you'd want.

75
00:03:47,710 --> 00:03:51,880
And this is more just literally
using the experience that it saw and

76
00:03:51,880 --> 00:03:53,870
ignoring the fact that there's
these intermediate states.

77
00:03:53,870 --> 00:03:55,250
>> Okay, well, when you put it that way,

78
00:03:55,250 --> 00:03:58,502
it makes it sound like you should
always use TD(0) and never use TD(1).

79
00:03:58,502 --> 00:04:04,047
>> Yes, but that's oversimplifying.

80
00:04:04,047 --> 00:04:06,390
>> Okay, can you under-simplify it?

81
00:04:06,390 --> 00:04:09,870
>> [LAUGH] We'll get to that.

82
00:04:09,870 --> 00:04:11,230
>> Oh, okay, good, I look forward to it.
