1
00:00:00,512 --> 00:00:01,980
All right so that's supervised learning,
Michael, and

2
00:00:01,980 --> 00:00:03,860
there's a whole bunch of
stuff that we covered.

3
00:00:03,860 --> 00:00:05,400
What about unsupervised learning?

4
00:00:05,400 --> 00:00:08,010
Is there anything there in that section

5
00:00:08,010 --> 00:00:10,070
that you wish we had been
able to talk about, too?

6
00:00:10,070 --> 00:00:11,820
>> It certainly seems
like there ought to be,

7
00:00:11,820 --> 00:00:14,320
in the sense that it was a lot
shorter than the other sections.

8
00:00:14,320 --> 00:00:16,370
>> A lot shorter than
the other sections.

9
00:00:16,370 --> 00:00:20,035
>> Yeah, one thing that I thought would
be worth bringing up in this context is

10
00:00:20,035 --> 00:00:21,750
tf-idf.

11
00:00:21,750 --> 00:00:24,950
So tf-idf is something that
I think is in one version

12
00:00:24,950 --> 00:00:26,840
of this class that you teach.

13
00:00:26,840 --> 00:00:28,540
But we didn't get a chance
to talk about it in here.

14
00:00:28,540 --> 00:00:31,040
And so, what does it have to do
with unsupervised learning, or

15
00:00:31,040 --> 00:00:33,050
randomized optimization?

16
00:00:33,050 --> 00:00:33,790
>> I don't know, Michael.

17
00:00:33,790 --> 00:00:35,890
What does it have to do with it?

18
00:00:35,890 --> 00:00:37,580
>> All right, so tf-idf, it stands for

19
00:00:37,580 --> 00:00:40,300
term frequency inverse
document frequency.

20
00:00:40,300 --> 00:00:43,300
And it is a way of applying
weight when youâ€™re doing,

21
00:00:44,470 --> 00:00:48,580
kind of a nearest neighbor
operation in textual data.

22
00:00:48,580 --> 00:00:52,090
So, if you have a big collection of
text like, web pages and queries,

23
00:00:52,090 --> 00:00:55,898
and you're making queries against
the webpages, you might want to know how

24
00:00:55,898 --> 00:00:59,171
close is this query is to all
the various documents that are in your

25
00:00:59,171 --> 00:01:02,586
collection, all of the various
webpages you might want to return.

26
00:01:02,586 --> 00:01:07,463
And it turns out that there's lots of
different similarity measures that you

27
00:01:07,463 --> 00:01:08,070
can use.

28
00:01:08,070 --> 00:01:11,680
You can use Euclidian distance, you can
use some kind of dot product-y thing.

29
00:01:11,680 --> 00:01:14,030
>> You can use the cosine of
the angle between documents.

30
00:01:14,030 --> 00:01:14,600
>> Yeah there we go.

31
00:01:14,600 --> 00:01:17,400
As a way of measuring how similar
this query is to a document so

32
00:01:17,400 --> 00:01:19,300
we can return the most similar ones.

33
00:01:19,300 --> 00:01:23,349
But it turns out that there's better and
worse ways of doing this waiting.

34
00:01:23,349 --> 00:01:26,316
So it has become apparent
through the years,

35
00:01:26,316 --> 00:01:31,312
I think back into the 60's actually
with Jerry Salton, that a really good

36
00:01:31,312 --> 00:01:36,464
way of doing this that's quite simple
but incredibly powerful, is to say that

37
00:01:36,464 --> 00:01:41,304
the amount of weight that you put on
the appearance of a term in a document,

38
00:01:41,304 --> 00:01:46,100
should be proportional, or positively
related to the term frequency.

39
00:01:46,100 --> 00:01:47,630
The number of times that
word that term appears.

40
00:01:47,630 --> 00:01:51,240
So if I'm searching for things about
snow, and we have a webpage that just

41
00:01:51,240 --> 00:01:53,660
mentions snow in passing,
that's not as important.

42
00:01:53,660 --> 00:01:57,120
As if we have a document that just
mentions snow all over the place.

43
00:01:57,120 --> 00:01:59,800
That the importance of that word grows
with the number of times that it appears

44
00:01:59,800 --> 00:02:00,410
in the document.

45
00:02:00,410 --> 00:02:01,210
>> Well that makes sense, but

46
00:02:01,210 --> 00:02:05,010
then that would imply that probably
the most important word is the.

47
00:02:05,010 --> 00:02:05,600
>> Aye, yes.

48
00:02:05,600 --> 00:02:07,180
Well first of all, no.

49
00:02:07,180 --> 00:02:08,280
It's not really that important.

50
00:02:08,280 --> 00:02:11,008
At least as far as determining
whether some document is relevant.

51
00:02:11,008 --> 00:02:12,580
So, exactly so.

52
00:02:12,580 --> 00:02:14,430
So that's the tf part
is the term frequency.

53
00:02:14,430 --> 00:02:17,070
The idf part,
inverse document frequency, says,

54
00:02:17,070 --> 00:02:21,300
well how many documents in your entire
collection have that word in them?

55
00:02:21,300 --> 00:02:23,870
If it appears indiscriminately
across a large number

56
00:02:23,870 --> 00:02:25,550
of documents then we
want to down-weight it.

57
00:02:25,550 --> 00:02:26,130
>> Like the.

58
00:02:26,130 --> 00:02:28,680
>> Like the, so
the appears almost everywhere.

59
00:02:28,680 --> 00:02:31,790
And so the document frequency,
the number of documents it appears in,

60
00:02:31,790 --> 00:02:32,500
is huge.

61
00:02:32,500 --> 00:02:34,870
The inverse document frequency
is therefore very small,

62
00:02:34,870 --> 00:02:37,740
so that gets very little weight when
you're doing this kind of comparison.

63
00:02:37,740 --> 00:02:40,430
>> That makes sense,
heck did a whole thesis about this.

64
00:02:40,430 --> 00:02:41,126
>> Oh, so
maybe you've heard of this before.

65
00:02:41,126 --> 00:02:42,272
>> I have heard of this before, and

66
00:02:42,272 --> 00:02:44,830
it's actually one of these things
that are just completely accepted.

67
00:02:44,830 --> 00:02:48,780
And it makes sense in the unsupervised
learning case because you're effectively

68
00:02:48,780 --> 00:02:50,620
in the ad hoc retrieval task,
as they call it.

69
00:02:50,620 --> 00:02:53,810
We would probably call it the Google
task if it had been invented.

70
00:02:53,810 --> 00:02:54,810
>> You know what was funny?

71
00:02:54,810 --> 00:02:58,070
I worked in information retrieval
before Google, and what was funny is-

72
00:02:58,070 --> 00:02:59,010
>> They stole all your ideas?

73
00:02:59,010 --> 00:03:02,115
>> No, No, No, No, No, No, No, No, Yes.

74
00:03:02,115 --> 00:03:07,110
No it was hard sometimes
writing papers to make

75
00:03:07,110 --> 00:03:09,880
the case that it was important to be
able to do information retrieval.

76
00:03:09,880 --> 00:03:11,460
Like what are we retrieving on?

77
00:03:11,460 --> 00:03:13,320
There's only ten documents in the world.

78
00:03:13,320 --> 00:03:15,810
Like it was a pretty funny thing and
now its so

79
00:03:15,810 --> 00:03:17,560
obvious to everyone
that this is important.

80
00:03:17,560 --> 00:03:19,110
In fact, they don't even think
about it anymore because-

81
00:03:19,110 --> 00:03:20,190
>> It changed the world.

82
00:03:20,190 --> 00:03:20,750
>> It did.
Okay, so

83
00:03:20,750 --> 00:03:22,640
that was something that we
could have talked about,

84
00:03:22,640 --> 00:03:25,670
but mainly we didn't have time and
we were tired.

85
00:03:25,670 --> 00:03:28,680
Let's see, we've talked about semi
supervised learning already, and

86
00:03:28,680 --> 00:03:31,680
that is something that could
have made sense in the section.

87
00:03:31,680 --> 00:03:35,370
And I guess, but that's all sort of
the unsupervised learning space.

88
00:03:35,370 --> 00:03:38,347
Spectral clustering, where as different
kinds of clustering we could've talked

89
00:03:38,347 --> 00:03:40,630
about but, we didn't
>> I like spectral clustering

90
00:03:40,630 --> 00:03:43,367
>> Yeah, it's kind of a neat thing, and

91
00:03:43,367 --> 00:03:48,172
pretty good right, it does pretty
well in the real world, right?

92
00:03:48,172 --> 00:03:49,662
>> Yeah, I think so
>> Okay, so

93
00:03:49,662 --> 00:03:52,296
you should look that up
>> What's neat about it is it actually

94
00:03:52,296 --> 00:03:53,846
doesn't get stuck in local optima.

95
00:03:53,846 --> 00:03:57,543
So it's a whole series of methods that
are based more on linear algebra,

96
00:03:57,543 --> 00:04:00,453
where you can kind of invert
matrixes and get one answer,

97
00:04:00,453 --> 00:04:04,290
than it is on search, and EM, and
gradient decent, and things like that.

98
00:04:04,290 --> 00:04:06,865
>> Yes, there's a claim you could
make that a lot of machine learning,

99
00:04:06,865 --> 00:04:09,990
particularly the unsupervised learning
space, is effectively linear algebra.

100
00:04:09,990 --> 00:04:11,230
At bottom.

101
00:04:11,230 --> 00:04:14,340
>> Well, and spectral clustering,
they fess up to that.

102
00:04:14,340 --> 00:04:16,209
>> Right, anything else we,
we didn't say anything new.

103
00:04:16,209 --> 00:04:18,600
Is there anything in randomized
optimization that we didn't really have

104
00:04:18,600 --> 00:04:20,079
a chance for talking about-
>> I think it's worth at least

105
00:04:20,079 --> 00:04:21,540
mentioning cross entropy,

106
00:04:21,540 --> 00:04:23,720
which is a method that's
really simple to implement.

107
00:04:23,720 --> 00:04:25,080
It's incredibly effective.

108
00:04:25,080 --> 00:04:28,680
Students that I work with have
implemented it in a number of settings,

109
00:04:28,680 --> 00:04:30,440
and it just always does well.

110
00:04:30,440 --> 00:04:33,500
It does better than our
friends simulated and

111
00:04:33,500 --> 00:04:35,790
kneeling and hill climbing and
things like that.

112
00:04:35,790 --> 00:04:37,490
It does really well.

113
00:04:37,490 --> 00:04:39,895
Well, so here's the thing,
I think it's an awful lot like MIMIC.

114
00:04:39,895 --> 00:04:43,295
[CROSSTALK] It's a MIMIC mimic.

115
00:04:43,295 --> 00:04:43,934
Yeah, very nice.

116
00:04:43,934 --> 00:04:48,811
But it seems to me that the two
communities weren't really

117
00:04:48,811 --> 00:04:50,850
aware of each other.

118
00:04:50,850 --> 00:04:52,770
So maybe we need to put
these two things together.

119
00:04:52,770 --> 00:04:53,590
You can be MIMIC, and

120
00:04:53,590 --> 00:04:56,200
I can be cross-entropy and we can figure
out how they relate to each other.

121
00:04:56,200 --> 00:04:58,210
>> And
we can cross fertilize each other.

122
00:04:58,210 --> 00:04:59,435
I don't like the sound of that at all.

123
00:04:59,435 --> 00:05:01,964
>> [LAUGHTER] Okay, so on that note,
we'll leave unsupervised learning and

124
00:05:01,964 --> 00:05:02,910
randomized optimization.

125
00:05:02,910 --> 00:05:05,098
>> Yes, see and this is what
happens when we're not supervised.

126
00:05:05,098 --> 00:05:07,210
>> [LAUGHTER] Well done.
