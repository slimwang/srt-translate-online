1
00:00:00,420 --> 00:00:04,200
So let's get started training
a logistic classifier.

2
00:00:04,200 --> 00:00:07,830
A logistic classifier is what's
called the linear classifier.

3
00:00:07,830 --> 00:00:12,130
It takes the input, for example,
the pixels in an image, and

4
00:00:12,130 --> 00:00:15,559
applies a linear function to them
to generate its predictions.

5
00:00:16,690 --> 00:00:20,130
A linear function is just
a giant matrix multiply.

6
00:00:20,130 --> 00:00:25,100
It take all the inputs as a big vector,
that will denote X, and multiplies

7
00:00:25,100 --> 00:00:30,390
them with a matrix to generate its
predictions, one per output class.

8
00:00:30,390 --> 00:00:35,280
Throughout, we'll denote the inputs
by X, the weights by W, and

9
00:00:35,280 --> 00:00:36,800
the biased term by b.

10
00:00:37,960 --> 00:00:41,830
The weights of the matrix and the bias,
is where the machine learning comes in.

11
00:00:41,830 --> 00:00:43,750
We're going to train that model.

12
00:00:43,750 --> 00:00:46,520
That means we're going to try to
find the values for the weights and

13
00:00:46,520 --> 00:00:50,480
bias, which are good at
performing those predictions.

14
00:00:50,480 --> 00:00:54,840
How are we going to use the scores
to perform the classification?

15
00:00:54,840 --> 00:00:56,620
Well, let's recap our task.

16
00:00:56,620 --> 00:01:03,430
Each image, that we have as an input can
have one and only one possible label.

17
00:01:03,430 --> 00:01:06,520
So, we're going to turn
the scores into probabilities.

18
00:01:06,520 --> 00:01:09,890
We're going to want the probability of
the correct class to be very close to

19
00:01:09,890 --> 00:01:14,490
one and the probability for
every other class to be close to zero.

20
00:01:14,490 --> 00:01:17,070
The way to turn scores
into probabilities is to

21
00:01:17,070 --> 00:01:20,990
use a softmax function,
which I'll denote here by S.

22
00:01:20,990 --> 00:01:22,500
This is what it looks like.

23
00:01:22,500 --> 00:01:25,170
But beyond the formula,
what's important to know about it

24
00:01:25,170 --> 00:01:30,800
is that it can take any kind of scores
and turn them into proper probabilities.

25
00:01:30,800 --> 00:01:35,320
Proper probabilities, sum to 1,
and they will be large

26
00:01:35,320 --> 00:01:39,820
when the scores are large and small when
the scores are comparatively smaller.

27
00:01:40,990 --> 00:01:45,502
Scores in the context of logistic
regression are often also called logits.
