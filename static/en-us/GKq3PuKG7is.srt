1
00:00:01,569 --> 00:00:05,600
So now that we have built a one versus all classifier,

2
00:00:05,600 --> 00:00:10,820
we will introduce to you, precision, recall, and F scores,

3
00:00:10,820 --> 00:00:15,880
which are other performance metrics to validate these kind of models.

4
00:00:17,430 --> 00:00:21,450
Let us now define this other metric called precision.

5
00:00:21,450 --> 00:00:26,980
Precision is the ratio of the predicted positives that are actually positive or

6
00:00:26,980 --> 00:00:32,840
true positives over the true positives plus the false positives.

7
00:00:32,840 --> 00:00:37,400
The denominator here is the number of examples that are predicted to

8
00:00:37,400 --> 00:00:38,940
be positive.

9
00:00:38,940 --> 00:00:41,900
Let's quickly recall our confusion matrix.

10
00:00:41,900 --> 00:00:44,620
In the top we have the actual values and

11
00:00:44,620 --> 00:00:47,720
on the side we have the predicted values.

12
00:00:47,720 --> 00:00:50,130
So actual positives and negatives.

13
00:00:50,130 --> 00:00:52,240
Predicted positives and negatives.

14
00:00:53,610 --> 00:00:55,660
In this case TP.

15
00:00:55,660 --> 00:01:00,300
FP, FN and TN refer to true positive, false positive,

16
00:01:00,300 --> 00:01:03,500
false negative and true negative respectively.

17
00:01:03,500 --> 00:01:08,360
So the precision is now the true positive divided by

18
00:01:08,360 --> 00:01:12,200
the true positive plus the false positives.

19
00:01:13,710 --> 00:01:19,330
You see this is somewhat different from the sensitivity value which was

20
00:01:19,330 --> 00:01:24,940
true positive divided by true positive plus false negative.

21
00:01:24,940 --> 00:01:31,590
In this case we are dividing by this row of predicted positives.

22
00:01:31,590 --> 00:01:37,120
You see precision is a good measure for classes where the same is skewed that is

23
00:01:37,120 --> 00:01:43,500
the instances of positive classes is smaller than the total number of instances.

24
00:01:43,500 --> 00:01:47,610
Can you think of an example of a classification problem where we

25
00:01:47,610 --> 00:01:52,680
can use precision as a good metric to evaluate the classifier performance?
