1
00:00:00,490 --> 00:00:02,160
Here's a simple way
of thinking about it.

2
00:00:02,160 --> 00:00:07,460
So our goal, let's suppose we're
just worrying about the intensity

3
00:00:07,460 --> 00:00:10,340
of a pixel where its intensity
is just single value.

4
00:00:10,340 --> 00:00:11,670
Right?
So it's a grayscale image,

5
00:00:11,670 --> 00:00:16,880
it goes from 0 to 1, 0 to 255,
minus 27 to plus 18.

6
00:00:16,880 --> 00:00:21,860
Whatever your range happens to be,
the idea is that you've got this linear

7
00:00:21,860 --> 00:00:24,130
that, dimension along which
the values are distributed.

8
00:00:24,130 --> 00:00:26,290
In this case we, we're going.

9
00:00:26,290 --> 00:00:28,650
So there's our distribution
of our pixels, right?

10
00:00:28,650 --> 00:00:31,420
So there's a little circle
down here let's say for

11
00:00:31,420 --> 00:00:34,290
every pixel that's in the image.

12
00:00:34,290 --> 00:00:34,990
All right?

13
00:00:34,990 --> 00:00:38,560
And the question is,
how do we find these centers,

14
00:00:38,560 --> 00:00:43,090
these clusters that we say okay,
those are our three cluster centers.

15
00:00:43,090 --> 00:00:44,920
And once we found them,
we could say all right,

16
00:00:44,920 --> 00:00:49,410
I've got these three different areas of
the image, one, two, and three again.

17
00:00:49,410 --> 00:00:51,570
What do we mean by good cluster?

18
00:00:51,570 --> 00:00:56,030
Well, generically here we're going to
talk about that our best clusters

19
00:00:56,030 --> 00:00:59,540
are clusters whose
centers minimize the SSD,

20
00:00:59,540 --> 00:01:04,364
the sum of squared differences, between,
the, all their points and their centers.

21
00:01:04,364 --> 00:01:07,260
Okay, so the idea here,
here is our SSD and

22
00:01:07,260 --> 00:01:12,720
it says summing over all the clusters,
given a point within the cluster these.

23
00:01:12,720 --> 00:01:14,210
Actually we'll make that square just so

24
00:01:14,210 --> 00:01:17,950
it's clear, the distance between
the point and the cluster center.

25
00:01:17,950 --> 00:01:19,120
All right?

26
00:01:19,120 --> 00:01:22,950
And the challenge in finding these
clusters is that it's a little bit

27
00:01:22,950 --> 00:01:24,870
of a chicken and the egg problem.

28
00:01:24,870 --> 00:01:25,720
All right?

29
00:01:25,720 --> 00:01:27,090
So here's, right?

30
00:01:27,090 --> 00:01:28,708
Suppose, that's what Q.

31
00:01:28,708 --> 00:01:32,130
Suppose we knew where
the cluster centers were, right?

32
00:01:32,130 --> 00:01:35,120
So they're here, here, and here.

33
00:01:35,120 --> 00:01:37,770
Well if we knew those where
the cluster centers were,

34
00:01:37,770 --> 00:01:40,770
which points would we
assign to each cluster?

35
00:01:40,770 --> 00:01:41,950
Well that would be pretty easy.

36
00:01:41,950 --> 00:01:43,770
Just as it says in the answer.

37
00:01:43,770 --> 00:01:46,690
We would assign the points
to the cluster depending

38
00:01:46,690 --> 00:01:49,120
upon which cluster center
they were closest to.

39
00:01:49,120 --> 00:01:52,990
So given the center,
it's obvious to assign the points.

40
00:01:52,990 --> 00:01:55,100
On the other hand, what if we,

41
00:01:55,100 --> 00:01:59,100
instead of knowing the cluster centers,
all we knew was the cluster memberships?

42
00:01:59,100 --> 00:02:02,450
That's what these ovals are signif,
showing, right.

43
00:02:02,450 --> 00:02:05,130
We know which points
belong to which clusters.

44
00:02:05,130 --> 00:02:07,830
Right, this is the chicken problem or
the egg prob, I dont know,

45
00:02:07,830 --> 00:02:09,139
it's the other problem.

46
00:02:09,139 --> 00:02:12,860
The question is, if i knew which
clusters the points belonged to,

47
00:02:12,860 --> 00:02:14,500
where would i put the centers?

48
00:02:14,500 --> 00:02:17,730
Well, i would put
the centers to be the mean,

49
00:02:17,730 --> 00:02:21,400
the average of each of
the points of their cluster.

50
00:02:21,400 --> 00:02:23,040
So everybody know why?

51
00:02:23,040 --> 00:02:28,030
Do you know why if I gave you a bunch of
scores that represented how people were

52
00:02:28,030 --> 00:02:31,170
doing on a test and I asked you sort of,
you know, what's, what, you know,

53
00:02:31,170 --> 00:02:34,270
how, how well do you think people do on
this test you would take the average?

54
00:02:34,270 --> 00:02:36,070
You know why you take the average?

55
00:02:36,070 --> 00:02:40,150
Turns out the average is
the value that minimizes

56
00:02:40,150 --> 00:02:42,070
the sum of squared differences.

57
00:02:42,070 --> 00:02:44,210
It's trivial to show.

58
00:02:44,210 --> 00:02:47,400
And the reason you do that is you
believe somewhere in your head that

59
00:02:47,400 --> 00:02:50,100
the actual thing you're
looking at is a distribution

60
00:02:50,100 --> 00:02:54,460
that's centered about a point with
a Gaussian fall off around it, right?

61
00:02:54,460 --> 00:02:58,900
Some, some value whose probability falls
off as the square of the distance.

62
00:02:58,900 --> 00:03:03,450
So the value that would generate
the maximum likelihood of the data

63
00:03:03,450 --> 00:03:07,360
is the mean value, cause it minimized
the sum of squared differences.

64
00:03:07,360 --> 00:03:11,060
I thought every masters student in
computer science would have known that.

65
00:03:11,060 --> 00:03:14,850
And I just gave this lecture, just
a little while ago, in class here, and

66
00:03:14,850 --> 00:03:17,410
it turned out, well there were
undergraduates in there, too, so

67
00:03:17,410 --> 00:03:20,450
maybe they were the ones who didn't
know that, but not everybody knew that.

68
00:03:20,450 --> 00:03:24,270
So, The reason we take the averages
of things, the mean values,

69
00:03:24,270 --> 00:03:26,570
is it minimizes the sum
of squared distance.

70
00:03:26,570 --> 00:03:29,950
And if you have a Gaussian model or
any other distribution,

71
00:03:29,950 --> 00:03:33,050
where the likelihood of
probability falls off as a square,

72
00:03:33,050 --> 00:03:37,900
then if you minimize the sum of the
squares you generate the, the value for

73
00:03:37,900 --> 00:03:41,480
which the data you saw would
have the maximum likelihood.

74
00:03:41,480 --> 00:03:43,450
Just tuck that away so
you'll remember it somewhere.
