1
00:00:00,240 --> 00:00:02,280
So first,
to be able to talk about generalization.

2
00:00:02,280 --> 00:00:05,640
It helps to think about
states as actually being

3
00:00:05,640 --> 00:00:08,940
collections of features instead
of just unanalyzed blobs.

4
00:00:08,940 --> 00:00:12,590
>> So we're no longer to refer to
states as state 17 and state 19?

5
00:00:12,590 --> 00:00:13,340
>> We can.

6
00:00:13,340 --> 00:00:16,450
But I think it's helpful actually to
give them additional information like

7
00:00:16,450 --> 00:00:16,970
these features.

8
00:00:16,970 --> 00:00:19,710
So here's a concrete example to
make this, try to make some sense.

9
00:00:19,710 --> 00:00:22,350
Do you remember our taxi problem
that we talked about before?

10
00:00:22,350 --> 00:00:23,180
>> How could I forget.

11
00:00:23,180 --> 00:00:27,500
>> In this particular environment,
there was something like, I don't know,

12
00:00:27,500 --> 00:00:32,119
500 states and in standard q learning
type approaches, we would just think of

13
00:00:32,119 --> 00:00:34,720
all those different states as being
completely different from one another.

14
00:00:34,720 --> 00:00:40,250
But we can also imagine tagging each of
the states with some particular value.

15
00:00:40,250 --> 00:00:41,880
Like whether or not well,

16
00:00:41,880 --> 00:00:45,140
we don't really have fuel when we talked
about it, but whether we're low on fuel.

17
00:00:45,140 --> 00:00:49,510
What is the X coordinate of
the taxi cab, this thing here.

18
00:00:49,510 --> 00:00:51,270
What's the Y coordinate the taxi cab?

19
00:00:52,390 --> 00:00:54,120
Is the taxi cab currently near a wall?

20
00:00:54,120 --> 00:00:57,630
Say to the you know to some direction,
north, south, east, west.

21
00:00:57,630 --> 00:01:01,280
Each of these different features
give us a little bit of a handle on

22
00:01:01,280 --> 00:01:03,180
the states and
how they relate to each other right.

23
00:01:03,180 --> 00:01:07,950
So if we have this state that we've got
right here and we say move the passenger

24
00:01:07,950 --> 00:01:11,920
to some other position,
we're still in a related state.

25
00:01:11,920 --> 00:01:14,610
The taxi hasn't moved and it would be
nice to be able to capture that and so

26
00:01:14,610 --> 00:01:17,780
this notion of features
gives us that idea.

27
00:01:17,780 --> 00:01:18,530
Does that makes sense?

28
00:01:18,530 --> 00:01:21,470
>> So that's a lot like how we
talked about machine learning and

29
00:01:21,470 --> 00:01:23,820
supervised learning in our
other machine learning class.

30
00:01:23,820 --> 00:01:24,470
>> Exactly.

31
00:01:24,470 --> 00:01:26,510
Yeah, in fact we had words for
these things.

32
00:01:26,510 --> 00:01:30,370
We talked about the,
boy what were those words?

33
00:01:30,370 --> 00:01:33,570
It's sort of the notion that
different machine learning

34
00:01:33,570 --> 00:01:36,800
representations cause different
things to be similar to each other.

35
00:01:36,800 --> 00:01:37,850
>> Right.

36
00:01:37,850 --> 00:01:40,620
And there are lots of different ways
that we could represent the state space

37
00:01:40,620 --> 00:01:43,540
other than whatever
the different 500 states are.

38
00:01:43,540 --> 00:01:47,280
You have some of them up here, but
we could have the coordinates of

39
00:01:47,280 --> 00:01:50,270
where the passenger is, not just
the coordinates of where the taxi is.

40
00:01:50,270 --> 00:01:53,790
We could have things represented and
sort of an egocentric way.

41
00:01:53,790 --> 00:01:58,850
What's the, I don't know, Manhattan
distance between a passenger and

42
00:01:58,850 --> 00:02:00,490
where the passenger wants to end up.

43
00:02:00,490 --> 00:02:04,560
There are all kinds of things that we
could the ways we could describe things

44
00:02:04,560 --> 00:02:07,410
and some of those things
make some states look

45
00:02:07,410 --> 00:02:11,645
more similar than other
representations would.

46
00:02:11,645 --> 00:02:12,935
>> I think I remember the term.

47
00:02:12,935 --> 00:02:15,295
I think it was inductive bias.

48
00:02:15,295 --> 00:02:18,625
>> Inductive bias was something that
referred to the way algorithms preferred

49
00:02:18,625 --> 00:02:20,245
one solution over another.

50
00:02:20,245 --> 00:02:21,465
>> But I think you're right, though,

51
00:02:21,465 --> 00:02:25,915
because different features
sort of create different

52
00:02:25,915 --> 00:02:29,855
kind of inductive biases by making
some states look more similar.

53
00:02:29,855 --> 00:02:34,180
So, for example,
if we had this egocentric notion of how

54
00:02:34,180 --> 00:02:38,960
far away a person is from a taxi,
then so

55
00:02:38,960 --> 00:02:44,790
long as a person is say two steps
away from a taxi, any state where

56
00:02:44,790 --> 00:02:49,670
a person is two steps away from a taxi
looks the same or looks very similar.

57
00:02:49,670 --> 00:02:52,490
In fact, if you look at this one you
have now if you just drew that right

58
00:02:52,490 --> 00:02:56,340
the red circle is close
to the orange square.

59
00:02:56,340 --> 00:02:58,140
If you were to move
both the red circle and

60
00:02:58,140 --> 00:03:02,580
the orange square up, those two states
would still be very similar because

61
00:03:02,580 --> 00:03:05,820
they're still two away from one
another or have two in between them.

62
00:03:05,820 --> 00:03:11,060
But even though I don't move the red
circle, if I take the orange square and

63
00:03:11,060 --> 00:03:16,600
move it to the right, then that's less
similar because they're farther apart.

64
00:03:18,660 --> 00:03:21,730
>> Okay, and they were ignoring
walls the way he described it right?

65
00:03:21,730 --> 00:03:22,730
>> Right, I'm ignoring walls.

66
00:03:22,730 --> 00:03:25,250
>> We move both of these up by two
we're so yeah we're two apart but

67
00:03:25,250 --> 00:03:27,720
it does feel kind of different
to go around the walls.

68
00:03:27,720 --> 00:03:28,560
>> And if it's different or

69
00:03:28,560 --> 00:03:30,890
not different depends upon
the state features that you use.

70
00:03:30,890 --> 00:03:33,290
>> Okay do we have a name for that or.

71
00:03:33,290 --> 00:03:35,390
Is inductive bias an okay stand in?

72
00:03:35,390 --> 00:03:37,010
>> It's a representation, right?

73
00:03:37,010 --> 00:03:38,020
It's a representation.

74
00:03:38,020 --> 00:03:42,560
Some representations are better for
some things than others and

75
00:03:42,560 --> 00:03:45,190
you can learn differently depending
upon which features you choose.

76
00:03:45,190 --> 00:03:46,800
>> Okay.
>> For example, here's a great feature.

77
00:03:46,800 --> 00:03:48,000
For every state,

78
00:03:48,000 --> 00:03:50,450
here's the set of actions you
should take to accomplish the goal.

79
00:03:50,450 --> 00:03:53,720
That's one representation that
makes learning really easy.

80
00:03:53,720 --> 00:03:54,450
>> Yes, good point.

81
00:03:54,450 --> 00:03:56,130
So let's talk about what we're
actually going to learn.

82
00:03:56,130 --> 00:03:56,630
>> Okay.
