1
00:00:00,000 --> 00:00:02,428
hi my name is Andrew Trask and the PhD

2
00:00:02,428 --> 00:00:03,569
student at the University of Oxford

3
00:00:03,569 --> 00:00:05,219
studying deep learning from natural

4
00:00:05,219 --> 00:00:07,378
language processing. Deep learning as i'm

5
00:00:07,378 --> 00:00:09,330
sure you come to understand is a suite

6
00:00:09,330 --> 00:00:10,800
of tools that allows us to take what we

7
00:00:10,800 --> 00:00:13,169
know and predict we want to know using

8
00:00:13,169 --> 00:00:14,820
their own networks natural language

9
00:00:14,820 --> 00:00:16,320
processing is a study of human language

10
00:00:16,320 --> 00:00:18,329
using tools such as machine learning in

11
00:00:18,329 --> 00:00:20,699
this case deep learning. In this tutorial, we're

12
00:00:20,699 --> 00:00:21,719
going to be learning about 

13
00:00:21,719 --> 00:00:23,160
sentiment classification, or classification of

14
00:00:23,160 --> 00:00:24,719
whether or not a section of human

15
00:00:24,719 --> 00:00:26,939
generated text is positive or negative

16
00:00:26,939 --> 00:00:28,980
in this case we're taking what we know

17
00:00:28,980 --> 00:00:31,859
which is a unit of text written by human and

18
00:00:31,859 --> 00:00:34,259
predicting we don't know which is all we

19
00:00:34,259 --> 00:00:36,210
want to know which is these two labels

20
00:00:36,210 --> 00:00:38,128
our goal is going to be to training

21
00:00:38,128 --> 00:00:40,880
neural network to make this prediction

22
00:00:40,880 --> 00:00:42,439
What we're really going to be learning in

23
00:00:42,439 --> 00:00:44,600
this course is about how to frame a

24
00:00:44,600 --> 00:00:46,969
problem for a neural network 

25
00:00:46,969 --> 00:00:48,320
sentiment classification is a good example for

26
00:00:48,320 --> 00:00:50,240
this because neural networks don't naturally

27
00:00:50,240 --> 00:00:53,359
accept text as input. They accept numbers. So what

28
00:00:53,359 --> 00:00:54,530
we're going to have to do is consider

29
00:00:54,530 --> 00:00:56,810
what underlying patterns in the text we

30
00:00:56,810 --> 00:00:58,280
want the neural network to be able to

31
00:00:58,280 --> 00:01:00,799
discover and then convert our text into

32
00:01:00,799 --> 00:01:02,450
a numerical form in such a way that that

33
00:01:02,450 --> 00:01:04,579
pattern is well preserved and easily

34
00:01:04,579 --> 00:01:06,950
discoverable in the neural network. You'll find that

35
00:01:06,950 --> 00:01:08,599
the frameworks such as tensorflow or

36
00:01:08,599 --> 00:01:11,209
torture you know many others are quite

37
00:01:11,209 --> 00:01:13,099
good at automating the process of

38
00:01:13,099 --> 00:01:14,780
backpropagation or choosing your error

39
00:01:14,780 --> 00:01:16,939
measures or regularization or any kind

40
00:01:16,939 --> 00:01:19,459
of suite of mechanical tools. However

41
00:01:19,459 --> 00:01:22,129
sort of that the art of understanding

42
00:01:22,129 --> 00:01:23,569
what's going on inside the neural net

43
00:01:23,569 --> 00:01:25,400
and then framing the problem such a way

44
00:01:25,400 --> 00:01:27,079
that your neural net can discover the

45
00:01:27,079 --> 00:01:29,000
patterns of the most important pretty

46
00:01:29,000 --> 00:01:30,560
much remains up to the scientists and

47
00:01:30,560 --> 00:01:32,659
it's really really important when you're

48
00:01:32,659 --> 00:01:33,739
actually helping the wild trying to

49
00:01:33,739 --> 00:01:35,329
train his neural Nets. So, that's what this

50
00:01:35,329 --> 00:01:37,700
course is going to be about. I'm already

51
00:01:37,700 --> 00:01:39,590
going to assume you know everything

52
00:01:39,590 --> 00:01:41,390
that's been provided to you in the

53
00:01:41,390 --> 00:01:43,129
Udacity course so far. So, neural nets

54
00:01:43,129 --> 00:01:44,929
stochastic gradient descent, mean squared

55
00:01:44,929 --> 00:01:47,359
error and training testing splits. If you

56
00:01:47,359 --> 00:01:49,189
need help along the way, just review

57
00:01:49,189 --> 00:01:51,140
previous Udacity lectures, and if you

58
00:01:51,140 --> 00:01:52,280
would like to have them explain

59
00:01:52,280 --> 00:01:54,170
in a different form or in a textual form, you

60
00:01:54,170 --> 00:01:55,700
also have recommended reading material

61
00:01:55,700 --> 00:01:57,409
along with Udacity course called 

62
00:01:57,409 --> 00:01:59,090
Grokking Deep Learning, which is a book that i

63
00:01:59,090 --> 00:02:00,859
wrote and my publisher, Manning 

64
00:02:00,859 --> 00:02:02,539
Publications is offering a forty to

65
00:02:02,539 --> 00:02:04,969
fifty percent discount code in which I

66
00:02:04,969 --> 00:02:06,709
will provide in the notebook accompanying

67
00:02:06,709 --> 00:02:09,169
this course. And that discount is just for

68
00:02:09,169 --> 00:02:12,318
Udacity students. So in this course we're

69
00:02:12,318 --> 00:02:13,520
going to be tackling six different

70
00:02:13,520 --> 00:02:15,469
projects. Now the first project is about

71
00:02:15,469 --> 00:02:17,959
curating a dataset and developing kind

72
00:02:17,959 --> 00:02:19,729
of predictive theory what we think is

73
00:02:19,729 --> 00:02:21,500
going to work when the pattern we think

74
00:02:21,500 --> 00:02:23,090
we want to be able to train our neural network

75
00:02:23,090 --> 00:02:25,189
defined. Then what we're going to, in the

76
00:02:25,189 --> 00:02:26,750
first project, validate this theory with

77
00:02:26,750 --> 00:02:29,719
some simple statistics. The next steps

78
00:02:29,719 --> 00:02:31,129
are going to simply be developing 

79
00:02:31,129 --> 00:02:32,539
neural networks and then iterating to

80
00:02:32,539 --> 00:02:34,909
increase the amount of signal as it's

81
00:02:34,909 --> 00:02:36,349
present and obvious to the neural network and

82
00:02:36,349 --> 00:02:38,568
reduce the amount of noise that is present to

83
00:02:38,568 --> 00:02:39,530
our neural networks

84
00:02:39,530 --> 00:02:41,719
By iterating in this process, what

85
00:02:41,719 --> 00:02:43,370
we're going to do is make it easier for

86
00:02:43,370 --> 00:02:44,810
our neural network to discover the underlying

87
00:02:44,810 --> 00:02:46,219
patterns that we think are most

88
00:02:46,219 --> 00:02:48,110
important and we're going to evaluate

89
00:02:48,110 --> 00:02:49,639
the quality of our network along the

90
00:02:49,639 --> 00:02:51,740
way to see how they do. So that's for all that

91
00:02:51,740 --> 00:02:57,670
we'll do. Let's jump in.

