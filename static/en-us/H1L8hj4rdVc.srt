1
00:00:00,270 --> 00:00:01,380
Okay Michael.

2
00:00:01,380 --> 00:00:06,000
So here's the definition of
the reward function (s, o).

3
00:00:06,000 --> 00:00:10,470
It's an expectation over
the set of rewards that

4
00:00:10,470 --> 00:00:14,190
I'm going to see discounted
at each step by gamma.

5
00:00:14,190 --> 00:00:19,030
So the first reward that I see,
r1 step one, is going to be R.

6
00:00:19,030 --> 00:00:21,460
In Step two, it's going to
be R 2 discounted by gamma.

7
00:00:21,460 --> 00:00:25,720
In step three, it's going to be
R 3 discounted by gamma squared.

8
00:00:25,720 --> 00:00:30,600
And so on and so forth,
until the option ends after K steps.

9
00:00:30,600 --> 00:00:33,589
And that final reward will be
discounted by gamma to the K minus 1.

10
00:00:33,589 --> 00:00:36,860
>>
>> So this is in some sense the average,

11
00:00:36,860 --> 00:00:41,080
in fact in a very particular sense
this is the average discounted reward

12
00:00:41,080 --> 00:00:44,930
that I will see by executing this
option in this particular state.

13
00:00:44,930 --> 00:00:45,700
Does that make sense?

14
00:00:47,820 --> 00:00:51,940
>> Yes I mean it's I guess it's weird
because it's kind of immediate reward

15
00:00:51,940 --> 00:00:56,190
in the sense that it's the reward that
comes from executing that one option.

16
00:00:56,190 --> 00:00:59,520
But it's also clearly not immediate
reward it's a sum of rewards.

17
00:01:00,620 --> 00:01:04,959
>> Right, so one way of thinking about
this is remember all an option is

18
00:01:04,959 --> 00:01:06,400
a policy that I'm going to execute.

19
00:01:07,430 --> 00:01:11,500
That policy will be realized in
specific actions which means it

20
00:01:11,500 --> 00:01:15,030
will be realized in
a particular states and

21
00:01:15,030 --> 00:01:17,930
rewards that I will see a bunch of
source, source, source, source.

22
00:01:17,930 --> 00:01:22,400
So even though from the outside,
I start here in this particular state.

23
00:01:22,400 --> 00:01:24,710
Some time passes I end up in this state.

24
00:01:24,710 --> 00:01:29,180
What's actually happening along
the way is I'm visiting every single

25
00:01:29,180 --> 00:01:32,920
one of these states and
picking up reward along the way.

26
00:01:32,920 --> 00:01:35,310
But we need to be able
to discount that reward.

27
00:01:35,310 --> 00:01:39,190
And so the discount factor is hidden
inside of the reward function.

28
00:01:39,190 --> 00:01:41,450
So this is as if rather
than executing the option.

29
00:01:41,450 --> 00:01:45,110
I had actually just executed all
the atomic actions at the option itself.

30
00:01:45,110 --> 00:01:46,410
Will in fact execute.

31
00:01:46,410 --> 00:01:49,520
>> And then you'll call that
sort of one bolus of reward?

32
00:01:49,520 --> 00:01:51,525
>> Right because all that's
really happening right is that

33
00:01:51,525 --> 00:01:52,315
>> Whenever you have this kind of

34
00:01:52,315 --> 00:01:55,115
summation over you taking you seeing
a bunch of states and taking a bunch of

35
00:01:55,115 --> 00:01:57,775
actions, all you're doing is adding up
all the rewards you're going to see.

36
00:01:57,775 --> 00:02:01,345
Remember we got the bellman
equation by just writing out that

37
00:02:01,345 --> 00:02:04,395
we're going to visit
a whole bunch of rewards.

38
00:02:04,395 --> 00:02:08,645
That the definition of the value of
being in the state is the expected

39
00:02:08,645 --> 00:02:09,830
discount of reward.

40
00:02:09,830 --> 00:02:13,350
So we're keeping that consistent
here in this in this abstracted

41
00:02:13,350 --> 00:02:16,250
temporally abstract actions setting

42
00:02:16,250 --> 00:02:19,730
by virtue of the fact that still
the immediate rewards are discounted.

43
00:02:19,730 --> 00:02:24,090
According to the true time steps of
the underlying process even though we,

44
00:02:24,090 --> 00:02:28,120
I guess as the agent get to think at
a higher level we get to think about

45
00:02:28,120 --> 00:02:29,980
an action taking multiple steps.

46
00:02:29,980 --> 00:02:34,140
>> Right, exactly and so
we do something very similar for F and

47
00:02:34,140 --> 00:02:35,230
we end up with this nice equation.
