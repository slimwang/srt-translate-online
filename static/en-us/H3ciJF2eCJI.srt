1
00:00:00,420 --> 00:00:04,260
You've already seen how convolutional
network uses shared parameters across

2
00:00:04,260 --> 00:00:06,800
space to extract patterns over an image.

3
00:00:07,990 --> 00:00:11,800
Now you're going to do the same thing
but over time instead of space.

4
00:00:11,800 --> 00:00:15,590
This is the idea behind
recurrent neural networks.

5
00:00:15,590 --> 00:00:19,460
Imagine that you have a sequence
of events, at each point in time

6
00:00:19,460 --> 00:00:22,840
you want to make a decision about what's
happened so far in this sequence.

7
00:00:22,840 --> 00:00:26,040
Imagine that you have a sequence
of events, at each point in time

8
00:00:26,040 --> 00:00:30,070
you want to make a decision about
what's happened so far in the sequence.

9
00:00:30,070 --> 00:00:32,540
If your sequence is
reasonably stationary,

10
00:00:32,540 --> 00:00:35,280
you can use the same classifier
at each point in time.

11
00:00:35,280 --> 00:00:37,060
That simplifies things a lot already.

12
00:00:37,060 --> 00:00:41,330
But since this is a sequence, you also
want to take into account the past.

13
00:00:41,330 --> 00:00:43,370
Everything that happened
before that point.

14
00:00:43,370 --> 00:00:47,310
One natural thing to do here is to use
the state of the previous classifier

15
00:00:47,310 --> 00:00:50,590
as a summary of what happened before,
recursively.

16
00:00:50,590 --> 00:00:55,140
Now, you would need a very deep neural
network to remember far in the past.

17
00:00:55,140 --> 00:00:58,610
Imagine that this sequence could
have hundreds, thousands of steps.

18
00:00:58,610 --> 00:01:01,510
It would basically mean to have
a deep network with hundreds or

19
00:01:01,510 --> 00:01:03,070
thousands of layers.

20
00:01:03,070 --> 00:01:05,690
But instead,
we're going to use tying again and

21
00:01:05,690 --> 00:01:09,070
have a single model responsible for
summarizing the past and

22
00:01:09,070 --> 00:01:11,650
providing that information
to your classifier.

23
00:01:11,650 --> 00:01:16,200
What you end up with is a network with a
relatively simple repeating pattern with

24
00:01:16,200 --> 00:01:20,360
part of your classifier connecting to
the input at each time step and another

25
00:01:20,360 --> 00:01:23,960
part called the recurrent connection
connecting you to the past at each step.
