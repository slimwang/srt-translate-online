1
00:00:00,080 --> 00:00:01,640
We're going to talk
a little more about this.

2
00:00:01,640 --> 00:00:04,430
But, basically,
what I just talked about, for example,

3
00:00:04,430 --> 00:00:08,100
whether you're doing the whole histogram
the histogram of the whole image,

4
00:00:08,100 --> 00:00:10,470
which doesn't work very well,
but you could do it.

5
00:00:10,470 --> 00:00:15,460
Or if you build like these orientations
of gradients in each of the quadrants,

6
00:00:15,460 --> 00:00:18,720
you're building a model,
a description of that instance.

7
00:00:18,720 --> 00:00:21,420
By the way, you take each of those four,
those histograms before and

8
00:00:21,420 --> 00:00:24,320
you just string them into one
great big feature vector.

9
00:00:24,320 --> 00:00:26,240
That's your description.

10
00:00:26,240 --> 00:00:29,020
So now what you have to
do is you have train, or

11
00:00:29,020 --> 00:00:32,170
sometimes referred to as learn,
a classifier.

12
00:00:32,170 --> 00:00:36,140
Given a bunch of those feature vectors
describing koalas and given a bunch of

13
00:00:36,140 --> 00:00:40,760
them describing pandas, figure out how
to discriminate one from the other.

14
00:00:40,760 --> 00:00:43,650
So here's the basic description,
all right?

15
00:00:43,650 --> 00:00:47,940
So let's suppose we're trying
to learn cars versus not cars.

16
00:00:47,940 --> 00:00:49,660
So how many classes is that?

17
00:00:49,660 --> 00:00:50,610
>> Two.

18
00:00:50,610 --> 00:00:52,740
>> Sorry, how many classes are that?

19
00:00:53,980 --> 00:00:54,780
How many is that?

20
00:00:55,790 --> 00:00:56,620
It is six.

21
00:00:56,620 --> 00:00:59,530
There are six classes, but
the number of classes is six.

22
00:00:59,530 --> 00:01:01,730
Actually, the number of classes is two.

23
00:01:01,730 --> 00:01:02,340
I don't know.

24
00:01:02,340 --> 00:01:06,340
It's called a binary classifier,
because there are two classes.

25
00:01:06,340 --> 00:01:09,602
So, for example, suppose I have
a little picture here of a car, and

26
00:01:09,602 --> 00:01:11,042
I have to decide, is it a car?

27
00:01:11,042 --> 00:01:12,834
And what do you think,
you think that's a car?

28
00:01:12,834 --> 00:01:13,400
>> Yes.

29
00:01:13,400 --> 00:01:14,167
>> Good, she got it right.

30
00:01:14,167 --> 00:01:14,690
Yes.

31
00:01:14,690 --> 00:01:15,750
It's a car.

32
00:01:15,750 --> 00:01:16,583
Okay?

33
00:01:16,583 --> 00:01:18,120
Now here's another picture.

34
00:01:18,120 --> 00:01:19,299
Okay?

35
00:01:19,299 --> 00:01:20,630
Taken.

36
00:01:20,630 --> 00:01:22,542
Oh, somewhere in China I think?

37
00:01:22,542 --> 00:01:23,460
I don't remember where.

38
00:01:23,460 --> 00:01:24,010
Sorry.

39
00:01:24,010 --> 00:01:25,375
So, is that a car or not a car?

40
00:01:25,375 --> 00:01:26,610
>> No.
>> No.

41
00:01:26,610 --> 00:01:27,410
Very good.

42
00:01:27,410 --> 00:01:30,480
What we need are methods of doing that.

43
00:01:30,480 --> 00:01:33,696
Now, there has been a huge
effort in machine learning and

44
00:01:33,696 --> 00:01:37,783
applied to computer vision over
the last really 15, 20 years,

45
00:01:37,783 --> 00:01:41,360
of methods of doing
discriminative classification.

46
00:01:41,360 --> 00:01:43,620
What you see listed here
are a whole bunch of methods,

47
00:01:43,620 --> 00:01:48,370
a couple of references below them
of how to go about doing that.

48
00:01:48,370 --> 00:01:50,120
We're going to talk about three of them.

49
00:01:50,120 --> 00:01:53,440
I've just highlighted them here,
nearest neighbor, SVMs, boosting.

50
00:01:53,440 --> 00:01:55,725
There are others as well.

51
00:01:55,725 --> 00:01:59,840
[COUGH] the general approach, and
there's a huge literature on this now,

52
00:01:59,840 --> 00:02:03,820
especially with these large scale
object recognition competitions, of

53
00:02:03,820 --> 00:02:07,740
what's the best feature vector you can
use, learning different feature vectors,

54
00:02:07,740 --> 00:02:09,758
sparse representations,
a bunch of other things.

55
00:02:09,758 --> 00:02:11,600
And then, given those representations,

56
00:02:11,600 --> 00:02:14,930
what are the methods that you should
use to make the classification?

57
00:02:14,930 --> 00:02:16,590
Here are the three that
we're going to talk about.

58
00:02:16,590 --> 00:02:19,330
If you take more machine learning,
just remember later

59
00:02:19,330 --> 00:02:23,940
you could apply those methods
to those other representations.

60
00:02:23,940 --> 00:02:29,180
Our fundamental assumption
is going to be someone and

61
00:02:29,180 --> 00:02:31,330
hopefully that's somebody
you don't have to pay.

62
00:02:31,330 --> 00:02:35,040
But somebody's going to give you
a database of lots of examples of

63
00:02:35,040 --> 00:02:36,720
the things you want to find and

64
00:02:36,720 --> 00:02:39,875
lots of examples of the things
you don't want to find.

65
00:02:39,875 --> 00:02:42,860
And then you're going to
train the classifier.

66
00:02:42,860 --> 00:02:46,680
Now once you do that, you have to
come up with some way of test,

67
00:02:46,680 --> 00:02:47,800
given some new image.

68
00:02:47,800 --> 00:02:48,930
How do you test it?

69
00:02:48,930 --> 00:02:51,490
So remember we talked about you have
to generate these new candidates and

70
00:02:51,490 --> 00:02:52,890
you have to score them.

71
00:02:52,890 --> 00:02:55,042
So here's the basic idea.

72
00:02:55,042 --> 00:02:57,346
You start off with a window,
and then, and

73
00:02:57,346 --> 00:03:00,100
Kristen is really good at
this animation, right?

74
00:03:00,100 --> 00:03:03,120
And you might take that window
of a particular size, and

75
00:03:03,120 --> 00:03:04,940
you scan it all over.

76
00:03:04,940 --> 00:03:07,955
And what you do is you apply your car,

77
00:03:07,955 --> 00:03:12,180
non-car classifier to every
one of those windows.

78
00:03:12,180 --> 00:03:13,170
And you say do I see any cars?

79
00:03:13,170 --> 00:03:13,760
Do I see any cars?

80
00:03:13,760 --> 00:03:15,140
Do I see any cars?

81
00:03:15,140 --> 00:03:18,060
Not very clever, but
it just works very well.

82
00:03:18,060 --> 00:03:19,840
Right?
Because the classifiers are quick and

83
00:03:19,840 --> 00:03:20,990
they're accurate.

84
00:03:20,990 --> 00:03:23,985
So, as opposed to having a different
way of trying to do detection versus

85
00:03:23,985 --> 00:03:27,790
labeling, I, basically,
do detection by labeling.

86
00:03:27,790 --> 00:03:29,350
There is a problem, of course, though.

87
00:03:29,350 --> 00:03:30,590
I picked a window.

88
00:03:30,590 --> 00:03:33,370
Well, suppose I'm nearer to a car.

89
00:03:33,370 --> 00:03:36,300
Does the picture get,
of the car get bigger or smaller?

90
00:03:36,300 --> 00:03:37,290
That one's an easy one Megan.

91
00:03:37,290 --> 00:03:40,570
If I get car, bigger does,
closer does it look bigger or smaller?

92
00:03:40,570 --> 00:03:41,070
>> Bigger.

93
00:03:41,070 --> 00:03:42,180
>> Bigger, very good.

94
00:03:42,180 --> 00:03:43,380
So what do I have to do?

95
00:03:43,380 --> 00:03:45,890
Well I just have to try bigger windows.

96
00:03:45,890 --> 00:03:48,220
And I slide those all over the place.

97
00:03:48,220 --> 00:03:50,520
And if this feels kind of
date intensive, you're right.

98
00:03:50,520 --> 00:03:52,140
And there's various tricks to doing it.

99
00:03:52,140 --> 00:03:55,030
But that's the general idea is that you
take these different sized windows and

100
00:03:55,030 --> 00:03:57,310
you apply them to different
places in the image.
