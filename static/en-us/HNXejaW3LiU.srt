1
00:00:00,290 --> 00:00:02,969
Here's what I like to do. You pointed out some issues that you

2
00:00:02,969 --> 00:00:05,790
were concerned about and I thought that maybe you could go and look into

3
00:00:05,790 --> 00:00:08,430
it a bit more and you did. And so why don't I turn

4
00:00:08,430 --> 00:00:10,940
things over to you so that you can tell us what you've found out.

5
00:00:10,940 --> 00:00:13,300
>> Okay, well thank you Michael. Hi again.

6
00:00:13,300 --> 00:00:13,940
>> Hi.

7
00:00:13,940 --> 00:00:18,050
>> Thank you Michael. So I did go and I started

8
00:00:18,050 --> 00:00:20,214
trying to deal with these issues. So just to recap a

9
00:00:20,214 --> 00:00:22,720
little bit, there were two problems that I had, more or

10
00:00:22,720 --> 00:00:26,020
less. And they were, that we had all these cool little randomized

11
00:00:26,020 --> 00:00:29,300
optimization algorithms. And, most of them seemed to share this property

12
00:00:29,300 --> 00:00:32,820
that the only thing that really happened over time, is you started

13
00:00:32,820 --> 00:00:34,960
out with some point and you ended up with some point,

14
00:00:34,960 --> 00:00:37,830
which was supposed to be, you know, the optimal point. And the

15
00:00:37,830 --> 00:00:41,520
only difference between the first point, and the last point that

16
00:00:41,520 --> 00:00:43,870
you did or, the one millionth point or however how many you

17
00:00:43,870 --> 00:00:46,670
iterations you had, is that, that point might have been closer

18
00:00:46,670 --> 00:00:51,210
to the optimum by some measure. And very little structure was actually

19
00:00:51,210 --> 00:00:53,870
being kept around or communicated. Only the point was being

20
00:00:53,870 --> 00:00:56,280
communicated. Now you could argue that that isn't quite true with

21
00:00:56,280 --> 00:00:59,000
genetic algorithms, but really you move from a single point

22
00:00:59,000 --> 00:01:02,310
to just a few points. The other problem that I had

23
00:01:02,310 --> 00:01:05,040
is that we had all of this sort of probability

24
00:01:05,040 --> 00:01:08,530
theory that was underneath what we were doing, all this randomization.

25
00:01:08,530 --> 00:01:10,770
But somehow it wasn't at all clear in most of

26
00:01:10,770 --> 00:01:16,320
the cases, exactly what probability distribution we were dealing with. Now,

27
00:01:16,320 --> 00:01:18,194
one thing I really liked about some [UNKNOWN]

28
00:01:18,194 --> 00:01:19,630
is that you were very clear about what

29
00:01:19,630 --> 00:01:23,140
the probability distribution was. So what I decided to do is to go out there in

30
00:01:23,140 --> 00:01:27,910
the world and see if I could find maybe a class of algorithms that took care

31
00:01:27,910 --> 00:01:30,420
of these two points for us. And I

32
00:01:30,420 --> 00:01:33,820
found something, you'll be very happy hear, Michael.

33
00:01:33,820 --> 00:01:35,590
>> Yeah, I would love to, to find out what it is.

34
00:01:35,590 --> 00:01:39,310
>> It turns out that I wrote a paper about this, almost 20 years ago.

35
00:01:39,310 --> 00:01:40,740
>> [LAUGH] How did you find that?

36
00:01:40,740 --> 00:01:41,480
>> I just

37
00:01:41,480 --> 00:01:43,350
said, well if I wanted to start looking someplace,

38
00:01:43,350 --> 00:01:45,476
I should look at home first. And I stumbled across

39
00:01:45,476 --> 00:01:45,556
>> Oh.

40
00:01:45,556 --> 00:01:46,280
>> this paper that I wrote.

41
00:01:46,280 --> 00:01:49,500
>> I see, I see. So learning about machine learning, really begins at home.

42
00:01:49,500 --> 00:01:51,550
>> That's exactly right. So, I had to

43
00:01:51,550 --> 00:01:53,454
re-read the paper because, you know, it is

44
00:01:53,454 --> 00:01:57,510
a coupl of decades old. And I will point that a lot of other work has been

45
00:01:57,510 --> 00:02:00,170
done since this that refines on these ideas.

46
00:02:00,170 --> 00:02:01,800
But, this is fairly straightforward and simple, so,

47
00:02:01,800 --> 00:02:03,380
I think I am just going to stick with

48
00:02:03,380 --> 00:02:06,790
this work. And the paper's available for everyone listening

49
00:02:06,790 --> 00:02:10,130
this to right now or watching this right now. So you can read it and

50
00:02:10,130 --> 00:02:13,130
all of it's gory details. But I just want to go over the, the high level bit

51
00:02:13,130 --> 00:02:14,900
here because I, I, I really think it

52
00:02:14,900 --> 00:02:18,010
kind of gets at this idea. So, in particular,

53
00:02:18,010 --> 00:02:19,970
the paper that I'm talking about introduced an

54
00:02:19,970 --> 00:02:22,740
algorithm called Mimic, which actually stands for something,

55
00:02:24,980 --> 00:02:28,820
though I forget what. And it really had a very simple structure

56
00:02:28,820 --> 00:02:34,370
to it. The basic idea was to directly model a probability distribution.

57
00:02:34,370 --> 00:02:35,600
>> Probability distribution of what?

58
00:02:35,600 --> 00:02:38,530
>> Well, I'll tell you. And, you know, like I said,

59
00:02:38,530 --> 00:02:42,430
Michael, I will, define exactly what this, probability distribution is for you

60
00:02:42,430 --> 00:02:45,280
for a second and, and hopefully you'll, you'll buy that it seems

61
00:02:45,280 --> 00:02:50,400
like a reasonable distribution to model. And given that you have this,

62
00:02:50,400 --> 00:02:53,380
probability distribution that you're directly modeling, the, the goal

63
00:02:53,380 --> 00:02:55,220
is to do this sort of search through space, just

64
00:02:55,220 --> 00:02:56,350
like we did with all the rest of these

65
00:02:56,350 --> 00:03:01,000
algorithms. And to successfully refine the estimate of that distribution.

66
00:03:01,000 --> 00:03:02,575
>> Hm.

67
00:03:02,575 --> 00:03:04,960
>> And the idea is that if you can directly model this

68
00:03:04,960 --> 00:03:09,710
distribution and refine it over time that, that will in fact convey structure.

69
00:03:09,710 --> 00:03:12,245
>> Structure in particular of what were learning

70
00:03:12,245 --> 00:03:15,510
about the search space while we're doing the search.

71
00:03:15,510 --> 00:03:17,900
>> Exactly, and not just simply the structure of the

72
00:03:17,900 --> 00:03:20,100
search space, but the structure of the parts of the

73
00:03:20,100 --> 00:03:22,900
space that represent good points or points that are more

74
00:03:22,900 --> 00:03:26,400
optimal than others. Yeah, that seems like a really useful thing.

75
00:03:26,400 --> 00:03:29,700
>> So I'm just going to give you, again, this, this simple mimic algorithm

76
00:03:29,700 --> 00:03:31,240
that, that sort of captures these basic

77
00:03:31,240 --> 00:03:32,930
ideas, because I think it's fairly simple

78
00:03:32,930 --> 00:03:34,860
and easy to understand, while still getting

79
00:03:34,860 --> 00:03:36,630
some of the underlying issues. But do

80
00:03:36,630 --> 00:03:38,220
keep in mind that there's been literally

81
00:03:38,220 --> 00:03:41,440
decades of work since then, and optimization

82
00:03:41,440 --> 00:03:44,240
space where people have really taken these kinds of ideas and

83
00:03:44,240 --> 00:03:47,810
refined them to be sort of much more mathematically precise. But this,

84
00:03:47,810 --> 00:03:49,920
I think, does get the idea across, and I happen to

85
00:03:49,920 --> 00:03:51,630
understand it, so I thought that I would share it with you.

86
00:03:51,630 --> 00:03:51,920
>> Hm.

87
00:03:51,920 --> 00:03:52,290
>> Seem fair?

88
00:03:52,290 --> 00:03:53,550
>> Yeah, that sounds really exciting.

89
00:03:53,550 --> 00:03:54,390
>> Excellent.
