1
00:00:00,100 --> 00:00:03,530
What we have just done here, is a very powerful idea in learning.

2
00:00:03,530 --> 00:00:08,420
Convergence is important. Because without convergence, a learning agent could

3
00:00:08,420 --> 00:00:13,660
zig zag forever in a large learning space. We want to ensure that the learning

4
00:00:13,660 --> 00:00:18,130
agent converges to some concept characterization, and that remains stable.

5
00:00:18,130 --> 00:00:22,210
This method guarantees convergence, as long as there is a sufficiently large

6
00:00:22,210 --> 00:00:27,050
number of examples. We needed five examples in this particular illustration, for

7
00:00:27,050 --> 00:00:31,290
the convergence to occur. This convergence would have occurred, irrespective of

8
00:00:31,290 --> 00:00:35,650
the order of the examples, as long as the five examples were there. Note that we

9
00:00:35,650 --> 00:00:39,360
did not use background knowledge like we did in incremental concept learning.

10
00:00:39,360 --> 00:00:43,180
Note also that we did not assume that the teacher was forwarding the examples in

11
00:00:43,180 --> 00:00:47,580
the right order. This is the benefit of version space learning. There is

12
00:00:47,580 --> 00:00:52,120
another feature to note. In incremental concept learning, we wanted each example

13
00:00:52,120 --> 00:00:56,460
different from the current concept characterization in exactly one feature, so

14
00:00:56,460 --> 00:01:00,970
that the learning agent could focus its attention. However inversion spaces,

15
00:01:00,970 --> 00:01:04,209
you can notice that each successful example, the first one,

16
00:01:04,209 --> 00:01:07,660
the previous one and many features, just look at the first two examples.

17
00:01:07,660 --> 00:01:11,466
They differ in many features in the name of the restaurant, in the meal,

18
00:01:11,466 --> 00:01:14,630
in the cost. Here is the algorithm for the version space technique.

19
00:01:14,630 --> 00:01:17,570
We'll go through it very quickly, because we've already illustrated it in

20
00:01:17,570 --> 00:01:23,900
detail. If the new example is positive, generalize all specific models included.

21
00:01:23,900 --> 00:01:26,960
Prune away the general models that cannot include the positive example.

22
00:01:28,250 --> 00:01:32,370
If the example is negative, specialize all the general models to include it.

23
00:01:32,370 --> 00:01:35,950
Prune away the specific models that cannot include the negative example.

24
00:01:35,950 --> 00:01:39,840
Prune away any models subsumed by the other models. Know that in this

25
00:01:39,840 --> 00:01:43,710
specific implementation of version space technique that we just illustrated,

26
00:01:43,710 --> 00:01:47,310
there is a single pathway coming from the most specialize concert model.

27
00:01:47,310 --> 00:01:53,390
And therefore there is no need to prune away specific models. In general,

28
00:01:53,390 --> 00:01:55,440
there could be multiple generalizations coming for

29
00:01:55,440 --> 00:01:57,770
the most specialized models, and this might be needed.
