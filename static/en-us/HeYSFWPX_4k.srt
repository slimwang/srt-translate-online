1
00:00:00,450 --> 00:00:00,979
Hello Charles.

2
00:00:00,979 --> 00:00:02,518
>> Hi Michael. How are you doing?

3
00:00:02,518 --> 00:00:03,987
>> I'm doing okay.

4
00:00:03,987 --> 00:00:04,979
>> Good.

5
00:00:04,979 --> 00:00:06,407
>> It's you know, exciting to

6
00:00:06,407 --> 00:00:08,661
be getting to talk about reinforcement learning.

7
00:00:08,661 --> 00:00:11,620
>> Mm. Reinforcement learning. It's my favorite type of learning.

8
00:00:11,620 --> 00:00:12,360
>> Is that true?

9
00:00:12,360 --> 00:00:13,070
>> It is true.

10
00:00:13,070 --> 00:00:14,910
>> Wow. I like machine learning.

11
00:00:14,910 --> 00:00:17,010
>> I like machine learning too. But of all the kinds

12
00:00:17,010 --> 00:00:20,240
of machine learning, reinforcement learning is my favorite kind of learning. So,

13
00:00:20,240 --> 00:00:23,360
let's, how bout, how bout, giving the opportunity for the students

14
00:00:23,360 --> 00:00:25,710
in the class to learn about reinforcement learning by having us tell

15
00:00:25,710 --> 00:00:26,170
them about it.

16
00:00:26,170 --> 00:00:27,920
>> Oh, let's do that. You first.

17
00:00:27,920 --> 00:00:31,190
>> Alright. We're going to build up on the stuff you told us about last time.

18
00:00:31,190 --> 00:00:32,990
>> You mean the fantastic, well defined, and

19
00:00:32,990 --> 00:00:34,580
well formalized stuff that we talked about last time?

20
00:00:34,580 --> 00:00:36,540
>> Yes, it was fantastic and it laid the

21
00:00:36,540 --> 00:00:38,180
groundwork for what I would like to talk about. So

22
00:00:38,180 --> 00:00:40,560
you set up Markov decision processes, and I'm going to

23
00:00:40,560 --> 00:00:42,690
talk about what it means to learn in that context.

24
00:00:42,690 --> 00:00:43,920
>> Excellent.

25
00:00:43,920 --> 00:00:45,450
>> I find it useful to start off by

26
00:00:45,450 --> 00:00:47,090
thinking about a reinforcement learning

27
00:00:47,090 --> 00:00:49,530
API, like application programmer interface.

28
00:00:50,620 --> 00:00:54,480
So what you talked about is, is this box here.

29
00:00:54,480 --> 00:00:56,752
The idea of being able to take a model of

30
00:00:56,752 --> 00:01:00,231
an MDP, which consists of a transition function, T, and

31
00:01:00,231 --> 00:01:03,606
a reward function R. And it goes through some code and,

32
00:01:03,606 --> 00:01:06,415
you know, it comes out and a policy comes out.

33
00:01:06,415 --> 00:01:09,386
Right? And a policy is, is like pi. It maps states

34
00:01:09,386 --> 00:01:13,038
to actions. And that whole activity, what would you call

35
00:01:13,038 --> 00:01:15,949
that? What would you call the, the, the problem of, or

36
00:01:15,949 --> 00:01:19,770
the approach of taking models and turning them into policies?

37
00:01:19,770 --> 00:01:21,030
>> Maybe I'd call them planning.

38
00:01:21,030 --> 00:01:23,112
>> Yeah that's what I, that's, that's what I was hoping you would say.

39
00:01:23,112 --> 00:01:24,130
>> Mmhm.

40
00:01:24,130 --> 00:01:25,858
>> Alright, now we're going to talk about

41
00:01:25,858 --> 00:01:27,548
a different set up here. We're still

42
00:01:27,548 --> 00:01:30,020
interested in spitting out policies, right? Figuring

43
00:01:30,020 --> 00:01:31,999
out how to behave to maximize reward. But

44
00:01:31,999 --> 00:01:34,194
a learner's going to do something different. Instead

45
00:01:34,194 --> 00:01:35,538
of taking a model as input, it's

46
00:01:35,538 --> 00:01:38,026
going to take transitions. It's going to take samples

47
00:01:38,026 --> 00:01:40,666
of, being in some state, taking some action,

48
00:01:40,666 --> 00:01:43,438
observing a reward and observing the state that is

49
00:01:43,438 --> 00:01:46,279
at the other end of that transition. Alright? And

50
00:01:46,279 --> 00:01:47,643
we'll, I put a little star on that to

51
00:01:47,643 --> 00:01:49,790
say well we're going to see a bunch of these transitions.

52
00:01:49,790 --> 00:01:50,140
>> Mm.

53
00:01:50,140 --> 00:01:52,930
>> And using that information we're going to instead

54
00:01:52,930 --> 00:01:54,930
of computing a policy, we're going to learn a policy.

55
00:01:54,930 --> 00:01:57,220
>> I see. So we call that learning?

56
00:01:57,220 --> 00:01:58,590
>> Yeah, or even reinforcement learning.

57
00:01:58,590 --> 00:02:01,600
>> Mm. By the way, what makes it reinforcement learning?

58
00:02:01,600 --> 00:02:04,577
>> That's a question. [LAUGH].

59
00:02:04,577 --> 00:02:06,120
>> It's not a good question,

60
00:02:06,120 --> 00:02:06,810
but it's a question.

61
00:02:06,810 --> 00:02:08,350
>> I was, I was going to say good question, but

62
00:02:08,350 --> 00:02:10,370
I'll, well maybe, maybe there, it's not that it was a

63
00:02:10,370 --> 00:02:12,370
bad question, it's that I don't have a particularly good answer

64
00:02:12,370 --> 00:02:14,640
for it. So, maybe we need another slide to discuss that.

65
00:02:14,640 --> 00:02:15,170
>> Okay.
