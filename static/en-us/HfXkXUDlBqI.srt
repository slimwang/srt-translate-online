1
00:00:00,000 --> 00:00:02,007
Let's analyze this algorithm.

2
00:00:02,007 --> 00:00:05,934
What we want to know is how much work is it going to take for us to complete this algorithm,

3
00:00:05,934 --> 00:00:10,035
how many operations are we going to do, and how many steps is that going to take?

4
00:00:10,035 --> 00:00:12,436
So the analysis is straightforward.

5
00:00:12,436 --> 00:00:17,396
For either flavor, inclusive or exclusive, for each of the n iterations of the loop,

6
00:00:17,396 --> 00:00:21,507
we take 1 step and we do 1 operation.

7
00:00:21,507 --> 00:00:28,934
Thus we require n operations to complete the algorithm, and we also require n steps.

8
00:00:28,934 --> 00:00:33,679
But the parallelization of this algorithm is more complex than with reduce.

9
00:00:33,679 --> 00:00:36,793
With reduce it was really clear that the different pieces of the computation tree

10
00:00:36,793 --> 00:00:40,926
could be computed in parallel. With scan this is much less clear.

11
00:00:40,926 --> 00:00:43,626
So the questions you should be asking yourself now are

12
00:00:43,626 --> 00:00:46,227
a) how do we compute scan and parallel at all,

13
00:00:46,227 --> 00:00:49,420
and b) how can we reduce the work and step complexity

14
00:00:49,420 --> 00:00:52,272
of a parallel implementation as much as possible?

15
00:00:52,288 --> 00:00:56,218
But even before we get to those 2 questions, let's answer a larger question.

16
00:00:56,218 --> 00:01:00,158
Why do I care about the parallelization of scan in the 1st place?

17
00:01:00,158 --> 00:01:02,917
So here's a high level, intuitive explanation.

18
00:01:02,917 --> 00:01:05,743
Many computations have the following form.

19
00:01:05,743 --> 00:01:09,717
We want to process a list of inputs, which I'm showing here with blue circles,

20
00:01:09,717 --> 00:01:13,809
and create a list of outputs, which I'm showing here with red circles.

21
00:01:13,809 --> 00:01:19,057
And we begin by computing this 1st output from this 1st input.

22
00:01:19,057 --> 00:01:24,753
Then we use that 1st output and the 2nd input to compute the 2nd output,

23
00:01:24,753 --> 00:01:31,226
use that 2nd output and the 3rd input to create the 3rd output, and so on.

24
00:01:31,226 --> 00:01:35,006
For instance, in our checkbook example we used the previous balance

25
00:01:35,006 --> 00:01:39,311
and the next check amount to compute the current balance.

26
00:01:39,311 --> 00:01:43,141
This creates a dependency structure that looks like what I've drawn in green.

27
00:01:43,141 --> 00:01:47,264
When we structure our computation like this, we see that it has no apparent concurrency.

28
00:01:47,264 --> 00:01:49,469
We're only doing 1 operation at a time.

29
00:01:49,469 --> 00:01:55,056
First we compute the 1st output, and then we use the 1st output as an input to get the 2nd output,

30
00:01:55,056 --> 00:01:58,801
we use this output to get the 3rd output, and so on.

31
00:01:58,801 --> 00:02:03,173
Now, this is a very serial structure, and if we implement it in this way

32
00:02:03,173 --> 00:02:06,029
it would be a poor fit for a GPU indeed.

33
00:02:06,029 --> 00:02:10,038
Computations like this can often be expressed as a scan.

34
00:02:10,038 --> 00:02:13,864
And if we can characterize our computation in terms of scan, that's great,

35
00:02:13,864 --> 00:02:17,228
because--let me tell you a little secret--we can parallelize scan

36
00:02:17,228 --> 00:02:20,880
and make it run fast on a parallel processor like a GPU.

37
00:02:20,880 --> 00:02:24,017
Now, not all computations with this sort of dependency structure

38
00:02:24,017 --> 00:02:26,601
will fit into the scan model, but many will.

39
00:02:26,601 --> 00:02:30,828
And for those computations, leveraging an efficient parallel scan implementation

40
00:02:30,828 --> 00:02:34,356
turns the problem from something that would be a poor fit for a GPU

41
00:02:34,356 --> 00:02:37,269
into something that is now a much better fit.

42
00:02:37,269 --> 00:02:40,843
So how do we parallelize a scan computation?
