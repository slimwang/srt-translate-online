1
00:00:00,070 --> 00:00:02,029
>> All right, Michael. What's the answer?

2
00:00:02,029 --> 00:00:04,725
>> All right. Well, let me start off with what I think the

3
00:00:04,725 --> 00:00:09,190
answer isn't. So, the last one, boosting tends to overfit, if boosting trains

4
00:00:09,190 --> 00:00:12,420
too long. You just told me a story about that not being true.

5
00:00:12,420 --> 00:00:14,000
So I'm going to eliminate that

6
00:00:14,000 --> 00:00:16,260
one from consideration. Boosting training too long.

7
00:00:16,260 --> 00:00:17,430
>> Oh, nice to know you were listening.

8
00:00:17,430 --> 00:00:19,670
>> [LAUGH] Boosting training too long, seems like

9
00:00:19,670 --> 00:00:21,810
not a good reason for it to overfit.

10
00:00:21,810 --> 00:00:22,430
>> You're correct.

11
00:00:22,430 --> 00:00:25,155
>> All right. Boosting tends to overfit if it's

12
00:00:25,155 --> 00:00:31,290
a nonlinear problem. So, that doesn't seem right. I

13
00:00:31,290 --> 00:00:33,250
mean I guess, no, this one just doesn't seem right

14
00:00:33,250 --> 00:00:34,860
at all. Like I don't see why, why the problem

15
00:00:34,860 --> 00:00:37,180
being linear or nonlinear, has anything to do with overfitting.,

16
00:00:37,180 --> 00:00:37,770
>> Okay.

17
00:00:37,770 --> 00:00:41,260
>> A whole lot of data is the opposite of what tends

18
00:00:41,260 --> 00:00:44,160
to cause overfitting. If there's lots of data then you'd think that it

19
00:00:44,160 --> 00:00:47,030
would actually do a pretty reasonable job of, you know, there's a

20
00:00:47,030 --> 00:00:50,120
lot to fit. There's a lot going on there. It's unlikely to overfit.

21
00:00:50,120 --> 00:00:50,410
>> Right, and

22
00:00:50,410 --> 00:00:54,840
in fact if a whole lot of data included all of the data, and you actually

23
00:00:54,840 --> 00:00:56,700
could get zero training error over it, then

24
00:00:56,700 --> 00:01:00,820
you know you have zero training zero generalization error.

25
00:01:00,820 --> 00:01:02,770
>> because it'll work on the testing data as well, because it's in there.

26
00:01:02,770 --> 00:01:03,282
>> Right.

27
00:01:03,282 --> 00:01:08,080
>> All right. Weak learner uses artificial neural network with

28
00:01:08,080 --> 00:01:10,520
many layers and nodes. So I'm guessing that you wanted

29
00:01:10,520 --> 00:01:13,170
me to think about that being something that, on its

30
00:01:13,170 --> 00:01:15,570
own, is prone to overfitting, because it's got a lot

31
00:01:15,570 --> 00:01:16,320
of parameters.

32
00:01:16,320 --> 00:01:17,010
>> Sure.

33
00:01:17,010 --> 00:01:19,584
>> So, if, and now we're doing boosting

34
00:01:19,584 --> 00:01:21,970
over that. So we fit a neural net, and

35
00:01:21,970 --> 00:01:24,140
then we fit another neural net, and we

36
00:01:24,140 --> 00:01:26,280
fit another neural net. And we're combining all the

37
00:01:26,280 --> 00:01:29,770
outputs together in the correct, weighted way. It's

38
00:01:29,770 --> 00:01:32,310
not obvious to me that that should be a

39
00:01:32,310 --> 00:01:36,170
good thing to do. I'm not sure it would overfit, but it seem like it sure could.

40
00:01:36,170 --> 00:01:38,770
>> OK, so you're, you're, so for now let's

41
00:01:38,770 --> 00:01:40,600
put a little question mark to it. You think that

42
00:01:40,600 --> 00:01:42,170
might be the right answer, but you want to think about it some more?

43
00:01:42,170 --> 00:01:43,440
>> Yeah let me, let me look at the first

44
00:01:43,440 --> 00:01:48,400
one. Weak learner chooses the weakest output. Well, I mean

45
00:01:48,400 --> 00:01:50,820
boosting is supposed to work as long as we have

46
00:01:50,820 --> 00:01:52,780
a weak learner. . And it doesn't matter if it

47
00:01:52,780 --> 00:01:55,630
chooses the weakest or the strongest. All that matters is

48
00:01:55,630 --> 00:01:58,130
it does significantly better than a half. So, like I

49
00:01:58,130 --> 00:01:59,850
feel like the only one, the only one of these

50
00:01:59,850 --> 00:02:02,800
choices that is likely to be true is the second one.

51
00:02:02,800 --> 00:02:05,770
>> And that is, in fact, correct. So let me give you an example

52
00:02:05,770 --> 00:02:07,590
of when that would be correct. So let's imagine

53
00:02:07,590 --> 00:02:09,940
I have a big powerful neural network that could represent

54
00:02:09,940 --> 00:02:12,730
any arbitrary functions. Okay, got lots of layers and lots

55
00:02:12,730 --> 00:02:16,440
of nodes. So, boosting calls it, and it perfectly fits

56
00:02:16,440 --> 00:02:19,720
the training data, but of course overfits. So then it

57
00:02:19,720 --> 00:02:23,750
returns, and it's got no error, which means all of

58
00:02:23,750 --> 00:02:27,760
the examples will have equal weight. And when you go

59
00:02:27,760 --> 00:02:31,290
through the loop again, you will just call the same

60
00:02:31,290 --> 00:02:32,850
learner, which will use the same neural

61
00:02:32,850 --> 00:02:35,030
network, and will return the same neural network.

62
00:02:35,030 --> 00:02:38,080
So every time you call the learner, you'll get zero training error, but you will

63
00:02:38,080 --> 00:02:42,230
just get the same neural network over and over and over again. And a weighted

64
00:02:42,230 --> 00:02:45,090
sum of the same function is just that

65
00:02:45,090 --> 00:02:47,700
function. So if it overfit, boosting will overfit.

66
00:02:47,700 --> 00:02:50,790
>> Interesting. And not only will it overfit, but it'll

67
00:02:50,790 --> 00:02:53,510
just, it'll be stuck in a horrible loop of error.

68
00:02:53,510 --> 00:02:56,390
>> Right. So that's why this

69
00:02:56,390 --> 00:02:58,700
is the sort of situation where you can imagine

70
00:02:58,700 --> 00:03:00,935
boosting a lower fit. If the underlying learners all

71
00:03:00,935 --> 00:03:02,955
overfit and you can never get them to stop

72
00:03:02,955 --> 00:03:04,990
overfitting, then there's really not much you can do.

73
00:03:04,990 --> 00:03:05,650
>> Interesting.

74
00:03:05,650 --> 00:03:08,028
>> Now, I do want to have a little semantic argument

75
00:03:08,028 --> 00:03:12,370
with you for a moment, Michael. You used the word strongest at

76
00:03:12,370 --> 00:03:15,450
some point, when you were talking about using the weakest output. And

77
00:03:15,450 --> 00:03:17,950
I just want to point out that, that doesn't really mean anything.

78
00:03:17,950 --> 00:03:19,980
>> What do you mean, it doesn't mean anything?

79
00:03:19,980 --> 00:03:21,570
>> Well, so what's a strong, what would you call

80
00:03:21,570 --> 00:03:22,510
a strong learner?

81
00:03:22,510 --> 00:03:24,830
>> One that is far away from it. If a

82
00:03:24,830 --> 00:03:26,900
weak learner just has to do a little bit better

83
00:03:26,900 --> 00:03:28,810
than a half, it seems like a strong learner would

84
00:03:28,810 --> 00:03:31,780
be something that would be very close to being accurate.

85
00:03:31,780 --> 00:03:33,900
>> Right. Of course, on the other hand, if

86
00:03:33,900 --> 00:03:36,289
by that definition all strong learners are also weak learners.

87
00:03:36,289 --> 00:03:36,774
>> Sure.

88
00:03:36,774 --> 00:03:39,230
>> Because anything that does better than a half is still doing better

89
00:03:39,230 --> 00:03:41,420
than a half, which is all it requires to be a weak learner.

90
00:03:41,420 --> 00:03:43,290
>> Yeah, but that's kind of true of people

91
00:03:43,290 --> 00:03:46,880
too. Like a strong person is also a weak person.

92
00:03:46,880 --> 00:03:47,610
>> No.

93
00:03:47,610 --> 00:03:49,030
>> Well it depends how you define it. So,

94
00:03:49,030 --> 00:03:51,060
if you say a weak person is someone who can

95
00:03:51,060 --> 00:03:54,660
at least lift their own arms, then strong people are

96
00:03:54,660 --> 00:03:56,810
also weak people in that they can lift their arms.

97
00:03:56,810 --> 00:03:58,750
>> Yes if you define it that way and if I define

98
00:03:58,750 --> 00:04:00,880
blue to be purple, then I can say blue is purple. But that's

99
00:04:00,880 --> 00:04:04,570
not how people define weak people. They define weak people, by saying they

100
00:04:04,570 --> 00:04:08,000
can't lift more than, not that they can lift at least as much.

101
00:04:08,000 --> 00:04:11,890
>> I see. So it's this piece of terminology that boosting uses that is in

102
00:04:11,890 --> 00:04:13,034
error, not me.

103
00:04:13,034 --> 00:04:16,300
>> That's one interpretation. It's not the one that I would use, but

104
00:04:16,300 --> 00:04:20,022
it's one interpretation. When you say something like a strong learner, I mean,

105
00:04:20,022 --> 00:04:22,466
it makes sense to use that kind of term, and sort of throw

106
00:04:22,466 --> 00:04:25,586
it around, and say, well, by a strong learner I mean someone who's,

107
00:04:25,586 --> 00:04:28,290
or a learner that's going to overfit, or is going to always do

108
00:04:28,290 --> 00:04:31,580
really well on the training data. But in kind of a technical definition

109
00:04:31,580 --> 00:04:34,531
it's very difficult to sort of pin down. So don't get too caught

110
00:04:34,531 --> 00:04:37,090
up what a strong learner means if you want to write a proof.

111
00:04:37,090 --> 00:04:37,890
Seems fair?

112
00:04:37,890 --> 00:04:42,570
>> Good point yeah, also, also that this whole notion that strong

113
00:04:42,570 --> 00:04:45,720
is sometimes defined as not weak. And it is not the case that

114
00:04:45,720 --> 00:04:47,970
if you have something that's not a weak learner that it's, then

115
00:04:47,970 --> 00:04:50,880
it's a strong learner. In fact, it's no lear, no learner at all.

116
00:04:50,880 --> 00:04:53,840
>> Exactly. So, a weak learner's just defined in a way that

117
00:04:53,840 --> 00:04:57,280
basically says, it gives me at least some information. Good. Let me

118
00:04:57,280 --> 00:04:59,040
just throw one more thing in here and then we can stop

119
00:04:59,040 --> 00:05:02,260
talking about this. There's another, a couple of other cases where boosting

120
00:05:02,260 --> 00:05:04,470
tends to overfit. The one that matters the most, or

121
00:05:04,470 --> 00:05:07,920
comes up the most, is in the case of pink noise.

122
00:05:07,920 --> 00:05:10,890
>> Did you say, peak noise?

123
00:05:10,890 --> 00:05:14,450
>> I said, pink noise. I even wrote it in red, which

124
00:05:14,450 --> 00:05:17,330
looks like pink. It's a strong pink as opposed to a weak pink.

125
00:05:17,330 --> 00:05:18,921
>> [LAUGH]

126
00:05:18,921 --> 00:05:20,910
>> I'm sorry. There's no way for that to be obvious from what we've

127
00:05:20,910 --> 00:05:22,500
talked about, but as a practical matter,

128
00:05:22,500 --> 00:05:25,710
pink noise tends to, cause boosting overfit.

129
00:05:25,710 --> 00:05:27,290
>> Okay, but this is not a term I'm familiar with

130
00:05:27,290 --> 00:05:31,830
unless you're critiquing the musical stylings of a particular performer.

131
00:05:31,830 --> 00:05:35,880
>> [LAUGH] No. Although I did recently see, see them in concert. But

132
00:05:35,880 --> 00:05:39,449
that's a whole other conversation. Okay, so pink noise just means uniform noise.

133
00:05:40,870 --> 00:05:42,510
>> I thought white noise was uniform noise.

134
00:05:42,510 --> 00:05:49,600
>> No, white noise is Gaussian noise. Okay, so pink noise is uniform

135
00:05:49,600 --> 00:05:52,410
noise and white noise is Gaussian noise. This is why, Michael, by the way,

136
00:05:52,410 --> 00:05:57,580
if you ever try to set up a studio or a cool stereo system in your house, you

137
00:05:57,580 --> 00:05:59,700
want a pink noise generator. So that it covers

138
00:05:59,700 --> 00:06:02,780
all the frequencies equally, not just the white noise. generated.

139
00:06:02,780 --> 00:06:03,520
>> Hm.

140
00:06:03,520 --> 00:06:08,400
>> But boosting tends to overfit in those sorts of circumstances. And you

141
00:06:08,400 --> 00:06:10,220
can read more about it in the notes if you want to. But

142
00:06:10,220 --> 00:06:13,180
the one that I want I really want people to get is, that

143
00:06:13,180 --> 00:06:17,590
if you have an underlying weak learner that overfits, then it is difficult

144
00:06:17,590 --> 00:06:20,490
for boosting to overcome that. Because fundamentally you've already done all of

145
00:06:20,490 --> 00:06:23,060
your overfitting and it's, there's really not much for those things to do.

146
00:06:23,060 --> 00:06:24,420
>> Okay. Got it?

147
00:06:24,420 --> 00:06:24,830
>> Got it.

148
00:06:24,830 --> 00:06:27,940
>> Excellent. It all ties back into margins, and it's all one

149
00:06:27,940 --> 00:06:30,828
big story, which I think is the lesson of all of machine learning.
