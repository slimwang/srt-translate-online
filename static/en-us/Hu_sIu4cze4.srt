1
00:00:00,690 --> 00:00:03,800
Recall that, if we have a random variable x, and

2
00:00:03,800 --> 00:00:07,320
we want to understand its relationship with another random variable, v.

3
00:00:07,320 --> 00:00:11,520
The way we do that is we examined the expected loss and

4
00:00:11,520 --> 00:00:13,930
entropy due to that variable.

5
00:00:13,930 --> 00:00:17,310
Specifically, to understand the relationship between x and

6
00:00:17,310 --> 00:00:22,520
y, we examine the expected loss and entropy due to y.

7
00:00:22,520 --> 00:00:25,840
Similarly, for z, we do exactly the same thing.

8
00:00:25,840 --> 00:00:29,680
We examine the expected loss and entropy due to z.

9
00:00:29,680 --> 00:00:32,600
After we know these two quantities the feature that we

10
00:00:32,600 --> 00:00:37,070
want is the one that results in the biggest drop from h of x.

11
00:00:37,070 --> 00:00:40,950
So as you can see in order to do this we'll have to calculate three quantities.

12
00:00:42,050 --> 00:00:47,740
h of x, the conditional entropy, the entropy of x.

13
00:00:47,740 --> 00:00:52,820
The conditional entropy of x given y and the conditional entropy of x given z.

14
00:00:53,980 --> 00:00:56,160
I've written them here, over off to the side, so

15
00:00:56,160 --> 00:00:59,690
that we can keep track of them as we do our calculations.

16
00:00:59,690 --> 00:01:01,750
Let's go ahead and find them.

17
00:01:01,750 --> 00:01:03,500
So h of x is just this expression

18
00:01:04,500 --> 00:01:10,160
minus one times the probability of x times log of probability of x.

19
00:01:10,160 --> 00:01:14,700
Minus the probability of x2 times the log of probability of x2.

20
00:01:15,820 --> 00:01:19,720
Now, remember the base of the logarithm doesn't really matter.

21
00:01:19,720 --> 00:01:22,670
Traditionally, either base two or base e is used.

22
00:01:22,670 --> 00:01:27,770
Let's assume for these calculations we're using base e.

23
00:01:27,770 --> 00:01:29,310
So, we can just pull the values for

24
00:01:29,310 --> 00:01:33,240
the probability of x1 and x2 straight from the joint probability table.

25
00:01:35,020 --> 00:01:40,802
And if we do that, we find that we get minus one times 0.472

26
00:01:40,802 --> 00:01:48,914
times log of 0.472 minus 0.528 times log of 0.528 which is approximately 0.691.

27
00:01:48,914 --> 00:01:52,460
So let's just keep track of that by writing it up here.

28
00:01:52,460 --> 00:01:54,210
So one of three down.

29
00:01:54,210 --> 00:01:57,830
Now we just have to do h of x given y and h of x given z.

30
00:01:59,320 --> 00:02:02,400
Recall that the conditional entropy of h of

31
00:02:02,400 --> 00:02:05,290
x given y is just a conditional expectation.

32
00:02:05,290 --> 00:02:11,520
It is the average of how much we expect h of x to change due to y1 and

33
00:02:11,520 --> 00:02:12,130
due to y2.

34
00:02:12,130 --> 00:02:16,900
So, the first term is just this expression below.

35
00:02:16,900 --> 00:02:20,380
So first, so first, let's tackle h of x given y1.

36
00:02:20,380 --> 00:02:25,790
H of x given y1 is just minus 1 times the probability of x1 given y1,

37
00:02:25,790 --> 00:02:30,060
times the log of probability x1 given y1,

38
00:02:30,060 --> 00:02:36,550
minus probability of x2 given y one times the log of probability of x2 given y1.

39
00:02:36,550 --> 00:02:39,860
And again we can just pull these numbers straight out of the joint probability

40
00:02:39,860 --> 00:02:41,360
table above and

41
00:02:41,360 --> 00:02:47,505
if we do that we find we get minus one times .0318 times log .0318.

42
00:02:47,505 --> 00:02:54,322
Minus 0.779 times log 0.779 and that's approximately 0.3043.

43
00:02:54,322 --> 00:02:55,927
Let's just make a note of that here.

44
00:02:55,927 --> 00:03:02,230
The calculation for h of x given y2 proceeds in exactly the same way.

45
00:03:02,230 --> 00:03:05,754
And if you evaluate that, you get approximately 0.6888.

46
00:03:06,950 --> 00:03:08,640
And let's just make a note of that here.

47
00:03:09,840 --> 00:03:16,404
So if we put it all together, h of x given y is .32075

48
00:03:16,404 --> 00:03:24,934
times .3043 plus .67925 times .6888, which is approximately .566.

49
00:03:24,934 --> 00:03:27,550
Let's make a note of our second term above.

50
00:03:28,690 --> 00:03:29,990
Now the calculation for

51
00:03:29,990 --> 00:03:35,390
the conditional entropy of x given z proceeds in exactly the same way.

52
00:03:35,390 --> 00:03:37,210
I'm not going to step through the calculation for

53
00:03:37,210 --> 00:03:40,580
this one as well, I'll leave that to you.

54
00:03:40,580 --> 00:03:44,400
But you'll find that if you do, you'll get approximately 0.5787.

55
00:03:44,400 --> 00:03:48,370
So now we have all three of our terms.

56
00:03:48,370 --> 00:03:53,639
Before knowing anything about y or z, h of x is .691.

57
00:03:53,639 --> 00:03:57,970
It drops to .566 when we condition on y and

58
00:03:57,970 --> 00:04:01,490
it drops to .5787 when we condition on z.

59
00:04:02,620 --> 00:04:06,500
So recall that the feature that we want is the one that results in

60
00:04:06,500 --> 00:04:09,040
the biggest change in entropy.

61
00:04:09,040 --> 00:04:13,890
Which is the arg max of h of x minus h of x given a variable.

62
00:04:13,890 --> 00:04:16,519
Where in this case the variables range over y and z.

63
00:04:17,890 --> 00:04:21,500
So you can see that in this case the answer is y.

64
00:04:21,500 --> 00:04:23,890
Although not by that much.

65
00:04:23,890 --> 00:04:26,890
This difference for y is 0.125.

66
00:04:26,890 --> 00:04:32,680
Whereas for z it's just slightly less at 0.1123.

67
00:04:32,680 --> 00:04:37,060
So although y is the winner here, z doesn't do that much worse.

68
00:04:37,060 --> 00:04:42,770
And so in practice, if I were going to build a model in order to understand x,

69
00:04:42,770 --> 00:04:46,030
I might consider including both y and z.

70
00:04:46,030 --> 00:04:48,220
But if I had to choose one, y is the winner.
