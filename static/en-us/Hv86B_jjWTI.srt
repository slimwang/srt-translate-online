1
00:00:00,240 --> 00:00:02,960
All right, so in this section,
we've made our network more efficient.

2
00:00:02,960 --> 00:00:06,529
We've done this by getting rid of
the multiplication by 1 because we don't

3
00:00:06,530 --> 00:00:07,950
need to,
because it doesn't change anything.

4
00:00:07,950 --> 00:00:12,132
And we've gotten rid of the processing
of any of the words that have 0 in

5
00:00:12,132 --> 00:00:13,779
them altogether.

6
00:00:13,779 --> 00:00:16,750
And this really should increase
the speed of both the training and

7
00:00:16,750 --> 00:00:20,039
the testing of our network, and
the running of our network,

8
00:00:20,039 --> 00:00:22,549
by a significant amount,
which we'll find out in a second.

9
00:00:22,550 --> 00:00:24,429
Now, in order to make this change,

10
00:00:24,429 --> 00:00:26,530
notice this top part of this class
didn't really change, right?

11
00:00:26,530 --> 00:00:29,130
So we still initialize the network,
we still create the wave matrix,

12
00:00:29,129 --> 00:00:31,679
still create the same word index lookup.

13
00:00:32,859 --> 00:00:36,000
I left these methods around but
we're not going to use them.

14
00:00:36,000 --> 00:00:39,000
And then we get to these train methods
and things really start to change.

15
00:00:39,000 --> 00:00:42,975
So I added kind of a pre processing
step to the train method,

16
00:00:42,975 --> 00:00:44,734
which was the following.

17
00:00:44,734 --> 00:00:49,115
It takes each of the kind
of training_reviews_raw, so

18
00:00:49,115 --> 00:00:53,150
that the raw text, and
it creates a set of indices.

19
00:00:53,149 --> 00:00:55,289
So it converts all the words to numbers,
right?

20
00:00:55,289 --> 00:00:57,320
But to the rows that
they're corresponding to.

21
00:00:57,320 --> 00:01:00,893
So in this case it's like what we did
up here with this 4 and 9, right?

22
00:01:00,893 --> 00:01:04,859
So then we had this list of indices and
we can just sum them into this vector.

23
00:01:04,859 --> 00:01:07,679
And this ends up being really,
really fast, which is great.

24
00:01:08,739 --> 00:01:12,409
So what we did here is just a kind
of a bigger version of that,

25
00:01:12,409 --> 00:01:17,759
where for each review we
convert the words to the row

26
00:01:17,760 --> 00:01:20,150
of our weight matrix that
it corresponds to, right?

27
00:01:20,150 --> 00:01:23,440
So our inputs correspond
to rows in the matrix and

28
00:01:23,439 --> 00:01:25,689
our outputs correspond to columns,
right?

29
00:01:25,689 --> 00:01:29,539
So in major matrix multiplication
is kind of how it works out.

30
00:01:29,540 --> 00:01:34,511
And that allows us to make it down here
to completely skip generating our input

31
00:01:34,510 --> 00:01:35,661
vector, right?

32
00:01:35,661 --> 00:01:38,609
So we don't have to do that in
forward propagation at all.

33
00:01:38,609 --> 00:01:41,310
We jump straight to
generating our hidden layer.

34
00:01:41,310 --> 00:01:46,189
All we do is we iterate through each
index in our review, each index,

35
00:01:46,188 --> 00:01:50,746
and we sum that row of
weights_0_1 into layer_1, right?

36
00:01:50,746 --> 00:01:53,219
So we take their 1,
which has been emptied out right here,

37
00:01:53,219 --> 00:01:56,546
and we say all right, for each row
in weights_0_1, add it into layer_1.

38
00:01:56,546 --> 00:02:02,189
What we're doing is we're saying for
each row, add it into thin layer.

39
00:02:02,189 --> 00:02:05,754
So add the row for horrible,
it's this one, add the row for terrible,

40
00:02:05,754 --> 00:02:07,789
it's this one, add it in here, right?

41
00:02:07,790 --> 00:02:09,631
We're skipping this entirely and

42
00:02:09,631 --> 00:02:12,879
the 70,000 other words
that were mentioned here.

43
00:02:12,879 --> 00:02:14,867
That is a huge, huge time saving and

44
00:02:14,867 --> 00:02:17,068
I think I'm really excited to see
how much speed that gives it.

45
00:02:18,229 --> 00:02:22,491
Now this part's exactly the same,
Layer_2 error is exactly the same,

46
00:02:22,491 --> 00:02:24,409
Layer_2_delta is exactly the same.

47
00:02:24,409 --> 00:02:28,668
Layer_1 error and 1_delta are exactly
the same, because nothing's changed.

48
00:02:28,668 --> 00:02:33,206
The only thing that's different
now is the way that we update

49
00:02:33,206 --> 00:02:38,201
the weights is basically just
the inverse of when we populated it.

50
00:02:38,201 --> 00:02:40,788
So we iterate through the same
indices and we say all right,

51
00:02:40,788 --> 00:02:43,635
we're just going to update the ones
that we forward propagated,

52
00:02:43,635 --> 00:02:47,269
the ones that we used and we're going to
leave all the rest of them alone.

53
00:02:47,270 --> 00:02:50,016
This actually makes even more
sense when you look at how

54
00:02:50,015 --> 00:02:53,304
back propagation happened or
how this happens, right?

55
00:02:53,305 --> 00:02:58,302
So when you're computing this final,
that's not a weight delta, your weight

56
00:02:58,301 --> 00:03:03,239
adjustment, you're multiplying the input
value by the delta that's here.

57
00:03:03,240 --> 00:03:05,870
So we have back propagate delta,
you have delta on hidden layer.

58
00:03:05,870 --> 00:03:10,566
So the way to update these four indices,
you say 1 times the delta that's

59
00:03:10,566 --> 00:03:14,140
right here, and
you subtract that from these weights.

60
00:03:14,140 --> 00:03:19,060
Now the thing is if it's a 0,
that subtraction's always going to be 0.

61
00:03:19,061 --> 00:03:23,099
So we can skip that on the way
back as well, which is great.

62
00:03:23,099 --> 00:03:25,759
So that's how we do our weight update.

63
00:03:25,759 --> 00:03:28,745
All the evaluation logic is the same,
the test logic is the same,

64
00:03:28,746 --> 00:03:29,951
the run logic is the same.

65
00:03:29,950 --> 00:03:30,650
And now we want to try it out.

66
00:03:30,650 --> 00:03:36,093
So we're going to go ahead and grab our
in the network, put it right there.

67
00:03:36,093 --> 00:03:40,689
Then we're going to grab the actual
training, so just a reference, right?

68
00:03:40,689 --> 00:03:46,551
This train did around
a 100 reviews per second.

69
00:03:46,551 --> 00:03:51,818
Go ahead and run that,
put that right there, grab the test.

70
00:03:51,818 --> 00:03:56,235
I guess they're going to train,
grab the test,

71
00:03:56,235 --> 00:04:00,975
I guess we kind of want to see that,
so I'll see if I can.

72
00:04:00,975 --> 00:04:05,974
A 1000 reviews a second,
it's ten times faster, over ten times,

73
00:04:05,974 --> 00:04:10,987
look at that 1300 and still training
at the same conversion speed.

74
00:04:10,987 --> 00:04:15,683
Look at that, 1300 reviews per second,
that is over and order of magnitude

75
00:04:15,683 --> 00:04:20,555
increase in speed, because we got rid of
all these problems, like COC testing.

76
00:04:20,555 --> 00:04:24,810
Okay testing, you almost didn't
even noticed it happened.

77
00:04:24,810 --> 00:04:25,689
Cool, awesome.

78
00:04:25,689 --> 00:04:28,812
And we're getting a great score,
actually, yeah, yeah.

79
00:04:28,812 --> 00:04:30,144
I mean it should be exactly identical,
right?

80
00:04:30,144 --> 00:04:33,800
because nothing has changed.

81
00:04:33,800 --> 00:04:35,310
Man, this is great, that's so
much faster, awesome.

82
00:04:37,129 --> 00:04:38,165
So this is great.

83
00:04:38,165 --> 00:04:40,231
If we wanted to, we could train
multiple iterations, right?

84
00:04:40,232 --> 00:04:42,181
So if I said times 2.

85
00:04:42,180 --> 00:04:46,353
[BLANK_AUDIO]

86
00:04:46,353 --> 00:04:48,492
So we'll hit train and
the second it starts, so

87
00:04:48,492 --> 00:04:50,964
it's converting everything
into indices right now.

88
00:04:50,964 --> 00:04:52,943
And then it's going to start training,
so

89
00:04:52,944 --> 00:04:54,816
I guess maybe I lost a little bit here.

90
00:04:54,815 --> 00:04:56,873
But if you're going to keep
doing more iterations,

91
00:04:56,874 --> 00:04:58,300
it's going to use the same indices.

92
00:04:58,300 --> 00:04:59,460
So it converts the indices once,
keeps going.

93
00:04:59,459 --> 00:05:02,810
Wow, reviews per second it's getting
up to 1500, it's crazy fast.

94
00:05:02,810 --> 00:05:06,910
So we can keep training,
training accuracy keeps going up.

95
00:05:06,910 --> 00:05:09,310
It's just the faster your network goes,

96
00:05:09,310 --> 00:05:12,490
the more iterations you can do in
a given period of time, right?

97
00:05:12,490 --> 00:05:16,394
And the way that we're able to
accomplish this is by just stripping out

98
00:05:16,394 --> 00:05:20,370
the stuff that the neural net is doing
that actually isn't helping us predict

99
00:05:20,370 --> 00:05:22,319
or really even to learn.

100
00:05:22,319 --> 00:05:26,189
That's really what the strategy
is all about, so yeah.

101
00:05:26,189 --> 00:05:29,920
Now, in the next section we're going
to kind of go back to our data and

102
00:05:29,920 --> 00:05:33,300
say okay,
can we kind of improve the modeling?

103
00:05:33,300 --> 00:05:36,600
And so we improved the way
that we frame the problem.

104
00:05:36,600 --> 00:05:40,879
Do we improve the way that
the neural net is for

105
00:05:40,879 --> 00:05:43,269
back propagation because
of how we train a problem?

106
00:05:43,269 --> 00:05:47,719
So because we set up just pushing
forward the vocabulary, this allowed us

107
00:05:47,720 --> 00:05:52,190
to trim out a lot of inefficient waste
that the neural net was computing.

108
00:05:52,189 --> 00:05:52,889
And now we're going to go back.

109
00:05:52,889 --> 00:05:57,889
Okay, can we reframe the problem
again to reduce even more noise and

110
00:05:57,889 --> 00:06:00,469
potentially even reduce more
weight propagations we have to do,

111
00:06:00,470 --> 00:06:04,160
to speed how fast it learns and
actually how fast it computes?

112
00:06:04,160 --> 00:06:07,250
So we're going to do one more
iteration of this in project six.

113
00:06:07,250 --> 00:06:07,709
I'll see you there.

