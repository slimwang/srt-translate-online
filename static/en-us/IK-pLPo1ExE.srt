1
00:00:00,260 --> 00:00:03,320
The event-driven model doesn't
come without any challenges.

2
00:00:03,320 --> 00:00:06,610
Recall that when we talked about
the many to one multithreading model,

3
00:00:06,610 --> 00:00:09,740
we said that a single blocking
I/O call that's coming from

4
00:00:09,740 --> 00:00:14,310
one of the user level threads
can block the entire process,

5
00:00:14,310 --> 00:00:18,230
although there may be other user level
threads that are ready to execute.

6
00:00:18,230 --> 00:00:20,682
A similar problem can
occur here as well.

7
00:00:20,682 --> 00:00:24,752
If one of the handlers issues
a blocking I/O call to read data from

8
00:00:24,752 --> 00:00:29,575
the network or from disk, the entire
event-driven process can be blocked.

9
00:00:29,575 --> 00:00:35,240
One way to circumvent this problem,
is to use asynchronous I/O operations.

10
00:00:35,240 --> 00:00:39,830
Asynchronous calls have the property
that when the system call is made,

11
00:00:39,830 --> 00:00:44,240
the kernel captures enough information
about the caller and where and

12
00:00:44,240 --> 00:00:48,070
how the data should be returned
once it becomes available.

13
00:00:48,070 --> 00:00:52,250
Async calls also provide the caller
with an opportunity to precede

14
00:00:52,250 --> 00:00:56,700
executing something, and then come back
at a later time to check if the results

15
00:00:56,700 --> 00:00:59,440
of the asynchronous operation
are already available.

16
00:00:59,440 --> 00:01:03,554
For instance, the process or the thread
can come back later to check if a file

17
00:01:03,554 --> 00:01:07,429
has already been read and the data is
available in the buffer in memory.

18
00:01:07,429 --> 00:01:11,546
One thing that makes asynchronous
calls possible is that the OS kernel

19
00:01:11,546 --> 00:01:12,740
is multithreaded.

20
00:01:12,740 --> 00:01:17,488
So while the caller thread continues
execution, another kernel thread does

21
00:01:17,488 --> 00:01:22,164
all the necessary work and all the
waiting that's needed to perform the I/O

22
00:01:22,164 --> 00:01:25,961
operation, to get the I/O data,
and then, to also make sure

23
00:01:25,961 --> 00:01:30,589
that the results become available to
the appropriate user level context.

24
00:01:30,589 --> 00:01:35,523
Also, asynchronous operations can
benefit by the actual I/O devices.

25
00:01:35,523 --> 00:01:39,963
For instance, the caller thread can
simply pass some request data structure

26
00:01:39,963 --> 00:01:43,856
to the device itself, and
then the device performs the operation,

27
00:01:43,856 --> 00:01:46,178
and the thread at a later
time can come and

28
00:01:46,178 --> 00:01:49,547
check to see whether device
has completed the operation.

29
00:01:49,547 --> 00:01:53,115
We will return to a synchronous
I/O operations in a later lecture.

30
00:01:53,115 --> 00:01:54,864
What you need to know for

31
00:01:54,864 --> 00:01:59,416
now is that when we're using
asynchronous I/O operations,

32
00:01:59,416 --> 00:02:04,423
our process will not be blocked in
the kernel when performing I/O.

33
00:02:04,423 --> 00:02:06,278
In the event-driven model,

34
00:02:06,278 --> 00:02:10,952
if the handler initiates an asynchronous
I/O operation for network or for

35
00:02:10,952 --> 00:02:15,846
disk, the operating system can simply
use the mechanism like select or poll or

36
00:02:15,846 --> 00:02:20,300
epoll like we've mentioned
before to catch such events.

37
00:02:20,300 --> 00:02:23,450
Since summary asynchronous
I/O operations fit

38
00:02:23,450 --> 00:02:26,030
very nicely with the event-driven model.

39
00:02:26,030 --> 00:02:29,503
The problem with asynchronous
I/O calls is that they weren't

40
00:02:29,503 --> 00:02:31,608
ubiquitously available in the past.

41
00:02:31,608 --> 00:02:36,020
And even today, they may not be
available for all types of devices.

42
00:02:36,020 --> 00:02:40,749
In a general case, maybe the processing
that needs to be performed by our server

43
00:02:40,749 --> 00:02:45,350
isn't to read data from a file, where
there are asynchronous system calls.

44
00:02:45,350 --> 00:02:49,445
But instead maybe to call
processing some accelerator,

45
00:02:49,445 --> 00:02:52,430
some device that only
the server has access to.

46
00:02:52,430 --> 00:02:57,410
To deal with this problem,
paper proposed the use of helpers.

47
00:02:57,410 --> 00:03:01,960
But a handler needs to issue
an I/O operation that can block,

48
00:03:01,960 --> 00:03:07,600
it passes it to the helper, and
returns to the event dispatcher.

49
00:03:07,600 --> 00:03:11,710
The helper will be the one that will
handle the blocking I/O operation, and

50
00:03:11,710 --> 00:03:14,220
interact with the dispatcher
as necessary.

51
00:03:14,220 --> 00:03:18,171
The communication with the helper can
be via socket based interface, or

52
00:03:18,171 --> 00:03:22,580
via another type of messaging interface
that's available in operating systems

53
00:03:22,580 --> 00:03:23,438
called pipes.

54
00:03:23,438 --> 00:03:28,150
And both of these present a file
descriptor-like interface.

55
00:03:28,150 --> 00:03:29,730
So the same kind of select or

56
00:03:29,730 --> 00:03:33,720
poll mechanism that we mentioned can
be used for the event dispatcher

57
00:03:33,720 --> 00:03:37,529
to keep track of various events
that are occurring in the system.

58
00:03:37,529 --> 00:03:41,459
This interface can be used to track
whether the helpers are providing any

59
00:03:41,459 --> 00:03:44,120
kind of events to the event dispatcher.

60
00:03:44,120 --> 00:03:48,280
In doing this, the synchronous I/O
call is handled by the helper.

61
00:03:48,280 --> 00:03:51,550
The helper will be the one
that will block, and

62
00:03:51,550 --> 00:03:57,330
the main event dispatcher in the main
process will continue uninterrupted.

63
00:03:57,330 --> 00:04:01,260
So this way although we don't
have asynchronous I/O calls,

64
00:04:01,260 --> 00:04:03,040
through the use of helpers,

65
00:04:03,040 --> 00:04:08,220
we achieve the same kind of behavior
as if we had asynchronous calls.

66
00:04:08,220 --> 00:04:10,330
At the time of the writing of the paper,

67
00:04:10,330 --> 00:04:14,230
another limitation was that not
all kernels were multi-threaded.

68
00:04:14,230 --> 00:04:14,830
So basically,

69
00:04:14,830 --> 00:04:19,040
not all kernels supported the one
to one model that we talked about.

70
00:04:19,040 --> 00:04:21,470
In order to deal with this limitation,

71
00:04:21,470 --> 00:04:26,630
the decision in the paper was to make
these helper entities processes.

72
00:04:26,630 --> 00:04:29,134
Therefore, they call this model AMPED,

73
00:04:29,134 --> 00:04:32,290
Asymmetric Multi-Process Event-Driven
model.

74
00:04:32,290 --> 00:04:34,050
It's an event-driven model.

75
00:04:34,050 --> 00:04:36,020
It has multiple processes.

76
00:04:36,020 --> 00:04:37,531
And these processes are asymmetric.

77
00:04:37,531 --> 00:04:41,050
The helper ones only deal
with blocking I/O operation.

78
00:04:41,050 --> 00:04:44,370
And then,
the main one performs everything else.

79
00:04:44,370 --> 00:04:48,924
In principle, the same kind of idea
could have applied to the multi-threaded

80
00:04:48,924 --> 00:04:52,305
scenario where the helpers are threads,
not processes,

81
00:04:52,305 --> 00:04:55,549
so asymmetric multi-threaded
event-driven model.

82
00:04:55,549 --> 00:05:00,195
And in fact, there is a follow-on on
the Flash work that actually does this

83
00:05:00,195 --> 00:05:02,200
exact thing, the AMTED model.

84
00:05:02,200 --> 00:05:06,730
The key benefits of the symmetric model
that we described is that it resolved

85
00:05:06,730 --> 00:05:10,197
some of the limitations of
the pure event-driven model in

86
00:05:10,197 --> 00:05:13,382
terms of what is required
from the operating system,

87
00:05:13,382 --> 00:05:17,450
the dependence on asynchronous
I/O calls and threading support.

88
00:05:17,450 --> 00:05:21,956
In addition, this motto lets us achieve
concurrency with a smaller memory

89
00:05:21,956 --> 00:05:26,544
footprint than either the multi-process
or the multi-threading model.

90
00:05:26,544 --> 00:05:30,158
In the multi-process or
multi-threading model,

91
00:05:30,158 --> 00:05:33,943
a worker has to perform everything for
a full request.

92
00:05:33,943 --> 00:05:38,547
So its memory requirements will be
much more significant than the memory

93
00:05:38,547 --> 00:05:41,430
requirements of a helper entity.

94
00:05:41,430 --> 00:05:46,830
In addition, with the AMPED model,
we will have a helper entity only for

95
00:05:46,830 --> 00:05:50,510
the number of concurrent
blocking I/O operations.

96
00:05:50,510 --> 00:05:53,280
Whereas, in the multi-threaded or
multi-process models,

97
00:05:53,280 --> 00:05:58,690
we will have as many current entities,
as many processes, or as many threads

98
00:05:58,690 --> 00:06:03,530
as there are concurrent requests
regardless of whether they block or not.

99
00:06:03,530 --> 00:06:08,181
The downside is that audit works well
with the server pipe applications.

100
00:06:08,181 --> 00:06:12,539
It is not necessarily as generally
applicable to arbitrary applications.

101
00:06:12,539 --> 00:06:16,461
In addition, there are also some
complexities with the routing of events

102
00:06:16,461 --> 00:06:17,670
in multi CPU systems.
