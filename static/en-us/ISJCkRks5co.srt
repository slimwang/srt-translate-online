1
00:00:00,180 --> 00:00:04,240
But what about the reliability and
the MTTF of Raid 1?

2
00:00:04,240 --> 00:00:08,490
Same as before, we'll assume that
some F is the failure rate for

3
00:00:08,490 --> 00:00:12,450
a single disk in failures
per disk per second.

4
00:00:12,450 --> 00:00:16,416
And for a single disk remember that
the MTTF is simply one over F,

5
00:00:16,416 --> 00:00:19,125
where F is that failure rate.

6
00:00:19,125 --> 00:00:23,485
And then the mean time to data loss,
which is the MTTF,

7
00:00:23,485 --> 00:00:25,375
really, of the whole disk array,

8
00:00:25,375 --> 00:00:30,155
in this case just one disk, would be
simply the MTTF of that one disk.

9
00:00:30,155 --> 00:00:34,200
Well, let's say now that we
have two disks in RAID 1.

10
00:00:34,200 --> 00:00:37,990
And remember that the failure rate
when we have more than one disk

11
00:00:37,990 --> 00:00:43,590
is N times the failure rate of one disk
as long as all of the disks are working.

12
00:00:43,590 --> 00:00:47,860
So what we have really is that
both of our disks in raid one

13
00:00:47,860 --> 00:00:52,150
are okay until the time,
entity of one disk over two.

14
00:00:52,150 --> 00:00:55,978
This is the expected time when
one of the disks will fail.

15
00:00:55,978 --> 00:00:57,530
Now, unlike raid zero,

16
00:00:57,530 --> 00:01:02,280
where this is the point where we would
lose data, now we have mirroring.

17
00:01:02,280 --> 00:01:07,750
So both of the disks need to fail in
order for the whole thing to lose data.

18
00:01:07,750 --> 00:01:12,824
One failure is not data loss yet,
so we don't have the failure

19
00:01:12,824 --> 00:01:17,520
of the array yet, but this time,
we are down to one disk.

20
00:01:18,540 --> 00:01:21,810
That one disk is a working disk,
and has this failure rate.

21
00:01:22,920 --> 00:01:29,190
So, that one remaining disk will live
on for another MTTF of a single disk.

22
00:01:29,190 --> 00:01:31,580
This is very important to understand.

23
00:01:31,580 --> 00:01:35,910
If we have a working disk at
some point in time, the MTTF for

24
00:01:35,910 --> 00:01:40,490
that disc is going to be,
the overall MTTF for one disc.

25
00:01:40,490 --> 00:01:44,010
This MTTF is not affected
by the fact that this disk

26
00:01:44,010 --> 00:01:46,420
has already been working for
all this time.

27
00:01:46,420 --> 00:01:47,080
Why?

28
00:01:47,080 --> 00:01:51,170
Well because the failure rate is
constant, it's not affected by time.

29
00:01:51,170 --> 00:01:54,500
So if we have a working disk,
its failure rate is always the same.

30
00:01:55,500 --> 00:01:59,079
The only thing that changes
the failure rate is if the disk fails.

31
00:02:00,250 --> 00:02:04,540
So if we had two disks and both work,
this is the expected time of them

32
00:02:04,540 --> 00:02:10,180
surviving, and then if we're down to
one disk that works, at the time when

33
00:02:10,180 --> 00:02:15,780
we have a single working disk this
is the MTTF that we can expect.

34
00:02:15,780 --> 00:02:22,300
So overall, the mean time to data loss
for RAID1 with two disks is going to be

35
00:02:22,300 --> 00:02:28,805
the MTTF of a single disk over two,
plus the MTTF of a single disk.

36
00:02:28,805 --> 00:02:30,765
And now, we bought two disks, and

37
00:02:30,765 --> 00:02:34,385
used them in an array that has
the capacity of only a single disk.

38
00:02:34,385 --> 00:02:37,515
So we basically paid for the extra disk,
just to get reliability.

39
00:02:37,515 --> 00:02:41,462
And what we get is just one
half more of the MTTF then

40
00:02:41,462 --> 00:02:44,082
we would normally get
with only a single disk.

41
00:02:44,082 --> 00:02:45,442
So what's the point?

42
00:02:45,442 --> 00:02:48,382
The point is that this calculation

43
00:02:48,382 --> 00:02:54,116
assumes that no disk is replaced
at the time when it fails.

44
00:02:54,116 --> 00:02:56,386
So we had the failure and

45
00:02:56,386 --> 00:03:01,006
we just let that fail disk just be
there and we were down to one disk and

46
00:03:01,006 --> 00:03:05,898
didn't fix it and then we were just down
to the entity F of one disk for awhile.

47
00:03:05,898 --> 00:03:09,528
But remember that when we want
reliability we definitely will

48
00:03:09,528 --> 00:03:12,358
replace the failed disks.

49
00:03:12,358 --> 00:03:16,848
So when we have the first failure we
will as soon as possible replace that

50
00:03:16,848 --> 00:03:18,328
failed disk.

51
00:03:18,328 --> 00:03:20,968
Copy the data from
the working disk to it so

52
00:03:20,968 --> 00:03:24,448
now we're back to RAID1
working properly.

53
00:03:24,448 --> 00:03:27,618
So what's the MTTF for that?
