1
00:00:00,080 --> 00:00:02,130
So now, we have an intuition of best, and how we

2
00:00:02,130 --> 00:00:07,210
want to split. We've, we've looked over, Michael's proposed, the high-level

3
00:00:07,210 --> 00:00:09,550
algorithm for how we would build a decision tree. And I

4
00:00:09,550 --> 00:00:12,600
think we have enough information now that we can actually do,

5
00:00:12,600 --> 00:00:15,770
a real specific algorithm. So, let's write that down. And the

6
00:00:15,770 --> 00:00:19,530
particular algorithm that Michael proposed is a kind of generic version

7
00:00:19,530 --> 00:00:22,270
of something that's called ID3. So let me write down what

8
00:00:22,270 --> 00:00:25,110
that algorithm is, and we can talk about it. Okay, so here's

9
00:00:25,110 --> 00:00:28,750
the ID3 algorithm. You're simply going to keep looping forever until you've

10
00:00:28,750 --> 00:00:33,440
solved the problem. At each step, you're going to pick the best attribute,

11
00:00:33,440 --> 00:00:35,650
and we're going to define what we mean by best. There are

12
00:00:35,650 --> 00:00:37,970
a couple of different ways we might, we might define best in

13
00:00:37,970 --> 00:00:40,970
a moment. And then, given the best attribute that splits the

14
00:00:40,970 --> 00:00:42,660
data the way that we want, it does all those things that

15
00:00:42,660 --> 00:00:46,860
we talked about, assign that as a decision attribute for node.

16
00:00:46,860 --> 00:00:50,130
And then for each value that the attribute A can take on,

17
00:00:50,130 --> 00:00:53,760
create a descendent of node. Sort the training examples

18
00:00:53,760 --> 00:00:56,840
to those leaves based upon exactly what values they

19
00:00:56,840 --> 00:01:00,310
take on, and if you've perfectly classified your training

20
00:01:00,310 --> 00:01:03,750
set, then you stop. Otherwise, you iterate over each

21
00:01:03,750 --> 00:01:07,140
of those leaves, picking the best attribute in turn

22
00:01:07,140 --> 00:01:09,380
for the training examples that were sorted into that

23
00:01:09,380 --> 00:01:11,460
leaf, and you keep doing that. Building up the

24
00:01:11,460 --> 00:01:15,310
tree until you're done. So that's the ID3 algorithm.

25
00:01:15,310 --> 00:01:18,290
And the key bit that we have to expand upon

26
00:01:18,290 --> 00:01:21,840
in this case, is exactly what it means to have a

27
00:01:21,840 --> 00:01:25,320
best attribute. All right so, what is exactly, what exactly

28
00:01:25,320 --> 00:01:27,730
is it that we mean by best attribute? So, there are

29
00:01:27,730 --> 00:01:30,090
lots of possibilities, that you can come up with. The

30
00:01:30,090 --> 00:01:32,570
one that is most common, and the one I want you

31
00:01:32,570 --> 00:01:36,080
to think about the most, is what's called information gain.

32
00:01:36,080 --> 00:01:41,070
So information gain is simply a mathematical way to capture the

33
00:01:41,070 --> 00:01:43,020
amount of information that i want to gain by

34
00:01:43,020 --> 00:01:46,540
picking particular attribute, funnily enough. But what it

35
00:01:46,540 --> 00:01:49,000
really talks about is the reduction in the

36
00:01:49,000 --> 00:01:52,490
randomness, over the labels that you have with

37
00:01:52,490 --> 00:01:55,190
set of data, based upon the knowing the

38
00:01:55,190 --> 00:01:58,270
value of particular attribute. So the formula's simply

39
00:01:58,270 --> 00:02:02,610
this. The information gain over S and A

40
00:02:02,610 --> 00:02:06,410
where S is the collection of training examples

41
00:02:06,410 --> 00:02:08,050
that you're looking at. And A, as

42
00:02:08,050 --> 00:02:11,490
a particular attribute, is simply defined as the

43
00:02:11,490 --> 00:02:14,970
entropy, with respect to the labels, of

44
00:02:14,970 --> 00:02:17,320
the set of training examples, you have S,

45
00:02:17,320 --> 00:02:21,605
minus, sort of, the expected or average

46
00:02:21,605 --> 00:02:24,950
entropy that you would have over each set

47
00:02:24,950 --> 00:02:28,020
of examples that you have with a particular

48
00:02:28,020 --> 00:02:30,150
value. Does that make sense to you, Michael?

49
00:02:30,150 --> 00:02:31,900
>> So what we're doing, we're picking an attribute and

50
00:02:31,900 --> 00:02:33,250
that attribute could have a bunch of different

51
00:02:33,250 --> 00:02:36,840
values, like true or false, or short, medium, tall?

52
00:02:36,840 --> 00:02:37,010
>> Right.

53
00:02:37,010 --> 00:02:37,170
>> And.

54
00:02:37,170 --> 00:02:38,320
>> And that's represented by v.

55
00:02:38,320 --> 00:02:41,670
>> Okay, each of those is a different v. And then we're

56
00:02:41,670 --> 00:02:45,470
saying okay, for over those leaves, we're going to do this entropy thing again.

57
00:02:45,470 --> 00:02:46,443
>> Mm-hm.

58
00:02:46,443 --> 00:02:52,420
>> And we right. So what, and what is entropy?

59
00:02:53,470 --> 00:02:57,140
>> entropy. So, we'll talk about entropy later on in the class

60
00:02:57,140 --> 00:03:00,820
in some detail and define it exactly and mathematically. And

61
00:03:00,820 --> 00:03:03,080
some of you probably already know what, what entropy is,

62
00:03:03,080 --> 00:03:05,070
but for those of you who don't, it's exactly a

63
00:03:05,070 --> 00:03:08,406
measure of randomness. So if I have a coin, let's

64
00:03:08,406 --> 00:03:11,030
say a two-headed coin. It can be heads or tails,

65
00:03:11,030 --> 00:03:14,070
and I don't know anything about the coin except that

66
00:03:14,070 --> 00:03:17,790
it's probably fair. If I were to flip the coin,

67
00:03:17,790 --> 00:03:21,170
what's the probability that it would end up heads versus tails?

68
00:03:21,170 --> 00:03:21,840
>> A half.

69
00:03:21,840 --> 00:03:22,600
>> It's

70
00:03:22,600 --> 00:03:24,900
a half, exactly, if it's a fair coin it's a half.

71
00:03:24,900 --> 00:03:28,530
Which means that I have no basis, going into flipping the coin,

72
00:03:28,530 --> 00:03:32,410
to guess either way whether it's heads or it's tails. And so

73
00:03:32,410 --> 00:03:34,730
that has a lot of entropy. In fact it has exactly what's

74
00:03:34,730 --> 00:03:38,220
called one bit of entropy. On the other hand, let's imagine

75
00:03:38,220 --> 00:03:41,930
that I have a coin that has heads on both sides. Then,

76
00:03:41,930 --> 00:03:44,890
before I even flip the coin, I already know what the outcome's

77
00:03:44,890 --> 00:03:47,620
going to be. It's going to come up heads. So what's the

78
00:03:47,620 --> 00:03:49,610
probability of it coming up with heads? It's.

79
00:03:49,610 --> 00:03:50,730
>> One.

80
00:03:50,730 --> 00:03:53,160
>> One. So that actually has no

81
00:03:53,160 --> 00:03:56,998
information, no randomness, no entropy whatsoever. And has

82
00:03:56,998 --> 00:04:01,710
zero bits of entropy. So, when I look at this set of examples that I

83
00:04:01,710 --> 00:04:06,870
have, and the set of labels I have, I can count the number that are coming up,

84
00:04:06,870 --> 00:04:13,050
lets say, red x's. Versus the ones that are coming up green o's.

85
00:04:13,050 --> 00:04:16,209
And if those are evenly split, then the entropy of them

86
00:04:16,209 --> 00:04:18,839
is maximal, because if I were to close my eyes and

87
00:04:18,839 --> 00:04:21,769
reach for an instance, I have no way of knowing beforehand

88
00:04:21,769 --> 00:04:24,660
whether I'm more likely to get an x or I'm more likely

89
00:04:24,660 --> 00:04:28,620
to get an o. On the other hand, if I have

90
00:04:28,620 --> 00:04:31,970
all the x's in together, then I already know before I even

91
00:04:31,970 --> 00:04:34,640
reach in that I'm going to end up with an x. So

92
00:04:34,640 --> 00:04:38,160
as I have more of one label than the other the amount

93
00:04:38,160 --> 00:04:40,810
of entropy goes down. That is, I have

94
00:04:40,810 --> 00:04:43,000
more information going in. Does that make sense, Michael?

95
00:04:43,000 --> 00:04:45,392
>> I think so. So, is there, can, maybe we

96
00:04:45,392 --> 00:04:48,968
can say what the formula is for this, or, or?

97
00:04:48,968 --> 00:04:51,560
>> Sure. What is the formula for it? You should remember.

98
00:04:52,570 --> 00:04:56,412
>> It's, if we have, well, I'm not sure what the notation ought to be with

99
00:04:56,412 --> 00:05:01,650
these S's but it has something to do with Log P log, no wait, it's P(log)P.

100
00:05:01,650 --> 00:05:03,160
>> Mm-hm. So the

101
00:05:03,160 --> 00:05:07,054
actual formula for entropy, using the same notation that we're

102
00:05:07,054 --> 00:05:10,090
using for information gain is simply the sum, over all

103
00:05:10,090 --> 00:05:13,390
the possible values that you might see, of the probability

104
00:05:13,390 --> 00:05:16,624
of you seeing that value, times the log of the probability

105
00:05:16,624 --> 00:05:19,800
of you seeing that value, times minus one. And I

106
00:05:19,800 --> 00:05:21,590
don't want to get into the details here. We're going to go

107
00:05:21,590 --> 00:05:24,080
into a lot more details about this later when we

108
00:05:24,080 --> 00:05:28,330
get further on in the class with randomize optimization, where entropy's

109
00:05:28,330 --> 00:05:30,440
going to matter a lot. But for now, I just, you

110
00:05:30,440 --> 00:05:33,230
have, I want you to have the intuition that this

111
00:05:33,230 --> 00:05:36,250
is a measure of information. This is the measure of

112
00:05:36,250 --> 00:05:39,690
randomness in some variable that you haven't seen. It's the likelihood

113
00:05:39,690 --> 00:05:42,090
of you already knowing what you're going to get if you

114
00:05:42,090 --> 00:05:45,450
close your eyes and pick one of the training examples, versus

115
00:05:45,450 --> 00:05:47,770
you not knowing what you're going to get. If you close

116
00:05:47,770 --> 00:05:50,500
your eyes and you picked one of the training examples. Okay?

117
00:05:50,500 --> 00:05:53,120
>> Alright. So, well, so, okay, so then in the practice,

118
00:05:54,300 --> 00:05:56,780
trees that you had given us before, it was the

119
00:05:56,780 --> 00:06:00,740
case that we worked, we wanted to prefer splits that I

120
00:06:00,740 --> 00:06:02,810
guess, made things less random, right? So if things were

121
00:06:02,810 --> 00:06:06,820
all mixed together, the reds and the greens, after the split

122
00:06:06,820 --> 00:06:08,300
if it was all reds on one side and all

123
00:06:08,300 --> 00:06:10,610
greens on the other. Then each of those two sides would

124
00:06:10,610 --> 00:06:13,660
have very, what? They would have very low entropy, even though

125
00:06:13,660 --> 00:06:17,050
when we started out before the split we had high entropy.

126
00:06:17,050 --> 00:06:19,187
>> Right, that's exactly right. So if you, if you remember the,

127
00:06:19,187 --> 00:06:22,320
the, three examples before. One of them, it was the case

128
00:06:22,320 --> 00:06:25,450
that all of the samples went down the left side of

129
00:06:25,450 --> 00:06:27,850
the tree. So the amount of entropy that we had, didn't

130
00:06:27,850 --> 00:06:31,460
change at all. So there was no gain in using that attribute.

131
00:06:31,460 --> 00:06:34,590
In another case, we split the data in half. But in

132
00:06:34,590 --> 00:06:37,510
each case, we had half of the x's and half of the

133
00:06:37,510 --> 00:06:40,820
o's together, on both sides of the split. Which means that

134
00:06:40,820 --> 00:06:44,430
the total amount of entropy actually didn't change at all. Even though

135
00:06:44,430 --> 00:06:49,290
we split the data. And in the final case, the best one, we still split

136
00:06:49,290 --> 00:06:52,330
the data in half, but since all of the x's ended up on one side

137
00:06:52,330 --> 00:06:55,730
and all of the o's ended up on the other side, we had to entropy

138
00:06:55,730 --> 00:06:57,940
or no randomness left whatsoever. And that

139
00:06:57,940 --> 00:06:59,710
gave us the maximum amount of information gain.

140
00:06:59,710 --> 00:07:01,010
>> So is that how we're choosing the

141
00:07:01,010 --> 00:07:03,050
best attribute? The one with the maximum gain?

142
00:07:03,050 --> 00:07:06,880
>> Exactly. So the goal is to maximize over the entropy gain.

143
00:07:09,160 --> 00:07:10,360
And that's the best attribute.
