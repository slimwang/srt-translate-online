1
00:00:00,210 --> 00:00:02,420
At this point I'm going to
skip over a bunch of details.

2
00:00:02,420 --> 00:00:07,248
For example which features are we
going to use to make their code words?

3
00:00:07,248 --> 00:00:10,561
What size should there be when
you do a vector quantization?

4
00:00:10,561 --> 00:00:14,455
Sometimes it'll tell you how many
code words it thinks it needs or

5
00:00:14,455 --> 00:00:16,241
sometimes you give it a number.

6
00:00:16,241 --> 00:00:18,764
We're not going to talk about that,
and we're not even going to talk about

7
00:00:18,764 --> 00:00:21,255
the clustering or
the vector quantization algorithm.

8
00:00:21,255 --> 00:00:25,405
Instead we're going to focus on how
would you use that in a way that relates

9
00:00:25,405 --> 00:00:31,270
to the SVMs that we just talked
about in order to do recognition.

10
00:00:31,270 --> 00:00:34,630
So, what we do is we're going to
continue with our analogy to documents.

11
00:00:34,630 --> 00:00:36,670
So here I have two different documents.

12
00:00:36,670 --> 00:00:39,330
And if you take a look at the document
on the left you'll see a whole bunch of

13
00:00:39,330 --> 00:00:40,090
words in here.

14
00:00:40,090 --> 00:00:42,950
Right, sensory, brain, visual,
perception, et cetera, nerve,

15
00:00:42,950 --> 00:00:44,580
image Hubel, Wiesel.

16
00:00:44,580 --> 00:00:46,930
Hubel and Wiesel, by the way,
are people who won Nobel Prize for

17
00:00:46,930 --> 00:00:51,010
understanding what goes on in the visual
cortex of the cat, all right?

18
00:00:51,010 --> 00:00:52,510
And here's another document.

19
00:00:52,510 --> 00:00:57,920
China, trade, surplus exports, domestic,
foreign, increase, trade, value.

20
00:00:57,920 --> 00:01:02,190
Notice that what's in the magnifying
glasses are not ordered.

21
00:01:02,190 --> 00:01:06,580
It's just the words that
happened to be in that document.

22
00:01:07,590 --> 00:01:09,510
It's like you stuck all
the words in a bag.

23
00:01:10,530 --> 00:01:13,910
And that's where we get this
notion of bag of words.

24
00:01:13,910 --> 00:01:17,851
Okay, and the goal of what we're
going to talk about now is to go from

25
00:01:17,851 --> 00:01:21,015
object recognition and
map that onto a bag of words.

26
00:01:21,015 --> 00:01:24,585
And in the document world, you know, one
way of doing this is I have my bag of

27
00:01:24,585 --> 00:01:27,889
words and I say, what is the
distribution of words in this document?

28
00:01:27,889 --> 00:01:30,339
And if I wanted to do
document retrieval,

29
00:01:30,339 --> 00:01:34,609
I want to find other similar documents,
a simple thing you might do is find

30
00:01:34,609 --> 00:01:39,160
other documents that have sort of the
same histogram distribution of words.

31
00:01:39,160 --> 00:01:41,910
And that,
that's likely to be a matching document.

32
00:01:41,910 --> 00:01:44,500
Things are much more sophisticated
now in terms of how that works.

33
00:01:44,500 --> 00:01:48,140
But it's, the basic idea is that I've
got these code words that represent or

34
00:01:48,140 --> 00:01:50,758
these words that represent
something about the semantics.

35
00:01:50,758 --> 00:01:54,170
And so a similarity in
distribution tells me that,

36
00:01:54,170 --> 00:01:57,250
maybe there's a good chance that
it's a similar type of document.

37
00:01:57,250 --> 00:01:59,690
So we want to do this
in computer vision.

38
00:01:59,690 --> 00:02:01,250
So that's notionally represented here.

39
00:02:01,250 --> 00:02:03,705
So I've got a bunch of
different objects and

40
00:02:03,705 --> 00:02:07,032
I compute my features over all
of these different objects.

41
00:02:07,032 --> 00:02:09,627
And I come up with what's
shown down here, and

42
00:02:09,627 --> 00:02:12,233
I think this side is
originally from [FOREIGN].

43
00:02:12,233 --> 00:02:13,140
I'm not sure.

44
00:02:13,140 --> 00:02:17,500
And all of these,
these are all of my code words for

45
00:02:17,500 --> 00:02:19,410
all of my different objects, right?

46
00:02:19,410 --> 00:02:22,150
So I take all my objects, and
it's kind of like if you take

47
00:02:22,150 --> 00:02:26,020
all the documents in a,
a library in United States,

48
00:02:26,020 --> 00:02:29,660
and if they're all in English, you
took all of those documents together.

49
00:02:29,660 --> 00:02:32,530
All of the words would be
the set of the words that span

50
00:02:32,530 --> 00:02:34,310
all of those documents, right?

51
00:02:34,310 --> 00:02:38,890
And in fact there are many more
documents than there are words.

52
00:02:38,890 --> 00:02:41,630
And in fact if you think of important
words, words that actually have a lot of

53
00:02:41,630 --> 00:02:45,440
meaning, there's way more
documents than words.

54
00:02:45,440 --> 00:02:50,870
So given a collection of words,
visual words,

55
00:02:50,870 --> 00:02:58,000
for every document, I can say how many
of each word is in each document.

56
00:02:58,000 --> 00:03:01,500
And that's what these histograms
are meant to represent, right?

57
00:03:01,500 --> 00:03:06,470
So you know, the bicycle has got
these little bicycle pieces in it.

58
00:03:06,470 --> 00:03:10,220
All right, and something that
might kind of look like this.

59
00:03:10,220 --> 00:03:11,820
I don't know, maybe, you know,

60
00:03:11,820 --> 00:03:15,560
this brown part down here looks a little
bit like that brown part down there.

61
00:03:15,560 --> 00:03:18,730
But the idea is that there's many more
of the bicycle elements than there

62
00:03:18,730 --> 00:03:21,568
are of the, the violin elements.

63
00:03:21,568 --> 00:03:25,080
This is referred to as a bag of
visual words, for obvious reasons.

64
00:03:25,080 --> 00:03:28,050
And the idea is that you're
describing the entire image

65
00:03:28,050 --> 00:03:31,630
just by the histogram or
the collection of those words.

66
00:03:31,630 --> 00:03:34,080
And the, and you want to do access
the same way that you did in

67
00:03:34,080 --> 00:03:34,930
the documents world.
