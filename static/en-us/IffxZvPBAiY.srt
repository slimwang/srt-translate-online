1
00:00:00,240 --> 00:00:02,020
So we have the allusion of shared

2
00:00:02,020 --> 00:00:04,800
memory, which is implemented by physical memories that

3
00:00:04,800 --> 00:00:08,310
is strewn all over the entire cluster, and

4
00:00:08,310 --> 00:00:11,490
the hope is that the application that is

5
00:00:11,490 --> 00:00:14,950
running on the different nodes of this

6
00:00:14,950 --> 00:00:19,380
cluster will actually get speed up with increasing

7
00:00:19,380 --> 00:00:21,920
number of processors, but such speed up is

8
00:00:21,920 --> 00:00:25,120
not automatic. If the sharing that we're doing,

9
00:00:25,120 --> 00:00:27,690
even though DSM gives you the ability to

10
00:00:27,690 --> 00:00:31,110
share memory across the network, recall what our

11
00:00:31,110 --> 00:00:34,460
good friend Chuck Thacker told us. Shared memory

12
00:00:34,460 --> 00:00:38,150
scales really well when you don't share memory.

13
00:00:38,150 --> 00:00:41,050
So, if the sharing is too fine-grained, then

14
00:00:41,050 --> 00:00:43,470
no hope of speed up, especially with DSM

15
00:00:43,470 --> 00:00:46,580
systems. Because it is only an illusion of

16
00:00:46,580 --> 00:00:50,110
shared memory via software, not even physical shared memory.

17
00:00:50,110 --> 00:00:53,650
Even physical shared memory can lead to overheads.

18
00:00:53,650 --> 00:00:57,520
So, this illusion through software can result in

19
00:00:57,520 --> 00:01:01,780
even more overhead so you've gotta be very careful on how you share and what you

20
00:01:01,780 --> 00:01:05,010
share. So the basic principle is that, the

21
00:01:05,010 --> 00:01:08,730
computation to communication ratio has to be very

22
00:01:08,730 --> 00:01:15,120
high if you want any hope of speed up. So in other words, the critical sections

23
00:01:15,120 --> 00:01:17,750
that are execute to modify data structures

24
00:01:17,750 --> 00:01:21,910
better be really, really hefty critical structures before

25
00:01:21,910 --> 00:01:24,800
somebody else needs to access the same portion

26
00:01:24,800 --> 00:01:27,190
of the data. So what does this mean

27
00:01:27,190 --> 00:01:33,240
for shared memory codes? Well basically, if the code has a lot of dynamic data

28
00:01:33,240 --> 00:01:37,190
structures that are manipulated with pointers, then it

29
00:01:37,190 --> 00:01:40,490
can lead to a lot of implicit communication

30
00:01:40,490 --> 00:01:43,520
across the local area network. You think you're

31
00:01:43,520 --> 00:01:47,160
executing code accessing a pointer, but the pointer

32
00:01:47,160 --> 00:01:49,690
happens to be pointing to memory that's in

33
00:01:49,690 --> 00:01:53,380
a remote processor. So that implicit access to

34
00:01:53,380 --> 00:01:56,100
a data structure pointed to by a pointer

35
00:01:56,100 --> 00:01:58,520
in your program, can result in a network

36
00:01:58,520 --> 00:02:02,550
communication across the network. Fetching something from here,

37
00:02:02,550 --> 00:02:06,100
into your local memory. This is the bane

38
00:02:06,100 --> 00:02:08,758
of distributed shared memory. That pointer

39
00:02:08,758 --> 00:02:13,270
codes may result in increasing overhead

40
00:02:13,270 --> 00:02:15,790
for coherence maintenance, for distributed shared

41
00:02:15,790 --> 00:02:18,220
memory in a local area network. So

42
00:02:18,220 --> 00:02:22,460
you have to be very careful on how you structure codes that can execute

43
00:02:22,460 --> 00:02:28,420
efficiently in a cluster using DSM as the vehicle for programming.
