1
00:00:00,280 --> 00:00:03,460
So what I'd like to do is actually step
through a concrete counterexample that

2
00:00:03,460 --> 00:00:07,250
really does show that a particular
way of doing this kind of training

3
00:00:07,250 --> 00:00:12,430
can fail in the worst case and this
is an example due to Lehman Baird and

4
00:00:12,430 --> 00:00:15,365
it's devious and clever.

5
00:00:15,365 --> 00:00:17,065
>> Hm, I like devious and clever.

6
00:00:17,065 --> 00:00:18,245
>> So here's a really simple MDP.

7
00:00:18,245 --> 00:00:19,045
You ready?

8
00:00:19,045 --> 00:00:22,335
It's got six states and
then a seventh absorbing state.

9
00:00:22,335 --> 00:00:23,185
>> Okay.
>> For each state,

10
00:00:23,185 --> 00:00:24,655
there's exactly one action.

11
00:00:24,655 --> 00:00:26,365
There's no stochasticity.

12
00:00:26,365 --> 00:00:28,255
There's no rewards of any kind.

13
00:00:28,255 --> 00:00:29,205
They're all zero.

14
00:00:29,205 --> 00:00:31,405
So this is a really simple example,
right?

15
00:00:31,405 --> 00:00:33,695
>> Sure,
that seems pretty straightforward.

16
00:00:33,695 --> 00:00:37,215
>> All right, so then what is the
optimal value function and policy for

17
00:00:37,215 --> 00:00:38,700
this example?

18
00:00:38,700 --> 00:00:41,400
>> The optimal value function or
optimal policy?

19
00:00:41,400 --> 00:00:42,260
>> I asked for both.

20
00:00:42,260 --> 00:00:44,590
>> There is no optimal policy,
it doesn't matter what you do.

21
00:00:44,590 --> 00:00:46,130
You just have to do one thing.

22
00:00:46,130 --> 00:00:49,720
>> There's only one policy and
it is optimal by definition of optimal.

23
00:00:49,720 --> 00:00:54,490
>> In the value function, the proper
value function would be zero everywhere.

24
00:00:54,490 --> 00:00:56,360
>> Zero's everywhere, right,

25
00:00:56,360 --> 00:00:59,510
because there's no
>> Rewards anywhere.

26
00:00:59,510 --> 00:01:02,200
So right so
this is super duper simple and

27
00:01:02,200 --> 00:01:03,590
if we had to learn this
using Q learning or

28
00:01:03,590 --> 00:01:07,020
something like that presumably this
would actually learn quite quickly.

29
00:01:07,020 --> 00:01:09,960
What we're going to do though
to make life difficult

30
00:01:09,960 --> 00:01:12,490
is we're going to do this in
a function approximation setting,

31
00:01:12,490 --> 00:01:16,545
linear function approximation with
this following set of features.

32
00:01:16,545 --> 00:01:20,210
So, there's there's the seven states
in the picture and associated with each

33
00:01:20,210 --> 00:01:24,280
state is a feature vector and there's
eight features that are part of that.

34
00:01:25,310 --> 00:01:29,110
Feature zero one, two, three, four,
five, six, seven, feature zero

35
00:01:29,110 --> 00:01:33,890
is one for all the states and seven for
this for the repeating state.

36
00:01:33,890 --> 00:01:36,110
And interestingly, we have it set up so

37
00:01:36,110 --> 00:01:40,330
that these other features are actually
completely indicative of which states

38
00:01:40,330 --> 00:01:44,680
you're in, so
state one has feature one being true.

39
00:01:44,680 --> 00:01:46,965
State two has feature two being true.

40
00:01:46,965 --> 00:01:49,125
State three has feature
three being true.

41
00:01:49,125 --> 00:01:49,630
Or at least.

42
00:01:49,630 --> 00:01:51,695
[LAUGH] And by true I mean two.

43
00:01:51,695 --> 00:01:52,245
I guess.

44
00:01:52,245 --> 00:01:53,335
>> Two.

45
00:01:53,335 --> 00:01:53,975
Yeah.
>> And so

46
00:01:53,975 --> 00:01:56,415
that's kind of the representation here.

47
00:01:56,415 --> 00:01:58,055
So state six looks like one.

48
00:01:58,055 --> 00:01:59,045
And then zeros is everywhere.

49
00:01:59,045 --> 00:02:02,577
Except for feature six which has a 2 and
then a 0, right?

50
00:02:02,577 --> 00:02:06,247
So this feature representation is
actually really close to being

51
00:02:06,247 --> 00:02:06,972
just a table.

52
00:02:06,972 --> 00:02:07,707
>> Yeah.

53
00:02:07,707 --> 00:02:10,937
>> So I'm going to say near tabular
in the sense that if we get rid of

54
00:02:10,937 --> 00:02:14,767
this first feature there's going to be
one weight that represents the value for

55
00:02:14,767 --> 00:02:18,410
state one and one weight that
represents a value for state two and so

56
00:02:18,410 --> 00:02:21,230
forth and one weight that represent
the value for state seven and so

57
00:02:21,230 --> 00:02:22,860
it's exactly just a table.

58
00:02:22,860 --> 00:02:26,010
One weight or one table entry per state.

59
00:02:26,010 --> 00:02:26,610
>> All right.

60
00:02:26,610 --> 00:02:31,080
>> And all we've done is kind of made it
a little bit more by having this extra

61
00:02:31,080 --> 00:02:33,810
feature that we ought to be
able to just ignore, right.

62
00:02:33,810 --> 00:02:35,710
And we should be able to
represent things pretty well.

63
00:02:35,710 --> 00:02:38,030
So in fact,
I think that it's worth asking.

64
00:02:38,030 --> 00:02:41,460
Can we represent the optimal
value function using this

65
00:02:41,460 --> 00:02:42,390
set of weights right?

66
00:02:42,390 --> 00:02:46,690
So if our parameters are W0 through W7,
is there a way to set them so

67
00:02:46,690 --> 00:02:51,240
that the value function that we get is
the actual value function for this MDP?

68
00:02:51,240 --> 00:02:52,266
>> Sure there is one easy one.

69
00:02:52,266 --> 00:02:53,050
>> And what's that?

70
00:02:53,050 --> 00:02:54,000
>> Set them all to zero.

71
00:02:54,000 --> 00:02:54,500
>> Right.
