1
00:00:00,080 --> 00:00:02,130
Here's another way of thinking about this trade-off.

2
00:00:02,130 --> 00:00:05,780
We want to balance the errors that our regression gives us,

3
00:00:05,780 --> 00:00:09,140
with the number of features that are required to get those errors.

4
00:00:09,140 --> 00:00:12,460
We're going to try to maximize the quality of the model that we're making.

5
00:00:12,460 --> 00:00:15,120
I haven't totally defined this yet, but let me fill out this graph, and

6
00:00:15,120 --> 00:00:17,230
you'll get a sense for what I mean by this.

7
00:00:17,230 --> 00:00:20,030
Let's suppose I have only one feature in my model.

8
00:00:20,030 --> 00:00:22,848
Then, I would say that the likelihood is pretty high that I'm

9
00:00:22,848 --> 00:00:24,050
under-fitting my data

10
00:00:24,050 --> 00:00:27,070
that I have something that's very high-bias.

11
00:00:27,070 --> 00:00:29,520
So, I'd say the quality of the model might be all right.

12
00:00:29,520 --> 00:00:32,100
But, not as good as it could be.

13
00:00:32,100 --> 00:00:35,610
Now, let's suppose that I allow a little bit more complexity into my model.

14
00:00:35,610 --> 00:00:37,310
I allow to use three features.

15
00:00:37,310 --> 00:00:40,330
And, I happen to pick three features that are good at

16
00:00:40,330 --> 00:00:41,730
describing the patterns in the data.

17
00:00:42,750 --> 00:00:44,810
Then, my error is going to go down,

18
00:00:44,810 --> 00:00:48,490
because it's my model is now fitting my data in a, in a more precise way.

19
00:00:49,500 --> 00:00:50,850
But it's not too complex yet.

20
00:00:50,850 --> 00:00:52,900
I'm only using three features, and I've checked each one of them.

21
00:00:52,900 --> 00:00:55,220
So, the quality of my model has now gone up.

22
00:00:55,220 --> 00:00:57,200
So, then I say, well, that's pretty sweet.

23
00:00:57,200 --> 00:01:00,540
I can improve my model, just by adding more features into it.

24
00:01:00,540 --> 00:01:02,290
So, let me try to add in a whole bunch of features.

25
00:01:02,290 --> 00:01:03,670
I'm going to jump up to 15.

26
00:01:03,670 --> 00:01:06,610
And, I'm hoping that that'll put the quality of my model,

27
00:01:06,610 --> 00:01:09,720
say somewhere up here, at the maximal value that it can have.

28
00:01:09,720 --> 00:01:14,070
What I'll actually find though, is that I'm starting to become susceptible to

29
00:01:14,070 --> 00:01:16,800
this over-fitting, high variance type regime.

30
00:01:16,800 --> 00:01:20,750
So, maybe the quality of my model has actually become a little bit

31
00:01:20,750 --> 00:01:24,660
lower than the quality that it was at, at three features.

32
00:01:24,660 --> 00:01:26,190
Because, now I'm starting to over fit,

33
00:01:26,190 --> 00:01:30,030
where before I was under-fitting with one, I'm now over fitting with 15.

34
00:01:30,030 --> 00:01:33,150
And, if I were to go all the way out to 20,

35
00:01:33,150 --> 00:01:37,210
then I might even be doing worse than if I had only one input feature.

36
00:01:37,210 --> 00:01:40,480
And, so you could imagine that if I sort of filled out this,

37
00:01:40,480 --> 00:01:45,280
this graph for all of the possible number of features that I could get, there

38
00:01:45,280 --> 00:01:51,100
would be some best point where the quality of my model is going to be maximized.

39
00:01:51,100 --> 00:01:54,530
So, a reasonable thing to think, and what we'll talk about for the next few

40
00:01:54,530 --> 00:01:59,450
videos, is how we can mathematically define what this arc might be so

41
00:01:59,450 --> 00:02:01,830
that we can find this maximum point of it.

42
00:02:01,830 --> 00:02:04,500
And, this is a process that's called regularization.

43
00:02:04,500 --> 00:02:08,479
It's an automatic form of feature selection that some algorithms can

44
00:02:08,479 --> 00:02:13,330
do completely on their own, that they can trade off between the precision,

45
00:02:13,330 --> 00:02:18,530
the goodness of fit, the very low errors, and, the complexity of,

46
00:02:18,530 --> 00:02:20,670
of fitting on lots and lots of different features.

47
00:02:20,670 --> 00:02:24,940
So, let me show you an example of how regularization works in regressions.
