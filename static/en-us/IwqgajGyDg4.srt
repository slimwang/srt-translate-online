1
00:00:00,100 --> 00:00:02,650
So, let's summarize what we have learned now. So

2
00:00:02,650 --> 00:00:05,900
we, we first understood what is information and we

3
00:00:05,900 --> 00:00:08,290
found out that information can be measured in some

4
00:00:08,290 --> 00:00:12,030
way and we meausred it in terms of entropy.

5
00:00:12,030 --> 00:00:14,560
Then we started to understand how we can measure

6
00:00:14,560 --> 00:00:17,822
the information between two way variables. And there we

7
00:00:17,822 --> 00:00:21,160
defined terms as, terms like joint entropy, conditional entropy

8
00:00:21,160 --> 00:00:25,680
and mutual information. And then finally we introduced ourselves

9
00:00:25,680 --> 00:00:28,140
to a term called a KL divergence, which

10
00:00:28,140 --> 00:00:30,420
is very famously used as a distance measured between

11
00:00:30,420 --> 00:00:33,620
two distributions. So this is just a primer

12
00:00:33,620 --> 00:00:36,320
to information theory and it forms as a base

13
00:00:36,320 --> 00:00:38,230
to what is required for you to go

14
00:00:38,230 --> 00:00:40,590
through this machine learning course. If you want to

15
00:00:40,590 --> 00:00:42,910
learn more about information theory, follow the links

16
00:00:42,910 --> 00:00:45,450
in, in the Comments sections. And yeah. Thank you.
