1
00:00:00,000 --> 00:00:05,000
[Thrun] It's interesting to see how to minimize a loss function using gradient descent.

2
00:00:05,000 --> 00:00:12,000
In our linear case, we have L equals sum over the correct labels

3
00:00:12,000 --> 00:00:16,000
minus our linear function to the square,

4
00:00:16,000 --> 00:00:18,000
which we seek to minimize.

5
00:00:18,000 --> 00:00:21,000
We already know that this has a closed form solution,

6
00:00:21,000 --> 00:00:25,000
but just for the fun of it, let's look at gradient descent.

7
00:00:25,000 --> 00:00:33,000
The gradient of L with respect to W1 is minus 2, sum of all J

8
00:00:33,000 --> 00:00:39,000
of the difference as before but without the square times Xj.

9
00:00:39,000 --> 00:00:43,000
The gradient with respect to W0 is very similar.

10
00:00:43,000 --> 00:00:49,000
So in gradient descent we start with W1 0 and W0 0

11
00:00:49,000 --> 00:00:55,000
where the upper cap 0 corresponds to the iteration index of gradient descent.

12
00:00:55,000 --> 00:00:57,000
And then we iterate.

13
00:00:57,000 --> 00:01:06,000
In the M iteration we get our new estimate by using the old estimate

14
00:01:06,000 --> 00:01:10,000
minus a learning rate of this gradient over here

15
00:01:10,000 --> 00:01:15,000
taking the position of the old estimate W1, M minus 1.

16
00:01:15,000 --> 00:01:20,000
Similarly, for W0 we get this expression over here.

17
00:01:20,000 --> 00:01:24,000
And these expressions look nasty,

18
00:01:24,000 --> 00:01:28,000
but what it really means is we subtract an expression like this

19
00:01:28,000 --> 00:01:31,000
every time we do gradient descent from W1

20
00:01:31,000 --> 00:01:36,000
and an expression like this every time we do gradient descent from W0,

21
00:01:36,000 --> 99:59:59,999
which is easy to implement, and that implements gradient descent.
