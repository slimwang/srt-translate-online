1
00:00:00,270 --> 00:00:01,000
Hi again Michael.

2
00:00:01,000 --> 00:00:02,590
>> Hey, how's it going?

3
00:00:02,590 --> 00:00:05,510
>> It's going pretty well. We are ready to do our

4
00:00:05,510 --> 00:00:08,410
last lesson of this mini course

5
00:00:08,410 --> 00:00:10,480
on unsupervised learning and randomized optimization.

6
00:00:10,480 --> 00:00:11,920
>> Cool, what's it about?

7
00:00:11,920 --> 00:00:16,470
>> It is about feature transformation. As opposed to feature selection.

8
00:00:16,470 --> 00:00:21,200
>> Cool. So they can change from vehicles into robots, I assume.

9
00:00:21,200 --> 00:00:23,400
>> yeah, typically. Usually it's from robots

10
00:00:23,400 --> 00:00:25,500
into cars, but that's a technical detail.

11
00:00:25,500 --> 00:00:26,360
>> Okay.

12
00:00:26,360 --> 00:00:29,820
>> Okay. So I'm going to define feature transformation for you, or at

13
00:00:29,820 --> 00:00:31,810
least I'm going to do my best to. It turns out that it's

14
00:00:31,810 --> 00:00:35,850
a slightly more slippery definition than the one that we used for

15
00:00:35,850 --> 00:00:38,075
feature selection but it has a lot of things in common. Okay?

16
00:00:38,075 --> 00:00:39,083
>> Yep.

17
00:00:39,083 --> 00:00:40,160
>> And you tell me if this makes sense.

18
00:00:40,160 --> 00:00:44,240
Okay, so feature transformation as opposed to feature selection which

19
00:00:44,240 --> 00:00:46,730
we talked about last time is the problem of

20
00:00:46,730 --> 00:00:50,680
doing some kind of pre-processing on a set of features.

21
00:00:52,010 --> 00:00:54,380
In order to create a new set of

22
00:00:54,380 --> 00:00:57,840
features. Now typically, we expect that new set of

23
00:00:57,840 --> 00:01:00,250
the future to be smaller or in some way

24
00:01:00,250 --> 00:01:04,209
more compact. Okay? But while we create this new

25
00:01:04,209 --> 00:01:07,270
set of features, we want to explicitly retain as

26
00:01:07,270 --> 00:01:10,360
much information as possible, and when I say retain

27
00:01:10,360 --> 00:01:14,150
as much information as possible, I probably mean, I

28
00:01:14,150 --> 00:01:17,050
think we'll, we'll see as we continue this conversation.

29
00:01:17,050 --> 00:01:20,260
Information that is relevant and hopefully useful.

30
00:01:20,260 --> 00:01:23,050
Now, the first thing you might ask is,

31
00:01:23,050 --> 00:01:25,990
what's the difference between this and feature selection.

32
00:01:25,990 --> 00:01:27,520
Is that a question you might ask, Michael?

33
00:01:27,520 --> 00:01:29,640
>> I was thinking about that, though I think you kind

34
00:01:29,640 --> 00:01:32,150
of told me at the beginning of the feature selection lecture.

35
00:01:32,150 --> 00:01:33,440
>> well, I told you that I was going to

36
00:01:33,440 --> 00:01:34,720
to tell you. I'm not sure I actually told you.

37
00:01:34,720 --> 00:01:35,930
>> Hm.

38
00:01:35,930 --> 00:01:38,260
>> So one thing you might say Michael, is that if I

39
00:01:38,260 --> 00:01:42,450
looked at this definition, this seems to actually be consistent with feature

40
00:01:42,450 --> 00:01:44,590
selection as well. I'd take a set

41
00:01:44,590 --> 00:01:46,850
of features, a feature selection. Then I'd create

42
00:01:46,850 --> 00:01:51,730
a new feature set which happens to be smaller. And my goal was to retain

43
00:01:51,730 --> 00:01:53,210
as much information as possible and we

44
00:01:53,210 --> 00:01:55,620
talked about the difference between relevant features and

45
00:01:55,620 --> 00:01:57,840
useful features, but really this sort of

46
00:01:57,840 --> 00:02:00,810
describes feature selection as well. You see that?

47
00:02:00,810 --> 00:02:01,430
>> Yeah, definitely.

48
00:02:01,430 --> 00:02:04,420
>> Now the difference is, in fact probably the right way to

49
00:02:04,420 --> 00:02:07,740
think about is this that feature selection is in fact a subset

50
00:02:07,740 --> 00:02:12,000
of feature transformation. Where the pre-processing you're doing is

51
00:02:12,000 --> 00:02:15,320
literally taking a subset of the features. Here, feature

52
00:02:15,320 --> 00:02:17,790
transformation can be more powerful, and can be an

53
00:02:17,790 --> 00:02:20,890
arbitrary pre-processing, not just something that goes from a set

54
00:02:20,890 --> 00:02:23,780
to to a subset. But what we're going to do

55
00:02:23,780 --> 00:02:26,730
is we're going to restrict our notion of future transformation to

56
00:02:26,730 --> 00:02:29,580
what's called linear future transformation. So, let me define

57
00:02:29,580 --> 00:02:32,840
that for you explicitly. So as before with the feature

58
00:02:32,840 --> 00:02:34,480
subset, we said that we were taking a

59
00:02:34,480 --> 00:02:38,800
bunch of features or a bunch of instances that

60
00:02:38,800 --> 00:02:42,000
were in some individual feature space and transforming

61
00:02:42,000 --> 00:02:45,670
it into another feature space of size M. Yup.

62
00:02:45,670 --> 00:02:48,890
>> And in the, the feature selection problem, m was meant

63
00:02:48,890 --> 00:02:52,160
to be less than N, hopefully much less than N. And that's

64
00:02:52,160 --> 00:02:55,450
typically the case of feature transformation, though as we'll discuss towards

65
00:02:55,450 --> 00:02:57,920
the very end, it doesn't have to be. But when I say

66
00:02:57,920 --> 00:03:01,580
usually we expect M to be less than N, usually means almost always.

67
00:03:01,580 --> 00:03:03,960
>> [LAUGH] Okay. And that's because of

68
00:03:03,960 --> 00:03:06,160
the cause of Dimensionality problem. But the difference

69
00:03:06,160 --> 00:03:08,100
between what we were doing with future selection

70
00:03:08,100 --> 00:03:10,020
and future transformation, because this is exactly what

71
00:03:10,020 --> 00:03:14,390
I wrote before, is that this transformation operator

72
00:03:14,390 --> 00:03:17,560
is usually something like this, in linear transformation

73
00:03:17,560 --> 00:03:23,870
operator. So the goal here is to find some matrix P, such that I can project.

74
00:03:23,870 --> 00:03:26,260
My examples my points in my instance

75
00:03:26,260 --> 00:03:29,670
space, into this new subspace, typically smaller

76
00:03:29,670 --> 00:03:34,000
than the original subspace. And I'll get some set of new features which are

77
00:03:34,000 --> 00:03:39,030
combinations in a particular linear combinations of the old features. Does that

78
00:03:39,030 --> 00:03:45,020
make sense? I think so. So then that p matrix would want to be N by M.

79
00:03:45,020 --> 00:03:46,210
>> Yes.

80
00:03:46,210 --> 00:03:49,770
>> So the transpose would be M by N and

81
00:03:49,770 --> 00:03:53,130
that multiplies by the N dimensional feature space in X

82
00:03:53,130 --> 00:03:55,480
and projects it out to an M dimensional feature space.

83
00:03:55,480 --> 00:03:55,660
>> Right.

84
00:03:55,660 --> 00:03:59,580
>> Okay. Yeah. Pretty good. So if we wanted to write that

85
00:03:59,580 --> 00:04:02,380
out. We could say that feature selection was about taking features like

86
00:04:02,380 --> 00:04:10,110
X1, X2, X3 and X4 and finding a subset like X1 and X2.

87
00:04:10,110 --> 00:04:15,090
And that would be feature selection. But feature transformation would be taking

88
00:04:15,090 --> 00:04:22,089
something like taking X1. X2, X3, and X4, and translating

89
00:04:22,089 --> 00:04:27,670
into something like 2X1 plus X2. Which creates

90
00:04:27,670 --> 00:04:30,380
a single new feature. Which is a linear combination

91
00:04:30,380 --> 00:04:32,370
of the subset of the. Does that make sense?

92
00:04:32,370 --> 00:04:34,900
>> Yeah, but it could actually make other feature

93
00:04:34,900 --> 00:04:37,610
combinations as well, right. It's just projected down to one.

94
00:04:37,610 --> 00:04:40,400
>> Right, it could be projected down to two, or it could be projected

95
00:04:40,400 --> 00:04:42,950
down to three Or could even project it out to a different for.

96
00:04:42,950 --> 00:04:43,530
>> Okay.

97
00:04:43,530 --> 00:04:46,720
>> And in principal you could imagine that you could even project

98
00:04:46,720 --> 00:04:48,370
up into other dimensions which is

99
00:04:48,370 --> 00:04:50,640
something that we've done before. Conceptually, anyway.

100
00:04:50,640 --> 00:04:53,110
>> When we talked about kernels?

101
00:04:53,110 --> 00:04:57,680
>> Yeah, although those were typically non-linear transformations implicit

102
00:04:57,680 --> 00:05:00,270
in the notion was doing a non-linear feature transformation

103
00:05:00,270 --> 00:05:01,570
but we even did it before we even learned

104
00:05:01,570 --> 00:05:06,160
about kernels. Very second thing I think we did. Perceptrons.

105
00:05:06,160 --> 00:05:08,760
How do perceptrons go into a higher dimensional space.

106
00:05:08,760 --> 00:05:10,420
>> Well, when we talked about XOR.

107
00:05:11,910 --> 00:05:13,470
>> Oh, right.

108
00:05:13,470 --> 00:05:15,700
>> What we effectively did was we showed that we could

109
00:05:15,700 --> 00:05:21,130
project the original. Two dimensional space into what looks like a three

110
00:05:21,130 --> 00:05:24,120
dimensional space where the third dimension was a combination of the first

111
00:05:24,120 --> 00:05:27,700
two. And then you could actually do it with a linear separator.

112
00:05:27,700 --> 00:05:29,110
>> But that wasn't a linear

113
00:05:29,110 --> 00:05:31,560
transformation, that c was a nonlinear transformation.

114
00:05:31,560 --> 00:05:33,360
>> Right. Because we were talking about Boolean verbs.

115
00:05:33,360 --> 00:05:33,970
>> Yeah.

116
00:05:33,970 --> 00:05:35,930
>> But, in the end of the day it

117
00:05:35,930 --> 00:05:38,250
was still a kind of feature transformation and in this

118
00:05:38,250 --> 00:05:40,560
case a feature transformation where we went to a higher

119
00:05:40,560 --> 00:05:43,420
number of features. And today, what we're going to be talking

120
00:05:43,420 --> 00:05:48,140
about is linear transformations as opposed to non-linear transformations.

121
00:05:48,140 --> 00:05:51,090
And we're going to be focusing specifically on the case

122
00:05:51,090 --> 00:05:53,520
where we're trying to reduce the number of dimensions. So

123
00:05:53,520 --> 00:05:57,010
the implicit assumption here, right, the kind of domain knowledge

124
00:05:57,010 --> 00:06:00,770
that we're bringing to here or the belief that we're bringing here is that. We

125
00:06:00,770 --> 00:06:04,990
don't need all N features. We need a much smaller set of features that'll still

126
00:06:04,990 --> 00:06:07,910
capture all the original information, and therefore,

127
00:06:07,910 --> 00:06:10,220
help us to overcome the curse of dimensionality.

128
00:06:10,220 --> 00:06:13,030
It's just that that smaller subset might require

129
00:06:13,030 --> 00:06:16,060
bringing in information across the various features. Okay?

130
00:06:16,060 --> 00:06:16,820
>> Yeah.

131
00:06:16,820 --> 00:06:17,600
>> Alright.
