1
00:00:00,240 --> 00:00:03,680
What I just showed you was a formula for
the one dimensional case.

2
00:00:03,680 --> 00:00:06,400
We almost always do this in the n
dimension because even when we were

3
00:00:06,400 --> 00:00:08,460
having x move in just one direction,

4
00:00:08,460 --> 00:00:12,280
we had a two dimensional vector
of position and velocity.

5
00:00:12,280 --> 00:00:16,040
In the two dimensional case,
these are the Kalman equations, okay.

6
00:00:16,040 --> 00:00:19,019
We have our state vector x.

7
00:00:19,019 --> 00:00:24,653
We assume, our prediction is just
based upon a linear transformation.

8
00:00:24,653 --> 00:00:30,020
Our covariance prediction, or the pre,
the covariance of the prediction,

9
00:00:30,020 --> 00:00:34,040
this is that squaring operator,
remember before, we did D squared sigma?

10
00:00:34,040 --> 00:00:36,630
Well, when you're doing
this in matrices,

11
00:00:36,630 --> 00:00:41,120
it's D times the old sigma times
D transpose, that's the squaring.

12
00:00:41,120 --> 00:00:43,720
Plus you add in the noise,
the process noise.

13
00:00:43,720 --> 00:00:45,850
So those are your
prediction measurements.

14
00:00:45,850 --> 00:00:49,190
And then your correction looks
like the things on the right.

15
00:00:49,190 --> 00:00:52,160
The thing for you to notice here,
is, you see the,

16
00:00:52,160 --> 00:00:54,580
the quantity in the green box?

17
00:00:54,580 --> 00:00:57,471
This is the residual, right there, and

18
00:00:57,471 --> 00:01:02,071
you're multiplying it by this
Kalman gain matrix, all right?

19
00:01:02,071 --> 00:01:06,757
And the Kalman gain matrix has
less weight on the residual

20
00:01:06,757 --> 00:01:10,868
as the prior covariance
estimate approaches 0.

21
00:01:10,868 --> 00:01:13,744
So as this value's, it's,
it's hard to say what it means for

22
00:01:13,744 --> 00:01:16,566
covariance to go to 0, but
let's talk about its traits, but

23
00:01:16,566 --> 00:01:18,780
the idea is that it's getting smaller.

24
00:01:18,780 --> 00:01:23,070
As it gets smaller,
the Kalman gain goes down,

25
00:01:23,070 --> 00:01:24,880
which is what you want to have happen,
right?

26
00:01:24,880 --> 00:01:28,130
If your covariance of your
prediction is very tight,

27
00:01:28,130 --> 00:01:30,480
I don't want to pay a lot of
attention to my measurements.

28
00:01:30,480 --> 00:01:33,160
So I don't want to weight
the residuals very much.

29
00:01:33,160 --> 00:01:39,300
On the other hand, if my measurement
covariance goes down, okay?

30
00:01:39,300 --> 00:01:43,460
So now, what happens,
is this value is very small,

31
00:01:43,460 --> 00:01:47,640
which means this value is what's
getting put in the inverse, right.

32
00:01:47,640 --> 00:01:51,840
So now this sigma t goes away.

33
00:01:51,840 --> 00:01:56,205
What happens is the Kalman gain matrix
goes up, its magnitude, if you will,

34
00:01:56,205 --> 00:01:58,485
goes up and
it weights the measurements more.

35
00:01:58,485 --> 00:02:02,255
So, these matrix equations
are doing exactly the same thing

36
00:02:02,255 --> 00:02:06,635
as the scalar equation did of balancing
the prediction, and the measurement.
