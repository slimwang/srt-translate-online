1
00:00:00,000 --> 00:00:03,073
So the examples that we're looking at get pretty good occupancy.

2
00:00:03,073 --> 00:00:05,875
In general, how do affect the occupancy of a kernel?

3
00:00:05,875 --> 00:00:10,948
Well, it's usually fairly easy to control the amount of shared memory needed by a thread block.

4
00:00:10,948 --> 00:00:15,916
For example, in our transpose kernel that would be the tile size, and, of course,

5
00:00:15,916 --> 00:00:19,862
you can change the number of threads and thread blocks that you launch when you launch your kernel.

6
00:00:19,862 --> 00:00:23,694
You can also work with a compiler to control the number of registers a kernel uses,

7
00:00:23,694 --> 00:00:28,264
though this qualifies as a ninja-level optimization and isn't usually worth the trouble.

8
00:00:28,264 --> 00:00:34,835
So increasing occupancy is usually, but not always, a good thing, and it only helps up to a point.

9
00:00:34,835 --> 00:00:39,758
It exposes more parallelism to the GPU and allows more memory transactions in flight,

10
00:00:39,758 --> 00:00:43,347
but it may force the GPU to run less efficiently when taken too far.

11
00:00:43,347 --> 00:00:50,884
This is always a tradeoff. For example, decreasing the tile size and the transpose code will let more thread blocks run.

12
00:00:50,884 --> 00:00:55,258
It decreases the amount of time spent waiting at barriers, but if we get to too small a tile,

13
00:00:55,258 --> 00:00:59,163
we'll lose the whole point of tiling, which was to coalesce the global memory accesses.

14
00:00:59,163 --> 00:01:01,736
So let's go back to that transpose code.

15
00:01:01,736 --> 00:01:06,738
So here's our kernel, and you'll recall that we concluded that the syncthreads was really the problem.

16
00:01:06,738 --> 00:01:10,674
We need it there, but we've got a large thread block, 32 by 32 threads,

17
00:01:10,674 --> 00:01:17,112
1024 total threads, and most of the time most of those threads are simply sitting at the barrier

18
00:01:17,112 --> 00:01:21,882
waiting for the rest of the threads to reach the barrier before they can proceed.

19
00:01:21,882 --> 00:01:31,660
So we conclude that maybe we can make this faster by simply going up and changing to 16 by 16 blocks,

20
00:01:31,660 --> 00:01:37,334
a very small change that should decrease the amount of time that we spend waiting at a barrier

21
00:01:37,334 --> 00:01:42,045
and therefore decrease the latency between memory operations, which,

22
00:01:42,045 --> 00:01:45,508
by Little's Law, we would expect to let us increase the total bandwidth.

23
00:01:45,508 --> 00:01:53,983
So now we'll compile and run, and sure enough, this time we took just over 0.53 milliseconds.

24
00:01:53,983 --> 00:01:58,384
So let's update our running tally of results. The tiled version of our code,

25
00:01:58,384 --> 00:02:02,659
we managed to get about half a millisecond by adjusting the tile size.

26
00:02:02,659 --> 00:02:04,759
So here's a programming exercise.

27
00:02:04,759 --> 00:02:10,268
Let's go back to the transpose code and try a few different tile sizes and see which ones work best.

28
00:02:10,268 --> 00:02:17,111
Try 8 by 8, 16 by 16, 32 by 32, and 64 by 64 tiles.

29
00:02:17,111 --> 00:02:21,857
This last one will be trickier because, of course, this is more elements than you can put threads in a thread block,

30
00:02:21,857 --> 00:02:24,644
so you'll have to think about how to pack this into a single thread block,

31
00:02:24,644 --> 00:02:31,723
and I should point out we are looking for the relative ranking of these on the Udacity servers,

32
00:02:31,723 --> 00:02:35,556
so using the Fermi-based GPUs as opposed to, for example,

33
00:02:35,556 --> 00:02:38,294
your own machine that you might be developing on at home.
