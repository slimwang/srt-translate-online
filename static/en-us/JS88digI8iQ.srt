1
00:00:00,460 --> 00:00:04,510
So now let's consider how different lock implementations

2
00:00:04,510 --> 00:00:07,660
interact with the coherence protocol and what performance

3
00:00:07,660 --> 00:00:13,808
do they achieve. Let's look at three cores, 0, 1, and 2. Let's first try to do

4
00:00:13,808 --> 00:00:17,260
this with atomic exchange instructions. Core 0 let's

5
00:00:17,260 --> 00:00:19,580
say, tries to grab the lock first. And

6
00:00:19,580 --> 00:00:22,500
it succeeds. It does an atomic exchange, gets

7
00:00:22,500 --> 00:00:25,590
that the lock was free. The lock var is

8
00:00:25,590 --> 00:00:31,190
now going to be in the modified state in the cache of core 0. And it's going to

9
00:00:31,190 --> 00:00:34,570
not be present in core 1 and core

10
00:00:34,570 --> 00:00:39,370
2's caches. Now eventually core 0, which grabbed the

11
00:00:39,370 --> 00:00:45,310
lock, will unlock by writing 0 to the lock var. But the question is what happens

12
00:00:45,310 --> 00:00:50,882
meanwhile on core 1 and core 2? Both core 1 and core 2 might want to also enter

13
00:00:50,882 --> 00:00:57,062
the same critical section. Let's say core 1 tries first. He does an exchange of

14
00:00:57,062 --> 00:01:03,166
R1 and lock var, see that lock var is one. So, it didn't grab the lock, but

15
00:01:03,166 --> 00:01:08,626
because there was a write to lock var, there is a transfer of lock var into this

16
00:01:08,626 --> 00:01:16,360
cache, where it's now modified. And now lock var is invalid in core 0's cache.

17
00:01:16,360 --> 00:01:18,880
Let's say that core 0 now does it's own

18
00:01:18,880 --> 00:01:22,540
exchange. Does the same thing. Tries to grab the lock,

19
00:01:22,540 --> 00:01:25,804
sees that it's one, will try again. But because it

20
00:01:25,804 --> 00:01:29,084
did the write to lockvar, now it grabs the block

21
00:01:29,084 --> 00:01:31,954
in the modified state and core 1 gets it in

22
00:01:31,954 --> 00:01:36,100
invalid state and core 2 stays invalid. And now, as

23
00:01:36,100 --> 00:01:38,390
long as the lock is busy, these two will be

24
00:01:38,390 --> 00:01:41,880
spinning in that loop. So what's going to happen is

25
00:01:41,880 --> 00:01:44,810
this block will move many many times between

26
00:01:44,810 --> 00:01:48,380
their caches, each time causing communication on the shared

27
00:01:48,380 --> 00:01:51,500
bus. And each time spending a lot of power.

28
00:01:51,500 --> 00:01:54,820
None of this activity here will actually succeed in

29
00:01:54,820 --> 00:01:58,470
grabbing the lock until the lockvar becomes 0.

30
00:01:58,470 --> 00:02:02,470
So at some point, let's say that core 1

31
00:02:02,470 --> 00:02:04,830
was the last one to grab the lock at

32
00:02:04,830 --> 00:02:07,370
the point where lock lockvar is written to 0.

33
00:02:07,370 --> 00:02:10,828
This write will cause yet another movement of

34
00:02:10,828 --> 00:02:14,741
the block, it becomes modified in core 0's cache

35
00:02:14,741 --> 00:02:17,994
now. And now, whoever tries to grab the lock

36
00:02:17,994 --> 00:02:21,122
first will succeed by obtaining the block, writing to

37
00:02:21,122 --> 00:02:24,008
it, but this time grabbing the lock. So,

38
00:02:24,008 --> 00:02:26,302
if core one now succeeds in grabbing the lock

39
00:02:26,302 --> 00:02:29,526
after it became available, core two continues this activity

40
00:02:29,526 --> 00:02:32,600
further. So, there is a lot of this going

41
00:02:32,600 --> 00:02:34,610
on as long as anybody's waiting on a

42
00:02:34,610 --> 00:02:38,161
lock. This is very power hungry because these

43
00:02:38,161 --> 00:02:41,163
movements of data between caches are going to

44
00:02:41,163 --> 00:02:44,160
cost us a lot of energy per transfer.

45
00:02:44,160 --> 00:02:48,920
And also, this activity here keeps the interconnect

46
00:02:48,920 --> 00:02:51,120
between the course busy, for example, the shared

47
00:02:51,120 --> 00:02:54,060
bus. So the one core that is actually

48
00:02:54,060 --> 00:02:57,900
doing useful work here while this is going on.

49
00:02:57,900 --> 00:02:59,694
If it has a cache miss will now be

50
00:02:59,694 --> 00:03:02,523
slower in handling this cache miss because it has

51
00:03:02,523 --> 00:03:04,662
to wait for the busy bus that these two

52
00:03:04,662 --> 00:03:07,760
are keeping busy. So not only is it power

53
00:03:07,760 --> 00:03:10,385
hungry to do this, it also by busy waiting

54
00:03:10,385 --> 00:03:13,160
in such an active way we're slowing down the

55
00:03:13,160 --> 00:03:15,785
useful work that is the only thing that can

56
00:03:15,785 --> 00:03:19,550
really result in us actually getting the lock eventually.
