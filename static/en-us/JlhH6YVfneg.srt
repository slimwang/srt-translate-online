1
00:00:00,370 --> 00:00:04,500
It's important to point out that
when we say reinforcement learning,

2
00:00:04,500 --> 00:00:08,320
we're really describing a problem,
not a solution.

3
00:00:08,320 --> 00:00:13,970
In the same way that linear regression
is one solution to the supervised

4
00:00:13,970 --> 00:00:19,600
regression problem, there are many
algorithms that solve the RL problem.

5
00:00:19,600 --> 00:00:21,930
Because I started out as a roboticists,

6
00:00:21,930 --> 00:00:26,560
I'm going to first explain this in
terms of a problem for a robot.

7
00:00:26,560 --> 00:00:27,980
So here's our robot here and

8
00:00:27,980 --> 00:00:30,600
our robot is going to interact
with the environment.

9
00:00:30,600 --> 00:00:35,450
So we represent the environment
as this sort of cloud up here.

10
00:00:35,450 --> 00:00:38,940
So the robots going to take actions
that'll change the environment.

11
00:00:38,940 --> 00:00:42,380
It will sense the environment,
reason over what it sees and

12
00:00:42,380 --> 00:00:43,160
take another action.

13
00:00:44,350 --> 00:00:49,420
In robotics, we call this the sense,
think, act cycle and

14
00:00:49,420 --> 00:00:52,330
you don't have to implement it
only using reinforcement learning.

15
00:00:52,330 --> 00:00:56,310
There's many ways that you could
implement sense, think, act, but

16
00:00:56,310 --> 00:01:00,150
we're going to focus on how to do
that with reinforcement learning.

17
00:01:00,150 --> 00:01:03,300
Okay, so
our robot observes the environment and

18
00:01:03,300 --> 00:01:07,540
some form of description of
the environment comes in.

19
00:01:07,540 --> 00:01:11,070
Let's call that the state s, so

20
00:01:11,070 --> 00:01:14,800
s is our letter that represents
what we see in the environment.

21
00:01:14,800 --> 00:01:17,904
Now the robot has to process that
state somehow to determine what to do.

22
00:01:17,904 --> 00:01:22,909
And we call this pi or
policy, so the robot

23
00:01:22,909 --> 00:01:29,520
takes in the state s and
then outputs an action.

24
00:01:29,520 --> 00:01:31,670
We'll call that action a and

25
00:01:31,670 --> 00:01:36,210
it affects the environment
in some way and changes it.

26
00:01:36,210 --> 00:01:41,870
Now this is a sort of circular process,
the action a is taken

27
00:01:41,870 --> 00:01:47,900
into the environment and the environment
then transitions to a new state.

28
00:01:47,900 --> 00:01:52,210
So T is this transition
function that takes in

29
00:01:52,210 --> 00:01:57,240
what its previous state was and
the action and moves to a new state.

30
00:01:57,240 --> 00:02:01,540
And that new state comes out,
boom, back into the robot.

31
00:02:01,540 --> 00:02:04,170
Robot looks at his policy,
action comes out.

32
00:02:05,270 --> 00:02:09,740
Now there's a question,
how do we arrive at this policy?

33
00:02:09,740 --> 00:02:11,300
How do we find pi?

34
00:02:12,380 --> 00:02:16,140
Well, that's what we're going to
spend a couple lessons on, but

35
00:02:16,140 --> 00:02:20,870
this whole puzzle is missing a piece and
that's the thing that helps us find pi.

36
00:02:20,870 --> 00:02:23,940
And part of that piece is well,

37
00:02:23,940 --> 00:02:28,002
there's this other part
called r which is the reward.

38
00:02:28,002 --> 00:02:35,335
So everytime the robot is in a
particular state and it takes an action.

39
00:02:35,335 --> 00:02:39,180
There's a particular reward associated

40
00:02:39,180 --> 00:02:44,300
with taking that action in that state
and that reward comes into the robot.

41
00:02:44,300 --> 00:02:46,870
And you can think of
the robot has having a little

42
00:02:46,870 --> 00:02:51,200
pocket where it keeps cash and
that's what that reward is.

43
00:02:51,200 --> 00:02:53,070
And the robot's objective is,

44
00:02:53,070 --> 00:02:58,360
over time, to take actions
that maximize this reward.

45
00:02:58,360 --> 00:03:02,280
And somewhere within the robot,
there's an algorithm that takes all this

46
00:03:02,280 --> 00:03:06,140
information over time to figure
out what that policy ought to be.

47
00:03:07,320 --> 00:03:09,290
So let me recap a little bit.

48
00:03:09,290 --> 00:03:11,830
S is the state of our environment and

49
00:03:11,830 --> 00:03:15,650
that's what the robot senses
in order to decide what to do.

50
00:03:15,650 --> 00:03:20,370
It uses its policy pi to figure
out what that action should be.

51
00:03:20,370 --> 00:03:24,830
And by the way,
pi can be a simple look up table.

52
00:03:24,830 --> 00:03:30,300
Over time, each time the robot takes
an action, it gets a reward and

53
00:03:30,300 --> 00:03:34,560
it's trying to find the pi that
will maximize its reward over time.

54
00:03:34,560 --> 00:03:40,720
Now in terms of trading, our environment
really is the market and our actions

55
00:03:40,720 --> 00:03:45,130
are actions we can take in the market,
like buying and selling or holding.

56
00:03:45,130 --> 00:03:50,870
S are factors about our stocks that
we might observe and know about.

57
00:03:50,870 --> 00:03:55,200
And r is the return we get for
making the proper trades.
