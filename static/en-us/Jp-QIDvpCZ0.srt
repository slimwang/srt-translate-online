1
00:00:00,120 --> 00:00:03,800
So at the bottom of this stack of
turtles is a turtle named Hoeffding and

2
00:00:03,800 --> 00:00:06,810
Hoeffding gives us the solution to
all of the rest of the turtles.

3
00:00:06,810 --> 00:00:08,950
There's actually a bunch of
different names for this.

4
00:00:08,950 --> 00:00:13,390
I've heard churn off,
I've heard other things, tail bounds,

5
00:00:13,390 --> 00:00:17,140
which you can see that the tale is
bound, I kind of forgot to draw it.

6
00:00:17,140 --> 00:00:20,910
That all kind of boiled
down to the same thing.

7
00:00:20,910 --> 00:00:24,860
So, I apologize to any person who may
have derived one of these bounds who I

8
00:00:24,860 --> 00:00:27,350
didn't show proper appreciation to.

9
00:00:27,350 --> 00:00:29,240
But, this is the general flavor of it.

10
00:00:29,240 --> 00:00:31,530
>> Well, I think the most important
thing is that you find someone,

11
00:00:31,530 --> 00:00:34,060
who did the derivation, who's Greek.

12
00:00:34,060 --> 00:00:35,350
Since all the turtles are greek.

13
00:00:35,350 --> 00:00:37,740
>> Except for Hoeffding,
as it turns out.

14
00:00:37,740 --> 00:00:38,980
So, here's the form of
the Hoeffding bound.

15
00:00:38,980 --> 00:00:43,030
So, Hoeffding bound says that if we
have a set of n random variables,

16
00:00:43,030 --> 00:00:44,740
X1 through Xn.

17
00:00:44,740 --> 00:00:47,770
And let's say that they're all iid,
which is independent and

18
00:00:47,770 --> 00:00:49,480
identically distributed.

19
00:00:49,480 --> 00:00:53,000
And each one has a mean of mu.

20
00:00:53,000 --> 00:00:55,500
And let's, so
let's say that we observe these values.

21
00:00:55,500 --> 00:00:57,270
So it's bunch of zero ones.

22
00:00:57,270 --> 00:01:00,850
Mu hat could be a good estimate of mu,
right?

23
00:01:00,850 --> 00:01:03,850
It's like add up all the variables that
you saw, all the zeroes and ones and

24
00:01:03,850 --> 00:01:05,360
divide by the number
of them that you saw.

25
00:01:05,360 --> 00:01:07,970
>> In other words,
the maximum likelihood mean.

26
00:01:07,970 --> 00:01:10,300
>> Yes, which is exactly
the maximum likelihood estimate.

27
00:01:10,300 --> 00:01:12,020
But so
it's the maximum likelihood estimate.

28
00:01:12,020 --> 00:01:17,660
But the bigger n gets, the more
confident we get as to how we are to it.

29
00:01:17,660 --> 00:01:20,620
So this gives us a way
of quantifying this.

30
00:01:20,620 --> 00:01:25,780
So the following is a 100 times 1 minus
delta percent confidence interval

31
00:01:25,780 --> 00:01:26,330
for mu.

32
00:01:26,330 --> 00:01:26,980
Right?

33
00:01:26,980 --> 00:01:28,450
So we don't know mu, but

34
00:01:28,450 --> 00:01:33,620
we know mu hat because we can get it
just by averaging these values together.

35
00:01:33,620 --> 00:01:36,150
We're going to say that the real mu

36
00:01:36,150 --> 00:01:41,740
is highly likely to be between mu
hat minus z delta over root n.

37
00:01:41,740 --> 00:01:44,710
And mu hat plus z delta over root n.

38
00:01:44,710 --> 00:01:49,180
Where z delta is the square root of
1/2 times log of two over delta.

39
00:01:49,180 --> 00:01:50,240
>> Oh right, that's obvious.

40
00:01:50,240 --> 00:01:53,510
>> Well, it's maybe not obvious but it
does have the right kind of properties.

41
00:01:53,510 --> 00:01:56,780
Like I don't think I could
justify this 2 without

42
00:01:56,780 --> 00:01:59,000
going through the detailed
derivation of the bound.

43
00:01:59,000 --> 00:02:01,370
But why is delta where it is?

44
00:02:01,370 --> 00:02:04,710
Well, the more confident
that we want to be.

45
00:02:04,710 --> 00:02:05,970
The smaller delta gets.

46
00:02:07,000 --> 00:02:09,180
And that means that this
quantity gets bigger.

47
00:02:09,180 --> 00:02:10,538
The log of that is still very big.

48
00:02:10,538 --> 00:02:11,940
The square root of
that is still very big.

49
00:02:11,940 --> 00:02:13,910
So this Z delta is very big.

50
00:02:13,910 --> 00:02:17,220
And so the width of this
bound ends up being very big.

51
00:02:17,220 --> 00:02:20,575
If we want to be really, really sure,
we need to give a big window, so

52
00:02:20,575 --> 00:02:22,795
that we're really sure
that we're in that window.

53
00:02:22,795 --> 00:02:25,545
But on the other hand,
if we have a lot of data,

54
00:02:25,545 --> 00:02:29,965
if we have a big n here,
that tends to make the bounds smaller.

55
00:02:29,965 --> 00:02:32,335
They shrink with the square root of n.

56
00:02:32,335 --> 00:02:37,255
So if we have a billion for n,
then it's going to be mu plus or

57
00:02:37,255 --> 00:02:40,525
minus some quantity,
divided by something really big.

58
00:02:40,525 --> 00:02:44,490
So it's going to be just a small
little interval around our estimate.

59
00:02:44,490 --> 00:02:45,840
So it has the right kind of properties.

60
00:02:45,840 --> 00:02:50,060
But the fact that it's a square root of
a log and there's a 2 and a one half,

61
00:02:50,060 --> 00:02:51,460
that's maybe not so obvious.

62
00:02:51,460 --> 00:02:52,510
>> Well, it looks a lot like.

63
00:02:53,550 --> 00:02:57,230
In fact, we use Hoeffding bounds
when we were doing pack learning.

64
00:02:57,230 --> 00:03:00,626
>> Yes, yes and again these learning
results that we were just talking about

65
00:03:00,626 --> 00:03:02,680
have a very pack kind of feel to them.

66
00:03:02,680 --> 00:03:03,620
So, it's maybe not so

67
00:03:03,620 --> 00:03:07,160
surprising that the bottom of the stack
of turtles is a turtle name Hoeffding.

68
00:03:07,160 --> 00:03:07,940
>> That's cool.
So basically,

69
00:03:07,940 --> 00:03:09,740
everything really does all tie together.

70
00:03:09,740 --> 00:03:11,640
>> Yes.
At least as long as you think about

71
00:03:11,640 --> 00:03:12,500
things this way.

72
00:03:12,500 --> 00:03:14,710
So, okay, but this isn't quite
answering our question, but

73
00:03:14,710 --> 00:03:16,800
this is going to be the key tool for
answering our question.

74
00:03:16,800 --> 00:03:20,460
So given that we have a way of
knowing how close we are to

75
00:03:20,460 --> 00:03:23,670
estimating a hidden
parameter from a sample.

76
00:03:23,670 --> 00:03:27,730
We can then use that to figure out how
many times do we have to pull an arm

77
00:03:27,730 --> 00:03:31,410
before we know fairly
accurately what its payoff is.

78
00:03:31,410 --> 00:03:32,040
And therefore,

79
00:03:32,040 --> 00:03:36,600
if we have K arms, how many times we
have to pull each of those arms, so

80
00:03:36,600 --> 00:03:40,590
that we are very confident that we find
something nearly optimal amongst them?

81
00:03:40,590 --> 00:03:41,230
>> Beautiful.

82
00:03:41,230 --> 00:03:43,420
I don't yet know how this is
going to solve everything, but

83
00:03:43,420 --> 00:03:45,310
I can see how it is a step
in the right direction.
