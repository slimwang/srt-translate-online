1
00:00:00,240 --> 00:00:02,840
Learning rate tuning
can be very strange.

2
00:00:02,840 --> 00:00:06,909
For example, you might think that using
a higher learning rate means you learn

3
00:00:06,909 --> 00:00:08,524
more or that you learn faster.

4
00:00:08,524 --> 00:00:10,260
That's just not true.

5
00:00:10,260 --> 00:00:14,270
In fact, you can often take a model,
lower the learning rate and

6
00:00:14,270 --> 00:00:16,379
get to a better model faster.

7
00:00:16,379 --> 00:00:17,558
It gets even worse.

8
00:00:17,558 --> 00:00:22,111
You might be tempted to look at the
curve that shows the loss over time to

9
00:00:22,111 --> 00:00:23,898
see how quickly you learn.

10
00:00:23,898 --> 00:00:27,325
Here the higher learning rate starts
faster, but then it plateaus,

11
00:00:27,325 --> 00:00:31,290
when the lower learning rate
keeps on going and gets better.

12
00:00:31,290 --> 00:00:35,410
It is a very familiar picture for
anyone who's trained neural networks.

13
00:00:35,410 --> 00:00:37,350
Never trust how quickly you learn.

14
00:00:37,350 --> 00:00:39,720
It has often little to do
with how well you train.

15
00:00:40,950 --> 00:00:44,540
This is where SGD gets its
reputation for being black magic.

16
00:00:44,540 --> 00:00:48,340
You have many, many hyper-parameters
that you could play with.

17
00:00:48,340 --> 00:00:52,510
Initialization parameters, learning
rate parameters, decay, momentum.

18
00:00:52,510 --> 00:00:54,870
And you have to get them right.

19
00:00:54,870 --> 00:00:56,820
In practice, it's not that bad.

20
00:00:56,820 --> 00:00:59,180
But if you have to
remember just one thing,

21
00:00:59,180 --> 00:01:03,760
it's that when things don't work, always
try to lower your learning rate first.

22
00:01:03,760 --> 00:01:06,140
There are lots of good solutions for
small models.

23
00:01:06,140 --> 00:01:09,830
But sadly, none that's completely
satisfactory so far for

24
00:01:09,830 --> 00:01:11,900
the very large models that
we really care about.

25
00:01:13,052 --> 00:01:17,640
I'll mention one approach called AdaGrad
that makes things a little bit easier.

26
00:01:17,640 --> 00:01:21,680
AdaGrad is a modification of SGD
which implicitly does momentum and

27
00:01:21,680 --> 00:01:23,620
learning rate decay for you.

28
00:01:23,620 --> 00:01:28,180
Using AdaGrad often makes learning
less sensitive to hyper-parameters.

29
00:01:28,180 --> 00:01:33,250
But it often tends to be a little worse
than precisely tuned SDG with momentum.

30
00:01:33,250 --> 00:01:34,540
It's still a very good option,

31
00:01:34,540 --> 00:01:36,535
though, if you're just trying
to get things to work.

32
00:01:37,595 --> 00:01:38,475
So, let's recap.

33
00:01:38,475 --> 00:01:42,195
We have this very simple linear
model which emits probabilities

34
00:01:42,195 --> 00:01:43,785
which we can use to classify things.

35
00:01:44,855 --> 00:01:47,715
We now know how to optimize
its parameters on lots and

36
00:01:47,715 --> 00:01:51,015
lots of data using SGD and its variants.

37
00:01:51,015 --> 00:01:53,615
It's still a linear,
shallow model, though, but

38
00:01:53,615 --> 00:01:55,775
now we have all the tools that we need.

39
00:01:55,775 --> 00:01:56,615
It's time to go deeper.
