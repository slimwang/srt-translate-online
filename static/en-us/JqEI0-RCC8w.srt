1
00:00:00,150 --> 00:00:03,380
So it's a little bit easier to
understand what these eigenvectors do,

2
00:00:03,380 --> 00:00:05,320
if we look at it this way.

3
00:00:05,320 --> 00:00:09,980
Here are the principle component
eigenvectors, one through whatever.

4
00:00:09,980 --> 00:00:12,980
Here's two different reconstructions,
all right.

5
00:00:12,980 --> 00:00:18,490
If I take the average face, and
I just add three times, and sigma

6
00:00:18,490 --> 00:00:22,390
k is just the standard deviation of
the coefficient, don't worry about that.

7
00:00:22,390 --> 00:00:23,980
If I take the average face and

8
00:00:23,980 --> 00:00:28,420
I add in some of that component,
I get this kind of thing.

9
00:00:28,420 --> 00:00:32,200
And if I subtract off that kind of
component, I get that thing.

10
00:00:32,200 --> 00:00:36,450
Notice, that that just moved
the lighting from the left to the right.

11
00:00:36,450 --> 00:00:39,170
So one way of thinking about
it is that these components,

12
00:00:39,170 --> 00:00:42,410
the first components just
change with the direction,

13
00:00:42,410 --> 00:00:44,670
lighting direction was from
the left to the right.

14
00:00:44,670 --> 00:00:47,770
Whereas, let's see,
let's take another one.

15
00:00:47,770 --> 00:00:51,340
Here, this component here,
if I add positive, I don't know,

16
00:00:51,340 --> 00:00:53,850
it tends to look a lot more feminine.

17
00:00:53,850 --> 00:00:57,120
When I go negative,
it tends to look a lot more masculine.

18
00:00:57,120 --> 00:01:00,260
Okay.
So each of these, different eigenvectors

19
00:01:00,260 --> 00:01:04,800
are adding in some other different
variation, some other different element.

20
00:01:04,800 --> 00:01:05,850
So, how do you use these?

21
00:01:05,850 --> 00:01:07,890
Well, a couple of things
we have to talk about.

22
00:01:07,890 --> 00:01:12,680
First is, I have to be able
to get the coefficients.

23
00:01:12,680 --> 00:01:15,230
Well, that's real easy,
just like I said before.

24
00:01:15,230 --> 00:01:18,350
I take in some image, x, here it is, and

25
00:01:18,350 --> 00:01:22,000
I just subtract off the mean,
the mean face.

26
00:01:22,000 --> 00:01:25,570
And I take the dot product with,
in this case, the first eigenvector,

27
00:01:25,570 --> 00:01:28,240
second, third,
all the way up to the kth eigenvector.

28
00:01:28,240 --> 00:01:31,790
Those dot products will give
me these coefficients, or

29
00:01:31,790 --> 00:01:34,660
weights, w1 through w k.

30
00:01:34,660 --> 00:01:38,710
What's cool is that vector of numbers,
just k of them, maybe it's 20,

31
00:01:38,710 --> 00:01:43,540
maybe it's 200, that's my entire
representation of this face.

32
00:01:43,540 --> 00:01:47,264
So I've taken this 10,000-dimensional
vector, 10,000 elements,

33
00:01:47,264 --> 00:01:51,560
10,000 pixels, and
I've reduced it to 20 numbers.

34
00:01:51,560 --> 00:01:57,450
So how do we make use of this vector,
these w's that are the coefficients?

35
00:01:57,450 --> 00:02:02,090
Well, the first thing is, I can actually
use them to reconstruct the face.

36
00:02:02,090 --> 00:02:02,620
Right?

37
00:02:02,620 --> 00:02:08,300
Each one of these w's tells me how much
of each eigenvector to add back in.

38
00:02:08,300 --> 00:02:11,740
And in general,
what I can do is I take a picture and

39
00:02:11,740 --> 00:02:16,325
I say this picture is represented
by the mean plus the w's,

40
00:02:16,325 --> 00:02:19,795
each of the w's times the eigenvector,
because that's what this is.

41
00:02:19,795 --> 00:02:25,025
And if I were to keep a lot of them, I
would get a really great reconstruction.

42
00:02:25,025 --> 00:02:28,305
If I keep some of them,
I get an okay reconstruction.

43
00:02:28,305 --> 00:02:30,825
Well, how many of them
do I want to keep?

44
00:02:30,825 --> 00:02:33,885
Well remember, I wasn't doing
this to do a reconstruction,

45
00:02:33,885 --> 00:02:35,145
I was trying to do recognition.
