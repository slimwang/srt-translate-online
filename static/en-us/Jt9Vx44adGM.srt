1
00:00:00,280 --> 00:00:05,930
We're now going to look at The Kullback-Liebler Divergence or Information.

2
00:00:05,930 --> 00:00:10,770
It's a general method to compare two probability density functions.

3
00:00:14,040 --> 00:00:19,730
Now let's assume we have two models, a model f which is given by a pdf and

4
00:00:19,730 --> 00:00:22,720
a model g, which is given by another pdf.

5
00:00:24,790 --> 00:00:30,280
We can calculate the information of the Kullback-Liebler Divergence

6
00:00:30,280 --> 00:00:34,420
between the model f and g using the following formula.

7
00:00:35,450 --> 00:00:42,620
Here we have taken the logarithm with the natural base of the ratio of f and g.

8
00:00:43,710 --> 00:00:48,650
The model g, can be estimated even by some parameter theta, and

9
00:00:48,650 --> 00:00:52,569
it could be the model that best estimates the model f.

10
00:00:53,630 --> 00:00:58,410
We can use this K-L information or K-L Divergence to

11
00:00:58,410 --> 00:01:03,440
measure the distance or difference between two models as well.

12
00:01:04,540 --> 00:01:09,510
We can think of the K-L Divergence as the information lost when g

13
00:01:09,510 --> 00:01:12,278
is used to approximate f.

14
00:01:12,278 --> 00:01:17,220
Sometimes the K-L Divergence is described as a distance.

15
00:01:17,220 --> 00:01:22,060
However, it's not strictly a distance, since the K-L Divergence between f and

16
00:01:22,060 --> 00:01:27,470
g is not the same as the K-L Divergence between g and f.

17
00:01:27,470 --> 00:01:30,960
So it is more of a directed distance.

18
00:01:30,960 --> 00:01:34,840
This function recognized that it's not symmetric.

19
00:01:36,190 --> 00:01:39,870
In statistical information theory, the K-L

20
00:01:39,870 --> 00:01:45,960
distance is considered one of the most fundamental of all information measures,

21
00:01:45,960 --> 00:01:50,160
in the sense of being derived from minimal of assumptions.

22
00:01:51,920 --> 00:01:56,630
In fact, you have seen the K-L distance before as

23
00:01:56,630 --> 00:02:00,480
relative entropy between two measurements in lesson two.

24
00:02:00,480 --> 00:02:05,580
We will not go into any more details of how we use the K-L Divergence or

25
00:02:05,580 --> 00:02:07,450
similar metrics.

26
00:02:07,450 --> 00:02:10,910
This subject is vast and we invite you to look into

27
00:02:10,910 --> 00:02:16,210
the reference material in the instructor's notes and discuss this in our forum.

28
00:02:17,410 --> 00:02:21,140
We'll now move on to estimating the kernel densities, for

29
00:02:21,140 --> 00:02:24,490
the various bandwidth we have introduced before.

30
00:02:24,490 --> 00:02:31,270
Take a look at the function called getAllKDE in the IPython note book.

31
00:02:31,270 --> 00:02:35,600
This function is used to generate various kernel density estimates.

32
00:02:35,600 --> 00:02:42,170
We use the Silverman's rule as well as various factors of Scott's Rule.

33
00:02:42,170 --> 00:02:46,240
We plot all of them together in the same plot.

34
00:02:46,240 --> 00:02:50,330
Once we have run the code above, we are going to use that function with our

35
00:02:50,330 --> 00:02:55,380
variable x to draw all of the kernel density estimated plots.

36
00:02:55,380 --> 00:02:57,180
Let's run this piece of code.

37
00:02:57,180 --> 00:03:01,320
In general, we have used various different bandwidths to

38
00:03:01,320 --> 00:03:06,110
make different kernel density estimates for variable x.

39
00:03:06,110 --> 00:03:12,160
The actual PDF, has been put in the red dashed lines using a histogram.

40
00:03:13,470 --> 00:03:15,350
In each of these cases,

41
00:03:15,350 --> 00:03:20,750
we have used the Gaussian kernel to do our density estimation.

42
00:03:20,750 --> 00:03:23,560
Now with a kernel density estimate,

43
00:03:23,560 --> 00:03:28,850
we have a general parameter free model of our data.

44
00:03:28,850 --> 00:03:31,910
We can now ask questions about probability.

45
00:03:31,910 --> 00:03:37,480
For example, we know values close to 0 are cases when medicare paid

46
00:03:37,480 --> 00:03:43,540
the health care provider the same, or close to the same amount that was charged.

47
00:03:43,540 --> 00:03:45,980
The rising peak here, to the right,

48
00:03:45,980 --> 00:03:51,060
shows that claims made by most providers, the faction of the total number of

49
00:03:51,060 --> 00:03:56,640
providers is given by the integral under this curve are denied by Medicare.

50
00:03:57,740 --> 00:04:02,750
So now we have formed a single variable kernel density estimate.

51
00:04:02,750 --> 00:04:06,480
We have looked at various ways to compare our estimates and

52
00:04:06,480 --> 00:04:10,300
make sure the values we are getting are optimal.

53
00:04:10,300 --> 00:04:15,560
We have also validated our univariate kernel density estimation model.

54
00:04:15,560 --> 00:04:18,540
We introduced mean integrated square error,

55
00:04:18,540 --> 00:04:23,860
as well as K-L Divergences to look for differences in PDFs.
