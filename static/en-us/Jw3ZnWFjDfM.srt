1
00:00:00,280 --> 00:00:02,570
Now, I actually snuck something important in here, I

2
00:00:02,570 --> 00:00:05,250
actually snuck two things that are important in here.

3
00:00:05,250 --> 00:00:08,250
The first is, the, what's called the Markovian property.

4
00:00:08,250 --> 00:00:10,070
Do you remember what the Markovian property is Michael?

5
00:00:10,070 --> 00:00:11,380
>> From what?

6
00:00:11,380 --> 00:00:15,390
>> From, I dunno. Actually, where does the Markovian property come from?

7
00:00:15,390 --> 00:00:16,740
>> I'm going to say Russia.

8
00:00:16,740 --> 00:00:19,330
>> Okay, yeah, from the, from the Russian.

9
00:00:19,330 --> 00:00:23,990
>> Yeah, so I have Russian ancestors, and they passed onto me this idea that.

10
00:00:25,210 --> 00:00:28,980
Markov means that you don't have to

11
00:00:28,980 --> 00:00:31,520
condition on anything past the most recent state.

12
00:00:31,520 --> 00:00:32,670
>> That's exactly right.

13
00:00:32,670 --> 00:00:36,290
>> The Markovian property is that only the present matters.

14
00:00:36,290 --> 00:00:39,100
>> And they had to pass that down to me you

15
00:00:39,100 --> 00:00:44,180
know, one generation at a time, because you know, Markov property.

16
00:00:44,180 --> 00:00:46,960
>> Exactly right, that was very good Michael. So what does this

17
00:00:46,960 --> 00:00:48,180
mean? What this means is our

18
00:00:48,180 --> 00:00:50,790
transition function, which shows you the probability

19
00:00:50,790 --> 00:00:53,570
you end up in some state as prime, given that you're in state

20
00:00:53,570 --> 00:00:59,400
S and took action A, only depends upon the current state S. If it

21
00:00:59,400 --> 00:01:03,050
also depended upon where you were 20 minutes before then, you would have

22
00:01:03,050 --> 00:01:06,930
to have more S's here. And then you would be violating the Markovian property.

23
00:01:06,930 --> 00:01:10,480
>> So is this like, do historians hate this?

24
00:01:10,480 --> 00:01:13,080
>> Well, you know, one never learns anything from history.

25
00:01:13,080 --> 00:01:16,860
>> No, you're supposed to learn from history, or you're doomed to,

26
00:01:16,860 --> 00:01:20,150
I don't know, let me make something up, repeat it.

27
00:01:20,150 --> 00:01:21,970
>> [LAUGH] Fair enough. Historians probably don't like

28
00:01:21,970 --> 00:01:24,130
this, but there is a way for mathematicians to

29
00:01:24,130 --> 00:01:25,880
convince them that they're okay with it. And

30
00:01:25,880 --> 00:01:27,960
the way that, you, mathematicians convince you that you're

31
00:01:27,960 --> 00:01:29,390
okay with this is to point out that

32
00:01:29,390 --> 00:01:33,420
you can turn almost anything, into a Markovian Process

33
00:01:33,420 --> 00:01:35,670
by simply making certain that your current state

34
00:01:35,670 --> 00:01:38,300
remembers everything you need to remember from the past.

35
00:01:38,300 --> 00:01:39,150
>> I see.

36
00:01:39,150 --> 00:01:41,940
>> So, in general, even if something isn't

37
00:01:41,940 --> 00:01:44,190
really Markovian, you need to know what you were, not only what

38
00:01:44,190 --> 00:01:46,900
you're doing now, but what you were doing five minutes ago. You could

39
00:01:46,900 --> 00:01:49,710
just turn your current state into what you're doing now and what you

40
00:01:49,710 --> 00:01:52,530
were doing five minutes ago. The obvious problem with that, of course, is

41
00:01:52,530 --> 00:01:55,580
that if you have to remember everything from the beginning of time.

42
00:01:55,580 --> 00:01:58,740
You're only going to see every state once and it's going to be very difficult

43
00:01:58,740 --> 00:02:03,260
to learn anything. But, that Markovian property turns out to be, turns out

44
00:02:03,260 --> 00:02:06,960
to be important and it actually allows us to solve these problems in

45
00:02:06,960 --> 00:02:09,660
a tractable way. But I snuck in something else, Michael.

46
00:02:09,660 --> 00:02:11,810
What I snuck in is this idea about the transition

47
00:02:11,810 --> 00:02:15,570
model, is that nothing ever changes. So this second property

48
00:02:15,570 --> 00:02:17,570
that matters for Markov decision processes, at least for the

49
00:02:17,570 --> 00:02:19,460
sets of things that we're going to be talking about in

50
00:02:19,460 --> 00:02:23,200
the beginning, is that things are stationary. That is these

51
00:02:23,200 --> 00:02:25,320
for the purposes of this discussion, it means that these

52
00:02:25,320 --> 00:02:30,480
rules don't change. Over time. That's one notion of stationary, okay?

53
00:02:30,480 --> 00:02:32,080
>> Does that mean that the agent can never leave

54
00:02:32,080 --> 00:02:33,620
the start state?

55
00:02:33,620 --> 00:02:35,080
>> No, the agent can leave the start state any

56
00:02:35,080 --> 00:02:37,650
time it takes the action, then it gets another start state.

57
00:02:37,650 --> 00:02:38,980
>> Then how is it, how is it stationary, then?

58
00:02:38,980 --> 00:02:42,480
>> It's not stationary, the world is stationary. The, the transition

59
00:02:42,480 --> 00:02:45,570
model is stationary, the physics are stationary, the rules don't change any.

60
00:02:45,570 --> 00:02:46,770
>> The rules don't change.

61
00:02:46,770 --> 00:02:47,190
>> Right.

62
00:02:47,190 --> 00:02:48,660
>> I, I, okay. I see.

63
00:02:48,660 --> 00:02:50,130
>> Then there's another notion of stationary

64
00:02:50,130 --> 00:02:51,169
that we'll see a little bit later.

65
00:02:52,500 --> 00:02:54,410
Okay, last thing to point out about

66
00:02:54,410 --> 00:02:56,980
the definition of a mark on decision process,

67
00:02:56,980 --> 00:03:02,310
is the notion of reward. So, reward is simply a

68
00:03:02,310 --> 00:03:05,940
scalr value, that you get for being in a state. So

69
00:03:05,940 --> 00:03:09,140
for example we might say, you know, this green goal

70
00:03:09,140 --> 00:03:11,670
is a really good goal. And so, if you get there,

71
00:03:11,670 --> 00:03:13,650
we're going to give you a dollar. This red dual,

72
00:03:13,650 --> 00:03:15,570
on the other hand, is a very, very bad state. We

73
00:03:15,570 --> 00:03:17,330
don't want you to end there. And so, if you end

74
00:03:17,330 --> 00:03:19,160
up there we're going to take away a dollar from you.

75
00:03:19,160 --> 00:03:20,490
>> What if I don't have a dollar?

76
00:03:20,490 --> 00:03:22,010
>> Someone will give you the dollar.

77
00:03:22,010 --> 00:03:23,140
The universe will give you the dollar.

78
00:03:23,140 --> 00:03:25,000
>> [LAUGH] And then take it away?

79
00:03:25,000 --> 00:03:27,580
>> Or, the universe will take it away. Even

80
00:03:27,580 --> 00:03:30,080
if you don't have it. You'll have negative dollars.

81
00:03:30,080 --> 00:03:30,510
>> Oh, man.

82
00:03:30,510 --> 00:03:33,450
>> Okay, so Reward is very important here in a of couple

83
00:03:33,450 --> 00:03:38,160
ways. Reward is one of the things that, as I'm always harping on

84
00:03:38,160 --> 00:03:41,950
encompasses our domain knowledge. So, the Rewards you get from the state tells

85
00:03:41,950 --> 00:03:47,020
you the usefulness of entering into that state. Now. I wrote out three

86
00:03:47,020 --> 00:03:50,030
different definitions of r here, because sometimes it's very useful to think

87
00:03:50,030 --> 00:03:52,460
about them differently. I've been talking about the reward you get for entering

88
00:03:52,460 --> 00:03:55,610
into the state, but there's also a notion of reward that you get

89
00:03:55,610 --> 00:03:59,210
for entering into a state and taking an action. There's a rewar, or,

90
00:03:59,210 --> 00:04:02,040
being in a state and taking an action, there's a reward that you

91
00:04:02,040 --> 00:04:05,220
could get for being in a state, taking an action, and then ending

92
00:04:05,220 --> 00:04:08,610
up in another state as prime. It turns out these are all mathematically

93
00:04:08,610 --> 00:04:12,390
equivalent. But often it's easier to think about one form or the other.

94
00:04:12,390 --> 00:04:16,760
But for the purposes of the, you know, for the rest of this discussion,

95
00:04:16,760 --> 00:04:19,860
really you can just focus on that one, the reward of the value entering

96
00:04:19,860 --> 00:04:21,959
into a state. And those four things,

97
00:04:21,959 --> 00:04:24,340
by themselves, along with this Markov property

98
00:04:24,340 --> 00:04:27,010
and non-stationarity, defines what's called the Markov

99
00:04:27,010 --> 00:04:29,830
Decision Process. Or an MDP. Got it?

100
00:04:29,830 --> 00:04:34,120
>> I'm a little stuck on the, how those could be mathematically equivalent.

101
00:04:34,120 --> 00:04:37,140
>> Well we'll get to that later. Would you like a little bit of intuition?

102
00:04:37,140 --> 00:04:38,030
>> Sure.

103
00:04:38,030 --> 00:04:40,400
>> Well, you can imagine that just as

104
00:04:40,400 --> 00:04:42,530
before we were dealing with the the notion

105
00:04:42,530 --> 00:04:45,730
of making a non-Markovian thing Markovian by putting

106
00:04:45,730 --> 00:04:47,240
a little bit of history into your state.

107
00:04:47,240 --> 00:04:50,810
You can always fold in the action that you took to be in a state or

108
00:04:50,810 --> 00:04:53,770
the action that you took to get to a state, as a part of your state.

109
00:04:53,770 --> 00:04:57,290
>> But that would be a different Markov Decision Process.

110
00:04:57,290 --> 00:05:00,020
>> It would, but they would work out to have the same solution.

111
00:05:00,020 --> 00:05:01,100
>> Oh, I see.
