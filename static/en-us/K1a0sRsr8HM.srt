1
00:00:00,400 --> 00:00:04,960
Let us look at another example of decision tree learning. Here is a data set of

2
00:00:04,960 --> 00:00:08,350
people who go to the beach, and some of them get sunburned, and

3
00:00:08,350 --> 00:00:13,090
others don't. In this data set, there are nine examples and

4
00:00:13,090 --> 00:00:17,640
each example is characterized by four features, hair, height, age and lotion.

5
00:00:17,640 --> 00:00:21,920
Once again, how can we construct an optimal decision tree that they classify all

6
00:00:21,920 --> 00:00:27,270
of those examples? One possible idea is to discriminate first on hair color.

7
00:00:27,270 --> 00:00:30,690
Hair color classifies all of these known examples into three categories,

8
00:00:30,690 --> 00:00:35,340
brown, red and blonde. The interesting thing about the choice of hair color is

9
00:00:35,340 --> 00:00:40,000
that in the case of brown, all of these sunburnt cases are negative. People with

10
00:00:40,000 --> 00:00:44,830
brown hair apparently don't get sunburned. In case of all the red haired people,

11
00:00:44,830 --> 00:00:49,620
there is sunburn. So hair color is a good choice for picking as a feature to

12
00:00:49,620 --> 00:00:53,870
discriminate on because it classifies things in such a way that some of

13
00:00:53,870 --> 00:00:58,540
the categories have only negative instances and no positive instances. And

14
00:00:58,540 --> 00:01:03,260
some of the categories are only positive instances and no negative instances.

15
00:01:03,260 --> 00:01:06,720
Of course, that still leaves blonde-haired people. In this case,

16
00:01:06,720 --> 00:01:10,270
there are both some positive instances and some negative instances, and

17
00:01:10,270 --> 00:01:13,600
therefore, will need another feature to discriminate between the positive and

18
00:01:13,600 --> 00:01:18,890
the negative instances. Here, lotion might be the second feature that we pick.

19
00:01:18,890 --> 00:01:22,690
Lotion now classifies the remaining examples into two categories, some people

20
00:01:22,690 --> 00:01:27,730
used lotion, other people did not. Those who used lotion did not get sunburnt.

21
00:01:27,730 --> 00:01:31,980
Those who did not use lotion did get sunburn. Once again, these are all

22
00:01:31,980 --> 00:01:36,080
negative instances. These are consisting of only positive instances.

23
00:01:36,080 --> 00:01:40,150
Thus, in this decision tree, simply by using two features, we were able

24
00:01:40,150 --> 00:01:44,450
to classify all of these nine examples. This is a different decision tree for

25
00:01:44,450 --> 00:01:49,340
this same data set. But because we use a different order, therefore, now we have

26
00:01:49,340 --> 00:01:55,040
to do more work. This decision tree is less optimal than the previous one.

27
00:01:55,040 --> 00:01:58,450
We could have chosen a different set of features in a different order. Perhaps,

28
00:01:58,450 --> 00:02:02,550
we could first discriminate on height then on hair color and age. In this case,

29
00:02:02,550 --> 00:02:08,600
we did a much bushier tree. Clearly, this tree is less optimal than this one.

30
00:02:08,600 --> 00:02:11,110
Note the trade off with the decision tree learning and

31
00:02:11,110 --> 00:02:13,947
discrimination tree learning that we covered in case-based reasoning.

32
00:02:14,960 --> 00:02:19,550
Decision tree learning leads to more optimal classification trees. But

33
00:02:19,550 --> 00:02:24,910
there is a requirement. You need all the examples right up front. Discrimination

34
00:02:24,910 --> 00:02:29,410
tree learning may lead to suboptimal trees, but you can learn incrementally.
