1
00:00:00,000 --> 00:00:03,000
We'll start by talking about language models.

2
00:00:03,000 --> 00:00:07,000
Historically, there have been two types of models that have been popular

3
00:00:07,000 --> 00:00:10,000
for natural language understanding within AI.

4
00:00:10,000 --> 00:00:16,000
One of the types of models has to do with sequences of letters or words?

5
00:00:16,000 --> 00:00:20,000
These types of models tend to be probabilistic

6
00:00:20,000 --> 00:00:24,000
in that we're talking about the probability of a sequence,

7
00:00:24,000 --> 00:00:30,000
word based in that mostly what we're dealing with is the surface words themselves,

8
00:00:30,000 --> 00:00:33,000
and sometimes letters.

9
00:00:33,000 --> 00:00:37,000
But we're dealing with the actual data of what we see,

10
00:00:37,000 --> 00:00:39,000
Rather than some underlying extractions,

11
00:00:39,000 --> 00:00:44,000
and these models are primarily learned from data.

12
00:00:44,000 --> 00:00:50,000
Now, in contrast to that is another type of model that you might have seen before,

13
00:00:50,000 --> 00:00:54,000
where we're primarily dealing with trees and with abstract structures.

14
00:00:54,000 --> 00:01:01,000
So we say we can have a sentence, which is composed of a noun phrase and a verb phrase,

15
00:01:01,000 --> 00:01:07,000
and a noun phrase might be a person's name, and that might be "Sam."

16
00:01:07,000 --> 00:01:14,000
And the verb phrase might be a verb and we might say "Sam slept"--

17
00:01:14,000 --> 00:01:16,000
a very simple sentence.

18
00:01:16,000 --> 00:01:20,000
Now, these types of models have different properties.

19
00:01:20,000 --> 00:01:25,000
For one, they tend to be logical rather than probabilistic--

20
00:01:25,000 --> 00:01:32,000
that is whereas on this side, we're talking about the probability of a sequence of words,

21
00:01:32,000 --> 00:01:40,000
on this side we're talking about a set of sentences and that set defines the language,

22
00:01:40,000 --> 00:01:44,000
and a sentence is either in the language or not.

23
00:01:44,000 --> 00:01:50,000
It's a Boolean logical distinction rather than on this side a probabilistic distinction.

24
00:01:50,000 --> 00:01:57,000
These models are based on abstraction such as trees and categories--

25
00:01:57,000 --> 00:02:02,000
categories like noun phrase and verb phrase and tree structures like this

26
00:02:02,000 --> 00:02:08,000
that don't actually occur in the surface form, so the words that we can observe.

27
00:02:08,000 --> 00:02:12,000
An agent can observe the words "Sam" and "slept,"

28
00:02:12,000 --> 00:02:19,000
but an agent can't directly observe the fact that slept is a verb or that it's part of this tree structure.

29
00:02:19,000 --> 00:02:25,000
Traditionally, these types of approaches have been primarily hand-coded.

30
00:02:25,000 --> 00:02:29,000
That is, rather than learning this type of structure from data,

31
00:02:29,000 --> 00:02:35,000
we learn it by going out and having linguists and other experts write down these rules.

32
00:02:35,000 --> 00:02:39,000
Now, these distinctions are not hard to cut.

33
00:02:39,000 --> 00:02:45,000
You could have trees and have a probabilistic model of them.

34
00:02:45,000 --> 00:02:48,000
You could learn trees.

35
00:02:48,000 --> 99:59:59,999
We can go back and forth, but traditionally the two camps have divided up in this way.
