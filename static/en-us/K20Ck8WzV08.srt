1
00:00:00,150 --> 00:00:03,740
So last time we were talking about
how contraction mappings make it so

2
00:00:03,740 --> 00:00:05,670
that value iteration converges you know,

3
00:00:05,670 --> 00:00:09,120
in the limit that we end up
solving the Bellman equations.

4
00:00:09,120 --> 00:00:11,550
I'm not going to want to say too
much more about value iteration,

5
00:00:11,550 --> 00:00:15,080
but on one slide I want to at least
summarize a couple interesting things

6
00:00:15,080 --> 00:00:16,360
that are worth knowing.

7
00:00:16,360 --> 00:00:17,470
All right, so here's the first one.

8
00:00:17,470 --> 00:00:20,704
If you think about QT star,
so that's the Q function

9
00:00:20,704 --> 00:00:24,610
that we get if we run value
iteration for T star iterations.

10
00:00:24,610 --> 00:00:25,110
>> Okay.

11
00:00:25,110 --> 00:00:26,960
>> We know that it
converges in the limit.

12
00:00:26,960 --> 00:00:30,510
We know that QT eventually
goes to Q star.

13
00:00:30,510 --> 00:00:35,220
But here's kind of an interesting thing
that we know about the finite horizon.

14
00:00:35,220 --> 00:00:40,680
So there is some T star,
less than infinity, that's polynomial in

15
00:00:40,680 --> 00:00:44,640
the number of states, the number of
actions, the magnitude of the rewards in

16
00:00:44,640 --> 00:00:48,600
the reward function, the number of bits
of precision that are used to specify

17
00:00:48,600 --> 00:00:52,460
the transition probabilities,
and one over one minus gamma.

18
00:00:52,460 --> 00:00:57,007
So that, if we run valuation for
that many steps,

19
00:00:57,007 --> 00:01:01,470
the Q function that we get
out is Q sub T star of SA.

20
00:01:01,470 --> 00:01:03,870
If we define a policy, pi SA,

21
00:01:03,870 --> 00:01:07,170
which is just the greedy policy
with respect to that Q function.

22
00:01:07,170 --> 00:01:09,400
That policy is optimal, okay?

23
00:01:09,400 --> 00:01:11,710
So what we're saying here is
that we know that in the limit,

24
00:01:11,710 --> 00:01:13,540
if we run value iteration for

25
00:01:13,540 --> 00:01:17,920
an infinite number of steps, then the Q
function that we get out at that point,

26
00:01:17,920 --> 00:01:21,800
the greedy policy with respect
to that Q function, is optimal.

27
00:01:21,800 --> 00:01:24,880
But what this is saying is that
there is some time before infinity

28
00:01:24,880 --> 00:01:28,930
where we get a Q function
that's close enough, so

29
00:01:28,930 --> 00:01:32,600
that if you do the greedy policy with
respect to it, it really is optimal.

30
00:01:32,600 --> 00:01:34,630
Like all the way 100% optimal.

31
00:01:34,630 --> 00:01:37,870
>> So what that really
means it's polynomial and

32
00:01:37,870 --> 00:01:40,270
the things you would expect to matter.

33
00:01:40,270 --> 00:01:42,800
And so that's saying that you
can actually solve this in

34
00:01:42,800 --> 00:01:44,250
a reasonable amount of time.

35
00:01:44,250 --> 00:01:44,950
>> Yeah.

36
00:01:44,950 --> 00:01:47,220
>> I mean any time I see the word
polynomial, I'm like okay,

37
00:01:47,220 --> 00:01:49,300
what you're saying is,
this is reasonable.

38
00:01:49,300 --> 00:01:51,320
>> Yeah,
now the trick in this particular think.

39
00:01:51,320 --> 00:01:55,220
The reason it's not just polynomial
is this one over one minus gamma.

40
00:01:55,220 --> 00:01:58,920
So as gamma gets close to one,
this blows up and that's not

41
00:01:58,920 --> 00:02:03,440
polynomial bounded in say the number of
bits that it takes to write down gamma.

42
00:02:03,440 --> 00:02:07,590
So it really is exponential in terms
of the number of bits it takes to write

43
00:02:07,590 --> 00:02:08,475
down the whole problem.

44
00:02:08,475 --> 00:02:09,258
>> Mm.

45
00:02:09,258 --> 00:02:13,030
>> But even so, if you take
gamma to be some fixed constant.

46
00:02:13,030 --> 00:02:16,570
Then one over one minus gamma might be
really big, but it's some fixed constant

47
00:02:16,570 --> 00:02:18,750
and we're polynomial in all
the rest of this stuff.

48
00:02:18,750 --> 00:02:23,680
So, what this really all boils down to
is the idea that once you fix an MDP and

49
00:02:23,680 --> 00:02:26,230
the number of bits of precision
that your using to write it down,

50
00:02:26,230 --> 00:02:28,390
then there's going to be
some optimal action and

51
00:02:28,390 --> 00:02:31,590
the might be other actions that are tied
for optimal in any given state.

52
00:02:31,590 --> 00:02:34,760
But the second best
action is going to be

53
00:02:34,760 --> 00:02:38,280
some actual bounded
amount away from optimal.

54
00:02:38,280 --> 00:02:40,530
So, it can't get arbitrarily
close to optimal.

55
00:02:40,530 --> 00:02:42,330
It's going to be some distance away.

56
00:02:42,330 --> 00:02:45,400
And what that gives us is that when
we run value iteration enough,

57
00:02:45,400 --> 00:02:47,730
eventually the distance
between the best action and

58
00:02:47,730 --> 00:02:50,520
the second best action
gets bigger than that gap.

59
00:02:51,560 --> 00:02:55,470
And once it's bigger than that gap, then
the greedy policy's going to be optimal.

60
00:02:55,470 --> 00:02:59,140
It's going to choose the actual
optimal action in all states.

61
00:02:59,140 --> 00:03:02,150
>> So VI converges and it converges
in a reasonable amount of time.

62
00:03:02,150 --> 00:03:04,660
>> It's not that it converges
in a reasonable amount of time.

63
00:03:04,660 --> 00:03:07,350
The greedy policy with
respect to value iteration

64
00:03:07,350 --> 00:03:09,600
converges in kind of
a reasonable amount of time.

65
00:03:09,600 --> 00:03:10,820
>> Okay.
That's a much better way of saying it.

66
00:03:10,820 --> 00:03:11,330
Okay.

67
00:03:11,330 --> 00:03:12,120
I buy that.

68
00:03:12,120 --> 00:03:14,170
>> This turns about to be
a consequence of Cramer's rule,

69
00:03:14,170 --> 00:03:15,270
which we're not going to talk about.

70
00:03:15,270 --> 00:03:19,460
But this is why, once we write down
everything with polynomial precision,

71
00:03:19,460 --> 00:03:23,370
that we are guaranteed to get some
fixed size gap, between the best and

72
00:03:23,370 --> 00:03:24,050
the second best.

73
00:03:24,050 --> 00:03:26,916
>> It always amazes me how you manage
to get Seinfeld into our conversations.

74
00:03:26,916 --> 00:03:29,022
>> [LAUGH] Kramer rules.

75
00:03:29,022 --> 00:03:29,962
>> [LAUGH]
