1
00:00:00,240 --> 00:00:03,710
Now that we've walked through solving
our original problem together,

2
00:00:03,710 --> 00:00:06,020
here's a new problem for
you to solve on your own.

3
00:00:06,020 --> 00:00:09,800
This will let you practice a bit more
with building MDPs in Burlap and

4
00:00:09,800 --> 00:00:11,910
let you self-assess your understanding.

5
00:00:11,910 --> 00:00:15,630
In this MDP we will
again begin in state S0.

6
00:00:15,630 --> 00:00:21,690
From state S0 we will have a choice of
two actions, action a and action b.

7
00:00:21,690 --> 00:00:25,200
Action a will have two
possible outcomes.

8
00:00:25,200 --> 00:00:33,195
With probability p1, we will loop back
into State S0 and obtain a reward of -1.

9
00:00:33,195 --> 00:00:40,410
With probability 1-p1, we will enter
state S1 and obtain a reward of +3.

10
00:00:40,410 --> 00:00:45,420
If we take action b from S0,
we will deterministically enter state

11
00:00:45,420 --> 00:00:49,370
S2 and we will obtain the reward +1.

12
00:00:49,370 --> 00:00:53,760
S2 has a single action that
deterministically takes us

13
00:00:53,760 --> 00:00:56,410
to state S1 with zero reward.

14
00:00:56,410 --> 00:01:00,170
At state S1 we have
another choice of actions.

15
00:01:00,170 --> 00:01:06,060
Action c will be stochastic and
action d will be deterministic.

16
00:01:06,060 --> 00:01:07,040
With probability p2,

17
00:01:08,140 --> 00:01:13,870
action c from S1 will take us to
the terminal state S5, with zero reward.

18
00:01:13,870 --> 00:01:19,870
And with probability 1-p2,
action c will take us into state S3 and

19
00:01:19,870 --> 00:01:22,110
we'll obtain a reward of +1.

20
00:01:22,110 --> 00:01:26,970
From S3 our only option is to
deterministically reenter S1

21
00:01:26,970 --> 00:01:29,290
with zero reward.

22
00:01:29,290 --> 00:01:34,250
If we take action d,
we enter S4 with probability 1,

23
00:01:34,250 --> 00:01:37,130
obtaining the reward +2.

24
00:01:37,130 --> 00:01:39,750
The only action available at state S4

25
00:01:39,750 --> 00:01:44,850
is to deterministically enter
the terminal state S5 with zero reward.

26
00:01:44,850 --> 00:01:49,510
Your problem will be to determine,
based on the parameters p1 and p2 and

27
00:01:49,510 --> 00:01:54,760
the discount factor gamma, the optimal
actions to take at states S0 and S1.

28
00:01:55,890 --> 00:02:00,690
Using the provided class SecondMDP,
fill in the constructor method and

29
00:02:00,690 --> 00:02:02,520
the method bestActions.

30
00:02:02,520 --> 00:02:05,690
Please feel free to add whatever
fields and helper methods or

31
00:02:05,690 --> 00:02:08,270
classes you need to solve the problem.

32
00:02:08,270 --> 00:02:11,620
And as always,
the Burlap javadocs will be your friend.

33
00:02:11,620 --> 00:02:12,120
Good luck.
