1
00:00:00,820 --> 00:00:04,820
You did it again, congratulations, your third machine algorithm decision trees,

2
00:00:04,820 --> 00:00:06,230
you got all the way to the end.

3
00:00:06,230 --> 00:00:07,810
>> We've learned a lot about decision trees.

4
00:00:07,810 --> 00:00:11,180
Let's add some of this new found knowledge to our list of things to

5
00:00:11,180 --> 00:00:13,190
consider when you're picking a classifier.

6
00:00:13,190 --> 00:00:16,055
>> So they're really easy to use and they're beautiful to grow on.

7
00:00:16,055 --> 00:00:18,155
They're, they're graphically, in some sense,

8
00:00:18,155 --> 00:00:20,111
allow you to interpret the data really well, and

9
00:00:20,111 --> 00:00:23,563
you can really understand them much better then say the result of a support vector machine.

10
00:00:23,563 --> 00:00:24,821
But they also have limitations.

11
00:00:24,821 --> 00:00:25,750
>> That's right.

12
00:00:25,750 --> 00:00:27,585
One of the things that's true about decision trees is

13
00:00:27,585 --> 00:00:29,090
they're prone to over fitting.

14
00:00:29,090 --> 00:00:31,880
Especially if you have data that has lots and lots of features and

15
00:00:31,880 --> 00:00:34,580
a complicated decision tree it can over fit the data.

16
00:00:34,580 --> 00:00:37,600
So you have to be careful with the parameter tunes that you're picking when you

17
00:00:37,600 --> 00:00:39,320
use the decision tree to prevent this from happening.

18
00:00:39,320 --> 00:00:41,300
>> Yeah. What comes out could look crazy if your node has,

19
00:00:41,300 --> 00:00:42,950
only did single data point.

20
00:00:42,950 --> 00:00:44,360
You almost always over-fit.

21
00:00:44,360 --> 00:00:47,150
So it is really important for you to measure how well you're doing,

22
00:00:47,150 --> 00:00:50,300
then stop the growth of the tree at the appropriate time.

23
00:00:50,300 --> 00:00:52,420
>> One of the things that's also really cool about decision trees,

24
00:00:52,420 --> 00:00:55,180
though, is that you can build bigger classifiers out of

25
00:00:55,180 --> 00:00:58,360
decision trees in something called ensemble methods.

26
00:00:58,360 --> 00:00:59,060
In the next lesson,

27
00:00:59,060 --> 00:01:02,310
we'll give you a chance to actually explore an algorithm completely on your own.

28
00:01:02,310 --> 00:01:05,600
And a couple of the ones that will give you as choices are examples of

29
00:01:05,600 --> 00:01:06,280
ensemble methods.

30
00:01:06,280 --> 00:01:07,550
So if this sounds interesting to you.

31
00:01:07,550 --> 00:01:11,413
Building a classifier out of classifier, then stay tuned in the next lesson.

32
00:01:11,413 --> 00:01:15,820
>> Yeah, we are super, duper So stay tuned.

33
00:01:15,820 --> 00:01:16,540
>> Let's get started.
