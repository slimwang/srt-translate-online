1
00:00:00,500 --> 00:00:04,566
(Peter) Welcome to office hours. You guys are doing a great job.

2
00:00:04,567 --> 00:00:08,899
And some really interesting questions this week, so let's get right to them.

3
00:00:08,900 --> 00:00:15,799
First one's from Jonathan in New York. He says, "Some websites represent a Captcha to type text that us blurred

4
00:00:15,800 --> 00:00:24,532
"or otherwise obscured, to let you in and make sure you're not a computer robot. Could you comment on this as an A.I. challenge?"

5
00:00:24,533 --> 00:00:29,732
So I think you're right, Jonathan, that that is a real A.I. challenge, and in fact,

6
00:00:29,733 --> 00:00:37,799
(Chander) Malik, one of my old colleagues at Berkeley who wrote some of the computer vision chapter in the textbook,

7
00:00:37,800 --> 00:00:42,132
did this with his student, Greg Mori. So it's a great challenge.

8
00:00:42,133 --> 00:00:50,732
They were able to do pretty well. They somewhere between 30 and 90% accuracy, depending on exactly what test you want to give them.

9
00:00:50,733 --> 00:00:57,566
And of course, even 30% is enough to foil these tests, because they allow you to try two or three or four times.

10
00:00:57,567 --> 00:01:06,466
So you could always get it right. But it does provide a lot of insight into what's difficult and kind of pushes things forward,

11
00:01:06,467 --> 00:01:11,466
and there's a never-ending battle to try to improve.

12
00:01:11,467 --> 00:01:16,866
(Sebastian) Yeah, and some of these Captchas are easily solved, really easy to decode, and some are really hard.

13
00:01:16,867 --> 00:01:24,666
My friend, Louis Menand, who runs a company in this space, used words from the New York Times archives as Captcha words,

14
00:01:24,667 --> 00:01:29,499
and specifically those that OCR--optical character recognition--failed to decode.

15
00:01:29,500 --> 00:01:35,499
And as a result, he not only solves the Captcha problem, with really hard examples that are really hard to solve,

16
00:01:35,500 --> 00:01:39,732
but also transcribes the old annals of the New York Times, which is really cool.

17
00:01:39,733 --> 00:01:45,832
So if you can solve this, let us know. Don't tell anybody else. I'd love to see a solution for the general Captcha problem,

18
00:01:45,833 --> 00:01:48,499
it would be quite amazing.

19
00:01:48,500 --> 00:01:55,399
The next question: "What do you think of OpenCV or other existing libraries as frameworks for doing (practical) computer vision?"

20
00:01:55,400 --> 00:02:01,766
By J&amp;M from Romania. I think it's a great idea. My good friend, Gary Bradski, of course, wrote OpenCV

21
00:02:01,767 --> 00:02:07,432
with the intent that everybody can use it. It's been a very powerful library. It's very fast, very efficient.

22
00:02:07,433 --> 00:02:12,132
Reasonably easy to program. There's multiple generations of interfaces, but once you get beyond, this is actually really useful,

23
00:02:12,133 --> 00:02:16,866
and it's really state of the art computer vision algorithms. I highly recommend using it in computer vision.

24
00:02:16,867 --> 00:02:21,966
It's an exercise in learning value to code stuff yourself, but when you do something practical, use it.

25
00:02:21,967 --> 00:02:24,699
There's other libraries as well. My favorite is actually MATLAB.

26
00:02:24,700 --> 00:02:30,832
Even though it's not meant for practical use in a highly efficient work environment, it's really amazing for rapid prototyping

27
00:02:30,833 --> 00:02:33,332
and the algorithms are really really well-coded.

28
00:02:33,333 --> 00:02:38,199
(Peter) "So in the lecture for stereo cameras, you discussed two cameras displaced in the horizontal plane.

29
00:02:38,200 --> 00:02:42,966
"Why not three cameras?" This comes from Johnny Boates in Philips, Maine. So is three better than two?

30
00:02:42,967 --> 00:02:49,132
(Sebastian) Why don't we have three eyes? You have two eyes, I have two eyes. Seems to me something magic about two eyes.

31
00:02:49,133 --> 00:02:55,232
The answer is it's actually useful to have three cameras. It turns out three cameras fail in horizontal lines.

32
00:02:55,233 --> 00:03:01,199
Now, nature isn't full of that many horizontal lines that really matter. Vertical lines are much more prevalent, like trees and so on.

33
00:03:01,200 --> 00:03:05,599
You can run into them. But that's probably the reason why nature didn't give us three eyes.

34
00:03:05,600 --> 00:03:10,732
In reality, there's a field called triocular vision, where people use three or more cameras,

35
00:03:10,733 --> 00:03:15,766
and they're arranged usually in an L shape, so they can see depth, and this continues in different directions,

36
00:03:15,767 --> 00:03:21,332
and they tend to be superior, but they take more processing power, so they're not very frequently used.

37
00:03:21,333 --> 00:03:26,766
(Peter) Yeah, and I should say in audio processing it is common to use a large array of microphones,

38
00:03:26,767 --> 00:03:33,032
and then you can zero in on exactly where a sound source is coming from, and from a crowded room you can pick out

39
00:03:33,033 --> 00:03:37,166
the one person that's talking and eliminate all the crowd noise.

40
00:03:37,167 --> 00:03:41,799
(Sebastian) And honestly, this is still a research problem. If you're able to make a microphone array that can localize

41
00:03:41,800 --> 00:03:49,166
a person and cut that out in a crowded room, then let me know, it's going to be really amazing, and it's going to be really useful.

42
00:03:49,167 --> 00:03:53,066
(Peter) So next question is from Nagwater in Los Angeles.

43
00:03:53,067 --> 00:03:57,932
"We learned about depth from sets of images, but what about using a single image?"

44
00:03:57,933 --> 00:04:02,066
(Sebastian) That's a wonderful question. It's a way under-researched field even though it's relatively old.

45
00:04:02,067 --> 00:04:09,666
Actually, single images have depth cues, like perspective, for example, (unintelligible) gets smaller at distance.

46
00:04:09,667 --> 00:04:13,699
Known objects, like you know how big people are, that really helps you understand how far things are away.

47
00:04:13,700 --> 00:04:19,298
And we all know that people can do this. We can look at a photograph and we can recover depth quite well.

48
00:04:19,300 --> 00:04:26,166
This is a beautiful field. Only two recent projects that I know of that deal with this, one at Carnegie Mellon, one at Stanford.

49
00:04:26,167 --> 00:04:32,932
But there's much more opportunity. So if you want to apply interesting A.I. technology, this is a great field to be in.

50
00:04:32,933 --> 00:04:39,566
A second though is even in human vision, we use stereo at near range because that's where the displacement works really well,

51
00:04:39,567 --> 00:04:46,432
but if you look really far away, like 12 feet or four meters, for my international friends, there's no stereo vision left.

52
00:04:46,433 --> 00:04:52,499
So we do all the depth recovery at a range, from motion cues, from parallax, move our head around.

53
00:04:52,500 --> 00:04:56,699
And of course, single images, we understand how big things are in the world.

54
00:04:56,700 --> 00:05:01,399
(Peter) "Running dynamic programming to find the optimal alignment is highly dependent on the cost of occlusion.

55
00:05:01,400 --> 00:05:06,166
"And bad correspondence pixels. How can we estimate the penalties for occlusion and bad correspondence?"

56
00:05:06,167 --> 00:05:11,632
(Sebastian) Well, that's a machine learning question, and it's a beautiful question because it's a really easy answer,

57
00:05:11,633 --> 00:05:20,732
but it's surprising, the simplicity of the answer: Which is, go and take images where you manually label the correct correspondence

58
00:05:20,733 --> 00:05:29,366
as good as you can. And then write a computer program that has costs assumed for occlusion and costs of the type we talked about here.

59
00:05:29,367 --> 00:05:34,432
And run it and see how well it does, and score how well it does. So every time it gets it right, add plus one,

60
00:05:34,433 --> 00:05:40,166
and if it gets it wrong, add minus one. And now I have a computer I would search in the space of these parameters,

61
00:05:40,167 --> 00:05:48,432
like your correspondence costs and so on, and propose new parameters, run the software again, and see if the score goes up or down.

62
00:05:48,433 --> 00:05:53,899
If it goes up, you've done a good twiddle of your parameter, and if it goes down, you've done a bad twiddle.

63
00:05:53,900 --> 00:05:57,066
This algorithm works like a charm, and all my students, I can tell you, are using it.

64
00:05:57,067 --> 00:06:03,599
We call it Twiddle in my lab. It tweaks individual parameters, one after another, and just looks at the outcome,

65
00:06:03,600 --> 00:06:07,966
how well the score (unintelligible) data, and if the score goes up, it keeps tweaking those.

66
00:06:07,967 --> 00:06:13,732
In fact, that's what humans also do when they tune their software, but you can automate this with a couple of label examples.

67
00:06:13,733 --> 00:06:16,566
Wonderful machine learning opportunity.

68
00:06:16,567 --> 00:06:21,099
(Peter) And we call it cross-validation rather than twiddling or tweaking to make it sound more official, but--

69
00:06:21,100 --> 00:06:25,066
(Sebastian) Yeah, you can have a theory on it and come up with theoretical bounds if you wish,

70
00:06:25,067 --> 00:06:27,899
or just implement it and run it.

71
00:06:27,900 --> 00:06:35,432
Okay. "So past content, would it be possible to provide us with a particle filter algorithm," in the ten lines of C code that I mentioned?

72
00:06:35,433 --> 00:06:42,232
By (Dee Dee) in New Jersey. And the answer is I was probably slightly inaccurate when I said ten lines,

73
00:06:42,233 --> 00:06:47,266
unless you have very long lines of C code. And the reason is the essence of the particle filter is ten lines long,

74
00:06:47,267 --> 00:06:52,999
which is the ten lines I gave you. But when you plug in things like the measurement model, those can be very complicated.

75
00:06:53,000 --> 00:07:00,199
They can be as complicated as computer vision is. Or if you have laser range finders in our robots, they are non-trivial.

76
00:07:00,200 --> 00:07:06,599
And can buy my book and find them described there, if you can spare the money to buy my book, Probability Robotics.

77
00:07:06,600 --> 00:07:13,332
In the robotics class, I will go through examples of using particle filters for localization.

78
00:07:13,333 --> 00:07:19,032
And there we work step by step through a particle filter with a very simple sensor model, and a simple motion model,

79
00:07:19,033 --> 00:07:24,767
but it can see each little step again. I hope that will help you understand the particle filter algorithm.
