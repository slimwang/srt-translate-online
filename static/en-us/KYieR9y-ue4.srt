1
00:00:00,360 --> 00:00:03,440
While I'm sure you're really excited about entropy right now,

2
00:00:03,440 --> 00:00:05,820
let's take this back to decision trees and

3
00:00:05,820 --> 00:00:09,250
see how entropy actually affects how a decision tree draws its boundaries.

4
00:00:10,680 --> 00:00:15,100
That involves a new piece of vocabulary, information gain.

5
00:00:16,290 --> 00:00:18,920
Information gain is defined as the entropy of the parent

6
00:00:20,130 --> 00:00:23,110
minus the weighted average of the entropy of the children that

7
00:00:23,110 --> 00:00:24,980
would result if you split that parent.

8
00:00:24,980 --> 00:00:30,300
The decision tree algorithm will maximize information gain.

9
00:00:31,830 --> 00:00:35,130
So this is how it will choose which feature to make a split on.

10
00:00:35,130 --> 00:00:37,760
And in cases where the feature has many different values that it can take,

11
00:00:37,760 --> 00:00:40,340
this will help it figure out where to make that split.

12
00:00:40,340 --> 00:00:43,640
It is going to try to maximize the information gain.

13
00:00:43,640 --> 00:00:44,480
Let's do an example.
