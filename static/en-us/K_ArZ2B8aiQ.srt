1
00:00:00,000 --> 00:00:04,000
So now we're ready to get back to the problem of extracting a list

2
00:00:04,000 --> 00:00:06,000
that contains all the links in a webpage.

3
00:00:06,000 --> 00:00:10,000
This is the first step towards our crawler, which will crawl a set of webpages,

4
00:00:10,000 --> 00:00:13,000
finding all of the links that can be found from a seed page.

5
00:00:13,000 --> 00:00:16,000
We're going to start with a webpage.

6
00:00:16,000 --> 00:00:19,000
We have the url of some seed page.

7
00:00:19,000 --> 00:00:24,000
We use the get_page procedure to get a string which is the text of that page.

8
00:00:24,000 --> 00:00:26,000
That's got lots of stuff in it that we don't care about,

9
00:00:26,000 --> 00:00:29,000
but it also includes some hyperlinks,

10
00:00:29,000 --> 00:00:33,000
and inside the hyperlinks are the url's to that page links too.

11
00:00:33,000 --> 00:00:36,000
Our goal is to define a procedure--we'll call it get<u>all</u>links--

12
00:00:36,000 --> 00:00:41,000
that takes as input, a string representing the text on a webpage,

13
00:00:41,000 --> 00:00:44,000
and produces as output, a list containing all of the url's that are linked to by that page.
