1
00:00:00,690 --> 00:00:03,310
Okay. So where are you guys now, with your respect to your project?

2
00:00:03,310 --> 00:00:10,760
>> So we wanted to do, to try to sort tweets on Ebola into certain buckets.

3
00:00:10,760 --> 00:00:13,840
The ones we came up with were based on certain words,

4
00:00:13,840 --> 00:00:17,018
whether we think they sound like misinformation, that's was >> Mm-hm.

5
00:00:17,018 --> 00:00:20,940
>> An idea you sort of prompted us with.

6
00:00:20,940 --> 00:00:24,295
Another one that we came up with was sort of panic mongering words.

7
00:00:24,295 --> 00:00:26,125
>> Uh-huh. >> Coz that there's a lot of that.

8
00:00:26,125 --> 00:00:27,620
>> Mm-hm.

9
00:00:27,620 --> 00:00:30,330
>> And it maybe is an offshoot of that coz before we came in

10
00:00:30,330 --> 00:00:34,480
here we were starting to think of if there's correlation of especially these

11
00:00:34,480 --> 00:00:36,860
panic mongering words with political words.

12
00:00:36,860 --> 00:00:37,415
>> Mm-hm.

13
00:00:37,415 --> 00:00:39,395
>> Coz it's becoming more and more of a political issue.

14
00:00:39,395 --> 00:00:40,990
>> Mm-hm.

15
00:00:40,990 --> 00:00:43,160
>> So that's the rough, sort of,

16
00:00:43,160 --> 00:00:44,820
structure, I think, of what we're trying to do is.

17
00:00:44,820 --> 00:00:46,310
>> So what are the buckets that you're applying

18
00:00:46,310 --> 00:00:50,210
to do that one is misinformation, and political tones, and.

19
00:00:50,210 --> 00:00:52,448
>> And maybe one was just critical information.

20
00:00:52,448 --> 00:00:53,163
>> Okay.

21
00:00:53,163 --> 00:00:54,231
>> Mm-hm.

22
00:00:54,231 --> 00:00:55,300
>> Mm-hm.

23
00:00:55,300 --> 00:00:56,220
>> And the basis for

24
00:00:56,220 --> 00:01:02,430
that was there are some Twitter handles that are being trusted more, with ,.

25
00:01:02,430 --> 00:01:04,900
Been coming from trusted sources.

26
00:01:04,900 --> 00:01:07,780
So, we could use those as references.

27
00:01:07,780 --> 00:01:09,010
>> Oh, okay.

28
00:01:09,010 --> 00:01:10,530
So, for example, cities might have a Twitter account.

29
00:01:10,530 --> 00:01:12,710
>> Right. >> And that might be verified by Twitter so,

30
00:01:12,710 --> 00:01:13,860
that is an authentic account.

31
00:01:13,860 --> 00:01:14,370
>> Exactly. Yeah.

32
00:01:14,370 --> 00:01:15,590
>> Okay.

33
00:01:15,590 --> 00:01:17,930
>> And in terms of the technology.

34
00:01:17,930 --> 00:01:19,850
So, basically, we're going to follow.

35
00:01:21,170 --> 00:01:24,397
We connect the spots as, to the [CROSS-TALK].

36
00:01:24,397 --> 00:01:24,897
>> Mm-hm. Hm.

37
00:01:26,290 --> 00:01:32,615
>> originally, we were planning to do some, lo, location,

38
00:01:32,615 --> 00:01:36,370
v, visualization, but since we know only a small part of the,

39
00:01:36,370 --> 00:01:39,230
the Twitter we have visual location data >> Mm-hm.

40
00:01:39,230 --> 00:01:44,240
>> And, we don't want to, e, e, e, exclude that all those other things, so.

41
00:01:44,240 --> 00:01:48,132
Well we, we probably got an inquiry audit where it's just for those of us-

42
00:01:48,132 --> 00:01:51,770
>> Okay. >> That's and the part.

43
00:01:51,770 --> 00:01:56,500
Later we can figure out if we want to add location visualization or not.

44
00:01:56,500 --> 00:01:57,210
>> Okay.

45
00:01:57,210 --> 00:01:58,930
I think like even this playing the bucket and

46
00:01:58,930 --> 00:02:04,480
coming up with some kind of score-able Percentage of misinformation versus.

47
00:02:04,480 --> 00:02:07,970
>> Right. >> Political tones, or even panic mongering and all, right?

48
00:02:07,970 --> 00:02:11,970
That itself is a good fate of how much, you can measure it.

49
00:02:11,970 --> 00:02:13,340
As long as you can measure it and

50
00:02:13,340 --> 00:02:17,380
give me the count, or give me the percentage, how much is going on, right?

51
00:02:17,380 --> 00:02:19,790
>> Yeah. >> That itself is a, a good thing to have.

52
00:02:19,790 --> 00:02:21,100
It's a very practical application.
