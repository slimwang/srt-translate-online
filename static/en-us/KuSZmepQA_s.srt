1
00:00:00,000 --> 00:00:04,000
The dimensionality reduction looks a little bit silly when you go

2
00:00:04,000 --> 00:00:05,000
from 2 dimensions to 1 dimension.

3
00:00:05,000 --> 00:00:09,000
But in truly high-dimensional space it has a very strong utility.

4
00:00:09,000 --> 00:00:13,000
Here's an example that goes back to MIT several decades ago

5
00:00:13,000 --> 00:00:15,000
on something called eigenfaces.

6
00:00:15,000 --> 00:00:17,000
These are all well-aligned faces.

7
00:00:17,000 --> 00:00:21,000
The objective in eigenface research has been to find

8
00:00:21,000 --> 00:00:25,000
simple ways to describe different people in a parameter space,

9
00:00:25,000 --> 00:00:27,000
in which we can easily identify the same person again.

10
00:00:27,000 --> 00:00:31,000
Images like these are very high-dimensional statistics.

11
00:00:31,000 --> 00:00:33,000
If each image is 50 by 50 pixels,

12
00:00:33,000 --> 00:00:39,000
each image itself becomes a data point in a 2500 dimensional feature space.

13
00:00:39,000 --> 00:00:43,000
Now obviously, we don't have random images.

14
00:00:43,000 --> 00:00:48,000
We don't fill the space of 2500 dimensions with all face images.

15
00:00:48,000 --> 00:00:54,000
Instead, it is reasonable to assume that all the faces live on a small subspace in that space.

16
00:00:54,000 --> 00:00:58,000
Obviously, you as a human can easily distinguish what is a valid image of a face

17
00:00:58,000 --> 00:01:02,000
and what is a valid image of a non face, like a car or a cloud or the sky.

18
00:01:02,000 --> 00:01:04,000
Therefore, there are many, many images that you can

19
00:01:04,000 --> 00:01:08,000
represent with 2500 pixels that are not faces.

20
00:01:08,000 --> 00:01:10,000
So research on eigenfaces has applied

21
00:01:10,000 --> 00:01:15,000
principle component analysis and eigenvalues to the space of faces.

22
00:01:15,000 --> 00:01:19,000
Here is a database in which faces are aligned.

23
00:01:19,000 --> 00:01:23,000
A researcher at Santiago Serrano extracted from it

24
00:01:23,000 --> 00:01:27,000
the average face after alignment on the right side.

25
00:01:27,000 --> 00:01:31,000
The truly interesting phenomenon occurs when you look at the eigenvalues.

26
00:01:31,000 --> 00:01:34,000
The face on the top left, over here, is the average face,

27
00:01:34,000 --> 00:01:37,000
and these are the variations,

28
00:01:37,000 --> 00:01:41,000
the eigenvectors that correspond to the largest eigenvalues over here.

29
00:01:41,000 --> 00:01:42,000
This is the strongest variation.

30
00:01:42,000 --> 00:01:46,000
You see a certain amount of different regions in and around the head shape

31
00:01:46,000 --> 00:01:48,000
and the hair that gets excited.

32
00:01:48,000 --> 00:01:50,000
That's the 2nd strongest one, where the shirt gets more excited.

33
00:01:50,000 --> 00:01:51,000
As you go down,

34
00:01:51,000 --> 00:01:56,000
you find more and more interesting variations that can be used to reconstruct faces.

35
00:01:56,000 --> 00:02:01,000
Typically a dozen or so will suffice to make a face completely reconstructable,

36
00:02:01,000 --> 00:02:05,000
which means you've just mapped a 2500 dimensional feature space

37
00:02:05,000 --> 00:02:08,000
into a, perhaps, 12 dimensional feature space

38
00:02:08,000 --> 99:59:59,999
on which we can now learn much, much easier.
