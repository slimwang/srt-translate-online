1
00:00:00,280 --> 00:00:03,700
Whenever we do recognition machine
learning, what's going to help me?

2
00:00:03,700 --> 00:00:04,780
It's the data.

3
00:00:04,780 --> 00:00:06,670
The data are the first thing
that's going to help me.

4
00:00:06,670 --> 00:00:07,880
There's somebody else
that's going to help me.

5
00:00:07,880 --> 00:00:09,540
I'm going to bring
them along in a minute.

6
00:00:09,540 --> 00:00:14,580
So here we have some images of pictures,
and as you can see in the red circles,

7
00:00:14,580 --> 00:00:17,667
there are some pixels that are from,
say, skin.

8
00:00:17,667 --> 00:00:20,600
And there are pixels
that are not from skin.

9
00:00:20,600 --> 00:00:25,330
And let's suppose I just want to
label a pixel or little patches.

10
00:00:25,330 --> 00:00:27,480
Is that skin, or is there not skin?

11
00:00:27,480 --> 00:00:31,084
By the way, this is a cute,
little thing for those of you out there.

12
00:00:31,084 --> 00:00:33,376
We did a bunch of skin
detection et cetera.

13
00:00:33,376 --> 00:00:38,406
And in Sandy Pentland's lab, and
I think Thad Starner was involved back

14
00:00:38,406 --> 00:00:44,310
when he was a student, it turns out
the color of skin is pretty constant.

15
00:00:44,310 --> 00:00:46,320
The luminance changes, right?

16
00:00:46,320 --> 00:00:49,530
So some people, you may have noticed
are darker than other people.

17
00:00:49,530 --> 00:00:50,910
But they're darker,

18
00:00:50,910 --> 00:00:56,550
they just have more of the same pignent,
pigment, right, the melanin pigment.

19
00:00:56,550 --> 00:01:01,620
So when you look at skin in a color
space, if you remove the illuminant,

20
00:01:01,620 --> 00:01:07,900
it's actually possible to detect
skin for a very wide range

21
00:01:07,900 --> 00:01:12,800
of how dark the skin is, using the same
color removing the illuminant.

22
00:01:12,800 --> 00:01:15,310
Now you have to worry about lighting and
a bunch of other things.

23
00:01:15,310 --> 00:01:17,520
But it makes sense, right,
because there's only one pigment.

24
00:01:17,520 --> 00:01:19,220
Anyway, we're going back,
we're looking at skin color.

25
00:01:19,220 --> 00:01:22,480
There's a bunch of pixels that are skin
color and a bunch of pixels are not.

26
00:01:22,480 --> 00:01:23,410
So, what am I going to do?

27
00:01:23,410 --> 00:01:24,210
Well, what I do is,

28
00:01:24,210 --> 00:01:28,290
I'm going to take the skin pixels, all
the pixels that I, somebody supervised,

29
00:01:28,290 --> 00:01:31,495
remember supervised learning,
have classified as being skin pixels.

30
00:01:31,495 --> 00:01:35,450
And I"m going to build a little
histogram of their feature.

31
00:01:35,450 --> 00:01:38,390
Now, the feature that
we're using here is hue.

32
00:01:38,390 --> 00:01:41,460
Hue is the, essentially the color
with the amount of brightness and

33
00:01:41,460 --> 00:01:42,550
saturation removed.

34
00:01:42,550 --> 00:01:46,860
It's, you know, how red, greenish,
bluish, yellowish is, is the color.

35
00:01:46,860 --> 00:01:51,920
And what this is, is this is a little
box that's essentially the percentage.

36
00:01:51,920 --> 00:01:56,050
It's a histogram that's
the percentage of pixels,

37
00:01:56,050 --> 00:01:59,830
of the skin pixels, that have this hue.

38
00:01:59,830 --> 00:02:05,020
And a histogram is an approximation
to the probability distribution.

39
00:02:05,020 --> 00:02:08,180
But notice that it's
the probability of getting

40
00:02:08,180 --> 00:02:11,320
that particular hue
given that it's skin.

41
00:02:11,320 --> 00:02:16,900
It is not the probability that
it's skin given these hues, okay?

42
00:02:16,900 --> 00:02:19,700
This is the,
remember the likelihood, all right?

43
00:02:19,700 --> 00:02:21,680
Well what do I do after
I've done the skin pixels,

44
00:02:21,680 --> 00:02:26,210
well I do the not skin pixels,
I build another graph, right?

45
00:02:26,210 --> 00:02:28,920
And in fact what's interesting
is you'll notice there's

46
00:02:28,920 --> 00:02:31,240
a little bit of overlap here, right?

47
00:02:31,240 --> 00:02:33,330
That's not too surprising.

48
00:02:33,330 --> 00:02:36,810
There are some, in fact in real life
there actually would have been some down

49
00:02:36,810 --> 00:02:38,270
here too, et cetera, whatever.

50
00:02:38,270 --> 00:02:38,890
Because, you know,

51
00:02:38,890 --> 00:02:43,340
there are skin colored pixels
in the non skin world, okay?

52
00:02:43,340 --> 00:02:46,940
And there are some skin that, you know,
people go and get tattooed or something.

53
00:02:46,940 --> 00:02:47,740
I dont know, whatever.

54
00:02:47,740 --> 00:02:51,600
You know, but in general I'm not
going to have a clean separation, but

55
00:02:51,600 --> 00:02:54,370
I'm going to have a distribution
that hopefully has some

56
00:02:54,370 --> 00:02:55,800
differences between them.

57
00:02:55,800 --> 00:02:56,770
And again,

58
00:02:56,770 --> 00:03:01,330
this is the probability of getting
that hue given that it's not skin.

59
00:03:01,330 --> 00:03:04,580
And it's not the probability
of the other way around.

60
00:03:04,580 --> 00:03:07,480
So all we have at the moment
is the probability of getting

61
00:03:07,480 --> 00:03:10,450
a hue given that it's skin, probability
of giving a hue given it's not skin.

62
00:03:10,450 --> 00:03:12,460
But what, what do we really want to do?

63
00:03:12,460 --> 00:03:14,490
Well, somebody gives us a picture.

64
00:03:14,490 --> 00:03:16,160
This is a stunning picture
of the Beatles, actually.

65
00:03:16,160 --> 00:03:17,583
I've never seen this picture before.

66
00:03:17,583 --> 00:03:18,460
Well, that's not totally true.

67
00:03:18,460 --> 00:03:20,833
I saw it the last time I gave this
lecture, but you know what I mean.

68
00:03:20,833 --> 00:03:23,134
Anyway, we get in a new picture, and

69
00:03:23,134 --> 00:03:27,960
what we want to do is we want to label
each pixel as being skin or not skin.

70
00:03:27,960 --> 00:03:32,180
Or more precisely, we want to know the
probability that it's skin or not skin,

71
00:03:32,180 --> 00:03:33,650
given say, some of the color.

72
00:03:33,650 --> 00:03:36,920
So, do I have the probability
that something's skin,

73
00:03:36,920 --> 00:03:39,030
given its hue on this slide?

74
00:03:39,030 --> 00:03:40,350
Megan, do I have it?

75
00:03:40,350 --> 00:03:40,990
Yes.

76
00:03:40,990 --> 00:03:41,560
>> Yes.

77
00:03:41,560 --> 00:03:42,810
>> Wrong.

78
00:03:42,810 --> 00:03:47,833
No, I have the probability of
a hue given that it's skin, right?

79
00:03:47,833 --> 00:03:48,625
What do we have to do?

80
00:03:48,625 --> 00:03:50,390
Well you've all seen this company wor,
coming.

81
00:03:50,390 --> 00:03:52,840
Where else do I get help from,
besides the data?

82
00:03:52,840 --> 00:03:56,570
I get a help from Dr.
Thomas Bayes, okay?

83
00:03:56,570 --> 00:03:58,120
Bayes rule, remember Bayes rule?

84
00:03:58,120 --> 00:04:00,160
We did Bayes rule now a couple of times?

85
00:04:00,160 --> 00:04:04,216
Bayes rule says that the probability
that it's skin, given x and

86
00:04:04,216 --> 00:04:06,500
that's referred to as the posterior.

87
00:04:06,500 --> 00:04:08,293
It can be written as a prior.

88
00:04:08,293 --> 00:04:10,451
If I pull out some pixel and
I don't tell you anything about it,

89
00:04:10,451 --> 00:04:12,300
what's the probability it's skin?

90
00:04:12,300 --> 00:04:13,520
That's the prior.

91
00:04:13,520 --> 00:04:18,820
And I multiply that by the likelihood,
P of x given that it is skin.

92
00:04:18,820 --> 00:04:20,899
Now that we do have that.

93
00:04:20,899 --> 00:04:22,340
Remember those histograms?

94
00:04:22,340 --> 00:04:25,630
That's the probability of getting
a hue given that it's skin.

95
00:04:25,630 --> 00:04:28,450
So that we have from the data,
all right?

96
00:04:28,450 --> 00:04:30,640
But the prior is added in here.

97
00:04:30,640 --> 00:04:33,690
And in fact, if we remove,
since we've got our measurement,

98
00:04:33,690 --> 00:04:35,090
we can sort of pull that out.

99
00:04:35,090 --> 00:04:37,870
We did that last time for
our particle filtering, actually.

100
00:04:37,870 --> 00:04:42,040
We can say that the probability that
something is skin, given a measurement,

101
00:04:42,040 --> 00:04:46,720
is the likelihood, or is proportional
to the likelihood times the prior.

102
00:04:46,720 --> 00:04:51,333
Which all brings us to the question
of where do you get your prior from?

103
00:04:51,333 --> 00:04:54,707
Okay, we've run into this problem before
when we told you to go to get in your

104
00:04:54,707 --> 00:04:57,850
car, drive to Greece,
go to the oracle, get a prior.

105
00:04:57,850 --> 00:05:01,590
Well that's one thing you could do,
but airfare to Greece is expensive.

106
00:05:01,590 --> 00:05:03,350
So we have to do something else and

107
00:05:03,350 --> 00:05:06,190
Steve sites,
wrote this Bayes rule in use/abuse.

108
00:05:06,190 --> 00:05:07,470
It was kind of cute, I kept the joke.

109
00:05:07,470 --> 00:05:07,970
Thanks Steve.
