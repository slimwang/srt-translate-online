1
00:00:00,000 --> 00:00:08,000
Here is seems to be a problem of not enough data and not a very good smoothing algorithm.

2
00:00:08,000 --> 00:00:13,000
Now the problem was even though I had 4 billion words from which I trained by probabilistic model,

3
00:00:13,000 --> 00:00:18,000
I had never seen the word "ginormous"--not once in those 4 billion.

4
00:00:18,000 --> 00:00:22,000
Yet, I should be able to deal with it even if I haven't seen the word before.

5
00:00:22,000 --> 00:00:26,000
So having more data might mean that I would've seen "ginormous"

6
00:00:26,000 --> 00:00:32,000
and I could have some probability for it rather than just making the Laplace smoothing assumption.

7
00:00:32,000 --> 00:00:35,000
And having better smoothing could also help--

8
00:00:35,000 --> 00:00:37,000
maybe something more sophisticated than Laplace,

9
00:00:37,000 --> 00:00:42,000
maybe something that looks more carefully at the content of the word.

10
00:00:42,000 --> 00:00:47,000
So it might have a letter model to say these letters look common,

11
00:00:47,000 --> 00:00:54,000
ending in "ous"--that's a common ending in English--so this looks more like a word,

12
00:00:54,000 --> 99:59:59,999
even if I haven't seen it before, than some other combination of letters.
