1
00:00:00,200 --> 00:00:03,880
So going back here, with out four issuing order processor with

2
00:00:03,880 --> 00:00:09,082
perfect branch prediction, we have seen that our loop, gets a CPI of 3 over 5,

3
00:00:09,082 --> 00:00:14,808
which is 0.6. Without scheduling, and it gets a CPI of 2 over 5.

4
00:00:14,808 --> 00:00:20,260
Which is 0.4, if we do a scheduling on the original loop. Now let's see what

5
00:00:20,260 --> 00:00:25,050
happens when we unroll the loop once, and try to execute it on this processor.

6
00:00:25,050 --> 00:00:29,250
This is the code we constructed, and now what happens in this loop is we,

7
00:00:29,250 --> 00:00:33,510
do the load. The add has to wait for that result. The store has to wait for

8
00:00:33,510 --> 00:00:39,280
that. This load is going to a different memory element, so it can proceed

9
00:00:39,280 --> 00:00:44,480
in parallel with the store. The R however, has to wait for that load. And then,

10
00:00:44,480 --> 00:00:48,470
this store has to wait for that add. And then, this decrement of the pointer,

11
00:00:48,470 --> 00:00:55,090
can proceed in parallel with the store, But the branch has to wait for R1.

12
00:00:55,090 --> 00:01:00,910
However then, the load can proceed. And now, we are back to where we started.

13
00:01:00,910 --> 00:01:05,900
So overall, it takes us five cycles. Do do these 8 instructions,

14
00:01:05,900 --> 00:01:11,440
which gives us a CP of 0.62 slightly lower than the original

15
00:01:11,440 --> 00:01:15,520
on schedule loop but not much lower. So these are very similar in CPI,

16
00:01:15,520 --> 00:01:19,510
we still gain significantly from eliminating some instructions. So

17
00:01:19,510 --> 00:01:24,630
overall this loop is going to perform better but the CPI has not improved. But

18
00:01:24,630 --> 00:01:28,900
let's see what scheduling does after enrolling. In the scheduled loop, we will

19
00:01:28,900 --> 00:01:34,590
have our load, for the first iteration of the two that we are really doing. And

20
00:01:34,590 --> 00:01:38,630
now if we put the add here, it would again have a dependence, so the load and

21
00:01:38,630 --> 00:01:43,180
whatever we did here would be the only thing that can be done in this cycle.

22
00:01:43,180 --> 00:01:48,240
So instead of doing the add next, we will look for what else can we do here that

23
00:01:48,240 --> 00:01:53,350
doesn't depend on this load. And it turns out that this load here, the load for

24
00:01:53,350 --> 00:01:58,440
the second iteration, can be done in parallel with this load. However, we cannot

25
00:01:58,440 --> 00:02:03,660
store the value into R2, so we will have to use another register, let's say R10,

26
00:02:03,660 --> 00:02:08,940
for that. So these two now can execute in the same cycle. And if you remember so

27
00:02:08,940 --> 00:02:13,320
could the branch. Now that we have done this we can do the add for the first

28
00:02:13,320 --> 00:02:17,510
iteration of the loop, and also the one for the second iteration of the loop,

29
00:02:17,510 --> 00:02:23,030
in the same cycle and in the same cycle you can do the add i instruction for

30
00:02:23,030 --> 00:02:28,950
moving the pointer. Now these two will produce the results that the stores need.

31
00:02:28,950 --> 00:02:32,600
So next cycle, we can do the stores. Note, however,

32
00:02:32,600 --> 00:02:38,330
that this is the store that was supposed to store with offset 0 from R1, but

33
00:02:38,330 --> 00:02:44,122
here, we have now decremented R1 by 8. So now we need to store to

34
00:02:44,122 --> 00:02:49,680
8 from R1. That's because his store needs to match the [UNKNOWN] of this load,

35
00:02:49,680 --> 00:02:53,920
but the R1 has moved in between. And then for the other store,

36
00:02:53,920 --> 00:02:58,880
we're storing R10 now, to the address that used to be minus 4 from R1.

37
00:03:00,330 --> 00:03:05,440
But R1 has already moved by minus 8. So now we need to store to 4 from R1,

38
00:03:05,440 --> 00:03:10,040
to match the address of this. This is basically minus 4 and then,

39
00:03:10,040 --> 00:03:15,270
we did minus 8, so now we need to add 4 to that, in order to get minus 4. And

40
00:03:15,270 --> 00:03:19,860
now, we can do our branch. So now let's see what can we get on our processor.

41
00:03:19,860 --> 00:03:23,770
In the first cycle we will be doing these two loads that can now proceed in

42
00:03:23,770 --> 00:03:29,070
parallel. The Add here, however, cannot proceed in that same cycle because

43
00:03:29,070 --> 00:03:33,860
it needs a result of this load. However, this Add, and the next one,

44
00:03:33,860 --> 00:03:38,790
and this one can proceed in the second cycle. Then the stores here have to

45
00:03:38,790 --> 00:03:42,470
wait the results of the adds so they cannot proceed in the same cycle.

46
00:03:42,470 --> 00:03:47,020
However they can be done in the next cycle and so can the branch because the R1

47
00:03:47,020 --> 00:03:52,820
has been produced, and so can the load because R1 has been produced. And

48
00:03:52,820 --> 00:03:57,660
unfortunately in this cycle we can not do this load because we already executed

49
00:03:57,660 --> 00:04:02,740
four things so unfortunately that load is done in the next cycle after which we

50
00:04:02,740 --> 00:04:08,090
get the same schedule as here. The three adds, and so on. So overall,

51
00:04:08,090 --> 00:04:14,160
it took us now 3 cycles to do 8 instructions. Which gives us a CP of 0.38,

52
00:04:14,160 --> 00:04:19,760
slightly better than the scheduled CPI for the loop that we didn't unroll.

53
00:04:19,760 --> 00:04:23,840
So what really happened is unrolling gives us more stuff that we can

54
00:04:23,840 --> 00:04:27,620
reorder here so that we can find more things that don't have dependencies and

55
00:04:27,620 --> 00:04:30,320
thus for example, we were able to do these two loads that were

56
00:04:30,320 --> 00:04:35,520
not available prior to unrolling. If we unroll this loop three times,

57
00:04:35,520 --> 00:04:40,000
then we will have four loads here that can all proceed in the same cycle.

58
00:04:40,000 --> 00:04:44,400
We will have four adds here that can proceed in the same cycle. Four stores that

59
00:04:44,400 --> 00:04:47,810
can proceed in the same cycle. And then the Add and branch equal. So

60
00:04:47,810 --> 00:04:52,770
the more we unroll, the more parallelism we can get, simply because there is no.

61
00:04:52,770 --> 00:04:57,760
More instructions among which we can try to find independent ones.

62
00:04:57,760 --> 00:05:02,440
Keep in mind, that unrolling already reduced a number of instructions and

63
00:05:02,440 --> 00:05:06,400
with scheduling it allows us to also reduce the CPI so

64
00:05:06,400 --> 00:05:10,340
it kind of creates a double whammy. For performance improvement our

65
00:05:10,340 --> 00:05:14,270
execution time gets reduced both because we have fewer instructions,

66
00:05:14,270 --> 00:05:19,380
8 instructions per 2 iterations as opposed to 5 instructions originally for

67
00:05:19,380 --> 00:05:24,750
one iteration. And also after scheduling it gives us usually a better CPI.
