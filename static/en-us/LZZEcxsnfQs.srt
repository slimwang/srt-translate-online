1
00:00:00,170 --> 00:00:02,520
We're going to look at K-means as an optimization problem. So

2
00:00:02,520 --> 00:00:06,440
remember when we talked about optimization we worried about configurations, I

3
00:00:06,440 --> 00:00:08,500
think we called them inputs at that time but I think

4
00:00:08,500 --> 00:00:10,950
it's going to be helpful to think of them as, as configurations

5
00:00:10,950 --> 00:00:13,360
here. There was a way of scoring configurations and we were

6
00:00:13,360 --> 00:00:17,240
trying to find configurations that had high score. And for some

7
00:00:17,240 --> 00:00:19,720
of the algorithms we needed a notion of a neighborhood so

8
00:00:19,720 --> 00:00:22,930
that we could move from configuration to configuration trying to improve.

9
00:00:22,930 --> 00:00:24,340
>> Mm-hm.

10
00:00:25,430 --> 00:00:27,150
>> So in this setting, the configuration, the thing

11
00:00:27,150 --> 00:00:30,680
that we're optimizing over, is the partitions, the clusters,

12
00:00:30,680 --> 00:00:32,740
and we also have this kind of auxiliary variable

13
00:00:32,740 --> 00:00:34,870
of where the centers are for those clusters. And

14
00:00:34,870 --> 00:00:37,100
what we need now is a notion of scores.

15
00:00:37,100 --> 00:00:40,920
How do we score a particular clustering? So do

16
00:00:40,920 --> 00:00:42,220
you have any thoughts about what would be a

17
00:00:42,220 --> 00:00:45,880
better or worse clustering according to the kind of K-means algorithm?

18
00:00:45,880 --> 00:00:46,950
>> Well, the thing about what you were

19
00:00:46,950 --> 00:00:50,430
saying earlier, about what we were trying to do

20
00:00:50,430 --> 00:00:52,310
with creating these clusters, and I look at

21
00:00:52,310 --> 00:00:54,330
this notion of a center, so something pops in

22
00:00:54,330 --> 00:00:58,260
my head. So, you would like to have centers or

23
00:00:59,520 --> 00:01:04,910
partitions that somehow are good representations of your

24
00:01:04,910 --> 00:01:07,190
data, and why does that matter? Because you

25
00:01:07,190 --> 00:01:09,220
said in the very beginning that we often think

26
00:01:09,220 --> 00:01:13,490
of unsupervised learning as compact representation. So if you

27
00:01:13,490 --> 00:01:15,520
want to have a compact representation it would be nice

28
00:01:15,520 --> 00:01:19,550
if you don't throw anything away. So, I'm going to say

29
00:01:19,550 --> 00:01:22,510
that a good score will be something that captures just

30
00:01:22,510 --> 00:01:27,520
how much error you introduce by representing these points as

31
00:01:27,520 --> 00:01:31,740
partitions or in this case as centers. Does that make sense?

32
00:01:31,740 --> 00:01:35,160
>> Okay. I guess that's a, that's a perfectly

33
00:01:35,160 --> 00:01:36,750
reasonable way to think of it. I, another way to

34
00:01:36,750 --> 00:01:39,256
think of it is in terms of error, right. Yeah,

35
00:01:39,256 --> 00:01:40,650
which I guess is the same idea. Like if we're,

36
00:01:40,650 --> 00:01:45,410
if you think about the object as being represented by the center of its cluster.

37
00:01:45,410 --> 00:01:46,023
>> Mm-hm.

38
00:01:46,023 --> 00:01:48,100
>> Then we want to know how far away from the center it is.

39
00:01:48,100 --> 00:01:50,100
>> Right. And the farther away it is,

40
00:01:50,100 --> 00:01:52,880
the more error you have in representing it.

41
00:01:52,880 --> 00:01:56,180
>> Right. So, here is a concrete of writing down what we

42
00:01:56,180 --> 00:01:59,610
think the scoring function could be. So we're going to say that the

43
00:01:59,610 --> 00:02:02,220
error, it's kind of the negative score, right? This is something that

44
00:02:02,220 --> 00:02:04,175
we want to minimize even though generally,

45
00:02:04,175 --> 00:02:06,120
we've been talking about optimization as maximizing.

46
00:02:06,120 --> 00:02:08,220
Here is something we want to minimize. That if you

47
00:02:08,220 --> 00:02:10,210
give me a particular way of clustering it and

48
00:02:10,210 --> 00:02:13,800
you define the centers based on, say, that cluster,

49
00:02:13,800 --> 00:02:14,660
what we're going to do is we're going to

50
00:02:14,660 --> 00:02:19,920
sum over all the objects the distance, the square distance actually, between

51
00:02:19,920 --> 00:02:25,470
the object and its center. Right? So p of x is the

52
00:02:25,470 --> 00:02:30,670
cluster for x and center sub that is the center for that cluster.

53
00:02:30,670 --> 00:02:31,170
>> So that drives

54
00:02:31,170 --> 00:02:34,880
home the idea that we're talking about Euclidean distance here because x would

55
00:02:34,880 --> 00:02:39,010
have to be in the same space as the centers are by definition. Okay.

56
00:02:41,400 --> 00:02:43,280
>> Yeah, and I'm not sure exactly how we're

57
00:02:43,280 --> 00:02:46,740
going to define neighborhood. But one way to define

58
00:02:46,740 --> 00:02:50,350
neighborhood is that the neighborhood of a configuration, which

59
00:02:50,350 --> 00:02:52,800
is a p and a center is the set

60
00:02:52,800 --> 00:02:55,770
of pairs where you keep the centers the same

61
00:02:55,770 --> 00:02:58,630
and you change the partitions. Or you keep the

62
00:02:58,630 --> 00:03:00,540
partitions the same and you move the centers. So

63
00:03:00,540 --> 00:03:02,350
you're basically changing one of these at a time.

64
00:03:02,350 --> 00:03:04,200
>> Oh, that's very clever, Michael.

65
00:03:04,200 --> 00:03:05,010
>> Thanks.
