1
00:00:00,340 --> 00:00:01,705
Hey Charles. How's it goin'?

2
00:00:01,705 --> 00:00:03,810
>> It's going pretty well Michael. How are things going with you?

3
00:00:03,810 --> 00:00:07,470
>> Good. You know I'm excited to tell you about neural networks today.

4
00:00:07,470 --> 00:00:11,180
You may be familiar with neural networks because you have one, in your head.

5
00:00:11,180 --> 00:00:12,550
>> I do?

6
00:00:12,550 --> 00:00:15,930
>> Well, yeah. I mean, you have a network neurons. Like, you know,

7
00:00:15,930 --> 00:00:19,150
you know neurons, like brain cells. Let me, let me, I'll draw you one.

8
00:00:19,150 --> 00:00:20,310
>> Okay.

9
00:00:20,310 --> 00:00:22,760
>> So this is my template drawing, a nerve cell,

10
00:00:22,760 --> 00:00:26,290
a neuron. And you can, you know, you've got billions and

11
00:00:26,290 --> 00:00:29,180
billions of these inside your head. And they have

12
00:00:29,180 --> 00:00:30,460
you know, most of them have a pretty similar

13
00:00:30,460 --> 00:00:32,759
structure, that there's the, there's the kind of the

14
00:00:32,759 --> 00:00:35,680
main part of the cell called the cell body. And

15
00:00:35,680 --> 00:00:41,190
then there's this thing called an axon which kind of is like a wire going

16
00:00:41,190 --> 00:00:46,880
forward to a set of synapses which are kind of little gaps between

17
00:00:46,880 --> 00:00:51,940
this neuron and some other neuron. And what happens is, information spike

18
00:00:51,940 --> 00:00:52,430
trains

19
00:00:52,430 --> 00:00:53,540
>> Woo woo!

20
00:00:53,540 --> 00:00:57,270
>> Travel down the axon. When the

21
00:00:57,270 --> 00:00:59,780
cell body fires it has an electrical impulse

22
00:00:59,780 --> 00:01:02,090
it travels down the, the, the axon

23
00:01:02,090 --> 00:01:06,220
and then causes across the synapses excitation to

24
00:01:06,220 --> 00:01:08,240
occur on other neurons which themselves can

25
00:01:08,240 --> 00:01:11,860
fire. Again by sending out spike trains. And

26
00:01:11,860 --> 00:01:13,890
so they're very much a kind of

27
00:01:13,890 --> 00:01:17,820
a computational unit and they're very, very complicated.

28
00:01:17,820 --> 00:01:20,600
To a first approximation, as is often true with

29
00:01:20,600 --> 00:01:23,740
first approximations they're very simple. Sort of by definition

30
00:01:23,740 --> 00:01:26,750
of first approximation. So what, what, in the field

31
00:01:26,750 --> 00:01:29,710
of artificial neural networks we have kind of a

32
00:01:29,710 --> 00:01:34,640
cartoonish version of the neuron and networks of neurons

33
00:01:34,640 --> 00:01:37,820
and we actually. Put them together to compute various

34
00:01:37,820 --> 00:01:40,170
things. And one of the nice things about the,

35
00:01:40,170 --> 00:01:43,010
the way that they're set up is that they

36
00:01:43,010 --> 00:01:46,300
can be tuned or changed so that they fire under

37
00:01:46,300 --> 00:01:49,740
different conditions and therefore compute different things. And they can

38
00:01:49,740 --> 00:01:52,320
be trained through a learning process. So that's what we're

39
00:01:52,320 --> 00:01:54,610
going to talk through if you haven't heard about this before.

40
00:01:54,610 --> 00:01:55,880
>> Okay.

41
00:01:55,880 --> 00:01:58,250
>> So we can replace this sort of detailed version of a

42
00:01:58,250 --> 00:02:00,970
neuron with a very abstracted way kind of notion of a neuron.

43
00:02:00,970 --> 00:02:04,950
And here's how it's going to work. We're going to have inputs

44
00:02:04,950 --> 00:02:08,560
that are kind of you know, think of them as firing rates or

45
00:02:10,139 --> 00:02:15,470
the strength of inputs. X1, X2, and X3 in this case. Those are

46
00:02:15,470 --> 00:02:20,790
multiplied by weight, w1, w2, w3 correspondingly. And so the weights

47
00:02:20,790 --> 00:02:25,990
kind of turn up the gain or the sensitivity of the neuron, this unit,

48
00:02:28,800 --> 00:02:32,200
to each of the inputs respectively. Then what we're going to do is

49
00:02:32,200 --> 00:02:36,970
we're going to sum them up. So we're going to sum. Over all the inputs.

50
00:02:39,640 --> 00:02:42,750
The strength of the input times the weight,

51
00:02:42,750 --> 00:02:45,420
and that's going to be the activation. Then

52
00:02:45,420 --> 00:02:47,320
we're going to ask is that greater than,

53
00:02:47,320 --> 00:02:50,370
or equal to the firing threshold. And if it

54
00:02:50,370 --> 00:02:57,310
is, then we're going to say the output is one, and if it's not, we're going to

55
00:02:57,310 --> 00:02:59,420
say the output is zero. So this is

56
00:02:59,420 --> 00:03:04,800
a particular kind of neural net unit called Perceptron.

57
00:03:04,800 --> 00:03:07,880
Which is a very sexy name because they had very sexy names in the 50s

58
00:03:07,880 --> 00:03:09,940
>> They did.

59
00:03:09,940 --> 00:03:13,270
>> When this was first developed. Alright?

60
00:03:13,270 --> 00:03:15,500
So this, this whole neuron concept gets

61
00:03:15,500 --> 00:03:19,970
boiled down to something much simpler, which is just, a linear sum followed by

62
00:03:19,970 --> 00:03:22,960
a threshold, thresholding operation, right? So it's

63
00:03:22,960 --> 00:03:25,410
worth kind of thinking, how can we,

64
00:03:25,410 --> 00:03:30,160
what sort of things can this, can networks of these kinds of units compute? So,

65
00:03:30,160 --> 00:03:32,120
let's see if we can figure some of those things out.
