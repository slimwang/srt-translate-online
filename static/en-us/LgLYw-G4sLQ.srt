1
00:00:00,080 --> 00:00:03,450
And now for the next few videos I'm going to show you an actually coded up

2
00:00:03,450 --> 00:00:06,620
example of PCA as applied to facial recognition.

3
00:00:06,620 --> 00:00:10,507
This is taken and adapted from an example on the sklearn documentation, so

4
00:00:10,507 --> 00:00:14,219
I'll include a link to the original code so you can go in and take a look at it,

5
00:00:14,219 --> 00:00:17,253
but I want to walk you through some of the most important parts.

6
00:00:17,253 --> 00:00:19,376
So the first thing that the code does is,

7
00:00:19,376 --> 00:00:23,930
import a set of pictures of famous world leaders from about 10 to 15 years ago.

8
00:00:23,930 --> 00:00:27,530
The next thing that it does is it splits it into a training and testing set.

9
00:00:27,530 --> 00:00:31,280
Very good, but then this block right here is where the PCA actually happens.

10
00:00:31,280 --> 00:00:34,380
You'll see in this example, and sometimes in the literature that PCA is

11
00:00:34,380 --> 00:00:37,880
also called eigenfaces when it's applied to facial recognition.

12
00:00:37,880 --> 00:00:41,230
So in this line here we actually see the PCA being created,

13
00:00:41,230 --> 00:00:44,020
it's called RandomizedPCA in sklearn.

14
00:00:44,020 --> 00:00:46,700
And then it's also being fit to the training data.

15
00:00:46,700 --> 00:00:49,850
Then what this line does here is it asks for the eigenfaces.

16
00:00:49,850 --> 00:00:53,320
The eigenfaces are basically the principle components of the face data.

17
00:00:53,320 --> 00:00:56,220
So it takes the pca.components, and then it reshapes them.

18
00:00:56,220 --> 00:00:58,340
because right now they're just strings of numbers, and

19
00:00:58,340 --> 00:01:03,250
it wants to turn those back into squares so that they look like pictures.

20
00:01:03,250 --> 00:01:07,168
You can also see that the number of principal components that are being used in

21
00:01:07,168 --> 00:01:08,443
this example is 150.

22
00:01:08,443 --> 00:01:12,348
And if you look at the example code on the sklearn documentation page,

23
00:01:12,348 --> 00:01:16,692
you'll see that the original dimensionality of these pictures is over 1,800.

24
00:01:16,692 --> 00:01:19,716
So we've gone from 1,800 features down to 150,

25
00:01:19,716 --> 00:01:21,800
a compression factor of more than 10.

26
00:01:21,800 --> 00:01:25,410
Then the last thing that I do with PCA is I transform my data.

27
00:01:25,410 --> 00:01:26,870
When I perform the fit command I'm

28
00:01:26,870 --> 00:01:29,435
just figuring out what the principle components are.

29
00:01:29,435 --> 00:01:32,640
It's these transform commands where I actually take my data and

30
00:01:32,640 --> 00:01:35,110
transform them into the principle components representation.

31
00:01:35,110 --> 00:01:37,830
And the rest of this code is creating an SVM.

32
00:01:37,830 --> 00:01:40,270
Remember the SVC is what its called in sklearn,

33
00:01:40,270 --> 00:01:41,970
a support vector classifier.

34
00:01:41,970 --> 00:01:45,080
They do a little bit of fancy footwork here to figure out exactly what

35
00:01:45,080 --> 00:01:49,140
parameters of the support vector machine they want to use.

36
00:01:49,140 --> 00:01:51,480
And then using the principle components as the features,

37
00:01:51,480 --> 00:01:55,770
they try to identify in the test set who appears in a given picture.

38
00:01:55,770 --> 00:01:58,240
Now I'll show you what this looks like when I run this code.

39
00:01:58,240 --> 00:01:59,990
First thing that it does is it prints out a doc string,

40
00:01:59,990 --> 00:02:01,460
just tells me what's going on.

41
00:02:01,460 --> 00:02:02,530
And then some information about the data set.

42
00:02:02,530 --> 00:02:04,420
So I have 1,288 samples,

43
00:02:04,420 --> 00:02:09,060
with 1,850 features available to me in the input feature space.

44
00:02:09,060 --> 00:02:10,740
Seven different people are appearing and

45
00:02:10,740 --> 00:02:14,940
then we use 150 eigenfaces from the 966 faces in our training set.

46
00:02:14,940 --> 00:02:18,320
The next thing that appears is a list of the people in our data set, and

47
00:02:18,320 --> 00:02:19,780
how often we get them correct.

48
00:02:19,780 --> 00:02:23,130
Precision, recall, f1-score and support are things that are sort of

49
00:02:23,130 --> 00:02:25,680
related to the accuracy, their evaluation matrix.

50
00:02:25,680 --> 00:02:28,170
We'll talk a lot about those in a coming lesson.

51
00:02:28,170 --> 00:02:31,670
You can see that in general though, it's getting things correct.

52
00:02:31,670 --> 00:02:33,880
Roughly 60% to almost 90% of the time.

53
00:02:33,880 --> 00:02:38,350
So even though we've reduced our dimensionality by a factor of ten,

54
00:02:38,350 --> 00:02:40,370
we're still having really good performance.

55
00:02:40,370 --> 00:02:43,100
Another thing that's really cool that I love about this example is they

56
00:02:43,100 --> 00:02:45,460
actually show you the eigenfaces.

57
00:02:45,460 --> 00:02:47,580
So this is the first principle component of our data, and

58
00:02:47,580 --> 00:02:50,180
this is the second principle component, the third one.

59
00:02:50,180 --> 00:02:53,670
So this image here represents the maximal variation that it

60
00:02:53,670 --> 00:02:55,800
sees across all the data.

61
00:02:55,800 --> 00:02:58,830
It's a little bit hard to understand exactly what that means in this example.

62
00:02:58,830 --> 00:03:01,020
It would be better if it said something like,

63
00:03:01,020 --> 00:03:05,920
how far apart the eyes are, or whether the person wears glasses or not.

64
00:03:05,920 --> 00:03:08,990
Instead, what we get are these kind of ghost images.

65
00:03:08,990 --> 00:03:13,838
But then using these composite images together as features in an SVM,

66
00:03:13,838 --> 00:03:17,832
it can be very powerful in predicting who's face we see.

67
00:03:17,832 --> 00:03:18,380
And then last but

68
00:03:18,380 --> 00:03:22,130
not least, there's a little printout that gives you 12 representative faces and

69
00:03:22,130 --> 00:03:25,010
the algorithms best guess about who it is, and the actual answer.

70
00:03:25,010 --> 00:03:27,100
So you can see it doesn't always get them correct.

71
00:03:27,100 --> 00:03:30,900
This one right here I think this is Tony Blair, but it's actually George W Bush,

72
00:03:30,900 --> 00:03:33,010
but on most of them it's getting it correct, pretty cool.
