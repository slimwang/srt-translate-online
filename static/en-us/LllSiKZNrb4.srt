1
00:00:00,460 --> 00:00:03,050
The main principle in structuring an operating

2
00:00:03,050 --> 00:00:06,010
system for a shared memory multiprocessor, is to

3
00:00:06,010 --> 00:00:09,330
limit sharing kernel data structures. Which both

4
00:00:09,330 --> 00:00:14,600
limits concurrency and increases contention. This principle finds

5
00:00:14,600 --> 00:00:17,770
additional corroboration in the Corey operating systems

6
00:00:17,770 --> 00:00:20,800
research that was done at MIT. Which wants

7
00:00:20,800 --> 00:00:23,640
to involve the applications that run on top

8
00:00:23,640 --> 00:00:25,510
of the operating system to give hints to

9
00:00:25,510 --> 00:00:28,390
the kernel. So let's talk about some of the ideas

10
00:00:28,390 --> 00:00:31,700
in the Corey System that is built at MIT, the

11
00:00:31,700 --> 00:00:35,620
ideas are similar to what we saw in Tornado, namely

12
00:00:35,620 --> 00:00:37,480
you want to reduce the amount of sharing. That's the

13
00:00:37,480 --> 00:00:39,890
key thing. If you reduce the amount of sharing it

14
00:00:39,890 --> 00:00:43,140
allows for scaling. And one of the things that is

15
00:00:43,140 --> 00:00:46,090
in, Corey System is his idea of address ranges in

16
00:00:46,090 --> 00:00:50,560
an application. And basically this is similar to the region concept

17
00:00:50,560 --> 00:00:53,520
in Tornado. The region concept in Tornado is

18
00:00:53,520 --> 00:00:55,890
under the covers. The application doesn't know anything about

19
00:00:55,890 --> 00:00:59,370
it. It knows that An application has an address

20
00:00:59,370 --> 00:01:02,460
space and the operating system decides that, well, this

21
00:01:02,460 --> 00:01:05,400
application has its address space, but the threads of

22
00:01:05,400 --> 00:01:09,405
this application are accessing different regions, and therefore I'm

23
00:01:09,405 --> 00:01:12,560
going to partition this data structure, the global data structure

24
00:01:12,560 --> 00:01:15,940
called. The page table into regions, and that way,

25
00:01:15,940 --> 00:01:19,080
I can ensure that there is concurrency among the

26
00:01:19,080 --> 00:01:23,150
page fault service handling for the different regions that,

27
00:01:23,150 --> 00:01:25,849
that the operating system has to deal with. Similar

28
00:01:25,849 --> 00:01:30,560
idea, except here the address ranges are exposed to the

29
00:01:30,560 --> 00:01:33,260
application. So in other words, a thread says that

30
00:01:33,260 --> 00:01:36,040
this is a region in which I'm going to operate

31
00:01:36,040 --> 00:01:38,280
and if the kernel knows the address range in

32
00:01:38,280 --> 00:01:41,080
which a particular thread is going to operate in, then it

33
00:01:41,080 --> 00:01:44,040
can actually use that as a hint in saying, well,

34
00:01:44,040 --> 00:01:45,750
where do I want to run this thread. If a

35
00:01:45,750 --> 00:01:48,780
bunch of threads are touching the same address range maybe

36
00:01:48,780 --> 00:01:51,160
you want to put it on the same processor. And these

37
00:01:51,160 --> 00:01:54,180
are the kinds of optimizations that the operating system can

38
00:01:54,180 --> 00:01:58,660
do. If this hint is provided by the application. Similarly,

39
00:01:58,660 --> 00:02:01,970
shares is another concept, and the idea here is that

40
00:02:01,970 --> 00:02:05,910
an application thread can say that here is the data structure,

41
00:02:07,070 --> 00:02:09,800
here is a system [INAUDIBLE] that I'm going to use,

42
00:02:09,800 --> 00:02:11,710
but I'm not going to share it with anybody. An

43
00:02:11,710 --> 00:02:15,710
example would be a file. A file, by definition,

44
00:02:15,710 --> 00:02:19,230
for a process is an operating system entity once

45
00:02:19,230 --> 00:02:21,960
the process opens that file. That file descriptor is

46
00:02:21,960 --> 00:02:25,910
a data structure that the operating system maintains and

47
00:02:25,910 --> 00:02:28,670
now if you have multiple threads in the same

48
00:02:28,670 --> 00:02:32,295
application, all of the threads have access to that file

49
00:02:32,295 --> 00:02:36,150
descriptor, but if a thread of that application opens

50
00:02:36,150 --> 00:02:38,870
the file and it knows that it's not going

51
00:02:38,870 --> 00:02:41,760
to share that file with anybody else. It can

52
00:02:41,760 --> 00:02:45,550
communicate that intent through the shares mechanism. Through the

53
00:02:45,550 --> 00:02:49,020
shares mechanism, it can communicate that intent to the

54
00:02:49,020 --> 00:02:52,130
operating system. Saying that, here is a file that

55
00:02:52,130 --> 00:02:53,620
I've opened, but I'm not going to share it

56
00:02:53,620 --> 00:02:57,860
with anybody else. That hint is useful once again

57
00:02:57,860 --> 00:03:01,610
for the kernel to optimize shared data structures

58
00:03:01,610 --> 00:03:04,790
and in particular if you have a multi-core

59
00:03:04,790 --> 00:03:08,200
processor and if I have threads of an

60
00:03:08,200 --> 00:03:11,050
application running on multiple cores I don't have to

61
00:03:11,050 --> 00:03:14,000
worry about the consistency of that file descriptor

62
00:03:14,000 --> 00:03:16,480
across all these cores. That gives an opportunity for

63
00:03:16,480 --> 00:03:19,640
reducing the amount of work that the operating

64
00:03:19,640 --> 00:03:23,280
system has to do in managing shared data structures.

65
00:03:23,280 --> 00:03:26,450
Another facility that is there in Corey's dedicated cores,

66
00:03:26,450 --> 00:03:29,180
here the idea is that if you have a multi-core,

67
00:03:29,180 --> 00:03:31,840
you have lots of cores, might as well dedicate

68
00:03:31,840 --> 00:03:35,210
some of the cores for kernel activity. And that way,

69
00:03:35,210 --> 00:03:39,120
we can confine the locality of kernel data structures

70
00:03:39,120 --> 00:03:41,690
to a few cores. And not have to worry about

71
00:03:41,690 --> 00:03:45,300
moving data between the cores. So all of these

72
00:03:45,300 --> 00:03:48,590
techniques that are being proposed in the Corey system is

73
00:03:48,590 --> 00:03:54,200
really trying to attack the fundamental problem that

74
00:03:54,200 --> 00:03:57,760
there is a huge latency involved when you

75
00:03:57,760 --> 00:04:00,480
have to communicate across cores. Or when you

76
00:04:00,480 --> 00:04:03,504
have to communicate outside of the core into

77
00:04:03,504 --> 00:04:06,125
the, into the memory subsystem and so on.

78
00:04:06,125 --> 00:04:08,000
And all of these facilities are trying to

79
00:04:08,000 --> 00:04:11,760
reduce the amount of inter-core communication and core

80
00:04:11,760 --> 00:04:13,514
to memory communication and so on and so forth.
