1
00:00:00,930 --> 00:00:03,980
Now let's look at how we can exploit the caches

2
00:00:03,980 --> 00:00:07,610
available. Now, it is a fact that a tested set

3
00:00:07,610 --> 00:00:11,910
instruction has to necessarily go to memory when we want

4
00:00:11,910 --> 00:00:13,790
to acquire the lock, we have to execute a tested

5
00:00:13,790 --> 00:00:16,570
set instructions so that we can atomically make sure that

6
00:00:16,570 --> 00:00:19,590
exactly one processor gets the lock. But on the other

7
00:00:19,590 --> 00:00:23,320
hand, the guys that don't have the lock. Could, in

8
00:00:23,320 --> 00:00:26,310
fact, exploit the caches in order to wait for the lock.

9
00:00:26,310 --> 00:00:28,480
And that's why this particular algorithm that I'm going to

10
00:00:28,480 --> 00:00:31,010
describe to you is what is called spin on read,

11
00:00:31,010 --> 00:00:33,660
and the assumption here is that you have a

12
00:00:33,660 --> 00:00:37,330
shared memory machine in which the architecture is providing cache

13
00:00:37,330 --> 00:00:40,170
coherence, or in other words, through the system bus

14
00:00:40,170 --> 00:00:44,320
or interconnection network, The hardware is ensuring that the caches

15
00:00:44,320 --> 00:00:47,780
are kept coherent. Well that gives us an idea

16
00:00:47,780 --> 00:00:51,350
as to how we can exploit the caches. The waiters,

17
00:00:51,350 --> 00:00:53,860
instead of executive a test [INAUDIBLE] instruction that has

18
00:00:53,860 --> 00:00:56,760
to go to memory, they can spin locally on the

19
00:00:56,760 --> 00:00:59,410
cached value of the lock. because when you are

20
00:00:59,410 --> 00:01:02,400
spinning on the local cached value of the lock. If

21
00:01:02,400 --> 00:01:04,900
that value changes in memory, these guys are going

22
00:01:04,900 --> 00:01:09,170
to notice that. That's the principle behind the cache coherence

23
00:01:09,170 --> 00:01:11,890
that is implemented in hardware. And so we can exploit

24
00:01:11,890 --> 00:01:16,790
that fact in implementing a more efficient way of spinning.

25
00:01:16,790 --> 00:01:19,700
Which is called spin on read. The idea is that the

26
00:01:19,700 --> 00:01:23,220
lock algorithm, the first thing it's going to do is go and

27
00:01:23,220 --> 00:01:26,090
do a check on the, the memory location to see if

28
00:01:26,090 --> 00:01:29,630
it is locked. So this is a, a normal atomic read operation

29
00:01:29,630 --> 00:01:32,540
that is being done, not a test and spin operation, so

30
00:01:32,540 --> 00:01:34,900
if it is not in the cache, you're going to go to memory

31
00:01:34,900 --> 00:01:37,770
and bring it in, and once you bring it in, so

32
00:01:37,770 --> 00:01:41,860
long as this value doesn't change, we're going to basically looking at the

33
00:01:41,860 --> 00:01:44,210
value that is in that cache in order to do

34
00:01:44,210 --> 00:01:46,650
the checking And I'm not going to go to the

35
00:01:46,650 --> 00:01:50,790
bus and therefore, I'm not producing any contention on the

36
00:01:50,790 --> 00:01:54,410
network. And there could be any number of processes waiting on

37
00:01:54,410 --> 00:01:56,960
the lock simultaneously. No problem with that because all of

38
00:01:56,960 --> 00:02:00,290
them are going to be spinning on the local value of

39
00:02:00,290 --> 00:02:03,790
L in their respective caches. And so if there is

40
00:02:03,790 --> 00:02:06,880
one processor that's actually doing useful work and it has to

41
00:02:06,880 --> 00:02:08,880
go to memory, it's not going to find that

42
00:02:08,880 --> 00:02:11,710
to be a problem. No contention on the network

43
00:02:11,710 --> 00:02:15,600
from the waiting processors because of this. Now,

44
00:02:15,600 --> 00:02:18,890
if the one processor that was having the lock

45
00:02:18,890 --> 00:02:22,260
eventually releases it, they go, everybody's going to notice

46
00:02:22,260 --> 00:02:24,990
that. And so if I'm waiting for the lock,

47
00:02:24,990 --> 00:02:26,610
and I've been spinning here locally in my

48
00:02:26,610 --> 00:02:31,990
cache, when the, the lock is released, I'll notice

49
00:02:31,990 --> 00:02:34,920
that through the cache coherence mechanism as I'll, I'll

50
00:02:34,920 --> 00:02:37,820
break out of this spin loop. But immediately, I

51
00:02:37,820 --> 00:02:40,260
want to check, I want check if the lock

52
00:02:40,260 --> 00:02:42,560
is available by doing this test and set and

53
00:02:42,560 --> 00:02:46,520
get it uniquely for myself. So multiple processors are

54
00:02:46,520 --> 00:02:51,030
trying to execute this testing set simultaneously. It's possible

55
00:02:51,030 --> 00:02:53,480
somebody else is going to beat me to the punch

56
00:02:53,480 --> 00:02:57,000
and if that happens, I simply go back and, and,

57
00:02:57,000 --> 00:03:04,630
and do the grouping on my private copy of L and wait for the guy who beat me

58
00:03:04,630 --> 00:03:07,760
to the punch to release a lock eventually. So

59
00:03:07,760 --> 00:03:10,530
that I can get it. So that's the idea.

60
00:03:10,530 --> 00:03:13,810
The idea is you spin locally. When you notice

61
00:03:13,810 --> 00:03:15,820
that the lock has been released you try and

62
00:03:15,820 --> 00:03:22,020
do a test and set. If you get lucky you win, if you lose you go back and spin

63
00:03:22,020 --> 00:03:24,870
again locally. So that's the idea behind spinning on

64
00:03:24,870 --> 00:03:29,230
read. The unlock operation of course is pretty straightforward. The

65
00:03:29,230 --> 00:03:32,420
guy that wants to unlock is simply going to change the

66
00:03:32,420 --> 00:03:35,880
memory location to indicate that L is no longer locked.

67
00:03:35,880 --> 00:03:37,960
So that's all it has to do. And then, and

68
00:03:37,960 --> 00:03:41,280
then the other processes can observe it through the cache

69
00:03:41,280 --> 00:03:44,280
coherence mechanism, and be able to acquire the lock. But

70
00:03:44,280 --> 00:03:47,070
note what happens when the lock is released. When the

71
00:03:47,070 --> 00:03:50,430
lock is released, all the processes that are stuck here

72
00:03:50,430 --> 00:03:52,880
in the spin loop, they are going to go and

73
00:03:52,880 --> 00:03:55,470
try to do this test and operation at the same

74
00:03:55,470 --> 00:03:59,110
time, and we know that test and set has to

75
00:03:59,110 --> 00:04:01,740
bypass the cache, everyone is hitting on the bus, right?

76
00:04:01,740 --> 00:04:03,610
Everybody is hitting on the bus, trying to go to

77
00:04:03,610 --> 00:04:07,210
memory, in order to do this test and operation. And

78
00:04:07,210 --> 00:04:12,278
so that essentially means that, in a right invalidate this

79
00:04:12,278 --> 00:04:16,630
cache coherence mechanism is going to result

80
00:04:16,630 --> 00:04:21,130
in order of n squared bus transactions.

81
00:04:21,130 --> 00:04:25,920
For all of these guys to stop chattering on the bus, because everyone of

82
00:04:25,920 --> 00:04:29,070
these test and set instructions is going to

83
00:04:29,070 --> 00:04:32,880
result in invalidating the caches, and as

84
00:04:32,880 --> 00:04:38,380
a result, you have an order of n squared operation that is going to result

85
00:04:38,380 --> 00:04:40,510
when a lock is released, where n is the number

86
00:04:40,510 --> 00:04:44,560
of processors that are simultaneously trying to get the lock.

87
00:04:44,560 --> 00:04:48,060
And, obviously, this is impeding that one guy that got

88
00:04:48,060 --> 00:04:51,290
the lock and can actually get some useful work done.

89
00:04:51,290 --> 00:04:54,820
And this is clearly disruptive. And earlier one of the

90
00:04:54,820 --> 00:04:57,670
things that we said is that we want to avoid

91
00:04:57,670 --> 00:05:00,830
or limit the amount of disruption to useful work that

92
00:05:00,830 --> 00:05:03,580
can be done by the process that acquired the lock.
