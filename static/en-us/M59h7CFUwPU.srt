1
00:00:00,280 --> 00:00:03,719
Alright, so what we'd like to do is work up to an algorithm that can

2
00:00:03,719 --> 00:00:05,660
actually do some of these inference steps instead

3
00:00:05,660 --> 00:00:08,140
of having to think it through each time

4
00:00:08,140 --> 00:00:12,820
de novo. So what I'm going to do is, let's hearken back to an example that we

5
00:00:12,820 --> 00:00:15,500
looked at before which is about spam detection.

6
00:00:15,500 --> 00:00:16,880
Do you, do you remember the spam example?

7
00:00:16,880 --> 00:00:18,640
>> I do remember the spam example. That

8
00:00:18,640 --> 00:00:20,270
was way back in the boosting lecture, right?

9
00:00:20,270 --> 00:00:23,510
>> Yes, I think you did that one. I did, it was an excellent example.

10
00:00:23,510 --> 00:00:25,420
>> There you go. So,

11
00:00:25,420 --> 00:00:27,530
we didn't think about it in a Bayes net setting,

12
00:00:27,530 --> 00:00:29,370
it was in a classification setting we were trying to

13
00:00:29,370 --> 00:00:31,260
come up with the rule, but let's think of this

14
00:00:31,260 --> 00:00:33,990
as a Bayes Net where there's a bunch of different variables

15
00:00:33,990 --> 00:00:36,860
that can be true or false about any given email

16
00:00:36,860 --> 00:00:39,930
message. It can either be spam or not. It can contain

17
00:00:39,930 --> 00:00:42,560
the word Viagra or not. It can contain the word

18
00:00:42,560 --> 00:00:46,240
prince or not. It maybe contains the word Udacity, or not.

19
00:00:46,240 --> 00:00:46,890
>> Mm.

20
00:00:46,890 --> 00:00:51,260
>> Right? And, so, just as we think about these as these random variables.

21
00:00:51,260 --> 00:00:54,060
If we're trying to build a belief net or a Bayes net

22
00:00:54,060 --> 00:00:57,740
with these variables. We have to say. kind of, what's dependent on what. In

23
00:00:57,740 --> 00:01:01,270
terms of representing the probabilities. So how would you, how do you

24
00:01:01,270 --> 00:01:05,099
think we should draw arrows to, to relate these to quantities to each other.

25
00:01:05,099 --> 00:01:10,740
>> I think that the arrows should go down from spam to

26
00:01:10,740 --> 00:01:17,090
the other features of spam mail and I'll tell you why. Because if,

27
00:01:17,090 --> 00:01:19,950
I like this notion of generation that you talked about a

28
00:01:19,950 --> 00:01:23,500
little bit earlier. It seems to me if you know. Spam

29
00:01:23,500 --> 00:01:29,730
mail or not. It sort of generates certain words. And as

30
00:01:29,730 --> 00:01:32,050
written as these are like words I mean I know the, the

31
00:01:32,050 --> 00:01:35,250
spam example these are you know, kind of stand ins for

32
00:01:35,250 --> 00:01:37,990
features. But they're sort of features of spam mail. Yeah I

33
00:01:37,990 --> 00:01:40,010
think that's a really good way to think about it. So,

34
00:01:40,010 --> 00:01:42,090
in some sense what we're saying if we draw the bayes net

35
00:01:42,090 --> 00:01:43,850
in this way, then any given email

36
00:01:43,850 --> 00:01:46,900
message has some probability of being spam. And

37
00:01:46,900 --> 00:01:48,990
given that it's spam, it has some

38
00:01:48,990 --> 00:01:51,730
probability of containing different sets of possible words.

39
00:01:51,730 --> 00:01:52,430
>> Right.

40
00:01:52,430 --> 00:01:56,710
>> So, I would say that, well what, so what do you, oh

41
00:01:56,710 --> 00:02:00,060
let's see if we can actually fill in some of these values. So

42
00:02:00,060 --> 00:02:02,120
given that we have a spam message, how likely do you think it

43
00:02:02,120 --> 00:02:05,350
would be to contain a word like, well let's say the word viagra.

44
00:02:05,350 --> 00:02:06,190
>> Fairly high.

45
00:02:06,190 --> 00:02:07,415
>> It might

46
00:02:07,415 --> 00:02:14,715
be 0.3, but a non-spam message might be, I don't know, like 0.001.

47
00:02:14,715 --> 00:02:15,280
>> Right.

48
00:02:15,280 --> 00:02:18,260
>> Something like that. So how about a word like prince?

49
00:02:18,260 --> 00:02:20,870
>> Well I get a lot of email about Prince because I'm a Prince fan.

50
00:02:20,870 --> 00:02:22,510
>> Yeah, I was thinking that. That's

51
00:02:22,510 --> 00:02:24,950
why I thought it would an interesting example.

52
00:02:24,950 --> 00:02:28,790
So, if in your spam messages, how likely is it for Prince to come up?

53
00:02:28,790 --> 00:02:30,213
>> Fairly low.

54
00:02:30,213 --> 00:02:33,130
>> Maybe like 0.2

55
00:02:33,130 --> 00:02:35,820
because you're talking about the Nigerian princes

56
00:02:35,820 --> 00:02:38,610
and whatnot. On the other hand among your

57
00:02:38,610 --> 00:02:42,540
non spam messages how likely is it for prince to come up, do you think?

58
00:02:42,540 --> 00:02:45,430
>> Well I get a lot of non spam, so,

59
00:02:45,430 --> 00:02:48,539
its still relatively low, but not as low as .001.

60
00:02:48,539 --> 00:02:50,923
>> Alright, so, let's say .1.

61
00:02:50,923 --> 00:02:51,150
>> Okay.

62
00:02:51,150 --> 00:02:54,120
>> That's a lot of prince spam.

63
00:02:54,120 --> 00:02:55,840
>> You can never have enough prince spam.

64
00:02:57,160 --> 00:02:58,380
>> Alright, so in the messages

65
00:02:58,380 --> 00:03:01,500
that you have that are spam, how often does the word Udacity come up?

66
00:03:01,500 --> 00:03:03,380
>> I guess, it's pretty low.

67
00:03:03,380 --> 00:03:05,120
>> I don't think I've ever seen a spam

68
00:03:05,120 --> 00:03:09,500
that mentions Udacity. Alright, what about your non-spam email?

69
00:03:09,500 --> 00:03:11,950
>> Again, increasingly, it's getting higher and higher.

70
00:03:11,950 --> 00:03:12,191
>> [LAUGH]

71
00:03:12,191 --> 00:03:14,370
>> Almost as much as I get prince mail.

72
00:03:14,370 --> 00:03:16,130
All right, so we'll call that .1 as well then.

73
00:03:16,130 --> 00:03:16,650
>> Okay.

74
00:03:16,650 --> 00:03:18,270
>> All right, so now we have, oh an,

75
00:03:18,270 --> 00:03:20,850
an what's the probability of spam versus not spam?

76
00:03:20,850 --> 00:03:23,730
>> [INAUDIBLE] Probability to have spam is pretty low,

77
00:03:23,730 --> 00:03:26,630
I'm going to say, at this point, actually; it's not

78
00:03:26,630 --> 00:03:29,890
that low. At this point, it's probably half my mail.

79
00:03:29,890 --> 00:03:33,290
>> Wow. All right, I'm going to say .4

80
00:03:33,290 --> 00:03:35,870
Alright, so this is now, Bayesian network structure

81
00:03:35,870 --> 00:03:38,690
that actually is, it's not exactly generating spam,

82
00:03:38,690 --> 00:03:40,500
but it is kind of capturing features of email

83
00:03:40,500 --> 00:03:42,420
messages as they come in. So, we should

84
00:03:42,420 --> 00:03:44,070
be able to answer questions like what's the

85
00:03:44,070 --> 00:03:46,400
probability that a given message is spam, given

86
00:03:46,400 --> 00:03:48,860
that the message has Viagra in it but not

87
00:03:48,860 --> 00:03:51,810
prince or udacity. So, how would we work this out?

88
00:03:51,810 --> 00:03:55,580
>> Well, Since it says Naive Bays I think I would use Bayes rule.

89
00:03:56,670 --> 00:03:59,260
>> That would be naive of you. Now we have applied

90
00:03:59,260 --> 00:04:02,080
Bayes rule, we have flipped things around, why is this giving

91
00:04:02,080 --> 00:04:05,640
us an advantage? For this kind of network structure it actually

92
00:04:05,640 --> 00:04:08,850
has a huge advantage because we can break this first quantity up.

93
00:04:08,850 --> 00:04:13,950
>> Oh I do see that, so this is where those conditional independences

94
00:04:13,950 --> 00:04:18,950
come into play If I'm reading this network right, each one of those attribute

95
00:04:18,950 --> 00:04:22,480
values is conditionally independent of each other,

96
00:04:22,480 --> 00:04:24,070
given that you know the value of SPAM.

97
00:04:24,070 --> 00:04:25,100
>> Excellent.

98
00:04:25,100 --> 00:04:29,210
>> So then that means that the first quantity there

99
00:04:29,210 --> 00:04:33,760
is actually a product of each of those conditional probabilities.

100
00:04:33,760 --> 00:04:36,400
>> Yeah, so this is a really convenient structure.

101
00:04:36,400 --> 00:04:39,500
Because it really just decomposes into all these separate

102
00:04:39,500 --> 00:04:42,880
helpful quantities. So in particular, we can actually derive

103
00:04:42,880 --> 00:04:46,260
this by applying the chain rule. But what we end

104
00:04:46,260 --> 00:04:49,380
up with is that this joint probability over these

105
00:04:49,380 --> 00:04:52,550
three variables decomposes into a product of three independent joint

106
00:04:52,550 --> 00:04:56,100
probabilities. The probability that's, Contains viagra given that it's

107
00:04:56,100 --> 00:04:59,230
spam, which we have. That number is 0.3. That probability

108
00:04:59,230 --> 00:05:01,040
that prince doesn't appear in it, given that it's

109
00:05:01,040 --> 00:05:04,830
spam and that is that it doesn't contain prince given

110
00:05:04,830 --> 00:05:09,655
that it is spam. So that should 0.8, cause 1 minus the

111
00:05:09,655 --> 00:05:14,870
0.2. And that it's not udacity given that it's spam. Is

112
00:05:14,870 --> 00:05:19,971
going to be 1 minus this 0.0001, should be 0.9999. All right.

113
00:05:19,971 --> 00:05:25,750
So this is the case when things, when it is spam, and if it's not spam, we

114
00:05:25,750 --> 00:05:27,840
can do this same thing and get a product,

115
00:05:27,840 --> 00:05:29,920
and that we can normalize, to get what the,

116
00:05:29,920 --> 00:05:34,280
the relative probabilities between it being spam and not spam. So then I'm a

117
00:05:34,280 --> 00:05:37,910
big fan of normalization, but of course this makes me think about, since it's

118
00:05:37,910 --> 00:05:40,030
sort of a classification problem, we only

119
00:05:40,030 --> 00:05:42,360
really care about knowing which one's more

120
00:05:42,360 --> 00:05:43,650
likely. We don't really care about the

121
00:05:43,650 --> 00:05:45,310
probability, right? Do we have to normalize?

122
00:05:45,310 --> 00:05:47,470
>> Yeah, yeah because we do care about the probability.

123
00:05:47,470 --> 00:05:48,390
>> Oh we do?

124
00:05:48,390 --> 00:05:50,260
>> Yeah because we're... I asked" What is the

125
00:05:50,260 --> 00:05:53,160
probability of spam given these other quantities. Oh, I see.

126
00:05:53,160 --> 00:05:55,090
>> But you're right. So the observation

127
00:05:55,090 --> 00:05:57,730
that you're making is a really good one. Which is that we

128
00:05:57,730 --> 00:05:59,610
can do probability calculations in this

129
00:05:59,610 --> 00:06:01,010
setting, and that's actually going to give

130
00:06:01,010 --> 00:06:03,810
us answers to classification problems. And we're going to connect this back to

131
00:06:03,810 --> 00:06:07,480
machine learning. But but first let's write a general form of this formula.

132
00:06:07,480 --> 00:06:08,220
>> Okay.

133
00:06:08,220 --> 00:06:11,770
>> Because this this seems a little bit specific. Alright so

134
00:06:11,770 --> 00:06:13,400
the general form for this, is that if we're trying to figure

135
00:06:13,400 --> 00:06:15,780
out the probability of, of some kind of a a root node

136
00:06:15,780 --> 00:06:20,250
like this, when you have all these little bristly things coming down.

137
00:06:20,250 --> 00:06:21,550
You can think of it as a probability of a

138
00:06:21,550 --> 00:06:25,110
value given a bunch of attributes. And that's going to be equal

139
00:06:25,110 --> 00:06:27,980
to the product of the probability that each of those

140
00:06:27,980 --> 00:06:32,640
attributes would be generated by that. Underlying this v. This, this

141
00:06:32,640 --> 00:06:35,740
the label or the or the underlying class. Times the

142
00:06:35,740 --> 00:06:38,500
prior probability that v and then we just normalize by all

143
00:06:38,500 --> 00:06:41,920
the different possible values of, of v. This, this quantity across

144
00:06:41,920 --> 00:06:45,660
all the possible types of v. So so this is one

145
00:06:45,660 --> 00:06:50,306
way of actually getting a very general kind of inference done,

146
00:06:50,306 --> 00:06:53,020
and there's, as you were pointing out, Charles, there's a. There's

147
00:06:53,020 --> 00:06:55,940
a really nice reason to think about things in this form,

148
00:06:55,940 --> 00:06:58,550
because it does let you do a kind of classification. So

149
00:06:58,550 --> 00:07:01,810
essentially if you think of, of this top node as being

150
00:07:01,810 --> 00:07:03,800
the class, this is what was playing the role of V

151
00:07:03,800 --> 00:07:07,570
here, and these are all a bunch of attributes, then even

152
00:07:07,570 --> 00:07:11,380
if, if we have a way of generating attribute values from classes.

153
00:07:11,380 --> 00:07:13,350
What this lets us do is to go the other way.

154
00:07:13,350 --> 00:07:16,310
That we observe the attribute values and we can infer the class.

155
00:07:16,310 --> 00:07:18,670
>> Nice, so what's the equation for that?

156
00:07:18,670 --> 00:07:20,980
>> Right, so the, the maximum oposterior

157
00:07:20,980 --> 00:07:22,700
class if you're just trying to find whats

158
00:07:22,700 --> 00:07:27,390
the most likely class given the, the data that you've seen. You can just take

159
00:07:27,390 --> 00:07:31,430
an arg max over all the different possible values of that, that root node of

160
00:07:31,430 --> 00:07:33,880
the prob, its probability times the product

161
00:07:33,880 --> 00:07:36,650
of all the attribute values given that class.

162
00:07:36,650 --> 00:07:39,820
So this would actually let us if you're, if you're been paying attention,

163
00:07:39,820 --> 00:07:45,770
we could, in this particular case, compute map spam. Which is a palindrome.

164
00:07:45,770 --> 00:07:48,650
>> Wow. That is spectacular.

165
00:07:48,650 --> 00:07:50,350
>> You did not see that coming did you?

166
00:07:50,350 --> 00:07:51,680
>> No I did not.
