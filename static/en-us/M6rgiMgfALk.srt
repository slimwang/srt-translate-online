1
00:00:00,067 --> 00:00:03,782
So for our next version, let's parallelize this per element, right?

2
00:00:03,782 --> 00:00:07,474
So we essentially replaced the inner loop with a thread launch.

3
00:00:07,474 --> 00:00:10,631
Let's replace the outer and inner loops with a thread launch.

4
00:00:10,631 --> 00:00:13,550
And I'm going to leave this as a programming exercise for you to do.

5
00:00:13,550 --> 00:00:15,828
We'll give you the same code that we've been using here

6
00:00:15,828 --> 00:00:18,936
with a little bit more instrumentation that should be pretty obvious.

7
00:00:18,936 --> 00:00:22,292
Go ahead and run this code, verify the timings.

8
00:00:22,292 --> 00:00:26,361
They might be different on whatever GPU and system you're running on.

9
00:00:26,361 --> 00:00:28,667
And then add a new kernel,

10
00:00:28,667 --> 00:00:33,400
which performs the transpose on a per element basis and see how it times out.

11
00:00:33,400 --> 00:00:36,806
Now one thing to be aware of when you're doing this programming exercise

12
00:00:36,806 --> 00:00:43,194
is that you can only launch up to a 1,024 threads in a thread block,

13
00:00:43,194 --> 00:00:46,861
so I suggest you organize your code to use K by K thread blocks,

14
00:00:46,861 --> 00:00:49,745
multiple thread blocks, each of dimension of K by K,

15
00:00:49,745 --> 00:00:52,355
and use a value of, like, 16 for K for now.

16
00:00:52,355 --> 00:00:56,000
We're going to play around with this value a little bit later.
