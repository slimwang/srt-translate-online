1
00:00:00,120 --> 00:00:03,430
Now, let's try to see what will
happen if we're considering two I/O

2
00:00:03,430 --> 00:00:04,410
bound tasks.

3
00:00:04,410 --> 00:00:09,143
And again, we'll think of two tasks that
have execution time of ten seconds, and

4
00:00:09,143 --> 00:00:12,858
in a system that has a context
switch overhead of 0.1 second.

5
00:00:12,858 --> 00:00:17,767
And let's also assume that the nature in
which these I/O calls are performed is

6
00:00:17,767 --> 00:00:21,690
that a task issues an I/O
operation every one second.

7
00:00:21,690 --> 00:00:24,210
And also let's assume that
every single one of those I/O

8
00:00:24,210 --> 00:00:27,330
operations complete in
exactly half a second.

9
00:00:27,330 --> 00:00:31,170
If we look at the timeline, it looks
identical as what we saw in the case for

10
00:00:31,170 --> 00:00:35,050
the CPU-bound jobs with
a time slice of one second.

11
00:00:35,050 --> 00:00:39,990
This makes sense, because exactly after
one second, the tasks are, in this case,

12
00:00:39,990 --> 00:00:41,260
not exactly preempted.

13
00:00:41,260 --> 00:00:45,070
They actually end up issuing an I/O
operation, so yielding voluntarily,

14
00:00:45,070 --> 00:00:48,020
regardless of the fact
that the time slice is 1.

15
00:00:48,020 --> 00:00:50,430
So if we look at the performance
metric for this case,

16
00:00:50,430 --> 00:00:55,190
they will be identical to the previous
case in the case of the CPU bound tasks.

17
00:00:55,190 --> 00:00:59,340
Now if we look at the second case that
has a time slice value of five seconds,

18
00:00:59,340 --> 00:01:04,160
we see that the timelines, the schedules
of the two tasks, are identical to

19
00:01:04,160 --> 00:01:08,070
the case when we had a much smaller
time slice value of one second.

20
00:01:08,070 --> 00:01:11,850
Similarly, obviously, if we compute the
matrix, they will be identical, it's for

21
00:01:11,850 --> 00:01:13,850
two identical schedules.

22
00:01:13,850 --> 00:01:14,530
The reason for

23
00:01:14,530 --> 00:01:19,300
this is that at this particular moment,
we're not exactly time slicing.

24
00:01:19,300 --> 00:01:22,990
We're not interrupting the tasks
in either one of these cases.

25
00:01:22,990 --> 00:01:26,410
The I/O operation, again,
is issued every one second.

26
00:01:26,410 --> 00:01:29,400
So regardless of the fact
that in this scenario,

27
00:01:29,400 --> 00:01:31,900
the time slice value is much longer,

28
00:01:31,900 --> 00:01:36,470
we still end up issuing an I/O operation
at the end of the first second.

29
00:01:36,470 --> 00:01:40,830
And therefore, the CPU is at
this point released from T1,

30
00:01:40,830 --> 00:01:45,940
T1 yields, and T2 is free to be
scheduled and to start executing.

31
00:01:45,940 --> 00:01:48,775
So one conclusion that you can
make from this is that for

32
00:01:48,775 --> 00:01:52,500
I/O bound tasks, the value of the time
slice is not really relevant.

33
00:01:53,620 --> 00:01:55,700
Well, let's not draw that
conclusion that fast.

34
00:01:55,700 --> 00:02:00,310
Let's look first at a scenario
where only T2 is an I/O bound task.

35
00:02:00,310 --> 00:02:04,580
T1 is a CPU bound task,
as what we saw in the previous morsel.

36
00:02:04,580 --> 00:02:07,990
In that case, the execution for
the two tasks T1 and

37
00:02:07,990 --> 00:02:11,860
T2 when the timeslice is
1 will look the same.

38
00:02:11,860 --> 00:02:17,650
The one difference is that in the event
of T1, we preempted after one second,

39
00:02:17,650 --> 00:02:22,810
where as in the case of T2,
the I/O bound task, after one second,

40
00:02:22,810 --> 00:02:27,820
it voluntarily yields since it has
to go and wait for an I/O operation.

41
00:02:27,820 --> 00:02:31,350
In the case of five seconds,
the execution of T1 and

42
00:02:31,350 --> 00:02:33,710
T2 will look something as follows.

43
00:02:33,710 --> 00:02:38,330
T1 will run for five seconds, and at
that point, its time slice will expire,

44
00:02:38,330 --> 00:02:39,880
so it will be preempted.

45
00:02:39,880 --> 00:02:43,380
T2 will be scheduled, and
as an I/O bound task,

46
00:02:43,380 --> 00:02:48,530
T2 will yield after one second because
of reading on an I/O operation.

47
00:02:48,530 --> 00:02:51,569
At that point,
T1 will be scheduled again.

48
00:02:51,569 --> 00:02:55,584
Now, at this point, T1 is actually
complete, so T2 is the only runnable

49
00:02:55,584 --> 00:03:00,300
task in the system, and that's why we
have this illustrated in this manner.

50
00:03:00,300 --> 00:03:03,180
If we work out the performance
metrics for this last case,

51
00:03:03,180 --> 00:03:05,020
the numbers will look as follows.

52
00:03:05,020 --> 00:03:06,020
And again, for all of these,

53
00:03:06,020 --> 00:03:08,730
the calculations are posted
in the instructor notes.

54
00:03:08,730 --> 00:03:12,680
We see that both with respect to
throughput and the average wait time,

55
00:03:12,680 --> 00:03:17,410
the use of a smaller time slice
results in better performance.

56
00:03:17,410 --> 00:03:21,440
The only reason why, in this case, the
average completion time is so low, then,

57
00:03:21,440 --> 00:03:24,560
if you look at the calculations,
there is a huge variance between

58
00:03:24,560 --> 00:03:29,620
the completion time of T1, which is at
11, and then T2, which is way later.

59
00:03:29,620 --> 00:03:32,360
We see from this that for
I/O bound tasks,

60
00:03:32,360 --> 00:03:35,380
using a smaller time slice is better.

61
00:03:35,380 --> 00:03:40,250
The I/O bound task with a smaller time
slice has a chance to run more quickly,

62
00:03:40,250 --> 00:03:43,920
to issue an I/O request,
or to respond to a user.

63
00:03:43,920 --> 00:03:45,660
And with a smaller time slice,

64
00:03:45,660 --> 00:03:50,150
we're able to keep both the CPU
as well as the I/O devices busy,

65
00:03:50,150 --> 00:03:53,830
which makes, obviously,
the system operator quite happy.
