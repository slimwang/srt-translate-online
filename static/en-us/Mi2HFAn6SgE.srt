1
00:00:00,000 --> 00:00:04,000
Now what that suggests is the design for an exploration agent

2
00:00:04,000 --> 00:00:09,000
that will be more proactive about exploring the world when it's uncertain,

3
00:00:09,000 --> 00:00:15,000
and will fall back to exploiting the optimal policy--or whatever policy it has as close to optimal--

4
00:00:15,000 --> 00:00:17,000
when it becomes more certain about the world.

5
00:00:17,000 --> 00:00:19,000
And what we can do is go through this

6
00:00:19,000 --> 00:00:21,000
normal cycle of TD learning--

7
00:00:21,000 --> 00:00:23,000
like we always did.

8
00:00:23,000 --> 00:00:25,000
But when we're looking for the estimate

9
00:00:25,000 --> 00:00:27,000
of the utility of the state,

10
00:00:27,000 --> 00:00:29,000
what we can do is say:

11
00:00:29,000 --> 00:00:33,000
The utility of the state estimate will be

12
00:00:33,000 --> 00:00:36,000
some large value, plus R--

13
00:00:36,000 --> 00:00:40,000
say, plus 1--in the case of this example--

14
00:00:40,000 --> 00:00:43,000
the largest reward we can expect to get.

15
00:00:43,000 --> 00:00:48,000
In every case, when the number of visits to the state

16
00:00:48,000 --> 00:00:52,000
is less than the sum threshold, E, the exploration threshold.

17
00:00:52,000 --> 00:00:55,000
And when we've visited a state E times,

18
00:00:55,000 --> 00:00:58,000
then we revert to the learned probabilities

19
00:00:58,000 --> 00:01:01,000
or the learned utilities, rather.

20
00:01:01,000 --> 00:01:05,000
So when we start out, we're going to explore from new states;

21
00:01:05,000 --> 00:01:09,000
and once we have a good estimate of what the true utility of the state actually is,

22
00:01:09,000 --> 99:59:59,999
then we stop exploring and we go with those utilities.
