1
00:00:00,420 --> 00:00:03,010
Okay, so, I want to talk about two things on this

2
00:00:03,010 --> 00:00:06,160
little slide, here. The first one is kind of a general

3
00:00:06,160 --> 00:00:10,300
observation about rewards and what makes the reinforcement learning MDP problem

4
00:00:10,300 --> 00:00:13,970
different from the supervised learning problems that we did before. And

5
00:00:13,970 --> 00:00:17,120
that's the notion of not just rewards, but this notion

6
00:00:17,120 --> 00:00:20,450
of delayed rewards. So, what do I mean by that. Well,

7
00:00:20,450 --> 00:00:22,540
so, if you think about the way we've set up kind

8
00:00:22,540 --> 00:00:25,120
of problem like, like we have here where we're trying to.

9
00:00:25,120 --> 00:00:29,440
Start in this little bottom left-hand square, and wind

10
00:00:29,440 --> 00:00:32,390
our way up into this plus one. There's really

11
00:00:32,390 --> 00:00:35,709
this notion of sequences of action. So in the

12
00:00:35,709 --> 00:00:39,863
reinforcement learning context, and all the things that we're talking

13
00:00:39,863 --> 00:00:43,213
about, at least in the foreseeable future, there's this

14
00:00:43,213 --> 00:00:45,759
sort of problem where you take some action and

15
00:00:45,759 --> 00:00:47,903
that puts you in some place and then you

16
00:00:47,903 --> 00:00:51,010
take another action and that puts you in some place.

17
00:00:51,010 --> 00:00:53,038
And then you take another action and that puts you in some place

18
00:00:53,038 --> 00:00:55,000
and maybe it puts you at a place where you get plus one.

19
00:00:55,000 --> 00:00:59,490
Or maybe it puts you at a place where you get minus one.

20
00:00:59,490 --> 00:01:03,200
And what really is going on here is this idea that you take

21
00:01:03,200 --> 00:01:06,420
actions that will set you up for other actions that will set you

22
00:01:06,420 --> 00:01:09,480
up for other actions. And only then do you know how good those

23
00:01:09,480 --> 00:01:13,600
particular actions you took were. So this reward is not just an idea

24
00:01:13,600 --> 00:01:16,710
of getting a reward at every state, it's an idea of getting delayed reward.

25
00:01:16,710 --> 00:01:19,130
So you don't know how your immediate action

26
00:01:19,130 --> 00:01:21,540
is going to lead to things down the road.

27
00:01:21,540 --> 00:01:22,830
So, let me give you a concrete example

28
00:01:22,830 --> 00:01:24,720
about that. So, have you ever played chess?

29
00:01:24,720 --> 00:01:25,470
>> Sure.

30
00:01:25,470 --> 00:01:26,770
>> So let's say we played a long game

31
00:01:26,770 --> 00:01:28,970
of chess and maybe took 60, 61, 62 moves. And

32
00:01:28,970 --> 00:01:33,270
then at the end, I win the game. So, what

33
00:01:33,270 --> 00:01:36,140
do you know about the way you played that game?

34
00:01:36,140 --> 00:01:39,350
>> That I probably made a bad decision around

35
00:01:39,350 --> 00:01:41,710
the time when I decided to play chess against you.

36
00:01:41,710 --> 00:01:44,040
>> That's possible, but maybe you made a

37
00:01:44,040 --> 00:01:47,080
good decision and you kept making good decisions, and

38
00:01:47,080 --> 00:01:49,450
you only messed up at the very last move,

39
00:01:49,450 --> 00:01:50,890
when you could have mated me but you didn't.

40
00:01:50,890 --> 00:01:52,970
>> I see. Well you, what, my

41
00:01:52,970 --> 00:01:55,290
experience in playing chess is that usually, that's

42
00:01:55,290 --> 00:01:58,900
not what happens. Usually it's not that I make a bad move and then there's

43
00:01:58,900 --> 00:02:02,250
a problem. It's usually I make a move that I think is reasonable that only

44
00:02:02,250 --> 00:02:05,980
later I discover put me in a position where I can't possibly do anything good.

45
00:02:05,980 --> 00:02:07,500
>> Right. Well that's actually

46
00:02:07,500 --> 00:02:09,229
the game that I played in New York many,

47
00:02:09,229 --> 00:02:11,840
many years ago that I lost was exactly like that.

48
00:02:11,840 --> 00:02:13,420
The game went on for like literally a hundred

49
00:02:13,420 --> 00:02:15,590
moves. I really lost the game on the third move.

50
00:02:15,590 --> 00:02:17,730
>> Oh.

51
00:02:17,730 --> 00:02:21,040
>> I made a mistake, i transposed two moves because it was a new

52
00:02:21,040 --> 00:02:24,800
opening that I was just learning and I knew at the time that i screwed

53
00:02:24,800 --> 00:02:28,340
up, i played beautiful chess from that point on, but the truth is the

54
00:02:28,340 --> 00:02:30,360
other player had a positional advantage from

55
00:02:30,360 --> 00:02:32,920
that point on that I could never overcome,

56
00:02:32,920 --> 00:02:36,710
so I lost the game, but not because I played poorly for,

57
00:02:36,710 --> 00:02:39,840
you know, 80 moves. It's because I paid, played poorly for one

58
00:02:39,840 --> 00:02:42,850
move, and that move happened to be fairly early. So this is

59
00:02:42,850 --> 00:02:45,810
this notion of delayed reward, that I played this long game of

60
00:02:45,810 --> 00:02:49,070
chess, and maybe it's I played well, and I screwed up in

61
00:02:49,070 --> 00:02:52,310
the end. Maybe I played medio, you know, mediocre game, but I

62
00:02:52,310 --> 00:02:54,960
had a couple of brilliant moves, and that's why I won, or

63
00:02:54,960 --> 00:02:58,130
maybe I played very well in the beginning, poorly at the end or

64
00:02:58,130 --> 00:03:00,170
the other way around. And the truth is you don't

65
00:03:00,170 --> 00:03:02,370
really know. All you know is that you're taking a bunch

66
00:03:02,370 --> 00:03:05,550
of actions. You get rewards signals back from the environment. Like

67
00:03:05,550 --> 00:03:07,710
I won the game or I lost the game. And then

68
00:03:07,710 --> 00:03:09,940
you have this problem of figuring out of all the actions

69
00:03:09,940 --> 00:03:13,990
that I took, what was the action that led to me

70
00:03:13,990 --> 00:03:17,990
ultimately winning or losing, or getting whatever reward that I, I

71
00:03:17,990 --> 00:03:20,020
got at the end of a sequence. Does that make sense?

72
00:03:20,020 --> 00:03:21,460
>> Yeah it seems like that could

73
00:03:21,460 --> 00:03:23,170
be really challenging. You probably need your

74
00:03:23,170 --> 00:03:26,310
own like sportscaster listening and, and commenting.

75
00:03:26,310 --> 00:03:29,100
>> Yeah and in fact the sportscaster who is listening and commenting

76
00:03:29,100 --> 00:03:32,500
at least in the NDP world is in fact the sequence of rewards

77
00:03:32,500 --> 00:03:34,750
that you get in the sequence of states that you see right?

78
00:03:34,750 --> 00:03:37,220
It really is sort of a play by play. In this state this

79
00:03:37,220 --> 00:03:39,680
action got this reward and you want to take all of that

80
00:03:39,680 --> 00:03:42,220
and figure out whether this was a good action you're first action or

81
00:03:42,220 --> 00:03:44,790
a poor action or your second action was good or poor and so

82
00:03:44,790 --> 00:03:48,170
on and so forth. Now contrast that with supervised learning, so in supervised

83
00:03:48,170 --> 00:03:51,720
learning what you would be getting is while I was in this state This

84
00:03:51,720 --> 00:03:54,840
first date and then this was the proper action I was supposed to take

85
00:03:54,840 --> 00:03:58,060
lets say action seventeen. And your goal then is just to simply learn a

86
00:03:58,060 --> 00:04:01,640
function from states to specific actions, that's

87
00:04:01,640 --> 00:04:03,400
how the supervised learning problem is setup, right?

88
00:04:03,400 --> 00:04:04,320
>> Yeah exactly.

89
00:04:04,320 --> 00:04:05,860
>> In this particular case you're in some

90
00:04:05,860 --> 00:04:09,130
state you take some action and you get

91
00:04:09,130 --> 00:04:13,090
some reward for the action you took. Or may be the state that you ended up

92
00:04:13,090 --> 00:04:15,590
in or something. And you get a sequence of these

93
00:04:15,590 --> 00:04:19,350
state action award triples, and ultimately you have to figure out

94
00:04:19,350 --> 00:04:23,900
for the given states you're in, what was the action

95
00:04:23,900 --> 00:04:26,460
you took that helped to deter, or actions you took that

96
00:04:26,460 --> 00:04:29,150
helped to determine the ultimate sequence of rewards that you

97
00:04:29,150 --> 00:04:32,780
saw. Perhaps this one plus one or this minus one that

98
00:04:32,780 --> 00:04:35,510
you got at the end. This is really difficult problem,

99
00:04:35,510 --> 00:04:38,560
its got its own name, its called the credit assignment problem.

100
00:04:38,560 --> 00:04:40,860
In fact, we're talking about a sequence

101
00:04:40,860 --> 00:04:42,930
of events over time, we typically refer to

102
00:04:42,930 --> 00:04:44,330
this type of problem as the temporal

103
00:04:44,330 --> 00:04:46,470
credit assignment problem. Does that make sense Michael.

104
00:04:46,470 --> 00:04:50,600
>> Yeah that's really cool. Although I guess its credit but also blame right?

105
00:04:50,600 --> 00:04:53,950
>> Well credit and blame you know, blame is just the minus of credit.

106
00:04:53,950 --> 00:04:56,490
So without loss of generality Assume that

107
00:04:56,490 --> 00:04:58,930
credit can assume a negative or positive vibe.

108
00:04:58,930 --> 00:05:01,990
>> And yet when I go to the supermarket they never say blame or credit.

109
00:05:01,990 --> 00:05:03,850
>> Well that's just because they don't understand

110
00:05:03,850 --> 00:05:04,940
the technical terms.

111
00:05:04,940 --> 00:05:05,160
>> Got it.
