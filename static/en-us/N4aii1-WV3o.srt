1
00:00:00,200 --> 00:00:04,120
Okay. Have you ever wondered how you could write the equivalent of

2
00:00:04,120 --> 00:00:08,300
a Watson program or a CD program for your own computer?

3
00:00:08,300 --> 00:00:13,200
Just imagine if you could talk to your machine in terms of stories.

4
00:00:13,200 --> 00:00:16,920
Simple stories. Perhaps just one sentence stories like Ashok ate a frog. Now,

5
00:00:16,920 --> 00:00:20,710
we've already seen how a machine can make sense of this particular sentence,

6
00:00:20,710 --> 00:00:24,780
Ashok ate a frog. We did that last time. But, of course, eat or

7
00:00:24,780 --> 00:00:29,130
ate can occur in many different ways in sentences. Here are some of the other

8
00:00:29,130 --> 00:00:35,410
ways in which I can use the verb eat. Ashok ate out at a restaurant.

9
00:00:35,410 --> 00:00:38,920
I could tell something was really eating him. And, the sense of eating him here,

10
00:00:38,920 --> 00:00:42,500
is very different from the sense of eating here. There's no physical object that

11
00:00:42,500 --> 00:00:46,730
is being eaten here. The manager would rather eat the losses. So now this is

12
00:00:46,730 --> 00:00:51,250
a very different notion of eat. The river gradually ate away at the riverbank.

13
00:00:51,250 --> 00:00:54,750
Yet another notion of eat. So just like we had in the previous lesson,

14
00:00:54,750 --> 00:00:58,400
take, the word take, which had so many different meanings, eat has so

15
00:00:58,400 --> 00:01:02,700
many different meanings. Now when we were discussing the word take,

16
00:01:02,700 --> 00:01:08,320
we discussed how we can use both the structure of sentences as well as

17
00:01:08,320 --> 00:01:13,150
background knowledge to disambiguate between different interpretations of take.

18
00:01:13,150 --> 00:01:15,160
We could do something similar with eat.

19
00:01:15,160 --> 00:01:20,050
We could enumerate all the different meanings of eat, then for each meaning of

20
00:01:20,050 --> 00:01:24,290
eat, we could ask ourselves what is it about the structure of the sentence and

21
00:01:24,290 --> 00:01:28,100
what is it about the background knowledge I have about things like rivers and

22
00:01:28,100 --> 00:01:31,730
river banks which tells me what the meaning of ate here is.

23
00:01:31,730 --> 00:01:36,330
To summarize this part, if you were to start talking to your machine in stories,

24
00:01:36,330 --> 00:01:41,560
in terms of simple stories designated by a single sentence, then one problem

25
00:01:41,560 --> 00:01:45,940
that will occur is, that words like ate or take will have large number of

26
00:01:45,940 --> 00:01:49,770
meanings. And we have seen how your machine can be programmed in such a way so

27
00:01:49,770 --> 00:01:53,950
as to disambiguate between different kinds of meanings of the same word.

28
00:01:53,950 --> 00:01:58,480
Now here is a different problem that occurs. Consider a number of stories again.

29
00:01:58,480 --> 00:02:02,420
Each story here is designated by a single sentence. Ashok ate a frog,

30
00:02:02,420 --> 00:02:05,690
Ashok devoured a frog, Ashok consumed a frog, Ashok ingested a frog, and

31
00:02:05,690 --> 00:02:10,270
so on. Several sentences here. And if we think about it a little bit,

32
00:02:10,270 --> 00:02:14,430
each of these sentences is using a different verb. But

33
00:02:14,430 --> 00:02:19,150
the meaning of each of these words in this sentence is pretty much the same.

34
00:02:19,150 --> 00:02:22,500
So whether Ashok ingested a frog or Ashok devoured a frog or

35
00:02:22,500 --> 00:02:26,060
Ashok dined on a frog, exactly the same thing happened in each case.

36
00:02:26,060 --> 00:02:29,080
There was a frog that was essentially outside of Ashok's body and

37
00:02:29,080 --> 00:02:32,500
then Ashok ate it up and at the end the frog was inside Ashok's body.

38
00:02:32,500 --> 00:02:37,440
The frog was dead and Ashok was happy. So the next challenge becomes, how can

39
00:02:37,440 --> 00:02:41,560
a machine understand that the meaning of each of these words here is exactly

40
00:02:41,560 --> 00:02:45,400
the same? In a way this problem is the inverse of the problem that we had here.

41
00:02:45,400 --> 00:02:49,310
Here the problem was that we had a single word like eat, which had got a lot of

42
00:02:49,310 --> 00:02:52,400
different meanings and different context in different sentences.

43
00:02:52,400 --> 00:02:55,980
Here, the issue is that we have a lot of different words, but

44
00:02:55,980 --> 00:03:00,010
they have the same meaning given the context of the sentences. So

45
00:03:00,010 --> 00:03:04,370
another question then becomes, well how can we make machines understand that

46
00:03:04,370 --> 00:03:08,280
the meaning of these words in the sentences is exactly the same?

47
00:03:08,280 --> 00:03:11,470
Each of the sentences telling us exactly the same story. There is

48
00:03:11,470 --> 00:03:15,360
one other thing that is important here, and that is the notion of context.

49
00:03:15,360 --> 00:03:19,970
One of the hardest things in AI is, how do we bring context into account?

50
00:03:19,970 --> 00:03:24,230
In both of these cases, context is playing an important role. On the left side,

51
00:03:24,230 --> 00:03:27,960
context is playing an important role because we understand that the meaning of

52
00:03:27,960 --> 00:03:30,930
eat is different in these different sentences, because the context of

53
00:03:30,930 --> 00:03:34,713
eat is different. Here context is playing a different role. We understand that

54
00:03:34,713 --> 00:03:38,502
the meaning of each of these words is practically the same because the context

55
00:03:38,502 --> 00:03:43,450
here is practically the same. A couple of quick qualifications here. First,

56
00:03:43,450 --> 00:03:47,430
right now we're dealing with stories that are just one sentences. Very soon in

57
00:03:47,430 --> 00:03:50,430
the next lesson, we'll deal with stories which are much more complicated,

58
00:03:50,430 --> 00:03:54,900
which really have a series of sentences. For now, just simple stories.

59
00:03:54,900 --> 00:04:00,330
The second is that here is structural all of these sentences is the same. So

60
00:04:00,330 --> 00:04:03,550
structure practically tells us something about the context. But

61
00:04:03,550 --> 00:04:08,550
a situation is a lot more complicated because often two sentences,

62
00:04:08,550 --> 00:04:12,440
which have very different kind of structures can still have the same meaning.

63
00:04:12,440 --> 00:04:18,560
So consider these two studies. Bob shot Bill, and Bob killed Bill with a gun.

64
00:04:18,560 --> 00:04:23,090
Now here the sentence structures are very different, but their interpretations,

65
00:04:23,090 --> 00:04:27,680
their meaning are the same. Bill was the one who got killed. Bob was the one who

66
00:04:27,680 --> 00:04:33,640
did the killing. And the killing was done by putting a bullet into Bill.

67
00:04:33,640 --> 00:04:37,840
So the question now becomes, how can we build AI agents that can meaningfully

68
00:04:37,840 --> 00:04:43,320
understand stories like this one? And stories are the kind that Bob shot Bill,

69
00:04:43,320 --> 00:04:47,720
and Bob killed Bill with a gun. One thing we could do is, that for

70
00:04:47,720 --> 00:04:52,510
each of the verbs here we could have a frame like we had a frame for take.

71
00:04:53,720 --> 00:04:57,090
So we could have a frame for ate, a frame for devoured, a frame for consumed,

72
00:04:57,090 --> 00:05:00,840
a frame for ingested, and so on. Well, we would have a lot of frames then.

73
00:05:01,940 --> 00:05:04,730
Because there are a large number of verbs in the English language or

74
00:05:04,730 --> 00:05:09,650
in any other modern language. Perhaps we could do something different.

75
00:05:09,650 --> 00:05:13,252
Perhaps we could look that there is a similarity between the interpretation of

76
00:05:13,252 --> 00:05:16,660
these things. Perhaps we could use the same primitive action for

77
00:05:16,660 --> 00:05:21,220
each one of them. So when we talk in English language, we might use for

78
00:05:21,220 --> 00:05:25,620
communication purposes all of these verbs. But perhaps we could compare AI

79
00:05:25,620 --> 00:05:30,400
agents that have a single internal representation for

80
00:05:30,400 --> 00:05:34,440
each one of them. What might that internal representation look like? We will

81
00:05:34,440 --> 00:05:39,970
call that representation a primitive action. Each one of these is an action. But

82
00:05:39,970 --> 00:05:45,830
many of these actions are equal in terms of our interpretation of those actions.

83
00:05:45,830 --> 00:05:47,760
Let's see what these primitive actions might look like.
