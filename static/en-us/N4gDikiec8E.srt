1
00:00:08,220 --> 00:00:10,040
Hey are you okay?

2
00:00:10,040 --> 00:00:12,680
Siraj, show them the math  behind deep learning.

3
00:00:13,320 --> 00:00:14,000
Totally.

4
00:00:14,000 --> 00:00:15,420
hello world it's SIraj and

5
00:00:15,430 --> 00:00:17,439
let's learn about the math needed to do

6
00:00:17,439 --> 00:00:20,170
deep learning. Math is in everything not

7
00:00:20,170 --> 00:00:21,939
just every field of engineering and

8
00:00:21,939 --> 00:00:24,070
science. It's between every note in a

9
00:00:24,070 --> 00:00:25,750
piece of music and hidden in the

10
00:00:25,750 --> 00:00:28,180
textures of a painting. Deep learning is no

11
00:00:28,180 --> 00:00:30,760
different. Math helps us define rules

12
00:00:30,760 --> 00:00:33,180
for our neural network so we can learn from our data.

13
00:00:33,190 --> 00:00:35,590
if you wanted to, you could use deep learning without

14
00:00:35,590 --> 00:00:37,440
ever knowing anything about math.

15
00:00:37,440 --> 00:00:38,440
There are a bunch of readily

16
00:00:38,440 --> 00:00:41,140
available APIs for tasks like computer

17
00:00:41,140 --> 00:00:43,180
vision and language translation, but if

18
00:00:43,180 --> 00:00:45,130
you want to use a library like TensorFlow

19
00:00:45,130 --> 00:00:48,160
to make a custom model to solve a problem

20
00:00:48,160 --> 00:00:50,040
knowing what math terms mean when

21
00:00:50,050 --> 00:00:51,800
you see them pop up is helpful

22
00:00:51,800 --> 00:00:53,280
and if you want to advance the

23
00:00:53,290 --> 00:00:54,730
field through research,

24
00:00:54,730 --> 00:00:57,040
don't even trip! You definitely need to

25
00:00:57,040 --> 00:00:59,020
know the math. Deep learning mainly pulls from

26
00:00:59,020 --> 00:01:01,210
three branches of math: linear algebra,

27
00:01:01,210 --> 00:01:03,790
statistics and calculus. if you don't

28
00:01:03,790 --> 00:01:05,320
know any of the topics, i recommend

29
00:01:05,320 --> 00:01:07,160
a cheat sheet of the important concepts

30
00:01:07,160 --> 00:01:09,520
and I've linked to one for each in the description

31
00:01:09,520 --> 00:01:11,049
so let's go over the four-step process

32
00:01:11,049 --> 00:01:13,109
of building a deep learning pipeline

33
00:01:13,109 --> 00:01:15,340
and talk about how math is used at  each step

34
00:01:15,340 --> 00:01:17,480
once we've got a dataset that we want to use

35
00:01:17,520 --> 00:01:19,659
we want you process it we can clean the

36
00:01:19,660 --> 00:01:21,700
data of any empty values, remove features

37
00:01:21,700 --> 00:01:23,710
that are not necessary but these steps

38
00:01:23,710 --> 00:01:26,200
don't require math. A step that does, though,

39
00:01:26,200 --> 00:01:28,000
i s called normalization.

40
00:01:28,000 --> 00:01:30,579
This is an optional step that can help our model

41
00:01:30,579 --> 00:01:32,530
reach convergence, which is that point

42
00:01:32,530 --> 00:01:34,299
when our prediction gives us the lowest

43
00:01:34,299 --> 00:01:36,759
error possible, faster, since all the

44
00:01:36,759 --> 00:01:38,359
values operate on the same scale.

45
00:01:38,360 --> 00:01:40,380
This idea comes from statistics.

46
00:01:40,420 --> 00:01:46,160
You have a 17.4 percent chance of making a straight.

47
00:01:46,160 --> 00:01:48,660
there are several strategies to normalize data

48
00:01:48,660 --> 00:01:51,620
although a popular one is called min max scaling.

49
00:01:51,820 --> 00:01:53,640
If we have some given data we can use the

50
00:01:53,640 --> 00:01:55,780
following equation to normalize it.

51
00:01:55,780 --> 00:01:57,940
We take each value in the list and subtract

52
00:01:57,940 --> 00:01:59,799
the minimum value from it, then divide

53
00:01:59,799 --> 00:02:01,540
that result by the maximum value minus

54
00:02:01,540 --> 00:02:04,000
the min value. we then have a new list of data

55
00:02:04,000 --> 00:02:06,159
within the range of 0 to 1 and we do

56
00:02:06,159 --> 00:02:07,690
this for every feature we have so

57
00:02:07,690 --> 00:02:09,150
they're all in the same scale

58
00:02:09,180 --> 00:02:11,200
after normalizing our data we have to ensure

59
00:02:11,200 --> 00:02:13,060
that it's in a format that our neural

60
00:02:13,060 --> 00:02:15,400
network will accept. This is where linear

61
00:02:15,400 --> 00:02:17,410
algebra comes in. There four terms in

62
00:02:17,410 --> 00:02:19,150
linear algebra that show up consistently.

63
00:02:19,150 --> 00:02:22,630
Scalars, vectors, matrices and tensors.

64
00:02:22,630 --> 00:02:24,670
A scalar is just a single number.

65
00:02:24,670 --> 00:02:27,090
A vector is a one-dimensional array of numbers.

66
00:02:27,090 --> 00:02:29,780
A matrix is a two-dimensional array of numbers.

67
00:02:29,780 --> 00:02:32,820
And a tensor is an N dimensional array of numbers.

68
00:02:32,820 --> 00:02:36,740
So a matrix, scalar,  vector and spectre, wait not spectre,

69
00:02:36,740 --> 00:02:39,020
can all be represented as a tensor.

70
00:02:39,020 --> 00:02:42,540
Want to convert data, whatever form it's in, be that images,

71
00:02:42,549 --> 00:02:45,459
words, videos, into tensors, where n is

72
00:02:45,459 --> 00:02:47,350
the number of features are data has

73
00:02:47,350 --> 00:02:49,780
and defines the dimensionality of our tensor.

74
00:02:49,780 --> 00:02:52,660
Let's use a three-layer feed-forward neural network

75
00:02:52,660 --> 00:02:55,150
capable of predicting a binary output

76
00:02:55,150 --> 00:02:57,450
given an input as our base example

77
00:02:57,450 --> 00:02:59,960
to illustrate some more math concepts going forward

78
00:02:59,960 --> 00:03:02,500
When do we use math and deep learning?
When we normalize during processing.

79
00:03:05,049 --> 00:03:09,289
Learn a model parameters by searching.
And random weights be initializing.

80
00:03:09,289 --> 00:03:14,220
Tensors flow... From input to out
Then measure the error to measure the doubt

81
00:03:14,220 --> 00:03:18,700
It gives us what's real and what's expected.
Back propogate to get cost corrected.

82
00:03:18,700 --> 00:03:21,180
We'll import our only dependency, Numpy,

83
00:03:21,180 --> 00:03:24,320
then initialize our input data and output data as matrices.

84
00:03:24,320 --> 00:03:28,620
Once our data is in the right format, we want to build our deep neural network.

85
00:03:28,620 --> 00:03:31,480
Deep nets have what are called hyperparameters. These are the

86
00:03:31,480 --> 00:03:34,209
high level tuning knobs of the network that

87
00:03:34,209 --> 00:03:36,100
we define and they help decide things

88
00:03:36,100 --> 00:03:38,410
like how fast our model runs, how many

89
00:03:38,410 --> 00:03:40,420
neurons per layer, how many hidden layers.

90
00:03:40,420 --> 00:03:42,430
Basically the more complex your neural

91
00:03:42,430 --> 00:03:45,510
network gets, the more hyperparameters you'll have.

92
00:03:45,510 --> 00:03:48,360
You can tune these manually using knowledge you have about

93
00:03:48,370 --> 00:03:50,230
the problem you're solving to guess

94
00:03:50,230 --> 00:03:52,150
probable values and observe the result.

95
00:03:52,150 --> 00:03:54,430
Based on the result, you can tweak them

96
00:03:54,430 --> 00:03:57,360
accordingly and repeat that process iteratively.

97
00:03:57,360 --> 00:04:00,860
But another strategy you could use is random search. You can

98
00:04:00,860 --> 00:04:03,290
identify ranges for each, then you can

99
00:04:03,290 --> 00:04:04,970
create a search algorithm that picks

100
00:04:04,970 --> 00:04:07,460
values from those ranges at random

101
00:04:07,460 --> 00:04:10,220
from a uniform distribution of possibilities

102
00:04:10,220 --> 00:04:12,590
which means all possible values have the

103
00:04:12,590 --> 00:04:14,910
same probability of being chosen.

104
00:04:14,910 --> 00:04:18,650
This process repeats until it finds the optimal hyperparameters.

105
00:04:18,740 --> 00:04:20,239
Yay for statistics!

106
00:04:20,239 --> 00:04:22,339
We only have number of epochs as our hyperparameter,

107
00:04:22,340 --> 00:04:24,540
since we have a very simple neural network

108
00:04:24,540 --> 00:04:27,260
We'll use probability to decide our weight values, too.

109
00:04:27,260 --> 00:04:30,340
One common method is randomly initializing samples

110
00:04:30,350 --> 00:04:32,180
of each weight from a normal distribution

111
00:04:32,180 --> 00:04:35,090
with a low deviation, meaning values are

112
00:04:35,090 --> 00:04:36,920
pretty close together. We'll use it to

113
00:04:36,920 --> 00:04:38,570
create a weight matrix with a dimension

114
00:04:38,570 --> 00:04:40,430
of three by four, since that's the size

115
00:04:40,430 --> 00:04:42,530
of our input. So every node in the input

116
00:04:42,530 --> 00:04:45,260
layer is connected to every node in the next layer.

117
00:04:45,260 --> 00:04:47,700
The weight values will be in the range from -1 to 1.

118
00:04:47,700 --> 00:04:51,020
Since we have three layers, we'll initialize two weight matrices.

119
00:04:51,020 --> 00:04:53,200
The next set of weights has a dimension four by one

120
00:04:53,200 --> 00:04:54,540
which is the size of our output.

121
00:04:54,540 --> 00:04:56,720
As data propagates forward in a neural network

122
00:04:56,720 --> 00:05:00,220
each layer applies its own respective operation to it.

123
00:05:00,220 --> 00:05:04,180
transforming it in some way,
until it eventually outputs a prediction

124
00:05:04,180 --> 00:05:05,520
This is all linear algebra.

125
00:05:05,520 --> 00:05:06,940
It's all tensor math.

126
00:05:06,940 --> 00:05:10,640
We'll initialize a for loop to train our network 60,000 iterations

127
00:05:10,640 --> 00:05:12,640
Then we'll want to initialize our layers.

128
00:05:12,640 --> 00:05:15,260
The first layer, our input, gets input data.

129
00:05:15,260 --> 00:05:17,240
The next layer computes the dot product of

130
00:05:17,240 --> 00:05:19,480
the first layer and the first weight matrix.

131
00:05:19,480 --> 00:05:21,460
When we multiply two matrices together,

132
00:05:21,460 --> 00:05:23,620
like in the case of applying weight values to input data,

133
00:05:23,620 --> 00:05:25,760
we call that the dot product.

134
00:05:25,760 --> 00:05:28,580
Then it applies a non-linearity to the result which we

135
00:05:28,580 --> 00:05:30,260
decided it's going to be a sigmoid.

136
00:05:30,260 --> 00:05:32,600
It takes a real value number and squashes

137
00:05:32,600 --> 00:05:34,610
it into a range between 0 and 1.

138
00:05:34,610 --> 00:05:36,500
So that's the operation that occurs in layer 1,

139
00:05:36,500 --> 00:05:38,020
and the same occurs in the next layer.

140
00:05:38,030 --> 00:05:40,010
We'll take that value from layer 1

141
00:05:40,010 --> 00:05:42,380
and propagate it forward to layer 2,

142
00:05:42,380 --> 00:05:45,800
computing the dot product of it and the next weight matrix,

143
00:05:45,800 --> 00:05:49,900
then squashing it into output probabilities with our non-linearity.

144
00:05:49,900 --> 00:05:53,500
Since we only have three layers, this output value is our prediction.

145
00:05:53,510 --> 00:05:55,520
The way we improve this prediction,

146
00:05:55,520 --> 00:05:57,950
the way our network learns, is by optimizing our network over time.

147
00:05:57,950 --> 00:06:00,390
So how do we optimize it?

148
00:06:00,390 --> 00:06:01,890
Enter calculus.

149
00:06:01,890 --> 00:06:04,490
The first prediction our model makes will be inaccurate.

150
00:06:04,490 --> 00:06:09,250
To improve it, we first need to quantify 
exactly how wrong our prediction is.

151
00:06:09,270 --> 00:06:11,820
We'll  do this by measuring the error, or cost.

152
00:06:11,820 --> 00:06:14,160
The error specifies how far off the

153
00:06:14,160 --> 00:06:17,020
predicted output is from the expected output.

154
00:06:17,160 --> 00:06:19,200
Once we have the error value we

155
00:06:19,200 --> 00:06:21,480
want to minimize it because the smaller

156
00:06:21,480 --> 00:06:23,550
the error the better our prediction.

157
00:06:23,550 --> 00:06:25,500
Training a neural network means

158
00:06:25,500 --> 00:06:27,440
minimizing the error over time.

159
00:06:27,460 --> 00:06:30,020
We don't want to change our input data but we can

160
00:06:30,030 --> 00:06:33,070
change our weights to help minimize this error.

161
00:06:33,100 --> 00:06:34,680
If we just brute forced all the

162
00:06:34,680 --> 00:06:36,210
possible weights to see what gave us the

163
00:06:36,210 --> 00:06:38,370
most accurate prediction, it would take a

164
00:06:38,370 --> 00:06:39,970
very long time to compute.

165
00:06:39,970 --> 00:06:42,790
Instead, we want some sense of direction for how we

166
00:06:42,810 --> 00:06:44,610
can update our weights such that in the

167
00:06:44,610 --> 00:06:47,850
next round of training our output is more accurate.

168
00:06:47,880 --> 00:06:49,280
To get this direction we'll

169
00:06:49,290 --> 00:06:51,450
want to calculate the gradient of our

170
00:06:51,450 --> 00:06:53,610
error with respect to our weight values.

171
00:06:53,610 --> 00:06:55,500
We can calculate this by using what's

172
00:06:55,500 --> 00:06:57,960
called the derivative in calculus.

173
00:06:57,960 --> 00:07:00,700
When we set deriv to true for our nonlin function,

174
00:07:00,700 --> 00:07:03,060
it'll calculate the derivative of a sigmoid.

175
00:07:03,060 --> 00:07:05,300
That means the slope of a sigmoid at a given point,

176
00:07:05,310 --> 00:07:08,120
which is the prediction values we give it from l2.

177
00:07:08,120 --> 00:07:10,020
We want to minimize our error

178
00:07:10,020 --> 00:07:11,760
as much as possible, and we can intuitively

179
00:07:11,760 --> 00:07:14,190
think of this process as dropping a ball

180
00:07:14,190 --> 00:07:16,050
into a bowl where the smallest error

181
00:07:16,050 --> 00:07:18,140
value is at the bottom of the bowl.

182
00:07:18,140 --> 00:07:20,160
Once we drop the ball in, we'll calculate the

183
00:07:20,160 --> 00:07:22,380
gradient at each of those positions,

184
00:07:22,380 --> 00:07:25,360
and if the gradient is negative, we'll move the ball to the right.

185
00:07:25,360 --> 00:07:27,320
If it's positive, we'll move the ball to the left.

186
00:07:27,320 --> 00:07:31,040
And we're using the gradient to update our weights accordingly each time.

187
00:07:31,040 --> 00:07:35,020
We'll keep repeating the process until eventually the gradient is zero,

188
00:07:35,020 --> 00:07:37,120
which will give us the smallest error value.

189
00:07:37,200 --> 00:07:39,860
This process is called gradient descent,

190
00:07:39,860 --> 00:07:42,960
because we are descending our gradient 
to approach zero

191
00:07:42,970 --> 00:07:45,620
and using it to update our weight values iteratively.

192
00:07:45,620 --> 00:07:46,920
I understand everything now.

193
00:07:48,040 --> 00:07:49,260
Still understand everything.

194
00:07:49,260 --> 00:07:51,720
So to do this programmatically, we'll multiply the

195
00:07:51,720 --> 00:07:53,670
derivative we calculated for our prediction by the error.

196
00:07:53,670 --> 00:07:59,140
This gives us our error waited derivative which we'll call l2_delta

197
00:07:59,140 --> 00:08:02,220
This is a matrix of values, one for each predicted output,

198
00:08:02,220 --> 00:08:03,860
and gives us a direction.

199
00:08:03,860 --> 00:08:08,040
We'll later use this direction to update this layer's associated weight values.

200
00:08:08,040 --> 00:08:10,800
This process of calculating the error at a given layer

201
00:08:10,800 --> 00:08:13,940
and using it to help calculate the error weighted gradient

202
00:08:13,940 --> 00:08:16,220
so that we can update our weights in the right direction

203
00:08:16,220 --> 00:08:18,800
will be done recursively for every layer

204
00:08:18,810 --> 00:08:20,850
starting from the last back to the first.

205
00:08:20,850 --> 00:08:23,220
We are propagating our error backwards

206
00:08:23,220 --> 00:08:26,040
after we've computed our prediction by propagating forward.

207
00:08:26,040 --> 00:08:27,880
This is called back propagation.

208
00:08:27,880 --> 00:08:32,840
So we'll multiply the l2_delta values 
by the transpose of its associated weight matrix

209
00:08:32,840 --> 00:08:34,420
to get the previous layer's error,

210
00:08:34,420 --> 00:08:37,099
then use that error to do the same operation as before,

211
00:08:37,099 --> 00:08:40,739
to get direction values to update the associated layers' weights.

212
00:08:40,739 --> 00:08:41,779
so error is minimized.

213
00:08:41,780 --> 00:08:45,620
Lastly, we'll update the weight matrices
for each associated layer

214
00:08:45,620 --> 00:08:48,080
by multiplying them by their respective deltas.

215
00:08:48,080 --> 00:08:49,519
When we run our code we can see that

216
00:08:49,519 --> 00:08:51,699
the error values decreased over time,

217
00:08:51,740 --> 00:08:54,620
and our prediction eventually became very accurate.

218
00:08:54,620 --> 00:08:55,520
So, to break it down.

219
00:08:55,520 --> 00:08:57,820
Deep learning borrows from three branches of math,

220
00:08:57,829 --> 00:09:00,829
linear algebra, statistics and calculus.

221
00:09:00,829 --> 00:09:02,540
A neural net performs a series of

222
00:09:02,540 --> 00:09:05,580
operations on an input tensor to compute a prediction

223
00:09:05,580 --> 00:09:08,840
and we can optimize a prediction by using gradient descent

224
00:09:08,840 --> 00:09:10,740
to back propagate our errors recursively,

225
00:09:10,760 --> 00:09:13,740
updating our weight values for every layer during training.

226
00:09:13,749 --> 00:09:17,180
The coding challenge winner from the last video is Jovian Lin.

227
00:09:17,180 --> 00:09:19,940
Jovian tried out a bunch of different models to predict sentiment

228
00:09:19,940 --> 00:09:22,660
from a dataset of video game reviews.
Wizard of the week!

229
00:09:22,660 --> 00:09:24,540
And the runner-up is Vishal Batchu.

230
00:09:24,540 --> 00:09:26,540
He tested out several different recurrent nets

231
00:09:26,540 --> 00:09:29,300
and eloquently recorded his experiment in his ReadMe.

232
00:09:29,300 --> 00:09:32,440
The coding challenge for this video is to train a deep neural net

233
00:09:32,440 --> 00:09:34,760
to predict the magnitude of an earthquake

234
00:09:34,760 --> 00:09:38,520
and use a strategy to learn the optimal hyperparameters.

235
00:09:38,520 --> 00:09:39,580
Details are in the ReadMe.

236
00:09:39,580 --> 00:09:42,820
Post your GitHub link in the comments, and I'll announce the winner next video.

237
00:09:42,820 --> 00:09:45,280
Please subscribe if you wanna see more videos like this.

238
00:09:45,280 --> 00:09:47,470
Check out this related video, and for now I got to get

239
00:09:47,480 --> 00:09:49,220
my math turned up to a million.

240
00:09:49,220 --> 00:09:55,420
So thanks for watching!