1
00:00:00,800 --> 00:00:04,658
Welcome back. In all these cases up to now, we've assumed that we know the

2
00:00:04,658 --> 00:00:10,438
population parameters, mu and sigma. But much of the time, we don't. We often

3
00:00:10,438 --> 00:00:16,350
only have samples, which we must then use to draw all our conclusions. In the

4
00:00:16,350 --> 00:00:19,427
next two lessons, we'll use samples to find out how different a sample mean is

5
00:00:19,427 --> 00:00:24,690
from a population, and how different two samples are from each other. Out of

6
00:00:24,690 --> 00:00:28,718
all the measures of center, we usually use the mean. Now, the two samples we're

7
00:00:28,718 --> 00:00:33,904
comparing in this case, can either be dependent or independent. We'll go over

8
00:00:33,904 --> 00:00:39,717
these differences later. In lesson ten, we're going to look at these. And then

9
00:00:39,717 --> 00:00:44,304
in lesson 11, we'll look at independent samples. It's going to be fun. When we

10
00:00:44,304 --> 00:00:48,525
work with samples, we have to estimate the population standard deviation using

11
00:00:48,525 --> 00:00:53,970
the samples standard deviation with Bessel's correction. Remember this from

12
00:00:53,970 --> 00:00:57,640
Lesson 4? Normally, to find out how typical or atypical a sample mean is, in

13
00:00:57,640 --> 00:01:00,880
what you did before, as we would find its location on the distribution of

14
00:01:00,880 --> 00:01:05,728
sample means, the sampling distribution. And we can determine the shape and

15
00:01:05,728 --> 00:01:09,220
parameters of this sampling distribution if we know the population parameters.

16
00:01:09,220 --> 00:01:14,708
Remember that for any sample mean, we can find where it falls on this

17
00:01:14,708 --> 00:01:21,395
distribution by standardizing. In other words, finding the z-score of the

18
00:01:21,395 --> 00:01:25,252
sample mean. We find the difference between the sample mean and mu, and then

19
00:01:25,252 --> 00:01:29,850
divide by the standard error. But now, the standard error depends on the

20
00:01:29,850 --> 00:01:35,431
sample, we can no longer us sigma if we have a sample. Therefore, we end up

21
00:01:35,431 --> 00:01:41,720
with a new distribution that is more prone to error. This is called the t

22
00:01:41,720 --> 00:01:46,140
distribution. Since it's more prone to error, then it's more spread out and

23
00:01:46,140 --> 00:01:51,989
thicker in the tails than a normal distribution. Remember from lesson seven

24
00:01:51,989 --> 00:01:55,116
when you learned that larger sample sizes result in skinnier sampling

25
00:01:55,116 --> 00:02:00,938
distributions? That same principal applies here. So what do you think happens

26
00:02:00,938 --> 00:02:06,690
as n, the sample size, increases? The standard error increases? The

27
00:02:06,690 --> 00:02:11,534
t-distribution approaches a normal distribution? The t-distribution gets

28
00:02:11,534 --> 00:02:17,862
skinnier tails? And finally, s, the sample center deviation, approaches sigma.
