1
00:00:00,100 --> 00:00:03,010
So, Mr. Algebra, what do you get?

2
00:00:03,010 --> 00:00:04,120
>> That's Dr. Algebra.

3
00:00:04,120 --> 00:00:07,822
I did not spend 12 years in evil algebra
school to be called Mr. Algebra.

4
00:00:07,822 --> 00:00:10,460
>> [LAUGH]
>> Okay.

5
00:00:10,460 --> 00:00:11,043
>> My apologies.

6
00:00:11,043 --> 00:00:13,803
>> [LAUGH] All right, well I think
there's actually only one answer here,

7
00:00:13,803 --> 00:00:14,850
and that answer is two.

8
00:00:14,850 --> 00:00:16,350
>> All right, which one is that?

9
00:00:16,350 --> 00:00:17,740
>> Both of them.
>> So there's two twos.

10
00:00:17,740 --> 00:00:18,360
>> Two twos.

11
00:00:18,360 --> 00:00:18,920
>> How do you get that?

12
00:00:18,920 --> 00:00:22,210
Okay, so you squared both sides, and
that causes the epsilon to be squared.

13
00:00:22,210 --> 00:00:24,100
Gets rid of all the square roots.

14
00:00:24,100 --> 00:00:26,870
Then, oh, but
then because we squared both sides,

15
00:00:26,870 --> 00:00:28,760
we get a divided by 4 over here.

16
00:00:28,760 --> 00:00:32,066
Which ultimately is going to meet
up with the half over here, and

17
00:00:32,066 --> 00:00:33,240
we're going to get a 2.

18
00:00:33,240 --> 00:00:34,772
Yeah, that's right, very good.

19
00:00:34,772 --> 00:00:38,610
>> And thus, more proof that every
single thing in computer science gets

20
00:00:38,610 --> 00:00:41,190
generalized once you have two of them.

21
00:00:41,190 --> 00:00:42,890
>> Two is very important
in computer science.

22
00:00:42,890 --> 00:00:44,480
So let's see what we actually get here.

23
00:00:44,480 --> 00:00:48,720
What we're saying is that if we sample
an arm at least this many times,

24
00:00:48,720 --> 00:00:50,585
2 times 1 over epsilon squared.

25
00:00:50,585 --> 00:00:53,295
Now, 1 over epsilon
squared is polynomial.

26
00:00:53,295 --> 00:00:59,325
And 1 over epsilon, 1 over delta n k,
times the logarithm of 2k over delta.

27
00:00:59,325 --> 00:01:03,303
So again, this quantity here is
polynomial n k, and 1 over delta.

28
00:01:03,303 --> 00:01:05,266
And we're logging it.

29
00:01:05,266 --> 00:01:09,387
And that gives us a count, that if we
do that enough times we're actually

30
00:01:09,387 --> 00:01:12,162
going to be sufficiently certain
that we're sufficiently close,

31
00:01:12,162 --> 00:01:15,077
that chosing the maximum arm
is going to be a good idea.

32
00:01:15,077 --> 00:01:18,300
And it's interesting to note
here that the dependence

33
00:01:18,300 --> 00:01:21,250
on the certainty parameter
is actually quite weak.

34
00:01:21,250 --> 00:01:24,160
Right, so it's 1 over delta,
and it's stuffed into the log.

35
00:01:24,160 --> 00:01:25,970
So this is having not a huge impact.

36
00:01:25,970 --> 00:01:28,990
But the dependence on the epsilon
parameter is actually quite profound.

37
00:01:28,990 --> 00:01:32,769
It's 1 over epsilon, so if we're talking
about 0.0001, that's like, I don't know,

38
00:01:32,769 --> 00:01:34,484
1,000 or 10,000 or something.

39
00:01:34,484 --> 00:01:36,820
And then we square that.

40
00:01:36,820 --> 00:01:40,310
And so, we could actually need a ton
of samples if we want to be very,

41
00:01:40,310 --> 00:01:42,390
very sure that we're very, very close.

42
00:01:42,390 --> 00:01:44,856
In fact, even if we want to be
moderately sure that we're very,

43
00:01:44,856 --> 00:01:47,331
very close, there's just this
big dependence on the epsilon.

44
00:01:47,331 --> 00:01:49,997
>> So what you're saying is that the
hard part is getting very, very close,

45
00:01:49,997 --> 00:01:51,450
not being certain how close you are?

46
00:01:51,450 --> 00:01:52,220
>> It looks that way, yeah.

47
00:01:52,220 --> 00:01:53,970
That's what this equation is telling us.

48
00:01:53,970 --> 00:01:57,830
>> So we've said this before, since it's
all hufting here, and hufting there, and

49
00:01:57,830 --> 00:01:58,880
hufting all the way down,

50
00:01:58,880 --> 00:02:01,660
that you're going to get stuff
that looks like PAC bounds.

51
00:02:01,660 --> 00:02:08,580
But this looks really similar to the PAC
bound for agnostic learning right?

52
00:02:08,580 --> 00:02:11,090
>> Hmm.
>> There was a 1 over epsilon squared,

53
00:02:11,090 --> 00:02:14,350
and a natural log of delta in there,
1 over delta or something in there.

54
00:02:14,350 --> 00:02:15,910
>> Yeah, and it uses a union bound too.

55
00:02:15,910 --> 00:02:19,660
It uses the union bound and the huffting
bound, just like we just did.

56
00:02:19,660 --> 00:02:22,070
But I mean, it's not exactly
the same thing, but you're right.

57
00:02:22,070 --> 00:02:23,490
It has a very similar flavor.

58
00:02:23,490 --> 00:02:26,210
>> I look at this and I just think,
one of these looks a lot like

59
00:02:26,210 --> 00:02:30,150
this says something about hypothesis
space, and the other is about the error.

60
00:02:30,150 --> 00:02:32,950
It just feels like there's
a connection there.

61
00:02:32,950 --> 00:02:35,620
>> Hm, that there's a sense
in which you can think about

62
00:02:35,620 --> 00:02:38,580
the payoffs of the arms as hypotheses,
in some sense.

63
00:02:38,580 --> 00:02:41,405
Or each arm is a hypothesis.

64
00:02:41,405 --> 00:02:43,200
That we want to find
the best hypothesis,

65
00:02:43,200 --> 00:02:44,980
it's like finding the best arm.

66
00:02:44,980 --> 00:02:48,590
It has the lowest error, or
the lowest suboptimality.

67
00:02:48,590 --> 00:02:50,000
Yeah, maybe, maybe.

68
00:02:50,000 --> 00:02:53,310
Other than using one kind of math to
solve the other kind of problem, I don't

69
00:02:53,310 --> 00:02:57,190
quite see what leverage we get from
that, but that's a good observation.

70
00:02:57,190 --> 00:02:58,620
>> Well, in that case,
k is the hypothesis.

71
00:02:58,620 --> 00:03:00,010
This is the size of
the hypothesis space.

72
00:03:00,010 --> 00:03:01,590
>> Yes, like the number r is, exactly.

73
00:03:01,590 --> 00:03:03,160
>> Right, it just seems like
there's something there.

74
00:03:03,160 --> 00:03:05,263
I don't know, I'm sure that if we
thought about it long enough or

75
00:03:05,263 --> 00:03:07,538
gave someone a homework problem,
they'd be able to connect the two.

76
00:03:07,538 --> 00:03:11,346
>> [LAUGH] So just to sum up here,
what we've been figuring out is a kind

77
00:03:11,346 --> 00:03:14,710
of PAC-style bound for
these kinds of bandit problems.

78
00:03:14,710 --> 00:03:19,990
That we want to say we can get near
optimal payoff after a polynomial number

79
00:03:19,990 --> 00:03:23,730
of pulls, or we can get a near optimal
arm after a polynomial number of pulls,

80
00:03:23,730 --> 00:03:27,310
or we can make only a polynomial
number of mistakes in an infinite run.

81
00:03:27,310 --> 00:03:31,040
That all these things actually all
boil down to essentially this result.

82
00:03:31,040 --> 00:03:34,130
We just have to pull each arm enough
times that we have an accurate estimate.

83
00:03:34,130 --> 00:03:35,280
>> That's pretty good.

84
00:03:35,280 --> 00:03:38,785
Well, PAC-style is better than
either monkey style or cream style.

85
00:03:38,785 --> 00:03:43,170
[NOISE]
>> Well, monkey style is quite good, hm.

86
00:03:43,170 --> 00:03:46,344
>> I'm impressed with your PAC-style.

87
00:03:46,344 --> 00:03:48,126
>> [LAUGH]
