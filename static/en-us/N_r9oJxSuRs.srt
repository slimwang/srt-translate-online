1
00:00:00,350 --> 00:00:05,400
Now comes the cool thing and
it's so cool we call it a trick.

2
00:00:05,400 --> 00:00:08,119
I mean, not too many things in
science are called a trick, and

3
00:00:08,119 --> 00:00:10,885
this is called the kernel trick,
all right?

4
00:00:10,885 --> 00:00:14,111
We saw that a,

5
00:00:14,111 --> 00:00:20,560
the linear classifier only depends
on dot products, remember that?

6
00:00:20,560 --> 00:00:21,360
I circled it.

7
00:00:21,360 --> 00:00:22,540
I jumped up and down about it.

8
00:00:22,540 --> 00:00:25,220
So let's define some function K.

9
00:00:25,220 --> 00:00:27,190
Of two points xi and

10
00:00:27,190 --> 00:00:32,850
xj as just being the dot product,
okay, between them, all right.

11
00:00:32,850 --> 00:00:36,680
If I map a data point to some
high dimensional space, so

12
00:00:36,680 --> 00:00:41,510
my function phi here maps x to phi of x,
all right?

13
00:00:41,510 --> 00:00:42,800
Then the dot product.

14
00:00:42,800 --> 00:00:45,770
Whoops, I actually should
write it this way, right?

15
00:00:45,770 --> 00:00:50,040
The dot product k of xi, xj would become

16
00:00:51,910 --> 00:00:54,630
phi of x, transpose phi of xj.

17
00:00:54,630 --> 00:00:56,220
That's what's written right there.

18
00:00:56,220 --> 00:00:57,280
Okay?

19
00:00:57,280 --> 00:01:00,980
And in fact, what I'm actually going to
do is, I'm just going to redefine k,

20
00:01:00,980 --> 00:01:05,910
instead of just being the dot product of
xi and xj, I'm going to have a new k,

21
00:01:05,910 --> 00:01:11,030
right, which is just this dot product
of the, the higher dimensional space.

22
00:01:12,500 --> 00:01:16,840
K stands for kernel, okay, and it's a,

23
00:01:16,840 --> 00:01:20,690
it's, it's a kernel function and it can
be thought of as a similarity function.

24
00:01:20,690 --> 00:01:24,040
Similarity functions the idea
is that they get bigger,

25
00:01:24,040 --> 00:01:26,230
the more similar things are and
you know,

26
00:01:26,230 --> 00:01:30,600
dot products can go from, you can even
think of it as minus 1 to 1, it's true.

27
00:01:30,600 --> 00:01:35,680
So the idea being the most similar is 1,
the least similar is minus 1, okay?

28
00:01:35,680 --> 00:01:40,730
And a kernel function is a similarity
function that is the dot product,

29
00:01:40,730 --> 00:01:45,066
sometimes called the inner product,
of the higher dimensional space.

30
00:01:45,066 --> 00:01:48,061
Okay.
So, I'm going to show you an example in

31
00:01:48,061 --> 00:01:53,528
a minute, but basically the idea is, if
I give you a function of x1, of xi and

32
00:01:53,528 --> 00:01:59,253
xj, some function K, as long as there's
some higher dimensional space in which

33
00:01:59,253 --> 00:02:04,423
that function is just the dot product
of that higher dimensional space.

34
00:02:04,423 --> 00:02:08,560
That kernel is a dot product,
and therefore,

35
00:02:08,560 --> 00:02:12,176
it can be used in our linear classifier.

36
00:02:12,176 --> 00:02:15,565
There's a whole bunch of theory there
about what are called Mercer kernels,

37
00:02:15,565 --> 00:02:19,020
which have to do with what are,
what types of kernels are acceptable.

38
00:02:19,020 --> 00:02:22,620
You basically have to end up with these
positive definite matrices between

39
00:02:22,620 --> 00:02:25,070
your points a couple
of other constraints.

40
00:02:25,070 --> 00:02:27,100
What, most of that doesn't matter.

41
00:02:27,100 --> 00:02:30,920
What matters is that you have to is,
is you, is that you can show that

42
00:02:30,920 --> 00:02:35,900
essentially it's a dot product and
it turns out a lot of functions are.
