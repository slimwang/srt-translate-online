1
00:00:00,390 --> 00:00:03,320
There's another important technique for
regularization

2
00:00:03,320 --> 00:00:06,740
that only emerged relatively
recently and works amazingly well.

3
00:00:08,020 --> 00:00:12,220
It also looks insane the first
time you see it, so bear with me.

4
00:00:12,220 --> 00:00:13,760
It's called dropout.

5
00:00:13,760 --> 00:00:15,850
Dropout works likes this.

6
00:00:15,850 --> 00:00:19,340
Imagine that you have one layer
that connects to another layer.

7
00:00:19,340 --> 00:00:23,089
The values that go from one layer to
the next are often called activations.

8
00:00:24,350 --> 00:00:27,260
Now take those activations and
randomly for

9
00:00:27,260 --> 00:00:31,760
every example you train your network on,
set half of them to zero.

10
00:00:31,760 --> 00:00:34,740
Completely randomly, you basically take

11
00:00:34,740 --> 00:00:39,110
half of the data that's flowing through
your network and just destroy it.

12
00:00:39,110 --> 00:00:40,170
And then randomly again.

13
00:00:41,370 --> 00:00:45,380
If that doesn't sound crazy to you then
you might qualify to become a student of

14
00:00:45,380 --> 00:00:48,110
Jeffery Hinton who
pioneered the technique.

15
00:00:48,110 --> 00:00:50,080
So what happens with dropout?

16
00:00:50,080 --> 00:00:55,310
Your network can never rely on any
given activation to be present

17
00:00:55,310 --> 00:00:57,790
because they might be
squashed at any given moment.

18
00:00:58,870 --> 00:01:01,820
So it is forced to learn
a redundant representation for

19
00:01:01,820 --> 00:01:06,130
everything to make sure that at least
some of the information remains.

20
00:01:06,130 --> 00:01:08,160
It's like a game of whack-a-mole.

21
00:01:08,160 --> 00:01:11,400
One activations gets smashed,
but there's always one or

22
00:01:11,400 --> 00:01:15,320
more that do the same job and
that don't get killed.

23
00:01:15,320 --> 00:01:18,100
So everything remains fine at the end.

24
00:01:18,100 --> 00:01:21,540
Forcing your network to learn redundant
representations might sound very

25
00:01:21,540 --> 00:01:22,985
inefficient.

26
00:01:22,985 --> 00:01:27,215
But in practice, it makes things more
robust and prevents over fitting.

27
00:01:27,215 --> 00:01:30,685
It also makes your network act
as if taking the consensus

28
00:01:30,685 --> 00:01:32,505
over an ensemble of networks.

29
00:01:32,505 --> 00:01:35,080
Which is always a good way
to improve performance.

30
00:01:35,080 --> 00:01:38,405
Dropout is one of the most important
techniques to emerge in the last few

31
00:01:38,405 --> 00:01:39,225
years.

32
00:01:39,225 --> 00:01:40,375
If drop out doesn't work for

33
00:01:40,375 --> 00:01:43,005
you, you should probably
be using a bigger network.
