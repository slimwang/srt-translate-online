1
00:00:00,320 --> 00:00:03,090
To compute the parameter
updates of a recurrent network,

2
00:00:03,090 --> 00:00:05,670
we need to backpropagate
the derivative through time,

3
00:00:05,670 --> 00:00:07,700
all the way to the beginning
of the sequence.

4
00:00:07,700 --> 00:00:11,450
Or in practice, more often, for
as many steps as we can afford.

5
00:00:11,450 --> 00:00:15,510
All these derivatives are going to
be applied to the same parameters.

6
00:00:15,510 --> 00:00:19,950
That's a lot of correlated updates
all at once, for the same weights.

7
00:00:19,950 --> 00:00:22,315
This is bad for
stochastic gradient descent.

8
00:00:22,315 --> 00:00:25,560
Stochastic gradient descent,
prefers to have uncorrelated updates,

9
00:00:25,560 --> 00:00:28,820
to its parameters, for
the stability of the training.

10
00:00:28,820 --> 00:00:31,300
This makes the math very unstable.

11
00:00:31,300 --> 00:00:35,170
Either the gradients grow exponentially
and you end up with infinities.

12
00:00:35,170 --> 00:00:37,550
Or they go down to zero very quickly.

13
00:00:37,550 --> 00:00:38,970
And you end up not training anything.
