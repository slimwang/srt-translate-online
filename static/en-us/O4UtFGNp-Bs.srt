1
00:00:00,230 --> 00:00:02,717
>> Alright, let's do another quiz. That previous example that

2
00:00:02,717 --> 00:00:06,198
we looked at of intervals, was nice and pedagogical, and reasonable

3
00:00:06,198 --> 00:00:09,384
to think about, but we actually hadn't really talked about any

4
00:00:09,384 --> 00:00:14,210
learning algorithms that used intervals. On the other hand, linear separators

5
00:00:14,210 --> 00:00:16,720
are a very big deal in machine learning. So, it's,

6
00:00:16,720 --> 00:00:19,370
it's very worthwhile, and it turns out to be not too

7
00:00:19,370 --> 00:00:22,210
bad to work out what the vc dimension is for linear

8
00:00:22,210 --> 00:00:26,350
separators. So, let's say that we're in two dimensional space, and

9
00:00:26,350 --> 00:00:29,150
so our hypotheses have the form that you've got

10
00:00:29,150 --> 00:00:32,860
a parameter, a weight parameter, w. And were going to just

11
00:00:32,860 --> 00:00:35,760
take that weight parameter, take the dot product with whatever

12
00:00:35,760 --> 00:00:37,710
the input is, and see whether its greater than or

13
00:00:37,710 --> 00:00:41,326
equal to some value, theta. And if it is,

14
00:00:41,326 --> 00:00:43,885
then we say that's a positive example, and if not

15
00:00:43,885 --> 00:00:46,562
it's a negative example, and geometrically that just means that

16
00:00:46,562 --> 00:00:51,410
we've, we end up specifying a line, and everything on

17
00:00:51,410 --> 00:00:53,380
one side of the line is going to be positive, and

18
00:00:53,380 --> 00:00:55,070
everything on the other side of the line's going to be negative.

19
00:00:55,070 --> 00:00:57,950
>> Got it. That makes sense. So what's the vc

20
00:00:57,950 --> 00:01:01,115
dimension? Oh, they're going to have to tell us. I like that.

21
00:01:01,115 --> 00:01:02,034
>> Alright.

22
00:01:02,034 --> 00:01:03,014
>> Go.
