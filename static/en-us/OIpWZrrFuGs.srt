1
00:00:00,250 --> 00:00:04,589
Most of the fields we can use as is
without any kind of transformation,

2
00:00:04,589 --> 00:00:07,570
since they are all mostly
continuous in nature.

3
00:00:07,570 --> 00:00:09,540
The two fields related to the first and

4
00:00:09,539 --> 00:00:14,099
last winter freeze will probably be
important to have in our analysis.

5
00:00:14,099 --> 00:00:16,128
But since they are both date fields,

6
00:00:16,129 --> 00:00:19,620
they will need to be transformed
into something that we can use.

7
00:00:19,620 --> 00:00:22,900
As I mentioned earlier in
reference to transforming dates,

8
00:00:22,899 --> 00:00:26,579
since these are dates where the year
itself is not significant, but

9
00:00:26,579 --> 00:00:30,250
rather just when they fall
within the yearly time frame,

10
00:00:30,250 --> 00:00:34,890
one thing we can do is to calculate
the mean or median of the fields and

11
00:00:34,890 --> 00:00:38,960
use that date as a reference
point to create a before and

12
00:00:38,960 --> 00:00:42,399
after value to the reference
point we calculate.

13
00:00:42,399 --> 00:00:46,920
In this case, I have already calculated
the median last winter freeze as

14
00:00:46,920 --> 00:00:52,820
being April 23rd, and the median first
winter freeze as being October 6th.

15
00:00:52,820 --> 00:00:57,573
With a Formula tool, I can create new
fields called LastWinterFreezeDiff and

16
00:00:57,573 --> 00:00:59,759
FirstWinterFreezeDiff.

17
00:00:59,759 --> 00:01:04,939
As you can see, it is basically the
difference between the calculated median

18
00:01:04,939 --> 00:01:07,679
date and the date for each store.

19
00:01:07,680 --> 00:01:09,240
It will provide a positive or

20
00:01:09,239 --> 00:01:12,750
negative number that we can then
use in the clustering process.

21
00:01:13,750 --> 00:01:18,799
Now when we run the process again,
we can see the newly calculated fields.

22
00:01:18,799 --> 00:01:22,019
And since the data is
sorted by latitude,

23
00:01:22,019 --> 00:01:25,479
then we can eyeball the data and
see that it makes sense.

24
00:01:25,480 --> 00:01:30,570
The closer the store is to the equator,
the earlier the last winter freeze is,

25
00:01:30,569 --> 00:01:33,019
as represented by a negative number, and

26
00:01:33,019 --> 00:01:37,869
the later the first winter freeze is,
as represented by a positive number.

27
00:01:37,870 --> 00:01:40,910
And now that we have our data
as we want it, we can save it

28
00:01:40,909 --> 00:01:44,959
to a new file that we will use for
the next step in the process.

29
00:01:44,959 --> 00:01:50,209
I just drop in an Output Data tool and
save the results to a new file.

30
00:01:50,209 --> 00:01:55,929
I'll save it as a CSV and
call mine Data Prep Exercise Results.

31
00:01:55,930 --> 00:01:58,910
To save that, you'll need to
run the process one more time.

