1
00:00:00,120 --> 00:00:01,690
All right.
So the next thing we want to show

2
00:00:01,690 --> 00:00:03,969
is if we have an algorithm that

3
00:00:03,969 --> 00:00:07,080
has a bounded number of mistakes in
the sense that we just talked about.

4
00:00:07,080 --> 00:00:10,443
That that algorithm will
eventually do well,

5
00:00:10,443 --> 00:00:15,873
in the sense that it will achieve a
total reward that is arbitrarily close,

6
00:00:15,873 --> 00:00:19,680
epsilon prime close to
the per step optimal reward.

7
00:00:19,680 --> 00:00:23,200
So let's imagine that we have an
algorithm that pulls epsilon suboptimal

8
00:00:23,200 --> 00:00:26,620
arms at most this bounded number
of times with high probability.

9
00:00:26,620 --> 00:00:28,590
So this is something that
has very few mistakes.

10
00:00:29,710 --> 00:00:31,640
Oh.
Except this wants to be tau.

11
00:00:31,640 --> 00:00:32,630
Right.
Given that algorithm,

12
00:00:32,630 --> 00:00:34,350
we want to make a new algorithm,

13
00:00:34,350 --> 00:00:38,060
that with high probability,
one minus delta prime, has a per step

14
00:00:38,060 --> 00:00:41,650
reward that is epsilon prime close
to the optimal per step award.

15
00:00:41,650 --> 00:00:43,220
In other words,
what we would've gotten for

16
00:00:43,220 --> 00:00:45,980
pulling the optimal arm on every step.

17
00:00:45,980 --> 00:00:50,150
And that bound starts to hold after
some number of steps, tau prime,

18
00:00:50,150 --> 00:00:51,780
k epsilon prime delta prime.

19
00:00:51,780 --> 00:00:53,830
>> So does that mean we get to do
the same thing that we did before?

20
00:00:53,830 --> 00:00:54,570
>> Let's try that.

21
00:00:54,570 --> 00:00:57,190
>> Okay.
>> We run this mistake bound algorithm

22
00:00:57,190 --> 00:00:58,980
that makes very few mistakes.

23
00:00:58,980 --> 00:01:00,560
What's our tau prime?

24
00:01:00,560 --> 00:01:05,000
When is it that we're going to
have achieved near optimality?

25
00:01:05,000 --> 00:01:08,820
>> Because algorithm b is the same as
algorithm a, it means that by running

26
00:01:08,820 --> 00:01:14,050
algorithm b, we're going to end up
with an arm that is epsilon optimal.

27
00:01:14,050 --> 00:01:16,900
>> Yes that's true, but
I don't want to think about it that way.

28
00:01:16,900 --> 00:01:20,190
I don't want to bring algorithm
A in to this at the moment.

29
00:01:20,190 --> 00:01:22,860
Lets stick with the algorithm
on the slide, algorithm B.

30
00:01:22,860 --> 00:01:27,210
>> So, we have an algorithm that pulls
epsilon suboptimal that many times.

31
00:01:27,210 --> 00:01:31,920
That implies the way it's written,
that you could have pulled

32
00:01:31,920 --> 00:01:36,530
a bunch of epsilon optimal arms,
many, many times during that.

33
00:01:36,530 --> 00:01:37,110
>> Yes.

34
00:01:37,110 --> 00:01:40,460
>> But not during that time period, like
you could have pulled epsilon optimal

35
00:01:40,460 --> 00:01:43,820
a zillion times and then pulled
epsilon sub optimal tau times.

36
00:01:43,820 --> 00:01:44,390
>> Yes.

37
00:01:44,390 --> 00:01:46,980
>> And
then it would be tau plus a bezillion.

38
00:01:46,980 --> 00:01:48,800
>> Yes.
>> Would be the total number of steps.

39
00:01:48,800 --> 00:01:50,720
>> Yes, so yeah, this is all true.

40
00:01:50,720 --> 00:01:51,440
>> So, okay, so

41
00:01:51,440 --> 00:01:55,380
we need to kind of think about how we
can use that fact to build a bound.

42
00:01:55,380 --> 00:01:55,880
>> Hm.
