1
00:00:00,230 --> 00:00:03,560
Now that we talked about doing
reinforcement learning in a POMDP

2
00:00:03,560 --> 00:00:07,220
setting, we're actually now going to
talk about a much weirder combination.

3
00:00:07,220 --> 00:00:10,370
We're going to talk about how we can
think of reinforcement learning as

4
00:00:10,370 --> 00:00:12,498
actually being itself a POMDP.

5
00:00:12,498 --> 00:00:14,360
>> Okay.
>> This is an area that

6
00:00:14,360 --> 00:00:16,420
is often referred to as
Bayesian Reinforcement Learning.

7
00:00:16,420 --> 00:00:19,180
The idea is that we're going to keep

8
00:00:19,180 --> 00:00:22,210
a a Bayesian posterior over
possible MDPs that we're in.

9
00:00:22,210 --> 00:00:26,690
And we're going to use that to actually
optimally balance exploration and

10
00:00:26,690 --> 00:00:27,330
exploitations.

11
00:00:27,330 --> 00:00:29,690
We're actually going to
behave optimally and

12
00:00:29,690 --> 00:00:31,444
reinforcement learning becomes planning.

13
00:00:31,444 --> 00:00:34,000
>> Okay, so does it mean we're
going to use Bayes rule?

14
00:00:34,000 --> 00:00:37,550
>> Well I guess technically in that
whenever your compute a posterior,

15
00:00:37,550 --> 00:00:39,359
you're actually doing
something like Bayes rule.

16
00:00:39,359 --> 00:00:41,470
Yeah, but maybe this is not so clear.

17
00:00:41,470 --> 00:00:42,860
Let me try to draw a picture.

18
00:00:42,860 --> 00:00:45,210
So I'm hoping this example might
make things a little bit clearer.

19
00:00:45,210 --> 00:00:49,140
So let's consider a really simple
reinforcement learning problem.

20
00:00:49,140 --> 00:00:54,020
We've got two states, state 1 and
state 2, two actions a black one and

21
00:00:54,020 --> 00:00:56,180
a, I don't know, purplish kind of one.

22
00:00:56,180 --> 00:00:59,280
And if we were doing reinforcement
learning in this environment, and

23
00:00:59,280 --> 00:01:01,740
you were say the reinforcement learner,
what would you do?

24
00:01:01,740 --> 00:01:04,200
Let's say we're in state 1,
what would you do?

25
00:01:04,200 --> 00:01:06,221
Would it be better to do purple or
black?

26
00:01:06,221 --> 00:01:07,360
>> I don't know.

27
00:01:07,360 --> 00:01:08,568
>> Okay so then what would you do?

28
00:01:08,568 --> 00:01:09,550
>> I'd pick one of them.

29
00:01:09,550 --> 00:01:12,331
>> All right, so
let's say you pick black and

30
00:01:12,331 --> 00:01:14,732
then we would observe a transition.

31
00:01:14,732 --> 00:01:16,560
And ultimately we can sort
of build out a model or

32
00:01:16,560 --> 00:01:19,630
use queue learning or something
like that to work out the values.

33
00:01:19,630 --> 00:01:23,090
So what I'd like to do is actually
try to have you think about this

34
00:01:23,090 --> 00:01:24,370
like a POMDP.

35
00:01:24,370 --> 00:01:26,760
And I think a simpler
way to show you that

36
00:01:26,760 --> 00:01:31,120
would be instead of imagining
all possible MDPs this could be,

37
00:01:31,120 --> 00:01:33,790
let's just pretend it comes
from of a small finite set.

38
00:01:33,790 --> 00:01:35,996
Okay, are there any small
sets that aren't finite?

39
00:01:35,996 --> 00:01:37,110
>> No.
>> Good,

40
00:01:37,110 --> 00:01:38,270
then I think we're on the same page.

41
00:01:38,270 --> 00:01:39,390
>> All right, then I agree with you.

42
00:01:39,390 --> 00:01:40,760
So here's the set that I had in mind,

43
00:01:40,760 --> 00:01:43,820
we've got three possible worlds
that we're in A, B and C.

44
00:01:43,820 --> 00:01:45,590
So these are three possible MDPs, right?

45
00:01:45,590 --> 00:01:50,490
>> Okay and they all look exactly the
same to me except where you get reward.

46
00:01:50,490 --> 00:01:53,510
>> Yeah, so it just so happens that
the actions all do the same thing in all

47
00:01:53,510 --> 00:01:55,210
three of these and we know that.

48
00:01:55,210 --> 00:01:56,600
We just don't know where the reward is.

49
00:01:56,600 --> 00:01:59,640
Is the reward something you get for
purple when you go from state 2?

50
00:01:59,640 --> 00:02:02,290
Is it something that you get for
purple from state 1?

51
00:02:02,290 --> 00:02:06,310
Or is it something that you get from
taking the black action from state 1?

52
00:02:06,310 --> 00:02:06,930
And we don't know.

53
00:02:06,930 --> 00:02:08,892
Now if you had to solve this
reinforcement learning problem,

54
00:02:08,892 --> 00:02:10,791
it's kind of a reinforcement
learning a problem, right?

55
00:02:10,791 --> 00:02:13,700
You're in some state you get to
see what state you're in, 1 or 1.

56
00:02:13,700 --> 00:02:15,980
You can try actions black or purple.

57
00:02:15,980 --> 00:02:18,960
And what you're trying to do is actually
figure out how to maximize reward.

58
00:02:18,960 --> 00:02:21,590
But now that it's just
a finite set of possibilities,

59
00:02:21,590 --> 00:02:23,090
we can think about this like a POMDP.

60
00:02:23,090 --> 00:02:25,300
>> Sure and in fact, I think this
one's pretty straightforward.

61
00:02:25,300 --> 00:02:28,350
If I'm in state 2,
I know what I should do.

62
00:02:28,350 --> 00:02:28,850
>> And what is that?

63
00:02:28,850 --> 00:02:30,280
>> I should take the purple action.

64
00:02:31,380 --> 00:02:31,970
>> And why is that?

65
00:02:31,970 --> 00:02:33,510
Explain.
>> Because I immediately either

66
00:02:33,510 --> 00:02:38,270
get reward for it or I know that I
get reward for being in state 1.

67
00:02:38,270 --> 00:02:40,170
And then when I'm in state 1,

68
00:02:40,170 --> 00:02:42,450
well actually it's not clear
which one I should do.

69
00:02:42,450 --> 00:02:46,040
>> All right, so
let's well let's just do two things.

70
00:02:46,040 --> 00:02:50,055
Let's start off by just tracking belief
states in this because I think that's

71
00:02:50,055 --> 00:02:52,205
already kind of an interesting
problem in and of itself.

72
00:02:52,205 --> 00:02:55,473
What we're thinking about here is
that we're in POMDP with six states.

73
00:02:55,473 --> 00:02:58,155
Often state 1, but
we don't know which MDP were in,

74
00:02:58,155 --> 00:02:59,973
then the belief state is going to
look something like this.

75
00:02:59,973 --> 00:03:00,511
Agreed?

76
00:03:00,511 --> 00:03:03,275
>> Agreed, if this is where we're
starting, we know nothing else, and

77
00:03:03,275 --> 00:03:06,760
we haven't done anything yet,
then we can be in any of those states,

78
00:03:06,760 --> 00:03:08,320
presumably with equal probability.

79
00:03:08,320 --> 00:03:11,990
>> Good, right, so
now the question is, if you were to do

80
00:03:11,990 --> 00:03:15,970
the purple action from here,
what would your expected reward be?

81
00:03:15,970 --> 00:03:20,250
>> My expected the reward
would be one-third.

82
00:03:20,250 --> 00:03:22,180
>> Right, so
if we happen to have been in MDP B,

83
00:03:22,180 --> 00:03:25,910
which we don't know if we are and we
took the action purple, we'd get a one.

84
00:03:25,910 --> 00:03:28,030
If not, then we get zero.

85
00:03:28,030 --> 00:03:30,260
But what also would
happen if we get the one?

86
00:03:30,260 --> 00:03:31,720
What's our new belief state?

87
00:03:31,720 --> 00:03:32,630
>> If we get the one?

88
00:03:32,630 --> 00:03:35,220
>> Yeah.
>> Well then we know that we are in B1.

89
00:03:35,220 --> 00:03:36,830
>> Good, B1, right.

90
00:03:36,830 --> 00:03:38,060
So we're in this state, so

91
00:03:38,060 --> 00:03:40,640
the belief state becomes basically
fully observable at this point.

92
00:03:40,640 --> 00:03:42,190
We know which MDP, we're in.

93
00:03:42,190 --> 00:03:43,820
And we know what state
of that MDP we're in.

94
00:03:43,820 --> 00:03:46,650
And we can just execute
the optimal policy for that MDP,

95
00:03:46,650 --> 00:03:49,357
which in this case, as you might guess,
is just purple all the time.

96
00:03:49,357 --> 00:03:52,916
>> Purple all the time,
purple all the time.

97
00:03:52,916 --> 00:03:53,900
Yeah, I agree.
>> Excellent.

98
00:03:53,900 --> 00:03:56,660
So let's make sure you get this,
we'll do a little quizzy kind of thing.

99
00:03:56,660 --> 00:03:57,160
>> Okay.
