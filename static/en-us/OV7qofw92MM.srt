1
00:00:00,000 --> 00:00:03,000
Welcome to the Unit 2 office hours.

2
00:00:03,000 --> 00:00:09,000
One of the first questions I'd like to address deals with the Ply library

3
00:00:09,000 --> 00:00:12,000
that we're using under the hood for this class

4
00:00:12,000 --> 00:00:15,000
to help us out with lexing and parsing.

5
00:00:15,000 --> 00:00:20,000
In the lectures we talked about how to write down a token definition.

6
00:00:20,000 --> 00:00:23,000
You define a function t_something--like a number--

7
00:00:23,000 --> 00:00:27,000
and then you'd write out the regular expression--0 through 9-plus.

8
00:00:27,000 --> 00:00:30,000
Then you might do a little work modifying the token value--

9
00:00:30,000 --> 00:00:33,000
maybe converting it from a string to an integer.

10
00:00:33,000 --> 00:00:35,000
Then eventually you return the token.

11
00:00:35,000 --> 00:00:40,000
The basic question from students is how does that work? What's going on?

12
00:00:40,000 --> 00:00:44,000
After you write down a number of these token definitions,

13
00:00:44,000 --> 00:00:49,000
there is a library behind the scenes--part of the grading scripts that we're using,

14
00:00:49,000 --> 00:00:52,000
but also available for you to download if you'd like to try it out on your own--

15
00:00:52,000 --> 00:00:56,000
that gathers up all of your token definitions.

16
00:00:56,000 --> 00:01:00,000
One way to do this is using a technique called "reflection,"

17
00:01:00,000 --> 00:01:05,000
which actually sounds very philosophical but is a way for a computer program

18
00:01:05,000 --> 00:01:08,000
to look at itself and all of it's own capabilities.

19
00:01:08,000 --> 00:01:13,000
In essence, our Python program asks, has anyone defined any functions recently

20
00:01:13,000 --> 00:01:15,000
that start with t_?

21
00:01:15,000 --> 00:01:21,000
If so, the next thing we have to do is get that regular expression out of them.

22
00:01:21,000 --> 00:01:27,000
It turns out that Python functions allow you to write documentation or a brief explanation

23
00:01:27,000 --> 00:01:32,000
at the beginning of any function. This is sometimes a good software engineering practice.

24
00:01:32,000 --> 00:01:38,000
Our Python parsing library and our Python lexing library reuse this power.

25
00:01:38,000 --> 00:01:42,000
We're writing down this regular expression, and the library is treating it

26
00:01:42,000 --> 00:01:45,000
as if it were documentation.

27
00:01:45,000 --> 00:01:50,000
We use that to get access to the regular expressions you've written down.

28
00:01:50,000 --> 00:01:53,000
So step one: you write down a bunch of these token definitions.

29
00:01:53,000 --> 00:01:57,000
Then we look--using reflection, for example--to find all of them.

30
00:01:57,000 --> 00:02:02,000
Step two: we go through all of them and ask, do they have any strings at the beginning-

31
00:02:02,000 --> 00:02:06,000
anything that looks like documentation. The answer is yes.

32
00:02:06,000 --> 00:02:11,000
But for us it's not documentation. It's a regular expression specification.

33
00:02:11,000 --> 00:02:17,000
The next thing to do is convert each of those regular expressions to a finite state machine.

34
00:02:17,000 --> 00:02:21,000
Now, I hinted at this in class. Didn't give a full, formal proof on how to do it.

35
00:02:21,000 --> 00:02:28,000
But it turns out that every one of the regular expressions--plus, star, disjunction, concatenation--

36
00:02:28,000 --> 00:02:31,000
can be written out in a finite state machine.

37
00:02:31,000 --> 00:02:35,000
For every regular expression there is at least one--and in fact, typically, infinitely many--

38
00:02:35,000 --> 00:02:39,000
finite state machines that accept exactly the same language.

39
00:02:39,000 --> 00:02:43,000
We'll just apply that conversion in the background.

40
00:02:43,000 --> 00:02:48,000
But if you have a bunch of different token definitions--one for number, one for string,

41
00:02:48,000 --> 00:02:51,000
one for some keywords in your language like "if," "then," or "else"--

42
00:02:51,000 --> 00:02:54,000
we're going to end up with a bunch of different finite state machines.

43
00:02:54,000 --> 00:02:58,000
Now we have to combine them all together.

44
00:02:58,000 --> 00:03:02,000
If you're willing to humor me with nondeterministic finite state machines,

45
00:03:02,000 --> 00:03:05,000
we could actually do that just by putting a special state at the beginning--

46
00:03:05,000 --> 00:03:08,000
a special new start state--and having an epsilon transition

47
00:03:08,000 --> 00:03:11,000
go to the beginning of each of our old states.

48
00:03:11,000 --> 00:03:14,000
We'll glue them all together into sort of a Frankenstein's monster--

49
00:03:14,000 --> 00:03:17,000
an amalgamation of everything we've looked at.

50
00:03:17,000 --> 00:03:23,000
We're almost done except that it's not enough for a lexer to know this string is a token.

51
00:03:23,000 --> 00:03:26,000
You have to know which one it is.

52
00:03:26,000 --> 00:03:31,000
A real life lexer will have one more bit of information that we didn't talk about in class.

53
00:03:31,000 --> 00:03:36,000
A state isn't just an accepting state, it's an accepting state with a little label.

54
00:03:36,000 --> 00:03:39,000
If you accept, here it was a string.

55
00:03:39,000 --> 00:03:42,000
If you accept here in this over there, it was the token then.

56
00:03:42,000 --> 00:03:45,000
If you accept in this state down here, it's a number.

57
00:03:45,000 --> 00:03:47,000
Instead of just known what the accepting states are,

58
00:03:47,000 --> 00:03:50,000
we need to know what the accepting states are

59
00:03:50,000 --> 00:03:54,000
and what token each one corresponds to.

60
00:03:54,000 --> 00:03:56,000
All right. You did all your token definitions. We found them all.

61
00:03:56,000 --> 00:03:59,000
Each one had a regular expression. We found that.

62
00:03:59,000 --> 00:04:02,000
We converted each of those down into finite state machines.

63
00:04:02,000 --> 00:04:05,000
We joined them all together. We labeled all the accepting states.

64
00:04:05,000 --> 00:04:12,000
Now we represent that big finite state machine internally as edges.

65
00:04:12,000 --> 00:04:15,000
This is sometimes formally called a transition table,

66
00:04:15,000 --> 00:04:19,000
but it's just like the edges in coding that we used in class.

67
00:04:19,000 --> 00:04:23,000
Now when it comes time to actually do lexing--to break a string down into

68
00:04:23,000 --> 00:04:28,000
into important words and tokens--you just feed the characters of the string

69
00:04:28,000 --> 00:04:31,000
one at a time into that big finite state machine.

70
00:04:31,000 --> 00:04:36,000
It's exactly the same as the FSM sim procedure we went over in class.

71
00:04:36,000 --> 00:04:40,000
It's just that their finite state machine is much bigger.

72
00:04:40,000 --> 00:04:44,000
In fact, the FSM sim code that we wrote would work just as well,

73
00:04:44,000 --> 00:04:46,000
even on bigger finite state machines.

74
00:04:46,000 --> 00:04:50,000
I just didn't show it because they're harder to draw on screen.

75
00:04:50,000 --> 00:04:53,000
Under the hood, this library is basically just doing a bunch

76
00:04:53,000 --> 00:04:57,000
of grungy details, chores, busy work in the background.

77
00:04:57,000 --> 00:05:01,000
We did exercises with one or two regular expression definitions.

78
00:05:01,000 --> 00:05:04,000
The library gathers them all up together in one place.

79
00:05:04,000 --> 00:05:10,000
You're already familiar with all of the key concepts in making a lexical analysis library.

80
00:05:10,000 --> 99:59:59,999
The library just does a lot of the busy work for you. And that's the explanation.
