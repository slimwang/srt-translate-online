1
00:00:00,742 --> 00:00:03,580
All right, so let's see if we can think
through, or maybe guess what's going to

2
00:00:03,580 --> 00:00:10,100
happen if TD lambda is run as an update
algorithm on some finite amount of data.

3
00:00:10,100 --> 00:00:13,300
So we it, we give the system
some finite amount of data, and

4
00:00:13,300 --> 00:00:16,170
then we run TD lambda with different
possible values of lambda.

5
00:00:16,170 --> 00:00:19,060
And then we ask, alright,
the V that you've learned,

6
00:00:19,060 --> 00:00:22,235
how far is that from what the V should
have been, if we had infinite data?

7
00:00:22,235 --> 00:00:24,055
>> Hm-mm.

8
00:00:24,055 --> 00:00:27,377
>> Alright so, I don't know,
let's see if we can figure out, or

9
00:00:27,377 --> 00:00:29,837
maybe even guess,
what this might look like.

10
00:00:29,837 --> 00:00:34,300
So, let's say if we use lambda
equals zero, so we do TD zero.

11
00:00:34,300 --> 00:00:35,605
So after some finite amount data,

12
00:00:35,605 --> 00:00:37,140
there's going to be
some amount of error.

13
00:00:37,140 --> 00:00:42,180
Let's just you know,
I don't know, call it that.

14
00:00:42,180 --> 00:00:42,790
>> Okay.

15
00:00:42,790 --> 00:00:47,622
>> Now it turns out, empirically, if you
play with this, it's pretty typical for

16
00:00:47,622 --> 00:00:52,880
TD one to give you worse error,
given finite data.

17
00:00:52,880 --> 00:00:55,460
And now the important thing
is what happens in between.

18
00:00:55,460 --> 00:00:58,081
So it could be like a straight line
if it was literally interpolating,

19
00:00:58,081 --> 00:01:02,130
it could be kind of bowed up, but
it's actually typically bowed down.

20
00:01:05,510 --> 00:01:10,400
So it's not so atypical to see a curve
kind of like this, that gets better.

21
00:01:10,400 --> 00:01:14,060
The amount of error goes down as
lambda gets to some intermediate value

22
00:01:14,060 --> 00:01:15,240
let's call it, I don't know .7.

23
00:01:15,240 --> 00:01:18,840
It depends of course on this,
the particular problem.

24
00:01:18,840 --> 00:01:22,060
And then kind of start to shoot up again
and it eventually hit the error for

25
00:01:22,060 --> 00:01:23,269
TD one as lambda approaches one.

26
00:01:24,430 --> 00:01:28,080
And so this is kind of the justification
for why we want to consider TD lambda.

27
00:01:28,080 --> 00:01:31,480
That it's actually combining these
two different sources of information.

28
00:01:31,480 --> 00:01:32,880
What happens at the end
of the episode and

29
00:01:32,880 --> 00:01:37,490
what happens at each step, in a way
that actually takes advantage of both.

30
00:01:37,490 --> 00:01:38,830
It gets you the benefits of both.

31
00:01:38,830 --> 00:01:43,150
We're rapidly propagating information
and we're not getting a biased estimate,

32
00:01:43,150 --> 00:01:45,660
because of the way that we're
using one step information.

33
00:01:45,660 --> 00:01:48,216
So, you know,
it has some attractiveness to it.

34
00:01:48,216 --> 00:01:51,320
>> Huh, did you pick 0.7 on purpose?

35
00:01:51,320 --> 00:01:54,020
I remember reading a paper once
that suggested that basically

36
00:01:54,020 --> 00:01:55,990
everyone uses lambda equals to 0.7.

37
00:01:55,990 --> 00:02:00,871
>> I typically see 0.3 to 0.7 as values
where things tend to work out where

38
00:02:00,871 --> 00:02:05,160
this bottom tends to fall,
somewhere between 0.3 and 0.7.

39
00:02:05,160 --> 00:02:10,100
Personally I always use zero, because
it works better with the control.

40
00:02:10,100 --> 00:02:12,180
I have had better performance and

41
00:02:12,180 --> 00:02:14,530
better understanding of what
happens in the zero case.

42
00:02:14,530 --> 00:02:17,530
But it is true that if you actually
run it with a finite amount of data,

43
00:02:17,530 --> 00:02:21,598
it tends to bottom out
somewhere between 0.3 and 0.7.

44
00:02:21,598 --> 00:02:23,340
>> Hmm, okay, that's worth note.

45
00:02:23,340 --> 00:02:26,387
>> So that's actually all I wanted
to tell you about TD lambda.

46
00:02:26,387 --> 00:02:30,110
I wanted to give you a sense of how the
equations are defined, and how they're

47
00:02:30,110 --> 00:02:34,350
derived, and empirically, why this
is something we actually care about.

48
00:02:34,350 --> 00:02:36,350
So, maybe we should sum up?

49
00:02:36,350 --> 00:02:37,000
>> I'm big fan of sum.

50
00:02:37,000 --> 00:02:38,055
Is this going to be an infinite sum?

51
00:02:38,055 --> 00:02:40,520
>> [LAUGH]
>> Thank you, I've been here all week.
