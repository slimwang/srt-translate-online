1
00:00:00,340 --> 00:00:04,050
Because we're lazy engineers, we're
going to take something that works,

2
00:00:04,050 --> 00:00:08,740
a logistic classifier and do the minimal
amount of change to make it non-linear.

3
00:00:08,740 --> 00:00:12,320
We're going to construct our new
function in the simplest way that we can

4
00:00:12,320 --> 00:00:13,310
think of.

5
00:00:13,310 --> 00:00:16,940
Instead of having a single matrix
multiplier as our classifier,

6
00:00:16,940 --> 00:00:19,780
we're going to insert
a RELU right in the middle.

7
00:00:19,780 --> 00:00:21,960
We now have two matrices.

8
00:00:21,960 --> 00:00:24,660
One going from the inputs to the RELUs,
and

9
00:00:24,660 --> 00:00:27,820
another one connecting
the RELUs to the classifier.

10
00:00:27,820 --> 00:00:29,650
We've solved two of our problems.

11
00:00:29,650 --> 00:00:33,450
Our function in now nonlinear thanks
to the RELU in the middle, and

12
00:00:33,450 --> 00:00:37,000
we now have a new knob that we can tune,
this number H which

13
00:00:37,000 --> 00:00:40,950
corresponds to the number of RELU
units that we have in the classifier.

14
00:00:40,950 --> 00:00:42,860
We can make it as big as we want.

15
00:00:42,860 --> 00:00:45,130
Congratulations, you've built
your first neural network.

16
00:00:46,150 --> 00:00:49,370
You might ask, wait a minute,
where's my neuron?

17
00:00:49,370 --> 00:00:53,170
In the past, when talking about neural
networks, I remember seeing diagrams

18
00:00:53,170 --> 00:00:58,900
with dendrites, axons, activation
functions, brains, neuroscience.

19
00:00:58,900 --> 00:00:59,500
Where is all that?
