1
00:00:00,260 --> 00:00:03,030
So what I'm basically arguing here is
that we can think of reinforcement

2
00:00:03,030 --> 00:00:04,730
learning as a kind of planning.

3
00:00:04,730 --> 00:00:05,550
It's not learning at all.

4
00:00:05,550 --> 00:00:09,260
It's actually planning in a kind
of continuous space in POMDP,

5
00:00:09,260 --> 00:00:12,510
where the hidden state
is the set of parameters

6
00:00:12,510 --> 00:00:14,490
of the MDP that we're trying to learn.

7
00:00:14,490 --> 00:00:18,960
Now there's an infinite number of those,
which makes things a little bit awkward.

8
00:00:18,960 --> 00:00:22,545
And so we don't really get the piecewise
linear and convex property anymore.

9
00:00:22,545 --> 00:00:24,940
But it can be shown that you can
actually get a result that shows that

10
00:00:24,940 --> 00:00:27,780
the value function in this
continuous space POMDP

11
00:00:27,780 --> 00:00:30,440
is actually piecewise polynomial and
convex.

12
00:00:30,440 --> 00:00:31,350
>> Okay.

13
00:00:31,350 --> 00:00:34,780
>> It's not as nice as linear,
but it's still representable.

14
00:00:34,780 --> 00:00:37,020
>> How big is the degree
of that polynomial?

15
00:00:37,020 --> 00:00:39,440
>> It grows with iterations
of value iteration.

16
00:00:39,440 --> 00:00:40,050
>> I see.

17
00:00:40,050 --> 00:00:43,830
>> Yeah, so it's not like piecewise
cubic and convex, it's like piecewise,

18
00:00:43,830 --> 00:00:47,930
it could be a lot of degree [LAUGH]
depending on how many iterations you do.

19
00:00:47,930 --> 00:00:51,800
So this is still pretty
awkward to work with.

20
00:00:51,800 --> 00:00:55,421
There's an algorithm that actually works
fairly nicely called BEETLE that tries

21
00:00:55,421 --> 00:00:58,790
to approximate this piecewise
polynomial and convex function.

22
00:00:58,790 --> 00:01:02,470
And can sometimes actually end up
learning very good approximations of

23
00:01:02,470 --> 00:01:07,020
the optimal way to do reinforcement
learning for those spaces of problems.

24
00:01:07,020 --> 00:01:08,052
>> Does BEETLE stand for something?

25
00:01:08,052 --> 00:01:13,512
>> Bayesian Exploration
Exploitation Tradeoff in learning.

26
00:01:13,512 --> 00:01:15,130
>> Which is cheating, I think we agree.

27
00:01:15,130 --> 00:01:15,950
>> I think we agree, yeah, but

28
00:01:15,950 --> 00:01:19,370
otherwise it would be like BEETLE,
which would be just way too hard to say.

29
00:01:19,370 --> 00:01:22,465
>> Bayesian Exploration Exploitation
Tradeoff In Learning Everything.

30
00:01:22,465 --> 00:01:24,130
>> [LAUGH] The E for everything.

31
00:01:24,130 --> 00:01:25,490
>> Yeah, and then that would be better.

32
00:01:25,490 --> 00:01:27,037
I think I could forgive that.

33
00:01:27,037 --> 00:01:29,486
>> [LAUGH] Learning excellently.

34
00:01:29,486 --> 00:01:31,600
>> No, it's too many,
that's just absurd.

35
00:01:31,600 --> 00:01:32,830
>> The point that I'd
like to make though or

36
00:01:32,830 --> 00:01:37,630
the point of the slide is that
there are Bayesian RL algorithms,

37
00:01:37,630 --> 00:01:40,220
BEETLE is one of them,
it is not by any means the only one.

38
00:01:40,220 --> 00:01:43,200
There's lots of different ways that
people have looked at actually keeping

39
00:01:43,200 --> 00:01:46,710
Bayesian posteriors over the MDPs
that are being learned and

40
00:01:46,710 --> 00:01:50,720
then trying to use that Bayesian
posterior to make better decisions than

41
00:01:50,720 --> 00:01:54,390
you would in just reinforcement
learning, where you don't have

42
00:01:54,390 --> 00:01:59,070
any kind of prior, any kind of structure
on what the possible underlying MDP is.

43
00:01:59,070 --> 00:02:00,630
So a lot of people
really like this stuff.

44
00:02:00,630 --> 00:02:01,860
It seems to, at the moment,

45
00:02:01,860 --> 00:02:06,600
be on the side of just kind of too
expensive to be practically useful.

46
00:02:06,600 --> 00:02:09,389
Q learning seems to tend to win.

47
00:02:09,389 --> 00:02:14,160
But it's very elegant, and it's a useful
way of sort of realizing that planning

48
00:02:14,160 --> 00:02:16,550
and learning actually are two
sides of the same coin.

49
00:02:16,550 --> 00:02:18,830
>> We've used it before in
some work that I've done.

50
00:02:18,830 --> 00:02:20,820
>> Okay.
>> But what we were taking advantage of

51
00:02:20,820 --> 00:02:23,840
is the fact that it gives you
probability distributions over

52
00:02:23,840 --> 00:02:25,900
value functions or
rather over Q functions.

53
00:02:25,900 --> 00:02:27,400
>> How does it do that?

54
00:02:27,400 --> 00:02:28,302
>> You're in a state, and

55
00:02:28,302 --> 00:02:31,067
it just tells you basically the
probability of an action being optimal.

56
00:02:31,067 --> 00:02:34,819
And that turns out to be very nice
because you can then compare various

57
00:02:34,819 --> 00:02:37,925
sources of information
together because they're all,

58
00:02:37,925 --> 00:02:41,016
if you can make the model
look like probabilities.

59
00:02:41,016 --> 00:02:45,592
>> That sounds like that really cool
policy sort of merging paper that I saw.

60
00:02:45,592 --> 00:02:46,452
>> Yeah, yeah, that one.

61
00:02:46,452 --> 00:02:47,019
>> Is that you?

62
00:02:47,019 --> 00:02:48,186
>> Yeah, it was by Isbell and
his colleagues.

63
00:02:48,186 --> 00:02:48,929
>> Woo.
