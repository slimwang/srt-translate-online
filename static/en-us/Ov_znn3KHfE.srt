1
00:00:00,540 --> 00:00:01,730
>> Okay, so I've the cleaned up the screen

2
00:00:01,730 --> 00:00:03,270
a little bit, Michael to, to make this a little

3
00:00:03,270 --> 00:00:06,820
bit clearer. Now, let's look at this xi transpose

4
00:00:06,820 --> 00:00:09,600
xj. And I'm now going to replace it with something.

5
00:00:12,580 --> 00:00:14,510
So, I've just replaced it with a function,

6
00:00:14,510 --> 00:00:17,660
which I'm going to call a kernel. Which takes

7
00:00:17,660 --> 00:00:21,450
xi and xj as parameters, and will return

8
00:00:21,450 --> 00:00:23,990
some number. And again, as we talked about

9
00:00:23,990 --> 00:00:28,390
before, we think of the xi transpose xj,

10
00:00:28,390 --> 00:00:30,815
as some notion of similarity, and so this

11
00:00:30,815 --> 00:00:36,220
kernel function is our representation, still, of similarity.

12
00:00:36,220 --> 00:00:37,830
Another way of thinking about that, by the way,

13
00:00:37,830 --> 00:00:41,360
is that this is the mechanism by which we

14
00:00:41,360 --> 00:00:45,600
inject domain knowledge into the support vector machine learning algorithm.

15
00:00:49,070 --> 00:00:50,730
>> Just like we were injecting domain

16
00:00:50,730 --> 00:00:53,910
knowledge when we were thinking about k-nearest neighbors.

17
00:00:53,910 --> 00:00:56,960
>> Yes, everything has domain knowledge and

18
00:00:56,960 --> 00:00:59,670
everything ultimately comes back to k-nearest neighbors.

19
00:00:59,670 --> 00:01:02,000
I don't know why and I don't know how, but it always seems to.

20
00:01:02,000 --> 00:01:04,111
>> So the k in k-nearest neighbors, and

21
00:01:04,111 --> 00:01:06,620
the k in kernel really stand for knowledge.

22
00:01:06,620 --> 00:01:09,435
>> Oh, wow, that's pretty good. We should write a paper with that title.

23
00:01:09,435 --> 00:01:14,860
>> [LAUGH] So the room neatness here, the neatness here two fold. One is,

24
00:01:14,860 --> 00:01:17,190
you can create these kernels and these kernels

25
00:01:17,190 --> 00:01:20,820
have arbitrary relationships to one another. so, what

26
00:01:20,820 --> 00:01:24,870
you're really doing is, projecting into some higher

27
00:01:24,870 --> 00:01:28,310
dimensional space, where, in that higher dimensional space, your

28
00:01:28,310 --> 00:01:30,700
points are in fact, linearly separable. But, the

29
00:01:30,700 --> 00:01:33,930
second bit is, because you're using this kernel function

30
00:01:33,930 --> 00:01:36,680
to represent your domain knowledge, you don't actually

31
00:01:36,680 --> 00:01:40,080
have to do the computation of transforming the points

32
00:01:40,080 --> 00:01:43,360
into this higher dimensional space. I mean, in fact if you think about it, with

33
00:01:43,360 --> 00:01:46,330
the last kernel that we used, computationally,

34
00:01:46,330 --> 00:01:47,460
there was actually no more work to be

35
00:01:47,460 --> 00:01:52,930
done. Before we were doing x transpose y, and now we're still doing x transpose

36
00:01:52,930 --> 00:01:54,990
y, except we're then squaring it. So

37
00:01:54,990 --> 00:01:57,038
that's just a constant bit more work. Right?

38
00:01:57,038 --> 00:02:00,865
>> So is, and that is a kernel and another kernel is X transpose Y?

39
00:02:00,865 --> 00:02:02,420
>> Yes, that's something I would call a kernel.

40
00:02:02,420 --> 00:02:05,680
>> And the other kernel we talked about was just X transpose Y by itself.

41
00:02:05,680 --> 00:02:06,940
That's, that's a kernel too, isn't it?

42
00:02:06,940 --> 00:02:08,479
>> Oh no, no. That's right. That's right.

43
00:02:08,479 --> 00:02:11,251
That's absolutely right. So, that's a different kernel,

44
00:02:11,251 --> 00:02:15,350
you're absolutely right. Just X transpose Y. Is

45
00:02:15,350 --> 00:02:18,170
another kernel. Actually we can write a general

46
00:02:18,170 --> 00:02:19,830
form of both of these. And as a

47
00:02:19,830 --> 00:02:23,280
very typical kernel, it's the polynomial kernel where

48
00:02:23,280 --> 00:02:27,500
you have x transpose y plus some constant,

49
00:02:27,500 --> 00:02:30,810
let's call it c, raised to some power p.

50
00:02:30,810 --> 00:02:32,748
And as you can see, both of those earlier

51
00:02:32,748 --> 00:02:35,480
kernels are, in fact, just a special case of this.

52
00:02:35,480 --> 00:02:37,030
>> Hm, and I would, yeah, okay, good.

53
00:02:37,030 --> 00:02:38,240
>> And that should look familiar.

54
00:02:38,240 --> 00:02:40,550
>> It reminds me of the regression lecture.

55
00:02:40,550 --> 00:02:41,600
>> Exactly, where we were doing

56
00:02:41,600 --> 00:02:43,680
polynomial regression. So now, rather than doing

57
00:02:43,680 --> 00:02:45,020
polynomial regression the way we were thinking

58
00:02:45,020 --> 00:02:47,210
about it before, we use a polynomial

59
00:02:47,210 --> 00:02:48,860
kernel and that will allow us

60
00:02:48,860 --> 00:02:51,460
to represent polynomial functions. And there're lots

61
00:02:51,460 --> 00:02:55,960
of other kernels you can come up with, Michael. So. Here's just a couple.

62
00:02:55,960 --> 00:02:58,530
I will just sort of leave em. Leave em up to

63
00:02:58,530 --> 00:03:02,690
you, to think about. And there's, there's tons of them. So,

64
00:03:02,690 --> 00:03:07,940
here's one that I, I happen to like. So, that's a

65
00:03:07,940 --> 00:03:12,010
a sort of, radial basis kernel. Does that look familiar to you?

66
00:03:12,010 --> 00:03:14,285
>> Well, to me, make sure I understand

67
00:03:14,285 --> 00:03:16,860
that it's doing the right thing. So if x

68
00:03:16,860 --> 00:03:18,260
and y are really close to each other, then

69
00:03:18,260 --> 00:03:21,140
it's like, e to the minus zero over something

70
00:03:21,140 --> 00:03:22,490
which is like e to the zero, which is like

71
00:03:22,490 --> 00:03:25,490
one. So there's similarities like one if they're on top

72
00:03:25,490 --> 00:03:28,160
of each other. If they're very far apart, then it's

73
00:03:28,160 --> 00:03:31,550
like their distance is something very big divided by something

74
00:03:31,550 --> 00:03:33,490
e to the minus something very big is very close

75
00:03:33,490 --> 00:03:37,410
to zero. So it does have that kind of property

76
00:03:37,410 --> 00:03:40,430
kind of like the sigmoid where it, it transitions between zero

77
00:03:40,430 --> 00:03:44,420
and one but it's not exactly the same shape as that.

78
00:03:44,420 --> 00:03:46,890
>> Right in fact it's symmetric, that the square

79
00:03:46,890 --> 00:03:49,610
of the of the distance between you in making an

80
00:03:49,610 --> 00:03:54,650
actual distance, makes it always a positive value there. Or at

81
00:03:54,650 --> 00:03:58,840
least a non-negative value there. And so it becomes symmetric,

82
00:03:58,840 --> 00:04:01,480
so it looks a lot more like a, like a gaussian

83
00:04:01,480 --> 00:04:04,350
with some kind of width which is represented by sigma.

84
00:04:04,350 --> 00:04:05,710
And there are tons and tons of these. Actually if you

85
00:04:05,710 --> 00:04:08,000
wanted to get something that looked like a sigmoid, here's

86
00:04:08,000 --> 00:04:12,120
one. Where alpha's different from the other alphas, but I couldn't

87
00:04:12,120 --> 00:04:15,280
think of a different Greek letter. And this function gives you

88
00:04:15,280 --> 00:04:18,850
something that looks a lot more like a sigmoid. And there're

89
00:04:18,850 --> 00:04:21,750
tons and tons of these you can come up with. And

90
00:04:21,750 --> 00:04:23,890
there's lots of, been a lot of research over the years

91
00:04:23,890 --> 00:04:27,330
on what makes a good kernel function. The most important thing

92
00:04:27,330 --> 00:04:31,260
here, I think, is that it really captures your, your domain

93
00:04:31,260 --> 00:04:34,050
knowledge. It really captures your notion of similarity. You might notice,

94
00:04:34,050 --> 00:04:37,840
Michael, that since it's just an arbitrary function that returns a number,

95
00:04:37,840 --> 00:04:42,440
it means that X and Y or the, the different data points you have, don't have

96
00:04:42,440 --> 00:04:45,080
to actually be points in a numerical space.

97
00:04:45,080 --> 00:04:49,790
They could be discrete variables. They could describe whether

98
00:04:49,790 --> 00:04:51,290
you're male or female. As long as you

99
00:04:51,290 --> 00:04:54,350
have some notion of similarity to play around

100
00:04:54,350 --> 00:04:56,870
with, that you can define, that returns a

101
00:04:56,870 --> 00:04:58,920
number, then it doesn't matter. It will always work.

102
00:04:58,920 --> 00:05:03,780
>> So can you do things like, I don't know, strings or graphs or images?

103
00:05:03,780 --> 00:05:06,400
>> Absolutely. You could think about two strings. How are

104
00:05:06,400 --> 00:05:09,290
two strings similar? Maybe they're, they're similar if their edit

105
00:05:09,290 --> 00:05:12,540
distance is small. The number of transformations that you have

106
00:05:12,540 --> 00:05:15,430
to give in order to transform one string to another. If

107
00:05:15,430 --> 00:05:17,960
there are few of those, then they're very similar. If

108
00:05:17,960 --> 00:05:20,690
there are a lot of those then they're very dissimilar.

109
00:05:20,690 --> 00:05:23,750
You could talk about words like cat and lion, and

110
00:05:23,750 --> 00:05:28,720
decide those are more similar than cat and mosquito, for example.

111
00:05:28,720 --> 00:05:29,440
>> because

112
00:05:29,440 --> 00:05:30,740
they, because of the ears?

113
00:05:30,740 --> 00:05:32,260
>> Yeah. Mainly because of the ears.

114
00:05:32,260 --> 00:05:34,130
>> All right. But then I, then I think I understand.

115
00:05:34,130 --> 00:05:36,580
>> Okay. Good. So you might be

116
00:05:36,580 --> 00:05:38,670
curious, Michael, whether there are any bad kernel

117
00:05:38,670 --> 00:05:41,150
functions. There is actually an answer to that.

118
00:05:41,150 --> 00:05:42,930
While it's not clear whether there are any

119
00:05:42,930 --> 00:05:47,540
bad kernel functions, it is the case that in order for all the math to

120
00:05:47,540 --> 00:05:51,200
go through, there is a specific technical requirement

121
00:05:51,200 --> 00:05:53,450
of a kernel function. It has a name.

122
00:05:55,090 --> 00:06:00,190
And it's the Mercer Condition. Have you ever heard of the Mercer Condition?

123
00:06:00,190 --> 00:06:03,020
>> I've heard the word. I actually used

124
00:06:03,020 --> 00:06:05,010
to live near Mercer County in New Jersey.

125
00:06:05,010 --> 00:06:05,370
>> You did?

126
00:06:05,370 --> 00:06:05,840
>> Yeah.

127
00:06:05,840 --> 00:06:09,430
>> Oh. So then I guess it's the condition of living near

128
00:06:09,430 --> 00:06:12,460
where Michael used to live. Now, so the Mercer condition is a very

129
00:06:12,460 --> 00:06:15,560
technical thing we'll talk about this again, a little bit in the

130
00:06:15,560 --> 00:06:17,450
homework assignment. But for your intuition

131
00:06:17,450 --> 00:06:20,290
in the meantime, it basically means it

132
00:06:20,290 --> 00:06:22,010
acts like a distance, or it acts like a

133
00:06:22,010 --> 00:06:25,450
similarity. It's not an arbitrary thing that doesn't relate the

134
00:06:25,450 --> 00:06:29,630
various points together. Being positive is something definite in

135
00:06:29,630 --> 00:06:32,530
in this context means it's a well behaved distance function.

136
00:06:32,530 --> 00:06:33,080
>> Gotcha.
