1
00:00:00,240 --> 00:00:02,160
>> Alright, so back to boosting, Michael. So as

2
00:00:02,160 --> 00:00:04,650
you recall the little teaser I left you with last

3
00:00:04,650 --> 00:00:07,800
time, is that it appears that boosting does not

4
00:00:07,800 --> 00:00:11,680
always over-fit. And a little graph. That's true, but it

5
00:00:11,680 --> 00:00:13,940
doesn't seem to over-fit in the ways that we

6
00:00:13,940 --> 00:00:16,910
would normally expect it to over-fit. And in particular we'd

7
00:00:16,910 --> 00:00:21,630
see a, you know, an error line on training And

8
00:00:21,630 --> 00:00:25,280
what we expect to see is a testing line that

9
00:00:25,280 --> 00:00:27,190
would, you know, hue pretty closely and then start to

10
00:00:27,190 --> 00:00:31,720
get bad. But what actually happens is that instead, this

11
00:00:31,720 --> 00:00:35,160
little bit at the end where you get over fitting

12
00:00:35,160 --> 00:00:39,470
seems to instead. Just keep doing well. In fact, getting better

13
00:00:39,470 --> 00:00:41,820
and better and better. And I promised you an explanation

14
00:00:41,820 --> 00:00:44,790
for why that was. So, given what we talked about

15
00:00:44,790 --> 00:00:47,000
with support vector machines, and what we spent most of

16
00:00:47,000 --> 00:00:49,960
our time thinking about, what do you think the answer is?

17
00:00:49,960 --> 00:00:49,960
>>

18
00:00:49,960 --> 00:00:53,180
Well I, I don't think I would have asked again if I, had

19
00:00:53,180 --> 00:00:55,880
a thought about it. But you mean You want me to connect it

20
00:00:55,880 --> 00:00:59,730
to support vector machines, somehow. Well the, the thing that was fighting over

21
00:00:59,730 --> 00:01:02,190
fitting in support vector machines, was

22
00:01:02,190 --> 00:01:05,050
trying to focus on maximum margin classifiers.

23
00:01:05,050 --> 00:01:07,360
>> Here, let me, let me try to explain to you

24
00:01:07,360 --> 00:01:10,550
why it is that you don't have this problem with With

25
00:01:10,550 --> 00:01:12,670
overfitting at least not in the, in the typical way as

26
00:01:12,670 --> 00:01:15,000
you keep applying it over and over again like you do

27
00:01:15,000 --> 00:01:17,990
with something like neural networks. And it really boils down

28
00:01:17,990 --> 00:01:21,950
to noticing that we've been ignoring some information. So, what we

29
00:01:21,950 --> 00:01:26,150
normally keep track of is error. So error on say

30
00:01:26,150 --> 00:01:30,300
a training set is just, you know, the The probability that

31
00:01:30,300 --> 00:01:32,510
you're going to come up with an incorrect answer or

32
00:01:32,510 --> 00:01:35,050
come up with an answer that disagrees with your training set.

33
00:01:35,050 --> 00:01:36,920
and that's a very natural thing to think about and it

34
00:01:36,920 --> 00:01:40,230
makes a lot of sense. But there's also something else that

35
00:01:40,230 --> 00:01:43,070
is actually captured inside of boosting and captured by a lot of learning

36
00:01:43,070 --> 00:01:44,620
algorithms we haven't been taking advantage

37
00:01:44,620 --> 00:01:46,209
of, and that's the notion of confidence.

38
00:01:47,620 --> 00:01:50,620
So confidence is not just whether you got it right or wrong. It's

39
00:01:50,620 --> 00:01:54,950
how strongly you believe in a particular answer that you given. Make sense?

40
00:01:54,950 --> 00:01:57,830
>> Yes, a lot of the algorithms we talked

41
00:01:57,830 --> 00:02:00,250
indirectly have something like that. So, like in a nearest

42
00:02:00,250 --> 00:02:02,620
neibhbor method, if you are doing five nearest neighbor

43
00:02:02,620 --> 00:02:05,420
and all five of the neighbor agree, that seems different

44
00:02:05,420 --> 00:02:08,570
than the case with vote one way and two vote the other.

45
00:02:08,570 --> 00:02:10,960
>> Right. And in fact, that's a really good example. If you think of that in

46
00:02:10,960 --> 00:02:13,550
terms of regression Then you could say something

47
00:02:13,550 --> 00:02:17,850
like the variance, between them is sort of

48
00:02:17,850 --> 00:02:20,230
a stand in for confidence. Low variance means

49
00:02:20,230 --> 00:02:23,780
everyone agrees, high variance means, there's some major

50
00:02:23,780 --> 00:02:26,270
disagreement. Okay. So what does that mean in

51
00:02:26,270 --> 00:02:30,600
the boosting case? Well as you recall, the final

52
00:02:30,600 --> 00:02:35,490
output of the boosted classifier is given by a very simple formula.

53
00:02:39,430 --> 00:02:44,510
And here's the equation here that h of x is equal to the sine of

54
00:02:44,510 --> 00:02:49,520
the sum over all of the weak hypothesis that you've gotten of alpha

55
00:02:49,520 --> 00:02:54,990
times h. So the weighted average of all of the hypothesis, right? And

56
00:02:54,990 --> 00:02:57,885
you just simply, if it's positive you produce a plus

57
00:02:57,885 --> 00:03:00,050
one. And if it's it negative you produce a minus and

58
00:03:00,050 --> 00:03:01,870
if it's exactly zero you don't know what to do

59
00:03:01,870 --> 00:03:05,190
so you just. Produces zero. Just throw up your hands. So

60
00:03:05,190 --> 00:03:08,211
I'm going to make a tiny change to this formula, Michael. Just, just for

61
00:03:08,211 --> 00:03:09,950
the purpose of sort of, explanation, that

62
00:03:09,950 --> 00:03:12,930
doesn't change the fundamental answer. And I'm

63
00:03:12,930 --> 00:03:16,420
just going to take exactly this equation as it is. And I'm going to divide

64
00:03:16,420 --> 00:03:19,980
it, by the weights that we use. Now what does that end up doing?

65
00:03:19,980 --> 00:03:23,380
>> Okay, so the weights. I'm getting a.

66
00:03:23,380 --> 00:03:26,110
There's Alphas in the SVM's too, so I'm

67
00:03:26,110 --> 00:03:28,130
getting a little confused. So that I'm. I

68
00:03:28,130 --> 00:03:30,563
think these Alphas all have to be non-negative.

69
00:03:30,563 --> 00:03:31,470
>> Right.

70
00:03:31,470 --> 00:03:35,270
>> But they kind of like this support vector values, in that

71
00:03:35,270 --> 00:03:38,750
there could be zero, if, if that hypothesis isn't come into play?

72
00:03:38,750 --> 00:03:41,230
>> Well, but they want in that case, the, the

73
00:03:41,230 --> 00:03:44,840
alpha is always set to be the natural log of something.

74
00:03:44,840 --> 00:03:48,620
>> Oh, oh, oh, and also these alphas are applied to hypothesis whereas

75
00:03:48,620 --> 00:03:51,980
the alphas in the, in the SVM settings were being applied to data points.

76
00:03:51,980 --> 00:03:54,030
>> That's right. So, unfortunately in machine

77
00:03:54,030 --> 00:03:56,415
learning, people in, invent things separately and

78
00:03:56,415 --> 00:04:00,970
re-use notation. Alpha's an easy Greek character to draw, so people use

79
00:04:00,970 --> 00:04:04,020
it all the time. But here, remember, alpha's the measure of how good

80
00:04:04,020 --> 00:04:07,670
a particular weak hypothesis was, and since it has to do better

81
00:04:07,670 --> 00:04:10,630
than chance, it works out that it will always be greater than zero.

82
00:04:10,630 --> 00:04:13,690
>> Gotcha, okay. So this, this normalization factor,

83
00:04:13,690 --> 00:04:16,930
this denominator doesn't, it's just a constant with respect

84
00:04:16,930 --> 00:04:19,220
to x, the input. So it won't actually change

85
00:04:19,220 --> 00:04:21,660
the answer. So it really is the same answer

86
00:04:21,660 --> 00:04:23,300
as we had before, just a different way of writing it.

87
00:04:23,300 --> 00:04:25,880
>> Right. And what it ends up doing like

88
00:04:25,880 --> 00:04:28,100
often is the case in these situations, is it

89
00:04:28,100 --> 00:04:31,190
normalizes the output. So it turns out that this

90
00:04:31,190 --> 00:04:35,010
value. Inside here is always going to be between minus

91
00:04:35,010 --> 00:04:39,640
one and plus one. Okay? But otherwise it doesn't

92
00:04:39,640 --> 00:04:42,880
change anything about what we've been doing for boosting. So

93
00:04:42,880 --> 00:04:44,510
you might ask why did I go through the

94
00:04:44,510 --> 00:04:46,850
trouble of normalizing it between minus one and plus one?

95
00:04:46,850 --> 00:04:47,940
>> Why indeed?

96
00:04:47,940 --> 00:04:50,020
>> Well it's makes it easier for me to draw

97
00:04:50,020 --> 00:04:53,000
what I want to draw next. So, we know that the

98
00:04:53,000 --> 00:04:56,760
output of this little bit inside the sign function is

99
00:04:56,760 --> 00:04:59,380
always going to be between minus one and plus one. Let's

100
00:04:59,380 --> 00:05:03,110
imagine that I take some particular data point x and

101
00:05:03,110 --> 00:05:05,190
I pass it through this function, I'm going to get some

102
00:05:05,190 --> 00:05:07,580
value between minus one and plus one. And let's just

103
00:05:07,580 --> 00:05:10,510
say for the sake of the argument, it ends up here.

104
00:05:11,990 --> 00:05:12,230
Okay?

105
00:05:12,230 --> 00:05:13,680
>> Is that an x or a plus?

106
00:05:13,680 --> 00:05:14,620
>> That's a plus.

107
00:05:14,620 --> 00:05:17,380
>> Okay. So it's a positive example and it's near plus one.

108
00:05:17,380 --> 00:05:18,290
>> Right.

109
00:05:18,290 --> 00:05:20,830
>> So this would be something that the algorithm is getting correct.

110
00:05:20,830 --> 00:05:22,580
>> Yes, and it's not just getting it correct,

111
00:05:22,580 --> 00:05:26,760
but it is very confident. In its correctness. because it

112
00:05:26,760 --> 00:05:29,460
gave it a very high value. By contrast there

113
00:05:29,460 --> 00:05:32,400
could have been another positive that ends up around here.

114
00:05:32,400 --> 00:05:33,580
>> Hmm.

115
00:05:33,580 --> 00:05:37,060
>> So it gets it correct but it doesn't have a lot

116
00:05:37,060 --> 00:05:39,550
of confidence so to speak in its correct answer because it's very

117
00:05:39,550 --> 00:05:44,140
near to zero. So that's the difference between error and confidence. Because

118
00:05:44,140 --> 00:05:47,050
for example I could also have a plus value way over here.

119
00:05:47,050 --> 00:05:51,770
So I am very, very confident in my very, very incorrect answer.

120
00:05:51,770 --> 00:05:52,100
>> Mm.

121
00:05:52,100 --> 00:05:54,270
>> So this is my daughter, for example.

122
00:05:54,270 --> 00:05:56,070
[LAUGH] She's very confident whether she's right or

123
00:05:56,070 --> 00:05:58,770
wrong. [LAUGH] Okay. And so now imagine there's

124
00:05:58,770 --> 00:06:02,100
lots of little points like this. And if

125
00:06:02,100 --> 00:06:04,140
you're doing well, you would expect that, you

126
00:06:04,140 --> 00:06:06,550
know, very, very often you're going to be

127
00:06:06,550 --> 00:06:08,520
correct. And so you end up shoving all

128
00:06:08,520 --> 00:06:10,330
the positives over here to the right, and all

129
00:06:10,330 --> 00:06:14,060
the negatives over here to the left. And it would be really nice if you were

130
00:06:14,060 --> 00:06:15,770
sort of confident in all of them. Okay,

131
00:06:15,770 --> 00:06:17,260
so does this make sense, Michael as a picture,

132
00:06:17,260 --> 00:06:17,550
>> Oh yeah.

133
00:06:17,550 --> 00:06:19,330
>> What, what might be going on? Absolutely.

134
00:06:19,330 --> 00:06:22,130
>> Okay, good. So now I want you to imagine that we've

135
00:06:22,130 --> 00:06:23,770
been going through these, these training

136
00:06:23,770 --> 00:06:27,260
examples, and we've gotten very, very good

137
00:06:27,260 --> 00:06:29,120
training error. In fact, let's imagine that

138
00:06:29,120 --> 00:06:31,953
we have negative training error. I'm [LAUGH]

139
00:06:31,953 --> 00:06:32,260
>> Wow.

140
00:06:32,260 --> 00:06:34,180
>> In fact, let's imagine that we have no

141
00:06:34,180 --> 00:06:37,360
training error at all. So we, we label everything correctly.

142
00:06:37,360 --> 00:06:38,700
So then the picture would look just a little

143
00:06:38,700 --> 00:06:41,840
bit different We're going to have all the pluses on one

144
00:06:41,840 --> 00:06:43,750
side, and all the minuses on the other. But

145
00:06:43,750 --> 00:06:47,350
we keep on training, we keep adding more and more

146
00:06:47,350 --> 00:06:50,230
weak learners into the mix. So here's what ends up

147
00:06:50,230 --> 00:06:52,810
happening in practice, right? What ends up happening in practice

148
00:06:52,810 --> 00:06:57,870
is, you have to do some kind of distribution

149
00:06:57,870 --> 00:07:00,680
on the hard examples. And the hard examples are

150
00:07:00,680 --> 00:07:02,800
going to be the one that are very near the

151
00:07:02,800 --> 00:07:06,340
boundary. So as you add more and more of these

152
00:07:06,340 --> 00:07:08,440
weak learners what seems to happen in practice is

153
00:07:08,440 --> 00:07:11,560
that these pluses that are near the boundary and these

154
00:07:11,560 --> 00:07:14,530
minuses that are near the boundary just start moving

155
00:07:14,530 --> 00:07:17,860
farther and farther away from the boundary. So, this minus

156
00:07:17,860 --> 00:07:20,340
starts drifting and drifting and drifting until it's all the way over

157
00:07:20,340 --> 00:07:23,660
here, this minus starts drifting and drifting and drifting untili it's all the

158
00:07:23,660 --> 00:07:27,340
way over here. And the same happens for the pluses. And as

159
00:07:27,340 --> 00:07:29,990
you keep going and you keep going, what ends up happening is that

160
00:07:29,990 --> 00:07:33,440
your error stays the same. It doesn't change at all, however your

161
00:07:33,440 --> 00:07:37,110
confidence keeps going up and up and up and up and up. Which

162
00:07:37,110 --> 00:07:39,410
has the effect, if you'll look at this little drawing over here of

163
00:07:39,410 --> 00:07:43,040
moving the pluses all around over here, so they're all in a bunch,

164
00:07:43,040 --> 00:07:44,870
and the minuses are on the other side.

165
00:07:44,870 --> 00:07:46,730
So what does that look like to you, Michael?

166
00:07:46,730 --> 00:07:47,380
>> This picture?

167
00:07:47,380 --> 00:07:47,820
>> Yeah.

168
00:07:47,820 --> 00:07:49,330
>> I mean that there's a, there's a

169
00:07:49,330 --> 00:07:52,590
big gap between the left most plus and the

170
00:07:52,590 --> 00:07:54,740
right most minus. Which, you know, in the

171
00:07:54,740 --> 00:07:56,710
context of this lecture reminds me of a margin.

172
00:07:56,710 --> 00:08:00,090
>> That's exactly right. Basically what ends up

173
00:08:00,090 --> 00:08:03,150
happening is that as you add more and more

174
00:08:03,150 --> 00:08:05,760
weak learners here the boosting out rhythm ends

175
00:08:05,760 --> 00:08:08,230
up becoming more and more confident in its answers

176
00:08:08,230 --> 00:08:12,460
which it's getting correct. And therefore effectively ends up creating a

177
00:08:12,460 --> 00:08:15,570
bigger and bigger margin. And what do we know about large margins?

178
00:08:15,570 --> 00:08:18,420
>> Large margins tend to minimize over fitting.

179
00:08:18,420 --> 00:08:22,870
>> That's exactly right. So it, counter intuitively, as we create

180
00:08:22,870 --> 00:08:26,530
more and more of these hypotheses, which you would think would

181
00:08:26,530 --> 00:08:29,620
make something more and more complicated, it turns out that you

182
00:08:29,620 --> 00:08:33,970
end up with something smoother, less likely to overfit and ultimately,

183
00:08:33,970 --> 00:08:38,200
less complicated. So the reason boosting tends to do well and tends to avoid

184
00:08:38,200 --> 00:08:42,308
over fitting even as you add more and more learners is that you're increasing

185
00:08:42,308 --> 00:08:45,840
the margin. And there you go. And if you look in the reading that

186
00:08:45,840 --> 00:08:49,000
we gave the students there's actually a

187
00:08:49,000 --> 00:08:51,070
detailed descritpion about this in a proof.

188
00:08:51,070 --> 00:08:51,440
>> Cool.

189
00:08:51,440 --> 00:08:53,060
>> Okay. So, there you go, Michael.

190
00:08:53,060 --> 00:08:55,530
Do you think, then, that boosting never overfits?

191
00:08:55,530 --> 00:08:59,790
>> [SOUND] Never seems like such a strong word.

192
00:08:59,790 --> 00:09:02,310
I mean, the story that you told says that it's going to try

193
00:09:02,310 --> 00:09:05,450
to separate those things out, but I guess I guess it doesn't have

194
00:09:05,450 --> 00:09:07,480
to be able to do that. I mean, it could be that

195
00:09:07,480 --> 00:09:09,140
for example all the weak learners

196
00:09:09,140 --> 00:09:13,220
are I dunno very unconfident very inconsistent.

197
00:09:13,220 --> 00:09:15,910
>> Hm. Okay, well you know, maybe, maybe it's worthwhile to

198
00:09:15,910 --> 00:09:18,380
take a little diversion here to take a five second quiz.

199
00:09:18,380 --> 00:09:20,780
>> I think it's worth the time.

200
00:09:20,780 --> 00:09:21,650
>> Done!
