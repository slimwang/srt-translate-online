1
00:00:00,270 --> 00:00:01,630
So the way I like to think about it,

2
00:00:01,630 --> 00:00:05,290
is there's three main families of
reinforcement learning algorithms.

3
00:00:05,290 --> 00:00:07,260
So, the one that's most like
what we've talked about so

4
00:00:07,260 --> 00:00:10,760
far is what we could call model based
reinforcement learning algorithm.

5
00:00:10,760 --> 00:00:14,610
So, model based reinforcement learning
algorithms takes the state action reward

6
00:00:14,610 --> 00:00:16,610
tupples that it gets, and

7
00:00:16,610 --> 00:00:21,349
sends them to a model learner, which
learns the transitions and rewards.

8
00:00:22,498 --> 00:00:24,580
Those transition and rewards,
once you've learned them,

9
00:00:24,580 --> 00:00:27,700
you can put through an MDPSolver,
like we talked about,

10
00:00:27,700 --> 00:00:32,170
which could be used to spit out a Q*,
a optimal value function.

11
00:00:32,170 --> 00:00:35,850
And once you have the optimal
value function, you can use,

12
00:00:35,850 --> 00:00:38,920
just by taking the argmax of the state
that you are in, you can choose which

13
00:00:38,920 --> 00:00:42,100
action you should take in any given
state and that gives you the policy.

14
00:00:42,100 --> 00:00:45,630
So it's still mapping the state action
reward sequence to policies, but

15
00:00:45,630 --> 00:00:48,210
it's doing it by creating all these
intermediate values in between.

16
00:00:52,180 --> 00:00:53,440
Cool, and
let me just add one more thing,

17
00:00:53,440 --> 00:00:57,980
which is that the model learner
takes the history that it's seeing.

18
00:00:57,980 --> 00:01:00,832
But it also takes its current estimate
of transition to rewards to produce

19
00:01:00,832 --> 00:01:02,582
the new estimate of
the transition rewards.

20
00:01:02,582 --> 00:01:05,769
So, this is how I want to kind of
represent the learning piece of

21
00:01:05,769 --> 00:01:06,578
the sequence.

22
00:01:06,578 --> 00:01:07,370
Does that make sense?

23
00:01:08,650 --> 00:01:09,820
All right, so this is one type.

24
00:01:09,820 --> 00:01:11,980
Now let me show you another type.

25
00:01:11,980 --> 00:01:14,770
So, the second class of
reinforcement learning

26
00:01:14,770 --> 00:01:16,510
algorithm that's
important to think about.

27
00:01:16,510 --> 00:01:19,568
They're referred to as
value-function-based, or actually,

28
00:01:19,568 --> 00:01:20,800
sometimes model-free.

29
00:01:20,800 --> 00:01:22,743
So, the beginning and
the end of it are still the same.

30
00:01:22,743 --> 00:01:26,964
We're taking sequences of state action
rewards and producing a policy and

31
00:01:26,964 --> 00:01:30,247
we even have this Q* in between
that we generate the policy

32
00:01:30,247 --> 00:01:31,588
from using the argmax.

33
00:01:31,588 --> 00:01:35,328
But now, instead of feeding back
the transitions and rewards,

34
00:01:35,328 --> 00:01:40,107
we're actually feeding back Q*, and we
have a direct value update equation that

35
00:01:40,107 --> 00:01:45,000
takes state action reward that it just
experienced, the current estimate of Q*.

36
00:01:45,000 --> 00:01:48,710
Actually, it's kind of more Q than Q*,

37
00:01:48,710 --> 00:01:52,719
and uses that to generate a new Q,
which is then used to generate a policy.

38
00:01:55,427 --> 00:01:58,278
Yeah.
So instead of explicitly building

39
00:01:58,278 --> 00:02:03,007
a model and using it,
it just somehow directly learns the Q

40
00:02:03,007 --> 00:02:06,303
values from state actions and rewards.

41
00:02:06,303 --> 00:02:09,794
And then the third class of
reinforcement learning algorithms comes

42
00:02:09,794 --> 00:02:13,405
from the idea that you can sometimes
actually take the policy itself and

43
00:02:13,405 --> 00:02:17,197
feed that back to a policy update that
directly modifies the policy based on

44
00:02:17,197 --> 00:02:19,450
the state action rewards
that you receive.

45
00:02:19,450 --> 00:02:22,960
So, in some sense, this is much, much
more direct, but the learning problem is

46
00:02:22,960 --> 00:02:26,380
very difficult because the kind of
feedback that you're getting about

47
00:02:26,380 --> 00:02:30,700
the policy isn't really very useful for
directly modifying the policy.

48
00:02:30,700 --> 00:02:33,789
So, here's these three models all
together so you can compare and

49
00:02:33,789 --> 00:02:34,351
contrast.

50
00:02:34,351 --> 00:02:35,029
You can see,

51
00:02:35,029 --> 00:02:39,105
in some sense, they're getting simpler
as [LAUGH] we go down to policy search.

52
00:02:39,105 --> 00:02:42,968
But you could also say that
the learning is more direct.

53
00:02:42,968 --> 00:02:48,290
And as we go up this way, the learning
problems become more supervised

54
00:02:48,290 --> 00:02:53,270
in the sense that you can imagine
learning to predict next dates and next

55
00:02:53,270 --> 00:02:57,800
rewards from previous dates and previous
rewards to learn T/R pretty directly.

56
00:02:57,800 --> 00:03:00,010
It's basically a supervised
learning problem.

57
00:03:00,010 --> 00:03:02,611
You get to see what
the output's supposed to be.

58
00:03:02,611 --> 00:03:05,905
Here, you don't quite get to see what
the output's supposed to be, but

59
00:03:05,905 --> 00:03:08,769
you do get values that you can
imagine propagating backwards.

60
00:03:08,769 --> 00:03:11,336
And here,
you get very little useful feedback,

61
00:03:11,336 --> 00:03:14,655
in terms of how to change your
policy to make it better.

62
00:03:14,655 --> 00:03:19,610
>> Wait, so, the way you put that
suggests that you should always do one.

63
00:03:19,610 --> 00:03:20,930
Well, so, yeah, but

64
00:03:20,930 --> 00:03:23,680
look at all the extra stuff
that you're doing in between.

65
00:03:23,680 --> 00:03:26,820
So, in fact, different problems
have different trade offs,

66
00:03:26,820 --> 00:03:30,410
as to whether or not which of
these is the best thing to do.

67
00:03:30,410 --> 00:03:32,540
A lot of the research has been
focused on number two, and

68
00:03:32,540 --> 00:03:35,770
that's what we're going to be diving
into, because in some ways it strikes

69
00:03:35,770 --> 00:03:39,630
a nice balance between keeping
the computations relatively simple and

70
00:03:39,630 --> 00:03:42,000
keeping the learning
updates relatively simple.

71
00:03:42,000 --> 00:03:44,040
But there are plenty of cases
where you'd rather do one and

72
00:03:44,040 --> 00:03:44,880
where you'd rather do three.

73
00:03:46,580 --> 00:03:48,310
>> I look forward to you
convincing me that that's true.
