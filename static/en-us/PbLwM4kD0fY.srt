1
00:00:00,000 --> 00:00:01,709
one of things that goes wrong when you

2
00:00:01,709 --> 00:00:04,740
try to actually run gradient descent on

3
00:00:04,740 --> 00:00:07,290
a complex network with a lot of data is

4
00:00:07,290 --> 00:00:08,580
that you can get stuck in these local

5
00:00:08,580 --> 00:00:10,169
minima and then you start to wonder boy

6
00:00:10,169 --> 00:00:11,580
is there some other way that I can

7
00:00:11,580 --> 00:00:13,019
optimize these weights i'm trying to

8
00:00:13,019 --> 00:00:14,460
find a set of weights for the neural

9
00:00:14,460 --> 00:00:17,670
network that well that what that that

10
00:00:17,670 --> 00:00:20,010
tries to minimize error on the training

11
00:00:20,010 --> 00:00:22,230
set and so gradient descent is one way

12
00:00:22,230 --> 00:00:23,910
to do it and it can get stuck but

13
00:00:23,910 --> 00:00:25,079
there's other kinds of advanced

14
00:00:25,079 --> 00:00:26,579
optimization methods have become very

15
00:00:26,579 --> 00:00:29,489
appropriate here in fact there's a lot

16
00:00:29,489 --> 00:00:31,260
of people in machine learning who think

17
00:00:31,260 --> 00:00:33,270
of optimization and learning is kind of

18
00:00:33,270 --> 00:00:34,770
being the same thing that what you're

19
00:00:34,770 --> 00:00:36,238
really trying to do in any kind of

20
00:00:36,238 --> 00:00:38,219
learning problem is solved this this

21
00:00:38,219 --> 00:00:40,350
high-order very difficult optimization

22
00:00:40,350 --> 00:00:42,988
problem to figure out what the learned

23
00:00:42,988 --> 00:00:45,869
representation needs to be so i just

24
00:00:45,869 --> 00:00:47,520
want to mention in passing so various

25
00:00:47,520 --> 00:00:49,109
kinds of advanced methods that that

26
00:00:49,109 --> 00:00:50,399
people brought to bear there's things

27
00:00:50,399 --> 00:00:53,399
like mom using momentum terms in the

28
00:00:53,399 --> 00:00:55,799
gradient which basically where the idea

29
00:00:55,799 --> 00:00:57,448
in the momentum is as we're doing

30
00:00:57,448 --> 00:00:59,009
gradient descents imagine this is our

31
00:00:59,009 --> 00:01:00,570
error surface we don't want to get stuck

32
00:01:00,570 --> 00:01:02,100
in this little Bowl here we want to kind

33
00:01:02,100 --> 00:01:03,359
of pass all the way through to get to

34
00:01:03,359 --> 00:01:05,159
this bowl so maybe we need to just you

35
00:01:05,159 --> 00:01:06,569
know continue in the direction we've

36
00:01:06,569 --> 00:01:08,430
been going so instead of you know think

37
00:01:08,430 --> 00:01:09,780
of it as a kind of physical analogy

38
00:01:09,780 --> 00:01:11,670
instead of just just going to the bottom

39
00:01:11,670 --> 00:01:12,868
of this hill and getting stuck it can

40
00:01:12,868 --> 00:01:14,640
kind of bounced out and pop over and

41
00:01:14,640 --> 00:01:16,829
come to what might be a lower minima

42
00:01:16,829 --> 00:01:19,769
later there's a lot of work in using

43
00:01:19,769 --> 00:01:21,478
higher order derivatives to to better

44
00:01:21,478 --> 00:01:23,430
optimize things instead of just thinking

45
00:01:23,430 --> 00:01:25,590
about the way that individual weights

46
00:01:25,590 --> 00:01:27,209
change the error function to look at

47
00:01:27,209 --> 00:01:29,969
combinations of weights Hamiltonians and

48
00:01:29,969 --> 00:01:32,340
whatnot there's various ideas from

49
00:01:32,340 --> 00:01:33,930
randomized optimization which were going

50
00:01:33,930 --> 00:01:37,469
to get to in a sister course that can be

51
00:01:37,469 --> 00:01:39,599
applied to two to make things more

52
00:01:39,599 --> 00:01:42,180
robust and sometimes it's worth thinking

53
00:01:42,180 --> 00:01:43,920
you know what we don't really want to

54
00:01:43,920 --> 00:01:45,688
just minimize the error on the training

55
00:01:45,688 --> 00:01:47,849
set we may actually want to have some

56
00:01:47,849 --> 00:01:50,519
kind of penalty for using using a

57
00:01:50,519 --> 00:01:54,118
structure that's too complex missed when

58
00:01:54,118 --> 00:01:55,290
do we when do we see something like this

59
00:01:55,290 --> 00:01:57,390
before Charles when we were doing

60
00:01:57,390 --> 00:02:00,030
regression and we were talking about

61
00:02:00,030 --> 00:02:02,399
overfitting what's a more or less

62
00:02:02,399 --> 00:02:04,049
complex network

63
00:02:04,049 --> 00:02:06,540
well there's two things you can do with

64
00:02:06,540 --> 00:02:08,429
network you can add more and more nodes

65
00:02:08,429 --> 00:02:10,800
and you can add more and more layers

66
00:02:10,800 --> 00:02:13,439
good so right so if we do

67
00:02:13,439 --> 00:02:15,000
more nodes that we put into network the

68
00:02:15,000 --> 00:02:17,099
more complicated the mapping becomes

69
00:02:17,099 --> 00:02:18,990
from input to output the more local

70
00:02:18,990 --> 00:02:20,699
minimum we get the more we get they have

71
00:02:20,699 --> 00:02:22,860
the ability to actually model the noise

72
00:02:22,860 --> 00:02:24,210
which brings up exactly the same

73
00:02:24,210 --> 00:02:26,280
overfitting issues it turns out there's

74
00:02:26,280 --> 00:02:27,360
another one that's actually really

75
00:02:27,360 --> 00:02:28,740
interesting in the neural net setting

76
00:02:28,740 --> 00:02:31,080
which I think didn't occur to people in

77
00:02:31,080 --> 00:02:32,580
the early days but it became clear and

78
00:02:32,580 --> 00:02:36,120
clear over time which is that you can

79
00:02:36,120 --> 00:02:38,400
also have a complex network just because

80
00:02:38,400 --> 00:02:40,860
the numbers the weights are very large

81
00:02:40,860 --> 00:02:43,110
so same number of weight same number of

82
00:02:43,110 --> 00:02:45,270
nodes same number of layers but larger

83
00:02:45,270 --> 00:02:47,219
numbers often leads to more complex

84
00:02:47,219 --> 00:02:50,250
network and the possibility of

85
00:02:50,250 --> 00:02:52,169
overfitting and so sometimes we want to

86
00:02:52,169 --> 00:02:53,460
penalize the network not just by giving

87
00:02:53,460 --> 00:02:55,080
it $PERCENT fewer nodes or layers but

88
00:02:55,080 --> 00:02:56,909
also by keeping the numbers in a

89
00:02:56,909 --> 00:02:59,340
reasonable range set that makes sense

90
00:02:59,340 --> 00:03:05,150
that makes perfect sense

