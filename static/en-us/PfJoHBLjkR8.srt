1
00:00:00,100 --> 00:00:00,590
HI Michael.

2
00:00:00,590 --> 00:00:01,580
>> Hey how's it going?

3
00:00:01,580 --> 00:00:04,270
>> So I want to talk about something today Michael.

4
00:00:04,270 --> 00:00:07,930
I want to talk about Bayesian Learning, and I've been inspired

5
00:00:07,930 --> 00:00:11,160
by our last discussion on learning theory to think a

6
00:00:11,160 --> 00:00:13,740
little bit more about what it is exactly that we're trying

7
00:00:13,740 --> 00:00:16,460
to do. I'm in the mood beyond specific algorithms to

8
00:00:16,460 --> 00:00:19,310
just think more generally The sort that learning people want us

9
00:00:19,310 --> 00:00:21,380
to do, learning theory people want us to do and I

10
00:00:21,380 --> 00:00:23,790
think Bayesian Learning is a nice place to start. Sound fair?

11
00:00:23,790 --> 00:00:25,350
>> Yeah, that sounds really cool, I think

12
00:00:25,350 --> 00:00:29,230
that might be a nice formal framework for thinking about some of these problems.

13
00:00:29,230 --> 00:00:31,930
>> Good. Good. So, I'm going to start out. By

14
00:00:31,930 --> 00:00:34,170
making a few assertions, which I hope you will

15
00:00:34,170 --> 00:00:37,260
agree with, and if you agree with this then

16
00:00:37,260 --> 00:00:39,000
we'll be able to kind of move forward and ask

17
00:00:39,000 --> 00:00:42,420
some pretty cool questions okay? So Bayesian learning, so

18
00:00:42,420 --> 00:00:44,950
the kind of idea here behind Bayesian learning is

19
00:00:44,950 --> 00:00:48,590
this sort of fundamental Underlying assumption about what we're

20
00:00:48,590 --> 00:00:50,420
trying to do with the machine learning. So, I've written

21
00:00:50,420 --> 00:00:52,310
it down here, here's what I'm going to claim

22
00:00:52,310 --> 00:00:54,090
we're trying to do. We are trying to learn

23
00:00:54,090 --> 00:00:59,310
the best hypothesis we can given some data and

24
00:00:59,310 --> 00:01:01,790
some domain knowledge. Do you buy that as an assertion?

25
00:01:01,790 --> 00:01:04,819
>> Yeah, it's, and pretty much everything we've talked about so far

26
00:01:04,819 --> 00:01:07,880
has had a form kind of like that. We're searching through a hypothesis

27
00:01:07,880 --> 00:01:12,330
base and As you've pointed out on multiple occasions there's this kind

28
00:01:12,330 --> 00:01:15,670
of extra domain knowledge that comes into play for example when you pick

29
00:01:15,670 --> 00:01:19,143
a like a similarity metric for something like k nearest neighbors

30
00:01:19,143 --> 00:01:20,830
>> Right and of course we always have the

31
00:01:20,830 --> 00:01:23,300
data because we're machine learning people and we always have

32
00:01:23,300 --> 00:01:26,580
data. So this is what we've been trying to do

33
00:01:26,580 --> 00:01:29,240
and I'm going to suggest that we can be a

34
00:01:29,240 --> 00:01:31,830
little bit more precise about what we mean by

35
00:01:31,830 --> 00:01:33,930
best and I'm going to try to do that and

36
00:01:33,930 --> 00:01:37,510
see if you agree with me. Okay, so I'm going

37
00:01:37,510 --> 00:01:40,680
to rewrite what I've written already except I'm replacing best

38
00:01:40,680 --> 00:01:44,760
with most probable. Okay. So what I'm going to claim is that what

39
00:01:44,760 --> 00:01:47,340
we've really been trying to do with all these algorithms we're doing is

40
00:01:47,340 --> 00:01:51,960
we're trying to learn the most likely or the most probable hypothesis

41
00:01:51,960 --> 00:01:55,440
given the data and whatever domain knowledge we bring to bear. You buy that?

42
00:01:55,440 --> 00:02:00,050
>> Interesting. I'm not sure yet. I mean, so is it

43
00:02:00,050 --> 00:02:03,780
the hypothesis that it's most likely to be returned by the algorithm?

44
00:02:03,780 --> 00:02:05,830
>> No, it's the hypothesis

45
00:02:05,830 --> 00:02:09,210
that we think is most likely, given the data that we've seen.

46
00:02:09,210 --> 00:02:12,570
Given the training set and given whatever domain knowledge that we bring to

47
00:02:12,570 --> 00:02:15,120
bear on the problem, the best hypothesis is the one that is

48
00:02:15,120 --> 00:02:20,310
most likely, that is Most probable. Or most l, probably the correct one.

49
00:02:20,310 --> 00:02:23,640
>> Interesting. Well, are we going to be able to connect that to what we

50
00:02:23,640 --> 00:02:25,410
were talking before? Which is generally we were

51
00:02:25,410 --> 00:02:28,490
selecting hypotheses based on things like their error.

52
00:02:28,490 --> 00:02:31,060
>> Yes. Exactly. We are going to be able to connect that.

53
00:02:31,060 --> 00:02:32,900
We are definitly going to be able to connect that. But.

54
00:02:32,900 --> 00:02:33,300
>> Okay.

55
00:02:33,300 --> 00:02:35,150
>> I can;t go forward unless I can convince

56
00:02:35,150 --> 00:02:38,080
you that it's reasonable to at least start out thinking

57
00:02:38,080 --> 00:02:41,440
about best being the same thing as most probable. Yeah,

58
00:02:41,440 --> 00:02:43,300
I'm willing to go forward with this. It sounds interesting.

59
00:02:43,300 --> 00:02:45,900
>> So if you're willing to move forward with this, then I want to write

60
00:02:45,900 --> 00:02:49,040
one more thing down and then we can sort of dive into it. So if

61
00:02:49,040 --> 00:02:52,400
you buy that we're trying to learn the most probable hypothesis, the most likely

62
00:02:52,400 --> 00:02:56,840
one, the one that has the highest chance of being correct given the data, and

63
00:02:56,840 --> 00:02:58,890
our domain knowledge, then we can write that

64
00:02:58,890 --> 00:03:00,950
down in math speak pretty simply. It's the

65
00:03:00,950 --> 00:03:04,710
probability of, some particular hypothesis h, drawn from

66
00:03:04,710 --> 00:03:07,410
some hypothesis class. Given some amount of data

67
00:03:07,410 --> 00:03:08,840
which I'm just going to refer to as D

68
00:03:08,840 --> 00:03:11,440
from now on. Okay? And that's just, exactly

69
00:03:11,440 --> 00:03:14,100
what we said just above when we talk

70
00:03:14,100 --> 00:03:17,120
about the most probable age, given the data. Okay?

71
00:03:17,120 --> 00:03:19,270
>> Well wait. Two things. One is so D

72
00:03:19,270 --> 00:03:21,420
is not distribution which we've had in the past.

73
00:03:21,420 --> 00:03:21,910
>> That's true.
>> So I

74
00:03:21,910 --> 00:03:25,130
guess as long as we keep that straight. And the other one is

75
00:03:25,130 --> 00:03:26,730
No that's, you're just telling me

76
00:03:26,730 --> 00:03:29,280
the probability of some particular hypothesis h.

77
00:03:29,280 --> 00:03:31,580
>> That's right. So, we want to somehow,

78
00:03:31,580 --> 00:03:34,620
given this quantity we want to find, the

79
00:03:34,620 --> 00:03:40,010
best or the, most likely, of the hypothesis given this. Does that make sense?

80
00:03:40,010 --> 00:03:40,360
>> Yes.

81
00:03:40,360 --> 00:03:47,220
>> So we want to find the argmax, of h, drawn from Your hypothesis class.

82
00:03:47,220 --> 00:03:49,260
That is we want to find the hypothesis drawn from

83
00:03:49,260 --> 00:03:52,800
the hypothesis class that has the highest probability given the data.

84
00:03:52,800 --> 00:03:53,400
>> Perfect.

85
00:03:53,400 --> 00:03:57,250
>> Okay, good. So we're going to spend the next 45 hours.

86
00:03:57,250 --> 00:03:59,039
>> [LAUGH]

87
00:03:59,039 --> 00:04:00,310
>> Exploring this expression.

88
00:04:00,310 --> 00:04:03,480
>> Okay so that's like what, like 6 hours per letter.

89
00:04:03,480 --> 00:04:06,000
>> [LAUGH] Yeah and that's fine because its important.
