1
00:00:00,180 --> 00:00:03,719
Modern parallel machines offer a lot of challenges

2
00:00:03,719 --> 00:00:07,020
in converting the algorithms and the techniques that

3
00:00:07,020 --> 00:00:11,355
we have learned so far into scalable implementations.

4
00:00:11,355 --> 00:00:13,850
Now what are some of these challenges? Well

5
00:00:13,850 --> 00:00:16,790
first of all, there's a size bloat of

6
00:00:16,790 --> 00:00:19,810
the operating system. And the size bloat comes

7
00:00:19,810 --> 00:00:22,930
because of additional features that we have to

8
00:00:22,930 --> 00:00:25,180
add to the operating system and so on.

9
00:00:25,180 --> 00:00:29,760
And that results in, in system software bottlenecks, especially

10
00:00:29,760 --> 00:00:32,930
for global data structures. And then, of course, we

11
00:00:32,930 --> 00:00:36,560
already have been discussing this quite a bit, that

12
00:00:36,560 --> 00:00:39,890
the memory latency to go from the processor to

13
00:00:39,890 --> 00:00:45,480
memory is, is huge. All the cores of the processor are on a chip, and if you go

14
00:00:45,480 --> 00:00:47,980
outside the chip to the memory, that latency is

15
00:00:47,980 --> 00:00:50,200
huge. 100 to one ratio is what we've been talking

16
00:00:50,200 --> 00:00:53,080
about and that latency is only growing. The

17
00:00:53,080 --> 00:00:55,370
other thing that happens in parallel machines is the

18
00:00:55,370 --> 00:00:58,200
fact that, this is a single node. And

19
00:00:58,200 --> 00:01:00,210
we're talking about the memory latency going from the

20
00:01:00,210 --> 00:01:05,519
processor to the memory. But in a parallel machine, it's typically constructed

21
00:01:05,519 --> 00:01:10,550
as a non-uniform memory access machine. And that is, you take

22
00:01:10,550 --> 00:01:15,280
individual nodes like this that contains a processor and memory and

23
00:01:15,280 --> 00:01:17,330
put all them together and connect them through an

24
00:01:17,330 --> 00:01:21,150
interconnection network. And what happens with this NUMA architecture

25
00:01:21,150 --> 00:01:25,520
is that access, there's differential access to memory whether

26
00:01:25,520 --> 00:01:29,840
this processor is accessing memory that is local to it,

27
00:01:29,840 --> 00:01:32,460
or it has to reach out into the network

28
00:01:32,460 --> 00:01:34,540
and access some memory that is farther away from

29
00:01:34,540 --> 00:01:37,150
where it is. In addition to the NUMA effect,

30
00:01:37,150 --> 00:01:40,570
there is also the memory hierarchy itself is very deep.

31
00:01:40,570 --> 00:01:43,570
We already talked about the fact a single

32
00:01:43,570 --> 00:01:48,180
processor these days contains multiple levels of caches before

33
00:01:48,180 --> 00:01:50,780
it goes to the memory. And this deep memory

34
00:01:50,780 --> 00:01:52,930
hierarchy is another thing that, that you have to

35
00:01:52,930 --> 00:01:55,350
worry about in building the operating system for a

36
00:01:55,350 --> 00:01:58,000
parallel machine. And there is the issue of false

37
00:01:58,000 --> 00:02:02,000
sharing. And false sharing is essentially saying that even

38
00:02:02,000 --> 00:02:06,080
though programmatically there is no connection between a piece

39
00:02:06,080 --> 00:02:09,330
of memory that is being touched by a

40
00:02:09,330 --> 00:02:12,530
particular thread executing on this core, another thread

41
00:02:12,530 --> 00:02:14,890
that is executing on, on this core. The

42
00:02:14,890 --> 00:02:19,300
cache hierarchy may make the block that contains the

43
00:02:19,300 --> 00:02:21,980
individual memory touched by different threads on different

44
00:02:21,980 --> 00:02:24,870
cores to be on the same cache block. So,

45
00:02:24,870 --> 00:02:28,050
programmatically, there's no sharing, but because of the

46
00:02:28,050 --> 00:02:31,150
fact that the memory that is being touched by

47
00:02:31,150 --> 00:02:33,210
a thread on this core, and a memory that

48
00:02:33,210 --> 00:02:35,430
is being touched by a thread on this core happen

49
00:02:35,430 --> 00:02:37,680
to be on the same cache line, they appear

50
00:02:37,680 --> 00:02:40,590
to be shared. That's what is false sharing. False sharing

51
00:02:40,590 --> 00:02:43,560
is essentially saying that there is no programmatic sharing

52
00:02:43,560 --> 00:02:47,930
But, because of the way the cache coherence mechanism operates,

53
00:02:47,930 --> 00:02:50,718
they appear shared. And this is happening more and more

54
00:02:50,718 --> 00:02:56,360
in modern processors, because modern processors tend to employ larger

55
00:02:56,360 --> 00:03:00,940
cache blocks. Why is that? Well, the analogy I'm

56
00:03:00,940 --> 00:03:03,050
gong to give you is that of a handyman. If

57
00:03:03,050 --> 00:03:05,940
you're good at doing chores around the house, then

58
00:03:05,940 --> 00:03:10,320
you might relate to this analogy quite well. You probably

59
00:03:10,320 --> 00:03:13,750
have a tool box if you're a handyman. And

60
00:03:13,750 --> 00:03:16,520
if you want to do some work, let's say a

61
00:03:16,520 --> 00:03:18,560
leaky faucet that you want to fix, what you

62
00:03:18,560 --> 00:03:21,600
do is you put the tools that you need into

63
00:03:21,600 --> 00:03:25,710
a, a tool tray and bring it from the tool box

64
00:03:25,710 --> 00:03:28,720
to the site where you're doing, doing the work. And basically, what

65
00:03:28,720 --> 00:03:31,790
you're doing there is, you know, collecting the set of tools

66
00:03:31,790 --> 00:03:34,020
that you need for the project so that you don't have to

67
00:03:34,020 --> 00:03:37,230
go running back to the tool tray all the time. That's

68
00:03:37,230 --> 00:03:40,888
the same sort of thing that's happening with caching and memory. Memory

69
00:03:40,888 --> 00:03:43,266
contains all this stuff but what I need, I want to

70
00:03:43,266 --> 00:03:46,620
bring it in. And the more I bring in from the memory,

71
00:03:46,620 --> 00:03:49,360
the less time that I have to go out to

72
00:03:49,360 --> 00:03:51,680
memory in order to fetch it. That means that I want

73
00:03:51,680 --> 00:03:54,480
to keep increasing the block size of the cache, in

74
00:03:54,480 --> 00:03:57,550
order to make sure that I take advantage of spatial locality

75
00:03:57,550 --> 00:04:00,830
in the cache design. And that increases the chances that

76
00:04:00,830 --> 00:04:04,320
false sharing is going to happen. The larger the cache line,

77
00:04:04,320 --> 00:04:08,090
the more chances are that memory that is being touched

78
00:04:08,090 --> 00:04:11,730
by different threads happen to be on the same cache block,

79
00:04:11,730 --> 00:04:14,460
and that results in false shading. So all of

80
00:04:14,460 --> 00:04:17,529
these effects, the NUMA effect, the deep memory hierarchy,

81
00:04:17,529 --> 00:04:21,290
and increasing block size leading to false sharing, all

82
00:04:21,290 --> 00:04:24,110
of these are things that the operating system designer

83
00:04:24,110 --> 00:04:27,430
has to worry about in making sure that the

84
00:04:27,430 --> 00:04:30,580
algorithms and the techniques that we have learned, when

85
00:04:30,580 --> 00:04:33,310
it is translated to a large scale parallel machine,

86
00:04:33,310 --> 00:04:36,960
it remains scalable. So that's really the challenge that

87
00:04:36,960 --> 00:04:39,730
the operating system designer faces. So some of

88
00:04:39,730 --> 00:04:42,260
the things that the OS designer would have to

89
00:04:42,260 --> 00:04:46,570
do is work hard to avoid false sharing, work

90
00:04:46,570 --> 00:04:51,620
hard to reduce write sharing the same cache line.

91
00:04:51,620 --> 00:04:53,220
Because if you write share the same cache

92
00:04:53,220 --> 00:04:55,680
line, then it is going to result among different

93
00:04:55,680 --> 00:04:57,950
cores of the same processor, then it's going to

94
00:04:57,950 --> 00:05:02,310
result in the cache line migrating from one processor

95
00:05:02,310 --> 00:05:05,510
to another. And even within the same core

96
00:05:05,510 --> 00:05:08,970
and even within the same processor, multiple cores,

97
00:05:08,970 --> 00:05:12,410
and across processors that are on different nodes

98
00:05:12,410 --> 00:05:14,970
of parallel machine, connected by the interconnection network.
