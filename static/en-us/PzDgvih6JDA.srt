1
00:00:00,000 --> 00:00:04,000
So the question, then, is: How do we get this learner out of its rut?

2
00:00:04,000 --> 00:00:07,000
It improved its policy for awhile,

3
00:00:07,000 --> 00:00:09,000
but then it got stuck in this policy

4
00:00:09,000 --> 00:00:13,000
where we go here, go up and then go right.

5
00:00:13,000 --> 00:00:16,000
Most of the time, that's a perfectly good policy.

6
00:00:16,000 --> 00:00:21,000
But if a stochastic error makes us slip into the minus 1, then it hurts us.

7
00:00:21,000 --> 00:00:25,000
We'd like to be able to say we're going to stop doing that

8
00:00:25,000 --> 00:00:28,000
and somehow find this route.

9
00:00:28,000 --> 00:00:30,000
But in order to find that new route,

10
00:00:30,000 --> 00:00:32,000
we'd have to spend some time executing a policy

11
00:00:32,000 --> 00:00:35,000
which was not the best policy known to us.

12
00:00:35,000 --> 00:00:38,000
In other words, we'd have to stop exploiting

13
00:00:38,000 --> 00:00:42,000
the best policy we'd found so far--which is this one--

14
00:00:42,000 --> 00:00:46,000
and start exploring, to see if maybe there's a better policy.

15
00:00:46,000 --> 00:00:48,000
And exploring could lead us astray

16
00:00:48,000 --> 00:00:51,000
and cause us to waste a lot of time.

17
00:00:51,000 --> 00:00:53,000
So we have to figure out: what's the right trade-off?

18
00:00:53,000 --> 00:00:57,000
When is it worth exploring to try to find something better for the long term--

19
00:00:57,000 --> 00:01:02,000
even though we know that exploring is going to hurt us in the short term?

20
00:01:02,000 --> 00:01:06,000
Now, one possibility is, certainly, random exploration.

21
00:01:06,000 --> 00:01:09,000
That is, we can follow our best policy

22
00:01:09,000 --> 00:01:11,000
some percentage of the time,

23
00:01:11,000 --> 00:01:14,000
and then randomly, at some point,

24
00:01:14,000 --> 00:01:17,000
we can decide to take an action which is not the optimal action.

25
00:01:17,000 --> 00:01:20,000
So we're here, the optimal action would be to go east;

26
00:01:20,000 --> 00:01:23,000
and we say, "Well, this time we're gong to choose something else--

27
00:01:23,000 --> 00:01:25,000
let's try going north.

28
00:01:25,000 --> 00:01:27,000
And then we explore from there

29
00:01:27,000 --> 00:01:29,000
and see if we've learned something.

30
00:01:29,000 --> 00:01:31,000
So that policy does, in fact, work--

31
00:01:31,000 --> 00:01:37,000
randomly making moves with some probability--but it tends to be slow to converge.

32
00:01:37,000 --> 00:01:39,000
In order to get something better, we have to really understand

33
00:01:39,000 --> 99:59:59,999
what's going on with our exploration, versus exploitation.
