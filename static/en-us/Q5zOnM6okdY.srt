1
00:00:00,290 --> 00:00:05,047
So going back to our mani-core challenges, we have seen that when the number of

2
00:00:05,047 --> 00:00:07,816
cores grows, so does the coherence traffic on

3
00:00:07,816 --> 00:00:10,899
the chip. And for that, we needed a

4
00:00:10,899 --> 00:00:13,980
scalable on-chip network such as a mesh, and

5
00:00:13,980 --> 00:00:17,535
the directory coherence, which we have already seen

6
00:00:17,535 --> 00:00:21,826
in our cache coherence lesson. Another thing that

7
00:00:21,826 --> 00:00:25,474
having more cores will make more difficult is

8
00:00:25,474 --> 00:00:28,162
that as the number of cores goes up

9
00:00:28,162 --> 00:00:32,415
the off-chip traffic that we need goes up. To

10
00:00:32,415 --> 00:00:35,409
have decent performance we will, as we increase

11
00:00:35,409 --> 00:00:38,694
the number of cores, increase the number of on-chip

12
00:00:38,694 --> 00:00:41,058
caches. So if we have four cores, we

13
00:00:41,058 --> 00:00:44,367
have four level one and possibly level two caches.

14
00:00:44,367 --> 00:00:46,817
When we have 64 cores, we'll have 64

15
00:00:46,817 --> 00:00:51,760
level one and level two caches. So each core

16
00:00:51,760 --> 00:00:55,400
doesn't really generate more misses, but it generates the

17
00:00:55,400 --> 00:00:58,330
same number of misses regardless of how many cores we

18
00:00:58,330 --> 00:01:01,050
have. So we have the same number of misses

19
00:01:01,050 --> 00:01:04,209
per core. So as we have more cores, the number

20
00:01:04,209 --> 00:01:06,870
of memory requests which is what we do when

21
00:01:06,870 --> 00:01:09,606
we have a miss, would go up in proportion to

22
00:01:09,606 --> 00:01:13,270
the number of cores. Now note that the number

23
00:01:13,270 --> 00:01:16,904
of pins, the number of those little wires sticking out

24
00:01:16,904 --> 00:01:19,195
of a chip, does go up slowly but not

25
00:01:19,195 --> 00:01:22,750
in proportion, or anywhere close to in proportion to

26
00:01:22,750 --> 00:01:25,685
the number of cores. So if we double the

27
00:01:25,685 --> 00:01:28,760
number of cores possibly we get maybe 10% more

28
00:01:28,760 --> 00:01:31,760
pins but that's it, because these pins have to

29
00:01:31,760 --> 00:01:34,685
physically be large enough to not break when we

30
00:01:34,685 --> 00:01:37,610
are moving the chip around and plugging it into

31
00:01:37,610 --> 00:01:42,370
a motherboard. So what happens is we get some little

32
00:01:42,370 --> 00:01:46,500
improvement in off-chip throughput while our demand for

33
00:01:46,500 --> 00:01:48,850
it basically grows in proportion to the number

34
00:01:48,850 --> 00:01:52,910
of cores. So our off-chip available throughput becomes

35
00:01:52,910 --> 00:01:55,752
a bottleneck. So we need to reduce the

36
00:01:55,752 --> 00:02:01,208
number of memory requests that we generate per core and the way we do that is

37
00:02:01,208 --> 00:02:04,024
to make the last level cache, which in

38
00:02:04,024 --> 00:02:07,984
most two day's processors is the level three cache,

39
00:02:07,984 --> 00:02:11,152
shared so that all cores go to that cache and

40
00:02:11,152 --> 00:02:15,270
its size is shared equally by all the cores. And

41
00:02:15,270 --> 00:02:17,580
the size of that cache needs to go up in

42
00:02:17,580 --> 00:02:20,570
proportion to the number of cores as it goes up.

43
00:02:20,570 --> 00:02:22,990
So as we double the number of cores we should

44
00:02:22,990 --> 00:02:26,190
at least double the size of the last level cache.

45
00:02:26,190 --> 00:02:30,000
But there is a problem to having one huge LLC

46
00:02:30,000 --> 00:02:33,516
like this. Such a large cache would be slow, and

47
00:02:33,516 --> 00:02:36,480
if it's really one single cache, it will have

48
00:02:36,480 --> 00:02:39,368
one entry point where we enter the address and

49
00:02:39,368 --> 00:02:42,732
expect the data out. And that entry point needs

50
00:02:42,732 --> 00:02:45,172
to be somewhere on our chip that now possibly has

51
00:02:45,172 --> 00:02:48,796
a mesh or another advanced network. And that means

52
00:02:48,796 --> 00:02:52,144
that that one point will be where we send all

53
00:02:52,144 --> 00:02:55,492
of the level two misses, so it would become

54
00:02:55,492 --> 00:02:58,670
a bottleneck. Pretty much now we don't get to use

55
00:02:58,670 --> 00:03:01,270
all of the links equally. This set of links

56
00:03:01,270 --> 00:03:03,610
that go to this entry point for the cache will

57
00:03:03,610 --> 00:03:06,900
get much more traffic than the other links. If

58
00:03:06,900 --> 00:03:09,830
we double the number of cores, then those links around

59
00:03:09,830 --> 00:03:13,110
the entry point get double the traffic. So what

60
00:03:13,110 --> 00:03:16,500
we do is we don't have just one big last

61
00:03:16,500 --> 00:03:19,490
level cache, we have what is called a distributed

62
00:03:19,490 --> 00:03:22,580
last level cache. Now let's see what that looks like.
