1
00:00:00,350 --> 00:00:03,050
Okay, so will you work this out? You will need

2
00:00:03,050 --> 00:00:05,980
to know the frequency of A, B, C, and D. But

3
00:00:05,980 --> 00:00:09,070
we already know that. You will also need to know

4
00:00:09,070 --> 00:00:12,190
how many bits each of those symbols require. Now we also

5
00:00:12,190 --> 00:00:15,840
know that. So, we will calculate the expected number of

6
00:00:15,840 --> 00:00:18,800
bits to transmit each symbol and then add them up. So

7
00:00:18,800 --> 00:00:21,820
for any symbol, the expected number of bits is given

8
00:00:21,820 --> 00:00:26,040
by the probability of seeing that symbol, and the size required

9
00:00:26,040 --> 00:00:31,940
to transmit that symbol. And we add them up for all the symbols in the language.

10
00:00:31,940 --> 00:00:39,300
This is going to give us 1.75 bits on an average. Now, since we had to ask

11
00:00:39,300 --> 00:00:42,170
less questions in this language, than the previous

12
00:00:42,170 --> 00:00:45,854
language, this language has less information. This is

13
00:00:45,854 --> 00:00:48,430
also called as variable length encoding. This should

14
00:00:48,430 --> 00:00:51,140
give you some idea into figuring out why some

15
00:00:51,140 --> 00:00:54,610
symbols in Morse code are smaller than others. In

16
00:00:54,610 --> 00:00:57,360
the English alphabet, the letters e and t occur

17
00:00:57,360 --> 00:01:00,340
most frequently. That's why, in the Morse code, e

18
00:01:00,340 --> 00:01:02,890
is generated by a dot and t is generated by

19
00:01:02,890 --> 00:01:06,220
a dash. Since e and t occur more frequently,

20
00:01:06,220 --> 00:01:09,658
they have the smallest message size. This measure, which calculates

21
00:01:09,658 --> 00:01:11,980
the number of bits per symbol, is also called

22
00:01:11,980 --> 00:01:16,720
as entropy. And it is mathematically given as this formula.

23
00:01:16,720 --> 00:01:19,740
To make it more legible, we need to find out how

24
00:01:19,740 --> 00:01:23,740
to denote the size of s more properly. The size of s

25
00:01:23,740 --> 00:01:27,250
is also given by the log of 1 upon the probability

26
00:01:27,250 --> 00:01:30,770
of that symbol. So the formula of entropy is given as this.
