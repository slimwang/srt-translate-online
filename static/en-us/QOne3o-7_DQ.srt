1
00:00:00,780 --> 00:00:03,330
So why don't we walk through our solution here. First we

2
00:00:03,330 --> 00:00:06,120
initialize this variable m, which we say is equal to length

3
00:00:06,120 --> 00:00:08,340
of values. So we're basically saying here hey, this is how

4
00:00:08,340 --> 00:00:11,940
many data points we have. We also initialize this variable called

5
00:00:11,940 --> 00:00:15,170
cost history, which is going to track how our cost function

6
00:00:15,170 --> 00:00:18,990
evolves over our iterations. We say for i in range num

7
00:00:18,990 --> 00:00:22,020
iterations, which basically says hey we're going to update theta in

8
00:00:22,020 --> 00:00:26,040
the costs. And number of times equal to the number of iterations.

9
00:00:26,040 --> 00:00:28,740
Let's calculate the predicted values. We'll do that by taking

10
00:00:28,740 --> 00:00:32,430
the dot product of the features and theta. Then we're

11
00:00:32,430 --> 00:00:35,360
going to update our values of theta. This looks pretty

12
00:00:35,360 --> 00:00:37,580
complicated, but you'll note that it's the same as the

13
00:00:37,580 --> 00:00:40,050
equation that we just discussed, which we'll pull up again

14
00:00:40,050 --> 00:00:44,700
here. So, we subtract our observed values from our predicted

15
00:00:44,700 --> 00:00:47,430
values, take the dot product of this vector with the

16
00:00:47,430 --> 00:00:51,140
features, and then essentially multiply by this alpha over m

17
00:00:51,140 --> 00:00:54,460
feature and subtract that from theta. This is effectively gradient

18
00:00:54,460 --> 00:00:57,240
descent in action, so we're taking a very small step

19
00:00:57,240 --> 00:01:00,490
in the direction of the steepest gradient. Then we're going

20
00:01:00,490 --> 00:01:03,930
to use another function called compute_cost which we see up

21
00:01:03,930 --> 00:01:07,310
here. So you compute the cost function given our features,

22
00:01:07,310 --> 00:01:10,900
our values and our theta. Then we'll append the newest

23
00:01:10,900 --> 00:01:14,450
cost to the cost history list. And once we've done

24
00:01:14,450 --> 00:01:16,760
this a number of times equal to the number of iterations,

25
00:01:16,760 --> 00:01:19,330
we'll not only return our final set of thetas,

26
00:01:19,330 --> 00:01:22,570
but also the entire cost history. You'll see that if

27
00:01:22,570 --> 00:01:25,320
we run this function, we get this array, which

28
00:01:25,320 --> 00:01:29,460
are theta values. About 45 minus 9 and 13, and

29
00:01:29,460 --> 00:01:32,320
these values kind of indicate how significant each factor

30
00:01:32,320 --> 00:01:35,870
is in determining our predicted value. So this 45 corresponds

31
00:01:35,870 --> 00:01:38,680
to a constant term, the minus 9 corresponds to

32
00:01:38,680 --> 00:01:41,800
height and the 13 and change corresponds to the weight.

33
00:01:41,800 --> 00:01:44,450
And then we see here how the cost function has evolved

34
00:01:44,450 --> 00:01:48,740
over time. Ideally, if we had a perfect model, this cost function

35
00:01:48,740 --> 00:01:51,640
would approach zero. You'll see that our cost function doesn't, you

36
00:01:51,640 --> 00:01:54,640
know, get close to zero. It, it does decrease a good amount

37
00:01:54,640 --> 00:01:57,480
as we train this model, but it doesn't do super well.

38
00:01:57,480 --> 00:02:01,100
And so this might lead you logically to a natural question. Which

39
00:02:01,100 --> 00:02:03,840
is, hey you know not all models are created equal. Maybe

40
00:02:03,840 --> 00:02:07,230
our model is actually bad. Is there some way we can evaluate

41
00:02:07,230 --> 00:02:09,660
our models? Why don't we discuss this a little bit more?
