1
00:00:00,360 --> 00:00:03,040
Okay Michael, so, let me see if I can convince

2
00:00:03,040 --> 00:00:04,850
you that we actually have some way of actually estimating

3
00:00:04,850 --> 00:00:07,810
these distributions. So, all I've written up here is the

4
00:00:07,810 --> 00:00:11,680
chain rule version of a joint probability distribution, okay? But just

5
00:00:11,680 --> 00:00:15,640
to be clear, what these subscripts mean here, every x

6
00:00:15,640 --> 00:00:17,849
that we have is made up of a set of features.

7
00:00:18,980 --> 00:00:21,120
So, there's, let's just say there is N of these

8
00:00:21,120 --> 00:00:25,650
features. And so really, X is a vector. There's feature one.

9
00:00:25,650 --> 00:00:29,210
There's feature two. There's feature three, dot, dot,

10
00:00:29,210 --> 00:00:32,530
dot. All the way up to feature N. Okay?

11
00:00:32,530 --> 00:00:33,900
>> Sure.

12
00:00:33,900 --> 00:00:37,220
>> What I really want to know for my piece of theta and I'm

13
00:00:37,220 --> 00:00:39,680
going to drop the theta's here for the

14
00:00:39,680 --> 00:00:42,070
purpose of just describing this generic distribution.

15
00:00:42,070 --> 00:00:44,520
Is I want to say, well the probability of me seeing all of the

16
00:00:44,520 --> 00:00:50,670
features yeah, all of the features of some particular example is just the joint

17
00:00:50,670 --> 00:00:53,570
distribution over all of those features. Now of course,

18
00:00:53,570 --> 00:00:55,610
we could just estimate this, but that's going to be hard.

19
00:00:55,610 --> 00:00:57,460
>> Why's it going to be hard? Because.

20
00:00:57,460 --> 00:01:01,290
>> Well, the first distribution is conditioned on a lot

21
00:01:01,290 --> 00:01:05,300
of things, so it's an exponential sized conditional probability table.

22
00:01:05,300 --> 00:01:08,090
>> Exactly, so this is exponential table and if

23
00:01:08,090 --> 00:01:10,970
it's an exponential table then we need to have

24
00:01:10,970 --> 00:01:13,030
an enormous amount of data in order to estimate

25
00:01:13,030 --> 00:01:15,510
it well and this is sort of the fundamental problem.

26
00:01:15,510 --> 00:01:19,440
But, we have addressed this in earlier lectures, earlier lessons Michael.

27
00:01:19,440 --> 00:01:21,940
>> In the inference lecture maybe?

28
00:01:21,940 --> 00:01:23,200
>> Yes, in the inference lecture.

29
00:01:23,200 --> 00:01:24,000
>> Whew.

30
00:01:24,000 --> 00:01:27,010
>> Where we can try to estimate this incredibly painful joint

31
00:01:27,010 --> 00:01:29,550
distribution by making some assumptions about

32
00:01:29,550 --> 00:01:32,100
conditional independence, and in particular I'm

33
00:01:32,100 --> 00:01:36,360
going to make, one kind of assumption. And that is, that we

34
00:01:36,360 --> 00:01:40,480
only care about what's called Dependency Trees. Okay, so what's a dependency

35
00:01:40,480 --> 00:01:41,450
tree Michael? Do you remember?

36
00:01:41,450 --> 00:01:42,670
>> No I have absolutely no idea, I

37
00:01:42,670 --> 00:01:44,330
don't think I've ever heard of such a thing.

38
00:01:44,330 --> 00:01:45,710
>> So the Dependency Tree is a special

39
00:01:45,710 --> 00:01:48,150
case of a Bayesian network, where the network

40
00:01:48,150 --> 00:01:52,520
itself is a tree. That is, every node,

41
00:01:52,520 --> 00:01:55,500
every variable in the tree, has exactly one parent.

42
00:01:55,500 --> 00:01:57,780
>> I see. So sometimes in Bayesian networks, they

43
00:01:57,780 --> 00:02:00,450
talk about polytrees. Polytrees just mean that if you

44
00:02:00,450 --> 00:02:02,040
ignore the direction of the edges, you have a

45
00:02:02,040 --> 00:02:05,550
tree. But you're actually talking about the, the edges have

46
00:02:05,550 --> 00:02:08,440
to go in a particular direction. Everybody's got one parent.

47
00:02:08,440 --> 00:02:10,570
>> Right, exactly right. And in, and in, so,

48
00:02:10,570 --> 00:02:14,760
you should see these as a directed graph, although I

49
00:02:14,760 --> 00:02:17,780
almost never draw them that way. Because, you know, it's

50
00:02:17,780 --> 00:02:19,290
sort of obvious by looking at it. That this is

51
00:02:19,290 --> 00:02:21,680
the root of the tree. So all we have here

52
00:02:21,680 --> 00:02:23,840
now is a tree. Every node has one parent, and

53
00:02:23,840 --> 00:02:26,460
what does that mean? That means that every node, every

54
00:02:26,460 --> 00:02:30,120
random variable, depends on exactly one other random variable. Yeah.

55
00:02:30,120 --> 00:02:30,860
>> So

56
00:02:30,860 --> 00:02:33,360
we can rewrite this joint distribution here now as a

57
00:02:33,360 --> 00:02:39,800
product over each of the features depending upon only its parent.

58
00:02:39,800 --> 00:02:42,360
>> I see. So the first capital pi there means

59
00:02:42,360 --> 00:02:45,880
product. And the second lower case pi there means parent.

60
00:02:45,880 --> 00:02:48,190
>> Exactly. So when I compare

61
00:02:48,190 --> 00:02:50,760
this representation of a distribution, dependency to

62
00:02:50,760 --> 00:02:56,040
representation versus the full joint. What nice properties do I get from doing

63
00:02:56,040 --> 00:03:00,690
it this way versus doing it that way? Well the, I guess the main positive is

64
00:03:00,690 --> 00:03:02,890
that you're only ever conditioned on, well it's

65
00:03:02,890 --> 00:03:05,020
gotta be at most one, right? Not exactly one?

66
00:03:05,020 --> 00:03:08,110
>> Right. At most one, so, so in fact you, you picked up on

67
00:03:08,110 --> 00:03:11,860
a nice rotational thing here. The parent of the tree, the root of the tree

68
00:03:11,860 --> 00:03:13,346
doesn't actually have a parent. So just

69
00:03:13,346 --> 00:03:15,810
assume that in this case Pi returns itself

70
00:03:15,810 --> 00:03:17,130
or something and so it's the unconditional

71
00:03:17,130 --> 00:03:18,810
distribution. And that could happen at any time.

72
00:03:18,810 --> 00:03:21,140
>> Got, gotcha, alright, yeah and so, like if

73
00:03:21,140 --> 00:03:23,900
nobody has any parents, then that's the same as Naive

74
00:03:23,900 --> 00:03:27,040
Bayes, but here it can happen with those one parents.

75
00:03:27,040 --> 00:03:27,100
>> No.

76
00:03:27,100 --> 00:03:27,460
>> No?

77
00:03:27,460 --> 00:03:29,180
>> Naive Bayes has one, exactly one, everyone

78
00:03:29,180 --> 00:03:30,800
has one parent and it's exactly the same one.

79
00:03:30,800 --> 00:03:33,670
>> Oh, right, good point.

80
00:03:33,670 --> 00:03:36,330
>> So, if everyone has no parents, then you're saying that

81
00:03:36,330 --> 00:03:39,800
all of the features are in fact, independent of one another.

82
00:03:39,800 --> 00:03:42,120
>> Gotcha. Okay. Alright. But the, but that's not what you

83
00:03:42,120 --> 00:03:46,090
asked. You asked comparing that probability distribution to the full joint,

84
00:03:46,090 --> 00:03:48,960
the main thing is that you're, no ones ever conditioned on more than

85
00:03:48,960 --> 00:03:51,080
one, other feature and therefore the

86
00:03:51,080 --> 00:03:53,520
conditional probability tables stay very, very small.

87
00:03:53,520 --> 00:03:56,540
>> Right. In fact do you, how small do they stay?

88
00:03:56,540 --> 00:03:58,040
>> Well it depends on how big this, are

89
00:03:58,040 --> 00:04:00,140
there, are there binary features that we're talking about?

90
00:04:00,140 --> 00:04:01,690
>> Yeah let's say they're binary. So

91
00:04:01,690 --> 00:04:04,040
then it's like two numbers. Right. It's like

92
00:04:04,040 --> 00:04:05,850
you're probability when your parent is true

93
00:04:05,850 --> 00:04:07,380
and your probability when your parent is false.

94
00:04:07,380 --> 00:04:09,880
>> Well that's each table. But what about representing the entire thing?

95
00:04:09,880 --> 00:04:11,130
>> So it's going

96
00:04:11,130 --> 00:04:13,920
to be linear in the size of the number of variables or.

97
00:04:13,920 --> 00:04:14,100
>> Right.

98
00:04:14,100 --> 00:04:14,530
>> Features.

99
00:04:14,530 --> 00:04:16,269
>> Right and the number, the amount of the data that you

100
00:04:16,269 --> 00:04:19,702
need is going to be fundamentally. In order to get this tape, in

101
00:04:19,702 --> 00:04:22,005
order to get this tree, it's going to turn out that you only

102
00:04:22,005 --> 00:04:25,110
need to keep track of quadratic set of numbers, quadratic in the number.

103
00:04:25,110 --> 00:04:26,500
>> We're not going to be given that

104
00:04:26,500 --> 00:04:27,850
tree, we're going to have to figure that out?

105
00:04:27,850 --> 00:04:29,160
>> Well, remember, one of the steps in

106
00:04:29,160 --> 00:04:31,040
the algorithm is you have to estimate the distribution.

107
00:04:32,200 --> 00:04:33,840
>> So we're going to have to figure out, of all

108
00:04:33,840 --> 00:04:36,178
the trees that I might have, which is the best tree?

109
00:04:36,178 --> 00:04:38,540
>> Yeah, that' doesn't exactly follow from the algorithm

110
00:04:38,540 --> 00:04:40,880
sketch because you could also say well, then you need

111
00:04:40,880 --> 00:04:42,280
to figure out that a tree is the right

112
00:04:42,280 --> 00:04:44,360
thing at all instead of something else like a box.

113
00:04:44,360 --> 00:04:45,990
>> Oh, that's fair, but what we're going to

114
00:04:45,990 --> 00:04:47,460
do is were just going to assume that

115
00:04:47,460 --> 00:04:48,820
we're going to do the dependency trees, now why

116
00:04:48,820 --> 00:04:49,840
are we doing this? Were actually doing this for

117
00:04:49,840 --> 00:04:51,250
a couple of reasons Michael, your, your points

118
00:04:51,250 --> 00:04:55,140
pretty your points well taken. Dependency trees have

119
00:04:55,140 --> 00:04:57,490
this nice feature that they actually let you

120
00:04:57,490 --> 00:05:01,300
represent relationships. The point is that you're actually able

121
00:05:01,300 --> 00:05:04,290
to represent relationships between variables. Which in this case are sort

122
00:05:04,290 --> 00:05:06,880
of features in our space. But you don't have to worry about

123
00:05:06,880 --> 00:05:10,620
too many of them, and the only question here then is how

124
00:05:10,620 --> 00:05:14,320
many relationships do you want, what kind, which of all the possible

125
00:05:14,320 --> 00:05:16,970
relationships you could have where you only are related to one

126
00:05:16,970 --> 00:05:19,730
other thing, do you want to have. Well, in some sense a

127
00:05:19,730 --> 00:05:22,410
dependency tree since you can depend on at most only one other

128
00:05:22,410 --> 00:05:26,910
thing, is the simplest set of relationships you can keep track of.

129
00:05:26,910 --> 00:05:29,360
The next simplest would be, as you point out, not having any

130
00:05:29,360 --> 00:05:34,230
parents at all. In which case, you would be estimating simply this.

131
00:05:34,230 --> 00:05:35,712
>> Yeah, I wouldn't say that the

132
00:05:35,712 --> 00:05:38,340
next simplest. That's even simpler. But it doesn't,

133
00:05:38,340 --> 00:05:40,490
doesn't allow any of the inter-relationships. Any

134
00:05:40,490 --> 00:05:44,110
of the co-variants essentially information to be captured.

135
00:05:44,110 --> 00:05:46,870
>> No, that's fair, that's a fair point. So, we could have started with

136
00:05:46,870 --> 00:05:49,800
something like this except here you must

137
00:05:49,800 --> 00:05:52,250
you you're forced to ignore all relationships because

138
00:05:52,250 --> 00:05:55,260
you're treating everything as independent. And we don't believe that things

139
00:05:55,260 --> 00:05:57,780
are independent or at least we think there a possibility there's some

140
00:05:57,780 --> 00:06:00,870
dependence. And so by allowing at most that you're connected to

141
00:06:00,870 --> 00:06:04,790
one other parent that's sort of the least committed to the idea

142
00:06:04,790 --> 00:06:07,400
you could still be while still allowing you to capture these

143
00:06:07,400 --> 00:06:10,630
relationships. So that's the basic idea. Now I want to be, I

144
00:06:10,630 --> 00:06:13,330
want to be strict here that the mimic algorithm or just this

145
00:06:13,330 --> 00:06:15,930
whole notion of representing probability distributions

146
00:06:15,930 --> 00:06:18,170
does not depend upon dependency trees.

147
00:06:18,170 --> 00:06:20,410
We're going to use dependency trees here because you have

148
00:06:20,410 --> 00:06:22,600
to make some decision about how to represent the

149
00:06:22,600 --> 00:06:25,230
probability distributions, and this is kind of the easiest

150
00:06:25,230 --> 00:06:28,670
thing to do that still allows you to capture relationships.

151
00:06:28,670 --> 00:06:29,120
>> I buy that.

152
00:06:29,120 --> 00:06:32,060
>> Okay and one other thing worth noting is I you'll I hope

153
00:06:32,060 --> 00:06:33,590
you'll see this is a couple of slides if you don't see it

154
00:06:33,590 --> 00:06:36,990
immediately, is that at the very least this will allow us to capture

155
00:06:36,990 --> 00:06:38,410
the same kind of relationships that we

156
00:06:38,410 --> 00:06:41,350
get from cross over in genetic algorithms.

157
00:06:41,350 --> 00:06:41,620
>> Huh?

158
00:06:41,620 --> 00:06:44,410
>> And that was a bit of the, the inspiration

159
00:06:44,410 --> 00:06:47,109
for this. That cross-over is representing

160
00:06:47,109 --> 00:06:49,190
structure. In this case structure that's

161
00:06:49,190 --> 00:06:53,010
measured by locality, and this is kind of the general form of that.

162
00:06:53,010 --> 00:06:57,610
>> So you are saying if there is some locality, then whatever,

163
00:06:57,610 --> 00:07:01,400
however we're going to learn the dependency tree from the samples is going to

164
00:07:01,400 --> 00:07:04,170
be able to capture that. And therefore, it will kind of wrap

165
00:07:04,170 --> 00:07:07,650
its head around the same kind of information that that cross over's exploiting.

166
00:07:07,650 --> 00:07:09,600
>> That's exactly right,

167
00:07:09,600 --> 00:07:11,860
and in fact, as we'll see in one of the examples

168
00:07:11,860 --> 00:07:13,640
that I'll give you in a moment, that it can do

169
00:07:13,640 --> 00:07:17,360
better than that because it doesn't actually require locality the way

170
00:07:17,360 --> 00:07:21,010
that crossover does. There's one other thing that's worth mentioning here

171
00:07:21,010 --> 00:07:24,500
about this distribution and why it's nice. It captures relationships. But

172
00:07:24,500 --> 00:07:27,880
the second thing is, something that you, you sort of alluded

173
00:07:27,880 --> 00:07:32,740
to earlier is that, it's very easy to sample from. Right?

174
00:07:32,740 --> 00:07:35,000
So given a dependency tree where each one of these features,

175
00:07:35,000 --> 00:07:37,500
each one of these nodes, represents a feature, it

176
00:07:37,500 --> 00:07:40,750
is very simple, very easy to generate samples consistent with

177
00:07:40,750 --> 00:07:43,080
it. Right? You just start at the root, generate

178
00:07:43,080 --> 00:07:46,820
a sample, unconditional sample according to whatever the distribution is

179
00:07:46,820 --> 00:07:48,360
and then you go through the parents and you

180
00:07:48,360 --> 00:07:51,690
do the same thing. So it's exactly a topological sort

181
00:07:51,690 --> 00:07:54,670
and in trees topological sorting is very easy and

182
00:07:54,670 --> 00:07:56,270
this is in fact, linear in the number of features.

183
00:07:56,270 --> 00:07:57,730
>> Right. I get that and it is

184
00:07:57,730 --> 00:08:00,090
exactly an instance of what we talked about when

185
00:08:00,090 --> 00:08:03,250
we did Bayesian inference The thing that is new is

186
00:08:03,250 --> 00:08:05,860
how do we figure out dependency tree from the sample.

187
00:08:05,860 --> 00:08:07,000
>> Exactly, that's what we're going to

188
00:08:07,000 --> 00:08:09,290
do next and that's going to involves math.
