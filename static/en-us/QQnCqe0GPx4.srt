1
00:00:00,280 --> 00:00:05,160
All right, so Dec-POMDPs give us a way
to optimally trade off acting and

2
00:00:05,160 --> 00:00:09,010
coordinating and communicating,
and I think we can now move on to

3
00:00:09,010 --> 00:00:11,400
the topic of coaching, so
communicating and coaching.

4
00:00:11,400 --> 00:00:14,880
So let's imagine instead of
the scenario we talked about before,

5
00:00:14,880 --> 00:00:17,890
where we had an agent that
himself wanted an apple and

6
00:00:17,890 --> 00:00:22,820
wanted to somehow use communication to
help bring about a positive outcome.

7
00:00:22,820 --> 00:00:27,450
Let's look more particularly at this
case of, okay, agent 1 needs agent 2 to

8
00:00:27,450 --> 00:00:30,670
learn a particular task,
like how to fetch an apple, all right.

9
00:00:30,670 --> 00:00:34,300
And so what we're going to try to
do is figure out how can agent 1

10
00:00:34,300 --> 00:00:36,190
convey the task to agent 2.

11
00:00:36,190 --> 00:00:39,030
If agent 2 has its own reward function,
it has its own task.

12
00:00:39,030 --> 00:00:42,870
So we're going to try to create
a a task or reward function for

13
00:00:42,870 --> 00:00:47,660
agent 2 in some way that is
consistent with agent 1's goals.

14
00:00:47,660 --> 00:00:48,390
>> Okay, so two things.

15
00:00:48,390 --> 00:00:52,730
One, so when you said, we're going to
try to create, you mean like agent 1

16
00:00:52,730 --> 00:00:55,500
should try to create a reward
function for agent 2, right?

17
00:00:55,500 --> 00:00:56,240
>> Yeah.
>> Okay.

18
00:00:56,240 --> 00:00:59,620
>> Or to convince agent 2 to create
such a reward function on its own.

19
00:00:59,620 --> 00:01:03,270
>> Okay, and so second, I think
the first thing agent 1 should do

20
00:01:03,270 --> 00:01:07,610
is not tell agent 2 that he's trying
to teach him to fetch things.

21
00:01:07,610 --> 00:01:09,280
I think he should probably.

22
00:01:09,280 --> 00:01:11,570
>> Yeah, fetch is not going to happen.

23
00:01:11,570 --> 00:01:14,930
>> Right, you should probably
use another nicer word.

24
00:01:14,930 --> 00:01:16,120
But okay, I think I'm with you.

25
00:01:16,120 --> 00:01:16,677
>> Yeah, that's why I said get.

26
00:01:16,677 --> 00:01:18,455
>> Yeah, except you said fetch.

27
00:01:18,455 --> 00:01:20,040
[LAUGH]
>> Yeah,

28
00:01:20,040 --> 00:01:21,410
it felt like fetch was a good word.

29
00:01:21,410 --> 00:01:23,380
All right, yes, okay, so
I agree with you on that.

30
00:01:23,380 --> 00:01:24,750
>> Okay, good.
So okay, so

31
00:01:24,750 --> 00:01:26,128
then I think I've got the task.

32
00:01:26,128 --> 00:01:31,526
I want to somehow get agent to
get a reward function that will

33
00:01:31,526 --> 00:01:36,494
get it to solve some task that
I wanted to solve or him.

34
00:01:36,494 --> 00:01:40,235
>> Yeah, exactly, and so there's a bunch
of different ways of thinking about

35
00:01:40,235 --> 00:01:43,742
this, but we're going to dive into
a particular one that I think is really

36
00:01:43,742 --> 00:01:47,790
interesting and actually quite useful
called inverse reinforcement learning.

37
00:01:47,790 --> 00:01:48,443
>> What, okay.

38
00:01:48,443 --> 00:01:50,457
>> All right, so
let me set this up for you.

39
00:01:50,457 --> 00:01:50,957
>> All right.
