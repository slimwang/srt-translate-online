1
00:00:00,403 --> 00:00:04,106
>> Steven, what kind of problems would be a good match for this dynamic parallelism capability?

2
00:00:04,106 --> 00:00:08,951
So as programmers are thinking about what techniques to use in designing the next-generation algorithms,

3
00:00:08,951 --> 00:00:13,052
what are the things where dynamic parallelism is going to really make a difference for them?

4
00:00:13,052 --> 00:00:17,520
>> I guess when I think about the features that I want to add to CUDA,

5
00:00:17,520 --> 00:00:21,791
I really want to make it easier to write the programs you want to write,

6
00:00:21,791 --> 00:00:29,177
and dynamic parallelism, I think, really gives you much more flexibility in how you write your code.

7
00:00:29,177 --> 00:00:35,911
So as well as things like recursive algorithms, which are extremely difficult without it,

8
00:00:35,911 --> 00:00:41,778
sometimes it's just easier to write a program wholly on the GPU, for example.

9
00:00:41,778 --> 00:00:46,449
If I know that my program is a small integrative loop launching lots of parallel work,

10
00:00:46,449 --> 00:00:49,445
it might be just simply be easier to write the whole thing on the GPU

11
00:00:49,445 --> 00:00:52,780
and have a GPU thread control the whole thing from scratch.

12
00:00:52,780 --> 00:00:56,677
So if I'm in a situation where my memory marshalling backwards and forwards

13
00:00:56,677 --> 00:00:59,813
between CPU and GPU would be complex or difficult--

14
00:00:59,813 --> 00:01:03,761
if I put my whole program on the GPU, all my memory's in one place, and it's much easier to write.

15
00:01:03,761 --> 00:01:06,453
So the first kind of problem I would look at

16
00:01:06,453 --> 00:01:11,225
is one which would just be made easier by keeping all of your code in one place.

17
00:01:11,225 --> 00:01:13,695
You may not necessarily go any faster.

18
00:01:13,695 --> 00:01:17,938
Remember serial execution on the GPU is slower than serial execution on the CPU.

19
00:01:17,938 --> 00:01:20,231
When you're developing code,

20
00:01:20,231 --> 00:01:22,967
even though you might be able to get something working faster

21
00:01:22,967 --> 00:01:25,874
if you do a CPU and GPU combination,

22
00:01:25,874 --> 00:01:28,673
typically--you know--as I said, I came from a science background.

23
00:01:28,673 --> 00:01:32,346
You don't have 6 months to tune your code; you've got 4 weeks before your paper deadline.

24
00:01:32,346 --> 00:01:34,947
So the question is not, how good can I get it?

25
00:01:34,947 --> 00:01:38,218
It's how good can I get it in 4 weeks before my paper is due?

26
00:01:38,218 --> 00:01:41,922
And so if I can give something like dynamic parallelism,

27
00:01:41,922 --> 00:01:45,792
which will make it easier to get as good as you can in 4 weeks--

28
00:01:45,792 --> 00:01:48,928
you might only get to 75% of performance in 4 weeks.

29
00:01:48,928 --> 00:01:51,672
But if without it, you'd only get to 50% of performance,

30
00:01:51,672 --> 00:01:54,399
you can do that much more science in the time you have available.

31
00:01:54,399 --> 00:01:59,075
So in spite of the new types of algorithm that it lets you do--and those are very interesting,

32
00:01:59,075 --> 00:02:02,776
because it lets you approach things on the GPU you simply couldn't do before.

33
00:02:02,776 --> 00:02:06,182
I think the first step is to say, "Well, does this just make my life easier?"

34
00:02:06,182 --> 00:02:09,780
If it makes your life easier, you can spend your time focusing on doing the science

35
00:02:09,780 --> 00:02:12,419
or the calculation you need to do,

36
00:02:12,419 --> 00:02:16,000
rather than wondering, how on Earth do I program this thing in the first place?
