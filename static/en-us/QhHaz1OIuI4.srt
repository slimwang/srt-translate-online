1
00:00:00,260 --> 00:00:03,250
Although conditional entropy can tell us when two variables

2
00:00:03,250 --> 00:00:06,210
are completely independent, it is not an adequate measure of

3
00:00:06,210 --> 00:00:09,832
dependence. Now consider the conditional entropy of y given the

4
00:00:09,832 --> 00:00:13,940
variable x. This conditional entropy may be small if x

5
00:00:13,940 --> 00:00:18,480
tells us a great deal about y or that x of y is very small to begin with. So we

6
00:00:18,480 --> 00:00:22,570
need another measure of dependence to measure the relationship between

7
00:00:22,570 --> 00:00:25,920
x and y and we call that as mutual information.

8
00:00:25,920 --> 00:00:32,479
It is denoted by the symbol I. And it is given as, the entropy of y, subtracted

9
00:00:32,479 --> 00:00:34,930
by the entropy of x given y. So

10
00:00:34,930 --> 00:00:37,470
mutual information is a measure of the reduction of

11
00:00:37,470 --> 00:00:40,400
randomness of a variable given knowledge of some

12
00:00:40,400 --> 00:00:43,350
other variable. If you like to understand the derivations

13
00:00:43,350 --> 00:00:47,500
for these particular identities, I'll refer you to Charles's

14
00:00:47,500 --> 00:00:51,080
notes on, on this topic. But we'll jump directly

15
00:00:51,080 --> 00:00:52,970
into an example and try to calculate these

16
00:00:52,970 --> 00:00:54,968
values and understand what it means to have a

17
00:00:54,968 --> 00:00:57,128
high value of mutual information or low value of

18
00:00:57,128 --> 00:01:01,140
mutual information. So let's do that as a quiz.
