1
00:00:00,090 --> 00:00:03,130
>> Okay Michael, so what I'm going to do, I'm going to step through how

2
00:00:03,130 --> 00:00:07,910
you find dependency trees and just stop me if it doesn't make any sense, okay?

3
00:00:07,910 --> 00:00:08,400
>> Sure.

4
00:00:08,400 --> 00:00:10,360
>> But hopefully it's fairly straightforward, at

5
00:00:10,360 --> 00:00:12,690
least if you understand information theory. And

6
00:00:12,690 --> 00:00:13,810
again I want to stress that although

7
00:00:13,810 --> 00:00:15,850
we're going to, we're going to spend some time doing

8
00:00:15,850 --> 00:00:18,210
this, this is just one example of

9
00:00:18,210 --> 00:00:20,620
ways that you could represent a probability distribution.

10
00:00:20,620 --> 00:00:21,520
It's just going to turn out that

11
00:00:21,520 --> 00:00:24,310
this one is particularly easy and powerful, okay?

12
00:00:24,310 --> 00:00:25,660
>> Exciting.

13
00:00:25,660 --> 00:00:27,680
>> Okay, so the first thing we have to remember, Michael,

14
00:00:27,680 --> 00:00:30,250
is that we have some true probability distribution which I'm just going

15
00:00:30,250 --> 00:00:33,070
to write as P, that we're trying to do. That's our P

16
00:00:33,070 --> 00:00:36,370
soup theta in this case. But the general question of how you

17
00:00:36,370 --> 00:00:39,310
represent a dependency tree doesn't depend on theta or anything else, there's

18
00:00:39,310 --> 00:00:43,040
just some underlying distribution we care about, let's call that P. Okay.

19
00:00:43,040 --> 00:00:46,505
And we're going to estimate it with another distribution which I'm going

20
00:00:46,505 --> 00:00:50,700
to represent as p-hat because, you know, for approximation. That's going to

21
00:00:50,700 --> 00:00:56,200
depend upon that parent function that we defined before. Okay?

22
00:00:56,200 --> 00:00:57,280
>> Alright.

23
00:00:57,280 --> 00:00:59,830
>> So somehow. You sound skeptical, Michael.

24
00:00:59,830 --> 00:01:02,200
>> Well, because I saw that you wrote those train tracks.

25
00:01:02,200 --> 00:01:04,060
>> And you figure they must mean something?

26
00:01:04,060 --> 00:01:05,360
>> Well I know they mean something. It's a,

27
00:01:05,360 --> 00:01:07,320
it's an information theory thing but I'm not going to

28
00:01:07,320 --> 00:01:09,470
remember what it is. You're going to, it's going to,

29
00:01:09,470 --> 00:01:11,130
you're going to say something like kl divergence or something.

30
00:01:11,130 --> 00:01:13,434
>> That's exactly right. So, somehow we want

31
00:01:13,434 --> 00:01:16,450
to not just find a p-hat sub theta,

32
00:01:16,450 --> 00:01:19,420
that is a dependency tree that represents the underlying

33
00:01:19,420 --> 00:01:22,050
distribution, but we want to find the best one. And

34
00:01:22,050 --> 00:01:24,350
so best one here sort of means closest one,

35
00:01:24,350 --> 00:01:26,590
or most similar, or the one that would generate the

36
00:01:26,590 --> 00:01:29,360
points in the best possible way. And it turns

37
00:01:29,360 --> 00:01:33,447
out, for those who remember information theory, that there's actually

38
00:01:33,447 --> 00:01:36,131
a particular measure for that and it's called the KL

39
00:01:36,131 --> 00:01:38,840
Divergence. You remember what the KL Divergence stands for, Michael?

40
00:01:38,840 --> 00:01:42,820
>> Kullback Leibler

41
00:01:42,820 --> 00:01:44,880
>> That's right. And it has a particular

42
00:01:44,880 --> 00:01:49,460
form and in this case noncontinuous variables. It

43
00:01:49,460 --> 00:01:53,480
has basically this form. And that's basically a

44
00:01:53,480 --> 00:01:56,220
measure of the divergence, that's what the D stands

45
00:01:56,220 --> 00:01:59,560
for, the divergence between the underlying distribution P

46
00:01:59,560 --> 00:02:01,855
that we care about and some other candidate distribution

47
00:02:01,855 --> 00:02:05,270
P-hat that we're trying to get as close

48
00:02:05,270 --> 00:02:08,342
as possible. So if these are the same distributions,

49
00:02:08,342 --> 00:02:10,070
if P-hat and P are the same

50
00:02:10,070 --> 00:02:12,930
distribution, the Kullback–Leibler divergence is equal to zero.

51
00:02:12,930 --> 00:02:14,234
>> Mm. Mm-hm.

52
00:02:14,234 --> 00:02:20,140
>> And as they differ or as they diverge this number gets larger and larger.

53
00:02:20,140 --> 00:02:24,609
Now it turns out that these numbers are unitless. They don't obey the triangle

54
00:02:24,609 --> 00:02:29,650
inequality. This is not a distance. This is truly a divergence. But if we

55
00:02:29,650 --> 00:02:33,915
can get this number to be minimized, then we know we have found a distribution

56
00:02:33,915 --> 00:02:38,490
P-hat, that is as close as we can get to P, okay? And

57
00:02:38,490 --> 00:02:42,820
we have a whole unit that Pushcar will do to remind everybody about information

58
00:02:42,820 --> 00:02:46,332
theory where this comes from. But basically this is the right way to define

59
00:02:46,332 --> 00:02:49,152
similarity between probability distributions. You just have

60
00:02:49,152 --> 00:02:50,500
to kind of take that on faith.

61
00:02:50,500 --> 00:02:53,080
>> Okay. Well, I'll, I'll pause this and go back and

62
00:02:53,080 --> 00:02:55,560
listen to Pushcar's lecture and then I'll be ready for you.

63
00:02:55,560 --> 00:02:59,140
>> Okay, beautiful. Okay. So what we really want to do is we want

64
00:02:59,140 --> 00:03:03,170
to minimize the Kullback–Leibler divergence the that is minimize the

65
00:03:03,170 --> 00:03:06,560
difference between the distribution that we're going to estimate with

66
00:03:06,560 --> 00:03:09,200
a dependency tree and the true underlying distribution. And just

67
00:03:09,200 --> 00:03:11,930
by doing some algebra, you end up getting down to

68
00:03:11,930 --> 00:03:15,640
what looks like a fairly simple function. So Michael, if

69
00:03:15,640 --> 00:03:17,880
you were, if you were paying close attention to the

70
00:03:17,880 --> 00:03:21,370
algebra, you will realize that well p log p, now

71
00:03:21,370 --> 00:03:24,330
that you've come back from Pushcar's lecture is just entropy.

72
00:03:24,330 --> 00:03:25,650
>> Yep.

73
00:03:25,650 --> 00:03:28,610
>> So or it's minus the entropy. And so

74
00:03:28,610 --> 00:03:31,340
I can rewrite this as simply minus the entropy

75
00:03:31,340 --> 00:03:35,270
of the underlying distribution, plus the sum of the

76
00:03:35,270 --> 00:03:39,740
conditional entropies, for each of the Xis, given its parent.

77
00:03:39,740 --> 00:03:42,454
Which has some, you know, sort of intuitive niceness

78
00:03:42,454 --> 00:03:44,250
do it, but, whatever. This is what you end

79
00:03:44,250 --> 00:03:46,400
up with just by doing the substitution. P log

80
00:03:46,400 --> 00:03:49,350
P gives you minus entropy of P minus P log

81
00:03:49,350 --> 00:03:53,330
P hat, which gives you the conditional entropy according

82
00:03:53,330 --> 00:03:57,950
to the function, the parent function pi. Okay. Well,

83
00:03:57,950 --> 00:04:00,480
in the end all we care about is finding

84
00:04:00,480 --> 00:04:03,500
the best pi. So, this term doesn't matter at all

85
00:04:03,500 --> 00:04:05,280
and so we end up with a kind of

86
00:04:05,280 --> 00:04:08,460
cost function that we would like to minimize. Which I'm

87
00:04:08,460 --> 00:04:11,700
going to call here J. Which depends upon pi

88
00:04:11,700 --> 00:04:13,880
which is just the sum of all the conditional entropies.

89
00:04:13,880 --> 00:04:16,260
Basically the best tree that we can find

90
00:04:16,260 --> 00:04:18,610
will be the one that minimizes all of the

91
00:04:18,610 --> 00:04:21,560
entropy for each of the features, given its

92
00:04:21,560 --> 00:04:24,230
parents. Does that make any intuitive sense to you?

93
00:04:24,230 --> 00:04:29,550
>> Yeah. I think so. Cause we want, we want to choose parents that are

94
00:04:29,550 --> 00:04:30,260
going to give us a lot of

95
00:04:30,260 --> 00:04:33,960
information about the values of the corresponding features.
