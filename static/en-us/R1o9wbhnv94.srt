1
00:00:00,000 --> 00:00:02,000
[Thrun] And the answer is yes,

2
00:00:02,000 --> 00:00:06,000
although, technically speaking, to reach the absolute global minimum

3
00:00:06,000 --> 00:00:11,000
we need the learning rates to become smaller and smaller over time.

4
00:00:11,000 --> 00:00:15,000
If they stay constant, there is a chance this thing might bounce around

5
00:00:15,000 --> 00:00:18,000
between 2 points in the end and never reach the global minimum.

6
00:00:18,000 --> 00:00:22,000
But assuming that we implement gradient descent correctly,

7
00:00:22,000 --> 00:00:24,000
we will finally reach the global minimum.

8
00:00:24,000 --> 00:00:29,000
That's not the case if you start over here, where we can get stuck over here

9
00:00:29,000 --> 00:00:32,000
and settle for the minimum over here, which is a local minimum

10
00:00:32,000 --> 00:00:35,000
and not the best solution to our optimization problem.

11
00:00:35,000 --> 00:00:38,000
So one of the important points to take away from this is

12
00:00:38,000 --> 00:00:43,000
gradient descent is universally applicable to more complicated problems--

13
00:00:43,000 --> 00:00:46,000
problems that don't have a plausible solution.

14
00:00:46,000 --> 00:00:49,000
But you have to check whether there is many local minima,

15
00:00:49,000 --> 00:00:51,000
and if so, you have to worry about this.

16
00:00:51,000 --> 00:00:55,000
Any optimization book can tell you tricks how to overcome this.

17
00:00:55,000 --> 99:59:59,999
I won't go into any more depth here in this class.
