1
00:00:00,300 --> 00:00:03,760
I think that just about covers it as
an introduction to generalization,

2
00:00:03,760 --> 00:00:06,510
so I think maybe it's a good time
to reflect on what we've learned.

3
00:00:06,510 --> 00:00:07,150
>> I'm willing to do that,

4
00:00:07,150 --> 00:00:09,300
especially if it means I don't
have to do another quiz.

5
00:00:09,300 --> 00:00:11,880
>> Okay, well, but I, eh, okay, no,
I'm not going to do another quiz, but

6
00:00:11,880 --> 00:00:14,400
I am going to ask you what we learned.

7
00:00:14,400 --> 00:00:18,540
>> Okay, I think I can remember some
of that, so I think the big thing we

8
00:00:18,540 --> 00:00:25,140
learned is that there is both a need for
and a way to do generalization.

9
00:00:25,140 --> 00:00:26,220
>> In reinforcement learning, yeah.

10
00:00:26,220 --> 00:00:28,580
>> And the need part comes
from the fact that for

11
00:00:28,580 --> 00:00:32,580
a lot of problems we care about, there
are zillions and zillions of states.

12
00:00:32,580 --> 00:00:34,940
>> You could be our
generation's Carl Sagan.

13
00:00:34,940 --> 00:00:37,780
>> Yes, zillion and zillions of states.

14
00:00:37,780 --> 00:00:41,170
And that's a problem because of course,
we'll visit states that we've never seen

15
00:00:41,170 --> 00:00:44,640
before, and the chances of seeing every
state an infinite number of times and

16
00:00:44,640 --> 00:00:48,340
all that other stuff that the theory
requires is just kind of unreasonable.

17
00:00:48,340 --> 00:00:50,318
Or at least slow, and so
it would be nice to overcome that.

18
00:00:50,318 --> 00:00:52,510
>> Good, okay, and methods for?

19
00:00:52,510 --> 00:00:53,590
>> We talked about a few methods, but

20
00:00:53,590 --> 00:00:58,540
I think at the end of the day, it really
boiled down to let's just take all

21
00:00:58,540 --> 00:01:02,020
that stuff we learned in
supervised learning and apply it.

22
00:01:02,020 --> 00:01:04,150
>> Cool, but
we did talk about some specific ones.

23
00:01:04,150 --> 00:01:07,030
Do you remember any particular
supervised learning methods that we

24
00:01:07,030 --> 00:01:08,820
looked at in the context
of generalization in

25
00:01:08,820 --> 00:01:09,800
reinforcement learning?

26
00:01:09,800 --> 00:01:12,040
>> Sure, we looked at linear
function approximation,

27
00:01:12,040 --> 00:01:15,480
which makes sense because linear
is nice and well behaved.

28
00:01:15,480 --> 00:01:17,390
Or at least we hoped it was.

29
00:01:17,390 --> 00:01:18,980
And it does seem like the simplest most

30
00:01:18,980 --> 00:01:20,610
straightforward thing to
do based on what we know.

31
00:01:20,610 --> 00:01:23,510
But I think it's worth mentioning
that we were function approximating

32
00:01:23,510 --> 00:01:24,630
something in particular, right?

33
00:01:24,630 --> 00:01:30,530
So we were deciding to do the value
functions and to try to learn those.

34
00:01:30,530 --> 00:01:31,960
That's how we were doing
our generalization.

35
00:01:31,960 --> 00:01:32,690
>> Right, that's a good point.

36
00:01:32,690 --> 00:01:35,370
We actually looked at different
functions that we might approximate,

37
00:01:35,370 --> 00:01:38,370
value functions,
policies and models, and

38
00:01:38,370 --> 00:01:40,720
we focused primarily on value functions.

39
00:01:40,720 --> 00:01:44,590
Models, there's interesting
things to say about that, but

40
00:01:44,590 --> 00:01:47,860
it's not nearly as well studied or
as well understood.

41
00:01:47,860 --> 00:01:51,610
The value function case, we just focused
on representing like the V function or

42
00:01:51,610 --> 00:01:55,190
the Q function, and
that could be done in a number of ways.

43
00:01:55,190 --> 00:01:57,905
And we looked at kind of
a general gradient form,

44
00:01:57,905 --> 00:02:02,158
where we turned the Bellman equation
itself into a kind of an error measure.

45
00:02:02,158 --> 00:02:06,085
And then we looked specifically at
linear value function approximation,

46
00:02:06,085 --> 00:02:09,758
how you could, if you have a set of
features, you can map them to values

47
00:02:09,758 --> 00:02:13,398
using a linear function and
how we might learn that linear function.

48
00:02:13,398 --> 00:02:15,850
>> Right, and that's pretty much what
has been almost done from the start.

49
00:02:15,850 --> 00:02:20,010
I don't think we did hardly anything
with the policy function approximation.

50
00:02:20,010 --> 00:02:20,878
>> Nope, and that's okay.

51
00:02:20,878 --> 00:02:21,696
[LAUGH]
>> [LAUGH] And

52
00:02:21,696 --> 00:02:26,407
one of the reasons it's okay is because
you're able to talk about lots of

53
00:02:26,407 --> 00:02:30,428
successes that people have had
doing function approximation

54
00:02:30,428 --> 00:02:31,990
with value functions.

55
00:02:31,990 --> 00:02:35,258
>> Yeah, and there are some successes
on doing function approximation with

56
00:02:35,258 --> 00:02:36,108
policies as well.

57
00:02:36,108 --> 00:02:40,592
In fact a lot of the coolest stuff in
robotics in the reinforcement learning

58
00:02:40,592 --> 00:02:45,221
setting has happened where there's
an actual gradient taken with respect to

59
00:02:45,221 --> 00:02:48,080
representation of the policy.

60
00:02:48,080 --> 00:02:52,280
It's not exactly clear why it seems
to be the more effective way to use

61
00:02:52,280 --> 00:02:56,820
feedback in the robotic setting,
but it's worth noting at least that

62
00:02:57,980 --> 00:03:02,770
policy gradient methods do seem to
work well in the robotics setting.

63
00:03:02,770 --> 00:03:04,700
>> That's a good point, but
it doesn't really matter.

64
00:03:04,700 --> 00:03:06,670
It's not like robotics is the future or
anything.

65
00:03:06,670 --> 00:03:07,866
>> Robotics is totally the future.

66
00:03:07,866 --> 00:03:09,180
>> Mm, we'll see.

67
00:03:09,180 --> 00:03:11,880
We'll have this conversation in 2050 and
see what we think.

68
00:03:11,880 --> 00:03:13,268
>> In 2015, it is 2015.

69
00:03:13,268 --> 00:03:14,574
>> No, 2050.

70
00:03:14,574 --> 00:03:16,008
>> 2050, phew.

71
00:03:16,008 --> 00:03:18,768
I'm like, my gosh, it's the future and
I wasn't even paying attention.

72
00:03:18,768 --> 00:03:21,186
>> [LAUGH] Well that's okay.

73
00:03:21,186 --> 00:03:24,726
I, for one,
welcome our new robotic overlords.

74
00:03:24,726 --> 00:03:25,964
>> Good.
>> In the meantime,

75
00:03:25,964 --> 00:03:28,833
we had all these successes,
and you named a few.

76
00:03:28,833 --> 00:03:31,462
Right, the one I remember
the most was TD gammon, but

77
00:03:31,462 --> 00:03:35,204
there were others, which if I went back
and looked at what we've recorded,

78
00:03:35,204 --> 00:03:37,261
I'm sure I could find
out what they were.

79
00:03:37,261 --> 00:03:38,840
>> [LAUGH]
>> Or maybe you'll just remember.

80
00:03:38,840 --> 00:03:43,780
>> Well, I mentioned several, but
Atari is one that actually came up

81
00:03:43,780 --> 00:03:47,328
relatively recently, and
hopefully will actually change

82
00:03:47,328 --> 00:03:51,150
the way people think about this whole
topic of generalization, but we'll see.

83
00:03:51,150 --> 00:03:52,670
It's still too new to know.

84
00:03:52,670 --> 00:03:53,500
>> Very good, prince.

85
00:03:53,500 --> 00:03:57,090
Okay, so now those successes
of course were followed by

86
00:03:57,090 --> 00:04:02,350
the downer of problems and
sort of problem cases,

87
00:04:02,350 --> 00:04:07,050
where things that seem like they would
obviously work do not obviously and

88
00:04:07,050 --> 00:04:10,140
easily work, even in the case
of linear function of arguments.

89
00:04:10,140 --> 00:04:11,480
>> Yes, that's exactly right.

90
00:04:11,480 --> 00:04:14,160
The good news is in the last few years,

91
00:04:14,160 --> 00:04:19,670
there's a new set of methods that
are based on TD and a gradient.

92
00:04:19,670 --> 00:04:21,720
But it's a different kind of
gradient that actually uses

93
00:04:21,720 --> 00:04:26,210
the function approximator as part of the
error metric that's being resolved and

94
00:04:26,210 --> 00:04:28,810
that these can actually
these provably converge.

95
00:04:28,810 --> 00:04:30,995
For the case of linear, in particular,

96
00:04:30,995 --> 00:04:35,307
the paper that talks about GTD2 actually
shows what happens when you apply this

97
00:04:35,307 --> 00:04:39,248
particular problem case, the Baird
counter example that we looked at.

98
00:04:39,248 --> 00:04:43,428
And you can actually correctly learn
it if you use this other update rule

99
00:04:43,428 --> 00:04:47,970
instead of kind of the classic Bellman
residual minimization kind of thing.
