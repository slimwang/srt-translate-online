1
00:00:00,130 --> 00:00:01,783
Okay, so let's quickly recap.

2
00:00:01,783 --> 00:00:06,250
We had a serial version of the code that did everything in a single thread--

3
00:00:06,250 --> 00:00:10,892
trivial to write that code, 0 parallelism, and pretty terrible performance.

4
00:00:10,892 --> 00:00:17,467
Then we had a parallel per row version of the code that was also pretty simple to write

5
00:00:17,467 --> 00:00:21,895
and assigned 1 thread per row and a single thread block--

6
00:00:21,895 --> 00:00:24,333
again did not get great performance,

7
00:00:24,333 --> 00:00:27,575
although a huge improvement factor of about a hundred.

8
00:00:27,575 --> 00:00:32,197
Then in the third version, we extracted pretty much all the parallelism that the problem has to

9
00:00:32,197 --> 00:00:34,796
offer and assigned one thread per element to the matrix.

10
00:00:34,796 --> 00:00:37,953
So we did this transposed operation now and sort of massive parallel.

11
00:00:37,953 --> 00:00:41,796
And now we got quite a bit better performance.

12
00:00:41,796 --> 00:00:49,823
Remember that all this is happening in the context of APOD, Analyze, Parallelize, Optimize and Deploy.

13
00:00:49,823 --> 00:00:54,707
And this actually might be fast enough that you're ready to deploy it.

14
00:00:54,707 --> 00:01:00,712
So we started by looking at analyzing the code and deciding that this function of

15
00:01:00,712 --> 00:01:02,565
transposing matrix need to be sped up.

16
00:01:02,565 --> 00:01:07,537
We started exploring ways to parallelize it. And this might be fast enough.

17
00:01:07,537 --> 00:01:11,707
This is already a place where you ought to look at this and say is this the bottleneck anymore?

18
00:01:11,707 --> 00:01:16,079
Is speeding this up going to make a big difference to my application? If not, then deploy.

19
00:01:16,079 --> 00:01:19,163
If so, then we can start thinking about optimizing it.

20
00:01:19,163 --> 00:01:21,268
And that was the next thing we did.

21
00:01:21,268 --> 00:01:26,163
We looked at the performance here we saw that we were getting pretty poor DRAM utilization.

22
00:01:26,163 --> 00:01:32,139
And we diagnosed that the problem must be that our global stores were getting bad coalescing.

23
00:01:32,139 --> 00:01:37,829
In other words, when we were writing the output matrix we were getting bad coalescing in those accesses to global memory.

24
00:01:37,829 --> 00:01:41,709
So to fix that we came up with the tiled version of our code, where each thread

25
00:01:41,709 --> 00:01:45,620
block is responsible for reading a tile of the input matrix, transposing it

26
00:01:45,620 --> 00:01:48,908
and writing that transposed matrix to its location in the output matrix.

27
00:01:48,915 --> 00:01:52,668
And it can perform these reads and writes in a coalesced fashion.

28
00:01:52,668 --> 00:01:58,207
We did this in blocks of 32 by 32 tiles and got excellent coalescing.

29
00:01:58,207 --> 00:02:03,340
This improved our speed a little bit. But we're still not getting great DRAM utilization.

30
00:02:03,340 --> 00:02:06,012
So, we looked into why.

31
00:02:06,012 --> 00:02:11,622
We thought a little bit more about why we wouldn't have taken great improvement in bandwidth.

32
00:02:11,622 --> 00:02:15,156
We concluded we probably had too many threads waiting at barriers.

33
00:02:15,187 --> 00:02:18,171
So we improved that by shrinking the tile size.

34
00:02:18,171 --> 00:02:21,765
Experimenting with different tile sizes led us to conclude that a 16 by 16

35
00:02:21,765 --> 00:02:27,285
thread block writing into a 16 by 16 tile ensured memory, struck a good balance

36
00:02:27,285 --> 00:02:31,404
between achieving coalesced writes, reads and writes to memory, and not spending

37
00:02:31,404 --> 00:02:34,000
too much time waiting at barriers.
