1
00:00:00,270 --> 00:00:03,250
So that actually brings us to
the end of the PomDP topic.

2
00:00:03,250 --> 00:00:05,230
>> How do you know at
the end of the PomDP topic?

3
00:00:05,230 --> 00:00:08,880
>> We have high probability of believing
that we're at the end of the pond E.P.

4
00:00:08,880 --> 00:00:11,990
topic because the observation that
we're making is the what have we

5
00:00:11,990 --> 00:00:12,525
learned slide.

6
00:00:12,525 --> 00:00:13,852
>> [LAUGH]
>> So what have we learned?

7
00:00:13,852 --> 00:00:16,391
>> Well, we learned about PomDPs.

8
00:00:16,391 --> 00:00:19,320
>> Partially Observable
Markov Decision Processes?

9
00:00:19,320 --> 00:00:22,170
>> Well we had been exposed to these
things before but we really kind of dove

10
00:00:22,170 --> 00:00:25,930
into sort of some detail about what
they are and how to solve them.

11
00:00:25,930 --> 00:00:26,815
And in particular,

12
00:00:26,815 --> 00:00:32,090
we've talked about belief states as kind
of a representation, which represents

13
00:00:32,090 --> 00:00:35,780
the states that we might be in and
how much we believe that were in them.

14
00:00:35,780 --> 00:00:39,286
We talked about PomDPs as
a strict generalization

15
00:00:39,286 --> 00:00:41,793
of MDPs which is sort of important.

16
00:00:41,793 --> 00:00:45,112
And that if we can solve a PomDP
that happens to be an MDP,

17
00:00:45,112 --> 00:00:47,710
we will solve the underlying MDP.

18
00:00:47,710 --> 00:00:52,810
One of the unfortunate consequences of
all that is that dealing with PomDPs

19
00:00:52,810 --> 00:00:57,500
is hard they're hard to solve they're
to sort of fundamentally difficult.

20
00:00:57,500 --> 00:00:59,160
I can't remember
the exact result now but

21
00:00:59,160 --> 00:01:02,210
there was something in
there about undecidability.

22
00:01:02,210 --> 00:01:04,830
>> Yeah.
>> And that just that was difficult.

23
00:01:04,830 --> 00:01:05,792
That was hard for me deal with.

24
00:01:05,792 --> 00:01:08,098
>> [LAUGH] It was hard for
you to accept hard to accept.

25
00:01:08,098 --> 00:01:09,841
[LAUGH]
>> Right,

26
00:01:09,841 --> 00:01:15,036
despite the fact that it's hard,
we did come up with some algorithms for

27
00:01:15,036 --> 00:01:17,780
solving MDPs like a value iteration.

28
00:01:17,780 --> 00:01:22,160
I remember something about
piecewise linear and convex.

29
00:01:22,160 --> 00:01:25,570
>> Right, and representing value
functions as piecewise linear and

30
00:01:25,570 --> 00:01:26,740
convex functions.

31
00:01:26,740 --> 00:01:31,052
>> Or pulc and one of the things that
was nice about that is that you show

32
00:01:31,052 --> 00:01:35,151
that you could build up these
sets of kind of linear functions.

33
00:01:35,151 --> 00:01:38,295
And because you only cared
about the max in a given point,

34
00:01:38,295 --> 00:01:41,834
you could throw away a bunch of
them that might be unnecessary and

35
00:01:41,834 --> 00:01:44,735
sort of keep the hard
problem possibly manageable.

36
00:01:44,735 --> 00:01:45,931
>> Good.
>> That was really nice.

37
00:01:45,931 --> 00:01:51,592
Then we went from the solving of PomDPs
to reinforcement learning with PomDPs,

38
00:01:51,592 --> 00:01:56,829
and if solving PomDPs is hard then RL
for PomDPs definitely has to be hard,

39
00:01:56,829 --> 00:02:01,380
and that turns out to be true but,
we tried to do it anyway.

40
00:02:01,380 --> 00:02:03,240
Let's see, we did memoryless.

41
00:02:03,240 --> 00:02:05,570
>> First we talked about
a model-based method.

42
00:02:05,570 --> 00:02:06,410
>> Right, model-based method.

43
00:02:06,410 --> 00:02:09,080
>> Where you try to actually
learn the PomDP from

44
00:02:09,080 --> 00:02:11,460
experience wandering around in it.

45
00:02:11,460 --> 00:02:14,290
And do you remember how we
how we did that learning?

46
00:02:14,290 --> 00:02:15,401
>> With great difficulty?

47
00:02:15,401 --> 00:02:19,170
>> Well sure, we talked about explicit
expectation maximization as a way of

48
00:02:19,170 --> 00:02:21,350
trying to organize that computation.

49
00:02:21,350 --> 00:02:24,782
>> Sure,
that's a favorite of my expectations.

50
00:02:24,782 --> 00:02:25,692
>> Your maximally favorite?

51
00:02:25,692 --> 00:02:27,359
>> [LAUGH] Yeah,
my maximally favorite one.

52
00:02:27,359 --> 00:02:30,052
We've got to go all the back and
try to remind ourselves how Worked.

53
00:02:30,052 --> 00:02:31,168
>> Yes.
>> That was fun.

54
00:02:31,168 --> 00:02:35,011
[LAUGH] And then we did memoryless?

55
00:02:35,011 --> 00:02:37,885
>> Yeah, and that we needed in
the memoryless case we might need to

56
00:02:37,885 --> 00:02:38,435
be random.

57
00:02:38,435 --> 00:02:40,540
>> Again, just like grad school.

58
00:02:40,540 --> 00:02:42,260
>> [LAUGH] I don't think that's true.

59
00:02:42,260 --> 00:02:43,990
You're saying that if
you're in grad school but

60
00:02:43,990 --> 00:02:48,350
you don't remember where you are in grad
school, then it helps to act randomly.

61
00:02:48,350 --> 00:02:50,449
>> Well it certainly explains
the behaviors on my grad.

62
00:02:51,480 --> 00:02:57,080
Okay, then we had, we probably did more
stuff there but I can't remember it.

63
00:02:57,080 --> 00:02:59,271
>> [LAUGH]
>> And so I do remember though that we

64
00:02:59,271 --> 00:03:01,650
started talking about Bayesian RL.

65
00:03:01,650 --> 00:03:02,252
>> Yes.

66
00:03:02,252 --> 00:03:06,402
And the cool thing about Bayesian RL
being that it blurs the line between

67
00:03:06,402 --> 00:03:08,850
planning and learning so.

68
00:03:08,850 --> 00:03:12,020
And Bayesian stuff does that in general,
this sort of notion that,

69
00:03:12,020 --> 00:03:16,670
well all learning really is is
estimating probabilities of things,

70
00:03:16,670 --> 00:03:22,036
it's not actually learning, so
just changing your posterior.

71
00:03:22,036 --> 00:03:26,780
And so, decision making in the Bayesian
RL setting ends up becoming just a kind

72
00:03:26,780 --> 00:03:30,820
of version of planning in
a continuous ugly kind of PomDP.

73
00:03:30,820 --> 00:03:34,490
>> Speaking of which, if we have
Bayesian RL, do we have frequentist RL?

74
00:03:34,490 --> 00:03:36,650
>> Well sure, I mean Bayesian and

75
00:03:36,650 --> 00:03:40,890
frequentist are kind
of rivals in a sense.

76
00:03:40,890 --> 00:03:44,239
So anything that's not
Bayesian maybe is frequentist.

77
00:03:44,239 --> 00:03:45,290
>> Okay.

78
00:03:45,290 --> 00:03:47,820
>> So all the other reinforcement
learning that we did like hue

79
00:03:47,820 --> 00:03:48,810
learning and whatnot.

80
00:03:48,810 --> 00:03:53,610
As it certainly has a frequentist flavor
in that what is frequentist statistics

81
00:03:53,610 --> 00:03:56,850
is about, it's about you know counting
up how many times something happens and

82
00:03:56,850 --> 00:03:59,010
divided by the number
of times you asked.

83
00:03:59,010 --> 00:04:03,420
And that is definitely, queue learning
has that flavor model based RL where

84
00:04:03,420 --> 00:04:06,070
you're trying to estimate
the parameters of the model by

85
00:04:06,070 --> 00:04:08,720
from experience they
tend to have that flavor.

86
00:04:08,720 --> 00:04:09,280
>> Okay, so good.

87
00:04:09,280 --> 00:04:11,580
So then we have been learning
about frequentist RL.

88
00:04:11,580 --> 00:04:12,860
And now that we have Bayesian RL,

89
00:04:12,860 --> 00:04:16,290
it reveals the frequentist things
that we'd been doing in the past.

90
00:04:16,290 --> 00:04:17,272
So I'm okay with that.

91
00:04:17,272 --> 00:04:20,485
>> Yeah and the Bayesian RL is very nice
because it doesn't make an exploration

92
00:04:20,485 --> 00:04:22,100
exploitation distinction anymore.

93
00:04:22,100 --> 00:04:26,650
You're always acting given your
knowledge of what the truth is and

94
00:04:26,650 --> 00:04:30,230
you're acting in a way that maximizes
your award given that knowledge.

95
00:04:30,230 --> 00:04:32,838
>> Right and so
exploration is exploitation.

96
00:04:32,838 --> 00:04:35,623
[SOUND]
>> Planning is learning.

97
00:04:35,623 --> 00:04:37,949
Exploration is exploitation.

98
00:04:37,949 --> 00:04:39,463
War is peace.

99
00:04:39,463 --> 00:04:45,730
>> [LAUGH] And we have always been
at war with supervised learning.
