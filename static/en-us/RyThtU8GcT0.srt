1
00:00:00,390 --> 00:00:04,169
Right, my next question is
about support vector machines.

2
00:00:04,169 --> 00:00:08,140
Let me draw a two dimensional
support vector machine scenario.

3
00:00:09,289 --> 00:00:12,959
Our data samples are laid
out on this 2D space.

4
00:00:12,960 --> 00:00:18,019
I'm going to use + signs for
the positive examples.

5
00:00:18,019 --> 00:00:20,789
And - signs for the negative examples.

6
00:00:22,030 --> 00:00:26,130
And as you might know support
vector machine tries to find

7
00:00:26,129 --> 00:00:30,289
like a boundary that
separates these two classes.

8
00:00:32,380 --> 00:00:37,080
Now my question for
you is what is the value or parameter or

9
00:00:37,079 --> 00:00:41,670
function that the support vector
machine is trying to optimize for,

10
00:00:41,670 --> 00:00:43,760
either minimize or maximize?

11
00:00:43,759 --> 00:00:49,181
>> The support vector machine is,
thank you,

12
00:00:49,182 --> 00:00:54,170
trying to operate on a boundary.

13
00:00:55,219 --> 00:00:59,839
And saying based on this
boundary on either side of where

14
00:00:59,840 --> 00:01:02,410
I decided to draw my division.

15
00:01:03,460 --> 00:01:06,560
I want to minimize the number
of misclassifications.

16
00:01:09,030 --> 00:01:14,439
Based on the training set,
where's the line I can draw such that

17
00:01:14,439 --> 00:01:18,899
I minimize the number of positives
on this side of the boundary, and

18
00:01:18,900 --> 00:01:22,410
negatives on the other
side of the boundary.

19
00:01:22,409 --> 00:01:24,489
>> That's a very good starting point.

20
00:01:24,489 --> 00:01:29,269
As you can see in this example, the
support vector machine still have a lot

21
00:01:29,269 --> 00:01:33,078
of freedom, in terms of where
it places the decision boundary.

22
00:01:35,079 --> 00:01:39,525
What else is it trying to look for
or strive for?

23
00:01:39,525 --> 00:01:44,190
>> Okay, so since there is a little
bit of freedom of movement like

24
00:01:44,189 --> 00:01:49,118
this line could have been drawn,
anywhere in sort of this space and

25
00:01:49,118 --> 00:01:53,977
still correctly classify all of
the data that we have in this set.

26
00:01:53,977 --> 00:01:59,303
We're going to try and
draw the line such that

27
00:01:59,304 --> 00:02:04,492
the distance between the classification,

28
00:02:04,492 --> 00:02:09,538
between these labels
is sort of optimal for

29
00:02:09,538 --> 00:02:12,550
all of these samples.

30
00:02:12,550 --> 00:02:18,320
>> Would you say that the algorithm is
trying to minimize these distances or

31
00:02:18,319 --> 00:02:20,959
maximize these distances?

32
00:02:20,960 --> 00:02:24,840
>> It's trying to keep the distance
from either side about the same.

33
00:02:24,840 --> 00:02:27,420
>> Okay, that's a very good point.

34
00:02:29,469 --> 00:02:34,969
In essence, the decision boundary
that comes up is sort of in between

35
00:02:37,240 --> 00:02:40,950
opposite examples that
are fairly close together.

36
00:02:42,810 --> 00:02:47,676
Cool, do you know what
the samples that are close

37
00:02:47,676 --> 00:02:51,483
to the decision boundary are called?

38
00:02:51,483 --> 00:02:53,051
>> No, I'm not familiar with that term.

39
00:02:53,050 --> 00:02:53,988
>> Okay.

40
00:02:53,989 --> 00:02:58,493
It's that's where sort of the term
support vector machine comes from

41
00:02:58,493 --> 00:03:02,759
because these points are referred
to as support vectors.

42
00:03:02,759 --> 00:03:07,227
In some sense they are supporting
the decision boundary.

43
00:03:07,227 --> 00:03:11,628
The other examples that you see
are not really affecting where

44
00:03:11,627 --> 00:03:13,072
the boundary lies.

45
00:03:13,073 --> 00:03:15,067
>> Okay.

46
00:03:15,067 --> 00:03:20,254
>> Cool, so one follow up
question would be what are some

47
00:03:20,253 --> 00:03:26,032
domains in which support vector
machines could be useful?

48
00:03:26,032 --> 00:03:31,239
>> Yeah, definitely, so support vector
machines have multiple use cases.

49
00:03:31,240 --> 00:03:37,730
Especially when your data clusters
around a certain value and

50
00:03:37,729 --> 00:03:45,604
it is easy to create a boundary between
multiple classifications of data.

51
00:03:45,604 --> 00:03:49,010
If this were, let's say,
an image classification and

52
00:03:49,010 --> 00:03:52,996
these were pictures of dolphins and
these were pictures of cats,

53
00:03:52,997 --> 00:03:56,773
we might see some very apparent
differences between the two.

54
00:03:56,772 --> 00:04:00,682
And the support vector machine
could easily determine

55
00:04:00,682 --> 00:04:03,120
the difference between the two.

56
00:04:03,120 --> 00:04:07,129
But let's say we were looking at
wolves and we're looking at dogs.

57
00:04:08,500 --> 00:04:13,219
We can imagine cases where
there is some very, there's

58
00:04:13,219 --> 00:04:16,810
dogs that look a lot like wolves and
wolves that maybe look a lot like dogs.

59
00:04:18,310 --> 00:04:21,870
There would be a lot of
misclassification along the boundary.

60
00:04:23,360 --> 00:04:28,449
And a way to work around that,
you could possibly see if you can tweak

61
00:04:28,449 --> 00:04:32,759
the kernel of your SVM such that instead
of let's say like a straight line,

62
00:04:32,759 --> 00:04:36,149
it can have some curvature to it, or

63
00:04:36,149 --> 00:04:38,929
try and use a different machine
learning algorithm completely.

64
00:04:40,000 --> 00:04:41,139
>> That's a very good point.

65
00:04:42,860 --> 00:04:46,490
SVMs, although they're very popular
in machine learning are still they

66
00:04:46,490 --> 00:04:49,420
can't be directly applied
to all scenarios.

67
00:04:49,420 --> 00:04:50,110
Very good.

68
00:04:50,110 --> 00:04:50,389
>> Thank you.
