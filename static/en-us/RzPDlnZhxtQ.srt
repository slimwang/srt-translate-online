1
00:00:00,000 --> 00:00:03,015
So now we've done all the preliminaries, so let's turn to some actual code.

2
00:00:03,015 --> 00:00:06,837
We're going to implement this twice with a similar strategy each time.

3
00:00:06,837 --> 00:00:11,659
In both we're going to implement a sum of a million--actually 2^20 elements,

4
00:00:11,659 --> 00:00:13,963
and we're going to do this in 2 stages.

5
00:00:13,963 --> 00:00:17,706
In the 1st stage we're going to launch 1024 blocks,

6
00:00:17,706 --> 00:00:23,833
each one of which will use 1024 threads to reduce 1024 elements.

7
00:00:23,833 --> 00:00:28,082
Each of those will produce 1 single item.

8
00:00:28,082 --> 00:00:31,210
So we're going to have 1024 items left when we're done.

9
00:00:31,210 --> 00:00:37,327
And we're going to launch 1 block to reduce the final 1024 elements into 1 single element.

10
00:00:37,327 --> 00:00:41,055
So I'll post all the code, of course. But the CPU side code is straightforward.

11
00:00:41,055 --> 00:00:44,867
Instead we're just going to take a look at the kernel. Let's see how that works.

12
00:00:44,867 --> 00:00:50,164
So each block is going to be responsible for a 1024 element chunk of floats,

13
00:00:50,164 --> 00:00:53,630
and we're going to run this loop within the kernel.

14
00:00:53,630 --> 00:00:59,003
On each iteration of this loop we're going to divide the active region in half.

15
00:00:59,003 --> 00:01:02,678
So on the 1st iteration, where we start with 1024 elements,

16
00:01:02,678 --> 00:01:06,544
we're going to have two 512-element regions.

17
00:01:06,544 --> 00:01:13,209
Then each of 512 threads will add its element in the 2nd half to its element in the 1st half,

18
00:01:13,209 --> 00:01:15,561
writing back to the 1st half.

19
00:01:15,561 --> 00:01:19,407
Now we're going to synchronize all threads, this syncthreads call right here,

20
00:01:19,407 --> 00:01:21,231
to make sure every one is done.

21
00:01:21,231 --> 00:01:24,015
We've got 512 elements remaining,

22
00:01:24,015 --> 00:01:27,798
and so we're going to loop again on this resulting region of 512 elements.

23
00:01:27,798 --> 00:01:33,613
Now we'll divide it into two 256-element chunks using 256 threads

24
00:01:33,613 --> 00:01:37,971
to sum these 256 items to these 256 items.

25
00:01:37,971 --> 00:01:41,819
And we're going to continue this loop, cutting it in half every time,

26
00:01:41,819 --> 00:01:46,967
until we have 1 element remaining at the very end of 10 iterations.

27
00:01:46,967 --> 00:01:50,659
And then we'll write that back out to global memory. So this works.

28
00:01:50,659 --> 00:01:52,784
We can run it on a computer in our lab.

29
00:01:52,784 --> 00:01:58,933
So we're doing that now, and we notice that it finishes in 0.65 milliseconds.

30
00:01:58,933 --> 00:02:04,082
Less than a millisecond. That's pretty great, but it's not as efficient as we might like.

31
00:02:04,082 --> 00:02:06,786
Specifically, if we take a look at the code again,

32
00:02:06,786 --> 00:02:10,441
we're going to global memory more often than we'd like.

33
00:02:10,441 --> 00:02:16,117
On each iteration of the loop, we read n items from global memory and we write back n/2 items.

34
00:02:16,117 --> 00:02:20,479
Then we read those n/2 items back from global memory and so on.

35
00:02:20,479 --> 00:02:27,778
In an ideal world, we'd do an original read where we read all of the 1024 items into the thread block,

36
00:02:27,778 --> 00:02:31,286
do all the reduction internally, and then write back the final value.

37
00:02:31,286 --> 00:02:35,700
And this should be faster because we would incur less memory traffic overall.

38
00:02:35,700 --> 00:02:38,387
The CUDA feature we use to do this is called shared memory

39
00:02:38,387 --> 00:02:42,651
and will store all intermediate values in shared memory where all threads can access them.

40
00:02:42,651 --> 00:02:45,645
Shared memory is considerably faster than global memory.

41
00:02:45,645 --> 00:02:49,332
So let's take a look at the kernel. It's going to look very similar.

42
00:02:49,332 --> 00:02:53,117
And in this kernel we're going to have the exact same loop structure.

43
00:02:53,117 --> 00:02:56,389
What's going to be different, though, is this little part right here.

44
00:02:56,389 --> 00:03:01,134
We have to 1st copy all the values from global memory into shared memory

45
00:03:01,134 --> 00:03:03,387
and that's done with this little block.

46
00:03:03,387 --> 00:03:06,932
And then all the further accesses here are from shared memory--

47
00:03:06,932 --> 00:03:11,203
this s data--as opposed to from global memory, which we did last time.

48
00:03:11,203 --> 00:03:15,260
And when we're done, we have to write this final value back to global memory again.

49
00:03:15,260 --> 00:03:19,879
The only other interesting part of this code is how we declare the amount of shared memory we need.

50
00:03:19,879 --> 00:03:21,481
And we do that here.

51
00:03:21,481 --> 00:03:26,447
We're declaring that we're going to have an externally defined amount of shared data.

52
00:03:26,447 --> 00:03:28,559
Now, we haven't actually said how much we do,

53
00:03:28,559 --> 00:03:32,058
so to do that, we're going to have to go down to where we actually call the kernel.

54
00:03:32,058 --> 00:03:35,372
So when we're calling the reduce kernel using the shared memory,

55
00:03:35,372 --> 00:03:40,606
we call it with now 3 arguments inside the triple chevrons, the normal blocks and the threads,

56
00:03:40,606 --> 00:03:45,180
but then we say how many bytes we need allocated in shared memory.

57
00:03:45,180 --> 00:03:50,899
In this case, every thread is going to ask for 1 float stored in shared memory.

58
00:03:50,899 --> 00:03:55,414
So the advantage of the shared memory version is that it saves global memory bandwidth.

59
00:03:55,414 --> 00:03:58,679
It's a good exercise to figure out how much memory bandwidth you'll save.

60
00:03:58,679 --> 00:04:00,421
So I'll ask that as a quiz.

61
00:04:00,421 --> 00:04:03,389
The global memory version uses how many times as much memory bandwidth

62
00:04:03,389 --> 00:04:05,023
as the shared memory version?

63
00:04:05,023 --> 00:04:06,866
Round to the nearest integer.
