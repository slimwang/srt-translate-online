1
00:00:00,300 --> 00:00:03,130
Next, we'll talk with Dr.
Juergen Fritsch.

2
00:00:03,130 --> 00:00:06,965
After earning a Ph.D degree at
Carnegie-Mellon University,

3
00:00:06,965 --> 00:00:09,040
he was a co-founder of M*Modal.

4
00:00:09,040 --> 00:00:11,860
Today, he's the company's
chief scientist.

5
00:00:11,860 --> 00:00:14,520
Juergen, thank you very much for
being with us today.

6
00:00:14,520 --> 00:00:16,970
>> Thanks for having me.

7
00:00:16,970 --> 00:00:20,190
>> You are the chief technology
officer of M*Modal, I believe.

8
00:00:20,190 --> 00:00:21,670
>> Chief scientist, actually.

9
00:00:21,670 --> 00:00:25,460
>> And the company which, as you know,

10
00:00:25,460 --> 00:00:30,060
I've been interested in for
years, was founded in 2001.

11
00:00:30,060 --> 00:00:34,030
But, it was quite a number of years
before the technology was built into

12
00:00:34,030 --> 00:00:38,220
electronic medical records systems for
direct use by physicians.

13
00:00:38,220 --> 00:00:42,710
Could you explain why
that happened that way?

14
00:00:42,710 --> 00:00:47,100
And more specifically,
how you went about training the system?

15
00:00:47,100 --> 00:00:47,920
>> Yes.
So

16
00:00:47,920 --> 00:00:51,120
this was actually very
intentional on our part.

17
00:00:51,120 --> 00:00:54,260
When we founded the company,
we were looking for

18
00:00:54,260 --> 00:00:59,000
an application of speech technologies
that would tolerate the error rate

19
00:00:59,000 --> 00:01:02,080
that's still present
in today's technology.

20
00:01:02,080 --> 00:01:07,470
If you're familiar with telephony
systems and other speech enabled

21
00:01:07,470 --> 00:01:11,540
dialogue systems, you're familiar with
how annoying it can be to an end user to

22
00:01:11,540 --> 00:01:15,160
have to deal with misinterpretations and
errors by the technology.

23
00:01:15,160 --> 00:01:16,590
So instead of doing that and

24
00:01:16,590 --> 00:01:20,060
going straight to the end users,
we decided to go a different path.

25
00:01:20,060 --> 00:01:24,170
And we used the technology
on the back end first.

26
00:01:24,170 --> 00:01:28,520
And we did this by enabling
an existing workflow,

27
00:01:28,520 --> 00:01:29,940
based on dictation and transcription.

28
00:01:29,940 --> 00:01:34,580
So in the US, there is billions of lines
of documentation created in healthcare

29
00:01:34,580 --> 00:01:38,660
every day or every year through
dictation and transcription workflows.

30
00:01:38,660 --> 00:01:42,770
And we enable those dictation and
transcription workflows, specifically

31
00:01:42,770 --> 00:01:46,260
the transcriptionists that were doing
that, to be much more productive.

32
00:01:46,260 --> 00:01:49,770
In that we produced a draft
document that they can review and

33
00:01:49,770 --> 00:01:55,210
correct, and thereby be twice as fast as
if they had to type it all from scratch.

34
00:01:55,210 --> 00:01:59,628
So that was an immediate use case for
the technology.

35
00:01:59,628 --> 00:02:01,400
In other words,
an ROI attached to it and

36
00:02:01,400 --> 00:02:03,220
it allowed us to build up a business.

37
00:02:03,220 --> 00:02:05,420
And at the same time,
collect huge amounts of data.

38
00:02:05,420 --> 00:02:10,380
We collected billions of audio
recordings and got them transcribed,

39
00:02:10,380 --> 00:02:13,970
corrected if you will, by these
transcriptionist as part of the process.

40
00:02:13,970 --> 00:02:20,690
So it was a fairly genius way to
basically collect the data, including

41
00:02:20,690 --> 00:02:24,960
correct the transcripts, to then train
the system to get better and better.

42
00:02:24,960 --> 00:02:27,470
And after doing this for
five, six, seven years,

43
00:02:27,470 --> 00:02:28,850
we got it to the point where it was so

44
00:02:28,850 --> 00:02:32,810
accurate that we could enable end users
to actually use it in the front end.

45
00:02:32,810 --> 00:02:35,160
And use it for
the biggest paycheck commission.

46
00:02:35,160 --> 00:02:38,160
So, when I first became
aware of M*Modal and

47
00:02:38,160 --> 00:02:40,670
traveled to your office
in Pittsburgh to see it.

48
00:02:41,830 --> 00:02:46,900
One of the most striking aspects of
the system to me was it's ability to

49
00:02:46,900 --> 00:02:55,200
go into the text that it had created
from the verbal transcriptions.

50
00:02:55,200 --> 00:02:58,580
Find clinical concepts, and
encode them into SNOMED.

51
00:02:58,580 --> 00:03:03,080
Now, the students are familiar with
SNOMED as we have this interview.

52
00:03:03,080 --> 00:03:06,990
They know how complicated it is, or
at least they have an idea of that.

53
00:03:06,990 --> 00:03:09,660
So how hard was it to actually do that?

54
00:03:09,660 --> 00:03:11,010
How technically challenging?

55
00:03:12,140 --> 00:03:13,250
>> It was fairly challenging.

56
00:03:14,770 --> 00:03:20,200
Apart from the usual challenges with
any natural language processing system.

57
00:03:20,200 --> 00:03:24,880
We had to deal with the complexities of
the ontology, of the SNOMED ontology,

58
00:03:24,880 --> 00:03:27,340
and with the medical
terminology to begin with.

59
00:03:27,340 --> 00:03:30,800
So, in addition to dealing with
things like syntax, semantics,

60
00:03:30,800 --> 00:03:35,340
pragmatics, and just the context
of the entire dialog happening.

61
00:03:35,340 --> 00:03:42,350
We had to also deal with similar words,
words that have different meanings.

62
00:03:42,350 --> 00:03:44,720
Or, it can have different
meanings in different contexts,

63
00:03:44,720 --> 00:03:45,840
things like that in healthcare.

64
00:03:46,910 --> 00:03:50,500
Just a simple term like cold, it can
have like three different meanings.

65
00:03:50,500 --> 00:03:52,880
It can be abbreviated COLD and
it's a disease.

66
00:03:52,880 --> 00:03:55,830
It could be the common cold, or
it just could be feeling cold.

67
00:03:55,830 --> 00:03:59,370
So there is all sorts of
disambiguation that needed to happen.

68
00:03:59,370 --> 00:04:01,970
And needed to be accurate
in order to codify

69
00:04:01,970 --> 00:04:04,240
these terms correctly
to their SNOMED codes.

70
00:04:04,240 --> 00:04:07,560
>> So how accurate is it at this point?

71
00:04:07,560 --> 00:04:10,304
>> That's a tough question to answer
because it really depends a lot on

72
00:04:10,304 --> 00:04:11,280
the use case.

73
00:04:11,280 --> 00:04:15,580
We are not attempting to codify
to the entirety of SNOMED,

74
00:04:15,580 --> 00:04:18,640
to the hundreds of thousands or
300,000 concepts in SNOMED.

75
00:04:18,640 --> 00:04:21,519
But, we're focusing on specific
subsets based on a use case.

76
00:04:21,519 --> 00:04:26,510
So one of the use cases we're pursuing
is chronic medical conditions.

77
00:04:26,510 --> 00:04:29,990
Things like diabetes or heart diseases.

78
00:04:29,990 --> 00:04:35,600
And in that realm, if you focus on
a particular subset of diseases and

79
00:04:35,600 --> 00:04:39,040
remodeling those with a data model
that's not just including the disease.

80
00:04:39,040 --> 00:04:42,410
But, also the temporality and
the specificity,

81
00:04:42,410 --> 00:04:45,500
and who is concerned with that disease.

82
00:04:45,500 --> 00:04:46,920
You can be extremely accurate.

83
00:04:46,920 --> 00:04:49,281
You can be up in the high
90s in discovering and

84
00:04:49,281 --> 00:04:52,190
detecting these things in
all sorts of contexts.

85
00:04:52,190 --> 00:04:57,070
But if you would attempt to codify each
and every concept in the SNOMED corpus

86
00:04:57,070 --> 00:05:02,600
and the SNOMED ontology, you would be
ending up in a much lower accuracy rate.

87
00:05:02,600 --> 00:05:05,720
And it's just not happening today,

88
00:05:05,720 --> 00:05:09,360
that people would automatically
codify to the entirety of SNOMED.

89
00:05:09,360 --> 00:05:11,720
I have not seen anybody
doing that accurately.

90
00:05:11,720 --> 00:05:14,550
And I think you really don't need to,
because at the end,

91
00:05:14,550 --> 00:05:16,410
you're always pursuing
a particular use case.

92
00:05:16,410 --> 00:05:18,160
You're always trying to
solve a particular problem.

93
00:05:18,160 --> 00:05:21,390
And for that, you really only
need a subset of that corpus.

94
00:05:21,390 --> 00:05:27,530
>> So what can you tell us about
the technology that's under the surface?

95
00:05:27,530 --> 00:05:33,740
What languages and databases,
and approaches are you using?

96
00:05:33,740 --> 00:05:35,490
>> So let's start on the speech side.

97
00:05:35,490 --> 00:05:41,400
As your students might know, all the
speech recognition, speech understanding

98
00:05:41,400 --> 00:05:45,710
systems that are out there today
are based on statistical algorithms.

99
00:05:45,710 --> 00:05:47,685
So based on machine learning,

100
00:05:47,685 --> 00:05:50,990
statistically-trained machine
learning algorithms.

101
00:05:50,990 --> 00:05:53,560
And there's no difference
here in our company.

102
00:05:53,560 --> 00:05:56,350
We do use those, a variety of those,

103
00:05:56,350 --> 00:06:00,420
starting with hidden Markov models,
neural networks, regression models.

104
00:06:01,810 --> 00:06:05,560
Even the more complex machine learning
algorithms that are on the market today,

105
00:06:05,560 --> 00:06:06,510
or that have been used.

106
00:06:06,510 --> 00:06:10,350
But we combine them in a fairly unique
way, in that we're not just seeing it

107
00:06:10,350 --> 00:06:14,250
as a pattern matching problem as
most speech recognition systems do.

108
00:06:14,250 --> 00:06:17,390
But, we combine speech technologies
with natural language processing

109
00:06:17,390 --> 00:06:18,450
technologies.

110
00:06:18,450 --> 00:06:21,650
The theory is that if you get
a little bit of an understanding of

111
00:06:21,650 --> 00:06:22,970
what you're trying to recognize,

112
00:06:22,970 --> 00:06:26,430
you can do better at the acoustic
identification of the sounds.

113
00:06:26,430 --> 00:06:27,330
And vice versa, right?

114
00:06:27,330 --> 00:06:30,880
So it's a constraint satisfaction
problem that you're solving.

115
00:06:30,880 --> 00:06:34,410
And the more knowledge sources you
bring in, the more accurate you can be.

116
00:06:34,410 --> 00:06:38,762
So we do combine syntactic power servers
and annotators, and cross taggers with

117
00:06:38,762 --> 00:06:43,200
the actual hidden Markov models new and
networks that do the speech recognition.

118
00:06:43,200 --> 00:06:48,060
And altogether by combing them,
achieve better results that way.

119
00:06:48,060 --> 00:06:51,320
When it comes to
identifying concepts and

120
00:06:51,320 --> 00:06:56,610
tagging them in a medical corpus such
as SNOMED, we use a lot of classifiers.

121
00:06:56,610 --> 00:07:00,880
We use anything from a statistical
classifier such as even the hidden

122
00:07:00,880 --> 00:07:05,180
Markov model, all the way to maximum
entropy models and things like that.

123
00:07:06,880 --> 00:07:12,100
>> I know you've begun to incorporate
M*Modal into commercial EMRs,

124
00:07:12,100 --> 00:07:15,280
I believe Greenway here
locally was the first.

125
00:07:16,720 --> 00:07:17,840
How's that going?

126
00:07:17,840 --> 00:07:19,510
>> It's going really well.

127
00:07:19,510 --> 00:07:24,000
Initially, there was a little bit of
a reluctance by the EMR community,

128
00:07:24,000 --> 00:07:27,720
I would say, to even incorporate speech
technologies into their systems.

129
00:07:27,720 --> 00:07:33,010
There was this perception that
physicians will eventually converge

130
00:07:33,010 --> 00:07:37,900
on structural data entry, and
will accept the drop down menus and

131
00:07:37,900 --> 00:07:40,130
the choice lists that they
have in their systems.

132
00:07:40,130 --> 00:07:41,210
But, the opposite did happen.

133
00:07:41,210 --> 00:07:43,280
The physicians were pushing
back harder and harder, and

134
00:07:43,280 --> 00:07:45,260
saying we need to
preserve the narrative.

135
00:07:45,260 --> 00:07:48,160
Not just from a productivity
perspective, they

136
00:07:48,160 --> 00:07:51,190
don't just want to be more productive
and faster in entering the data.

137
00:07:51,190 --> 00:07:52,750
But, they also want to tell a story.

138
00:07:52,750 --> 00:07:55,990
They want to be able to explain
what they thought about,

139
00:07:55,990 --> 00:07:58,470
what the thought process
was in the diagnosis.

140
00:07:58,470 --> 00:08:01,910
Why they tried certain things, and
why they worked or didn't work.

141
00:08:01,910 --> 00:08:05,620
And that, you just simply cannot
do by picking from a choice list.

142
00:08:05,620 --> 00:08:09,220
You have to be able to
narrate a paragraph or two.

143
00:08:09,220 --> 00:08:12,990
And so, this need for, not just speech
technologies, but just a narrative.

144
00:08:12,990 --> 00:08:16,410
A way to answer a narrative, whether
it's typing, speech recognition, or

145
00:08:16,410 --> 00:08:19,650
other ways, was really something
that they pushed hard for.

146
00:08:19,650 --> 00:08:21,820
The EMR community has come around.

147
00:08:23,000 --> 00:08:27,100
We're not aware of a single big vendor,
whether it's Epic,

148
00:08:27,100 --> 00:08:31,420
Cerner, Greenway Athena, or Allscripts.

149
00:08:31,420 --> 00:08:35,610
All these companies have come around,
and are now heavily investing into

150
00:08:35,610 --> 00:08:40,200
preserving a narrative part of the chart
of the physician's documentation.

151
00:08:40,200 --> 00:08:42,880
And so as part of that,
they are all including

152
00:08:42,880 --> 00:08:45,680
some sort of a speech technology for
authoring these things.

153
00:08:45,680 --> 00:08:46,690
But also, they are more and

154
00:08:46,690 --> 00:08:50,760
more incorporating natural language
processing technologies in order

155
00:08:50,760 --> 00:08:53,030
to search the narrative and
find the relevant information.

156
00:08:53,030 --> 00:08:57,170
And combine it with the structure
data that they have.

157
00:08:57,170 --> 00:09:00,679
>> When you look forward,
beyond where we are today,

158
00:09:00,679 --> 00:09:04,516
at the future of healthcare,
and the potential role and

159
00:09:04,516 --> 00:09:09,010
future of voice recognition
technologies such as yours.

160
00:09:09,010 --> 00:09:10,530
What do you see?

161
00:09:10,530 --> 00:09:12,580
>> I wouldn't limit it to
just the speech technologies.

162
00:09:12,580 --> 00:09:14,990
I think for me, again,
it's the narrative.

163
00:09:14,990 --> 00:09:17,680
And what I find exciting in
healthcare is that there is this

164
00:09:17,680 --> 00:09:22,330
renaissance almost, of the narrative
in physician documentation.

165
00:09:22,330 --> 00:09:23,510
It started out with that.

166
00:09:23,510 --> 00:09:27,740
Since 100 years ago, physicians
have been writing and scribbling.

167
00:09:27,740 --> 00:09:29,980
And really telling a story
about their patients.

168
00:09:29,980 --> 00:09:34,620
And then, it was kind of on the verge
of being abandoned with all these

169
00:09:34,620 --> 00:09:39,150
initial EMR systems,
using drop down menus, pick lists.

170
00:09:39,150 --> 00:09:40,470
And now, you see the opposite happening.

171
00:09:40,470 --> 00:09:44,260
Again, physicians are creating
much richer storyline and

172
00:09:44,260 --> 00:09:46,170
narrative around their patients.

173
00:09:46,170 --> 00:09:48,770
>> And the more these
technologies mature, the more,

174
00:09:48,770 --> 00:09:50,130
the better the speech recognition gets,

175
00:09:50,130 --> 00:09:53,530
the better the natural language
processing taggers and identifiers get.

176
00:09:53,530 --> 00:09:57,000
The more use cases you see appearing.

177
00:09:57,000 --> 00:10:00,890
And to me, the exciting thing is that
we can get to real evidence-based

178
00:10:00,890 --> 00:10:02,350
medicine in the next few years.

179
00:10:02,350 --> 00:10:06,000
Where you can use these millions and
millions of

180
00:10:06,000 --> 00:10:09,650
clinical documents that the physicians
are creating in any given hospital.

181
00:10:09,650 --> 00:10:10,530
That we can use those.

182
00:10:10,530 --> 00:10:12,160
That we can scan those.

183
00:10:12,160 --> 00:10:13,230
That we can find trends.

184
00:10:13,230 --> 00:10:17,150
That we can tell exactly for this
patient of that gender, of that age,

185
00:10:17,150 --> 00:10:19,650
with that disease and that outcome.

186
00:10:19,650 --> 00:10:21,880
This medication worked or
this medication didn't work, and

187
00:10:21,880 --> 00:10:23,050
in what context.

188
00:10:23,050 --> 00:10:26,160
And we can use that information
to derive better treatments,

189
00:10:26,160 --> 00:10:30,460
and more accurate diagnoses for
other patients.

190
00:10:30,460 --> 00:10:35,440
And I think that's when you really have
reached that evidence-based medicine

191
00:10:35,440 --> 00:10:39,130
realm that we're all talking about,
and really haven't achieved yet.

192
00:10:39,130 --> 00:10:42,990
>> Yeah, I think there's a very
exciting landscape that you portray.

193
00:10:42,990 --> 00:10:47,040
I hope the students remember
these comments when in Lesson 1.

194
00:10:47,040 --> 00:10:51,020
We look at some of the contemporary
research efforts in things like clinical

195
00:10:51,020 --> 00:10:52,550
student support.

196
00:10:52,550 --> 00:10:53,840
>> So thank you very much.

197
00:10:53,840 --> 00:10:54,470
>> Pleasure.

198
00:10:54,470 --> 00:10:58,110
>> I'm thrilled to see how far you guys
have come since I first visited you,

199
00:10:58,110 --> 00:11:00,330
I don't remember how many years ago.

200
00:11:00,330 --> 00:11:03,150
And perhaps,
we can do this interview again in two or

201
00:11:03,150 --> 00:11:06,100
three years, and
see what you've done then.

202
00:11:06,100 --> 00:11:07,030
>> Any time, my pleasure.

203
00:11:07,030 --> 00:11:08,080
>> Thank you.

204
00:11:08,080 --> 00:11:08,580
>> Thank you, Mark.
