1
00:00:00,025 --> 00:00:04,995
Finally we need to make some assumptions in order to use one way Anova. The

2
00:00:04,995 --> 00:00:09,421
first in normality. All the populations from which the samples are from are

3
00:00:09,421 --> 00:00:14,837
normally distributed. Another is homogeneity of variance. The data come from

4
00:00:14,837 --> 00:00:19,325
populations that have equal amounts of variability and finally independence of

5
00:00:19,325 --> 00:00:24,985
observations. The results found from one sample won't affect the others.

6
00:00:24,985 --> 00:00:31,175
However, we can violate these assumptions under certain conditions. We can

7
00:00:31,175 --> 00:00:35,872
violate the normality assumption if the sample size is large. We can violate

8
00:00:35,872 --> 00:00:41,185
the homogeneity of variance assumption. If all the samples have nearly equal

9
00:00:41,185 --> 00:00:47,302
sample sizes, and the ratio of any two variances does not exceed 4. We have to

10
00:00:47,302 --> 00:00:51,019
maintain independence of observations but we can use randon assignment to

11
00:00:51,019 --> 00:00:56,154
conditions to help up meet this assumption. Let's do a quick summary of ANOVA

12
00:00:56,154 --> 00:01:00,748
to wrap up this lesson. I'm not actually going to rap this time though, we're

13
00:01:00,748 --> 00:01:05,935
just going to wrap up the lesson. If we have three or more samples and we want

14
00:01:05,935 --> 00:01:10,022
to know if any two of them are significantly different, we look at both the

15
00:01:10,022 --> 00:01:16,637
between group variability, and the within group variability. Between group

16
00:01:16,637 --> 00:01:21,181
variability is a measure of how spaced apart these sample means are from each

17
00:01:21,181 --> 00:01:27,636
other. And we do that by finding the grand mean and each squared deviation from

18
00:01:27,636 --> 00:01:33,721
the grand mean for each sample mean. We multiply each sample size by the

19
00:01:33,721 --> 00:01:40,919
squared deviation of each sample mean from the grand mean. Then we add them up.

20
00:01:40,919 --> 00:01:46,295
Then we have to look at the within group variability which is essentially the

21
00:01:46,295 --> 00:01:54,030
square deviation of each value in each sample from the respective sample mean.

22
00:01:54,030 --> 00:01:59,007
So we add up all sums of squares from their respective sample mean and then we

23
00:01:59,007 --> 00:02:03,984
have to find the average sum of squares for each by dividing by the degrees of

24
00:02:03,984 --> 00:02:11,599
freedom. In the case of the between groups, this is the number of samples minus

25
00:02:11,599 --> 00:02:16,527
1, and for within groups this is the total number of values minus the number of

26
00:02:16,527 --> 00:02:23,860
groups. This is the same as adding the degrees of freedom for each group. There

27
00:02:23,860 --> 00:02:28,219
we have our F statistic. And if it falls out here in the critical region, past

28
00:02:28,219 --> 00:02:33,343
the F critical value, we'll reject the null. After making a statistical

29
00:02:33,343 --> 00:02:37,745
decision, we can use the multiple comparison test, one of which is Tukey's

30
00:02:37,745 --> 00:02:43,824
Honestly Significant Difference. Which is a value that if any two sample means

31
00:02:43,824 --> 00:02:47,118
have a difference greater than that value, they're considered honestly

32
00:02:47,118 --> 00:02:51,974
significantly different. You've also learned how to determine what proportion

33
00:02:51,974 --> 00:02:56,169
of the difference between mean is due the independent variable. You've also

34
00:02:56,169 --> 00:02:58,313
learned how to determine what proportion of the difference between means is due

35
00:02:58,313 --> 00:03:02,209
the independent variable. That's theta squared and that's a wrap.
