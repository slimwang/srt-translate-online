1
00:00:00,250 --> 00:00:03,300
Now as a programmer, you have a certain expectation

2
00:00:03,300 --> 00:00:06,150
as you add more processors to the system. Your

3
00:00:06,150 --> 00:00:08,685
expectation is natural if you think that if you

4
00:00:08,685 --> 00:00:11,935
add, if you add more processors your performance should go

5
00:00:11,935 --> 00:00:14,408
up. So this is expectation. This is what is

6
00:00:14,408 --> 00:00:17,842
called scalability. That the performance of a parallel machine

7
00:00:17,842 --> 00:00:20,146
is going to scale up as you increase the number

8
00:00:20,146 --> 00:00:25,480
of processes. Reasonable. Reasonable to expect that. However, I mentioned

9
00:00:25,480 --> 00:00:28,160
just now that the overhead associated with

10
00:00:28,160 --> 00:00:30,070
increasing the number of processes in terms

11
00:00:30,070 --> 00:00:33,300
of maintaining cache coherence when you have

12
00:00:33,300 --> 00:00:37,360
sharing that is happening for shared data. And

13
00:00:37,360 --> 00:00:42,790
so therefore, the pro in adding more processors is the fact that you can

14
00:00:42,790 --> 00:00:45,710
exploit parallelism. That's the reason why you're

15
00:00:45,710 --> 00:00:50,510
able to get this expectation of increased performance

16
00:00:50,510 --> 00:00:54,850
with processors. But unfortunately, as you increase the

17
00:00:54,850 --> 00:00:58,760
number of processors, there is increased overhead. And,

18
00:00:58,760 --> 00:01:01,520
and the increased overhead also grows. As you

19
00:01:01,520 --> 00:01:05,000
increase the number of processors more, overhead is

20
00:01:05,000 --> 00:01:10,070
going to be incurred by the system. If we have an eight processor SMP the

21
00:01:10,070 --> 00:01:15,540
overhead for cash coherence is less than if we have a 16 processor SMP or a 32

22
00:01:15,540 --> 00:01:17,722
processor or a 64 processor, so the overhead is

23
00:01:17,722 --> 00:01:22,400
going to grow. As a result, you can see that you

24
00:01:22,400 --> 00:01:25,390
have the pro of exploiting parallelism but you have

25
00:01:25,390 --> 00:01:28,220
the con of increased overhead and you end up with

26
00:01:28,220 --> 00:01:31,680
an actual performance that's somewhere in the middle between

27
00:01:31,680 --> 00:01:34,860
your expectation and the overhead. So, in some sense this

28
00:01:34,860 --> 00:01:37,760
is a difference between what your expectation is and

29
00:01:37,760 --> 00:01:41,080
what the overhead you're paying. And that becomes the actual

30
00:01:41,080 --> 00:01:44,910
delivered performance of a parallel machine. And this

31
00:01:44,910 --> 00:01:47,790
is very important to remember, that your delivered

32
00:01:47,790 --> 00:01:51,590
performance may not necessarily be linear in the

33
00:01:51,590 --> 00:01:53,380
number of processors that you add to the

34
00:01:53,380 --> 00:01:56,680
system. So what should we do to get

35
00:01:56,680 --> 00:02:00,500
good performance? Don't share memory across threads as

36
00:02:00,500 --> 00:02:03,190
much as possible, If you want good performance

37
00:02:03,190 --> 00:02:06,120
from the parallel machine. A quote that is attributed

38
00:02:06,120 --> 00:02:08,180
to a famous computer scientist Chuck Thacker

39
00:02:08,180 --> 00:02:12,700
comes to mind, shared memory machines scale well

40
00:02:12,700 --> 00:02:15,870
when you don't share memory. Of course as

41
00:02:15,870 --> 00:02:18,790
operating system designers, we have no control over

42
00:02:18,790 --> 00:02:21,800
what the application programmer does. All we can

43
00:02:21,800 --> 00:02:25,020
do is to ensure that the users share

44
00:02:25,020 --> 00:02:27,470
data structures is kept to a minimum and

45
00:02:27,470 --> 00:02:31,030
the implementation of the oppressing system caught itself.

46
00:02:31,030 --> 00:02:35,690
You will see how relevant Chuck Thackers quote is as we visit operating

47
00:02:35,690 --> 00:02:40,040
system synchronization, communication and scheduling algotithums and

48
00:02:40,040 --> 00:02:42,680
more generally. The structure of the operating

49
00:02:42,680 --> 00:02:46,670
system itself in this lesson. See if you can remind yourself of this

50
00:02:46,670 --> 00:02:51,910
quote, and how often it permeates our discussion as we go through this lesson.
