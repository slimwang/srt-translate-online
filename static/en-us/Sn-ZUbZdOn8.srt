1
00:00:00,000 --> 00:00:04,000
Let's look at a very simple piece of code that implements this planning algorithm.

2
00:00:04,000 --> 00:00:09,000
We have a grid here as before with 0s and 1s. You're familiar with it.

3
00:00:09,000 --> 00:00:13,000
The start location on the top left, the goal location on the bottom right.

4
00:00:13,000 --> 00:00:19,000
We can set up arbitrary obstacles like a wall over here and a wall over here

5
00:00:19,000 --> 00:00:23,000
that forces the robot into kind of an S-curve around the corner.

6
00:00:23,000 --> 00:00:29,000
If I run this code with this table, what I get is a table that looks like this.

7
00:00:29,000 --> 00:00:33,000
This for each of the states that is a non-wall state.

8
00:00:33,000 --> 00:00:35,000
It tells me what's the optimal thing to do.

9
00:00:35,000 --> 00:00:40,000
So over here it says go south. Here is says go right. Here is says go up.

10
00:00:40,000 --> 00:00:43,000
Here is says go right again and go south.

11
00:00:43,000 --> 00:00:46,000
Realize that even states I'm not likely to every reach,

12
00:00:46,000 --> 00:00:49,000
like the one over here and the on over here, have an optimal policy

13
00:00:49,000 --> 00:00:53,000
and action associated, because there is really no start state.

14
00:00:53,000 --> 00:00:55,000
There is just a goal state over here.

15
00:00:55,000 --> 00:01:00,000
The specification of the initial state has no bearing on this result.

16
00:01:00,000 --> 00:01:04,000
How can we compute this efficiently?

17
00:01:04,000 --> 00:01:09,000
Let me make a simple example of a world like this with an obstacle over here.

18
00:01:09,000 --> 00:01:12,000
Say our goal state is the one in the corner over here.

19
00:01:12,000 --> 00:01:15,000
Rather than telling you how to compute the optimal policy,

20
00:01:15,000 --> 00:01:18,000
which assigns an action to each of these cells,

21
00:01:18,000 --> 00:01:20,000
let me instead teach you about "value."

22
00:01:20,000 --> 00:01:28,000
A "value function" associates to each grid cell the length of the shortest path to the goal.

23
00:01:28,000 --> 00:01:31,000
For the goal, obviously, it is 0.

24
00:01:31,000 --> 00:01:34,000
For each adjacent cell to the goal, it's obviously 1.

25
00:01:34,000 --> 00:01:44,000
For the guys over here, 2, 3, 4, 5, 6, and 7.

26
00:01:44,000 --> 00:01:51,000
This is recursively calculated by taking the optimal neighbor x-prime, y-prime,

27
00:01:51,000 --> 00:01:56,000
considering its value, and by adding the costs it takes to get there,

28
00:01:56,000 --> 00:01:59,000
which in our example will be plus 1.

29
00:01:59,000 --> 00:02:02,000
By applying this update equation recursively,

30
00:02:02,000 --> 00:02:04,000
we can attain this value function over here.

31
00:02:04,000 --> 00:02:08,000
Once we have this value function, we find that the optimal control action

32
00:02:08,000 --> 00:02:15,000
is obtained by minimizing the value, which is a hill-climbing type of action.

33
00:02:15,000 --> 00:02:18,000
Let me give you a quiz.

34
00:02:18,000 --> 00:02:22,000
In this world here with the goal location over here,

35
00:02:22,000 --> 99:59:59,999
I'd like to understand what is the value of the cell in the bottom right.
