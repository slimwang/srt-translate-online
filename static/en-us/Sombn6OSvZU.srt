1
00:00:00,440 --> 00:00:03,210
Does splitting your data into
three separate data sets

2
00:00:03,210 --> 00:00:04,990
sound overly complicated?

3
00:00:04,990 --> 00:00:07,660
Let's look at this in
action in the real world.

4
00:00:07,660 --> 00:00:10,970
Kaggle is a competition platform for
machine learning.

5
00:00:10,970 --> 00:00:13,336
People compete on classification tasks,
and

6
00:00:13,336 --> 00:00:15,648
whoever gets the highest
performance wins.

7
00:00:15,648 --> 00:00:20,147
Here, I'm showing the example of
a scientific competition on data that

8
00:00:20,147 --> 00:00:21,928
relates to the Higgs Boson.

9
00:00:21,928 --> 00:00:26,822
Kaggle also has three data sets,
the training data, the public validation

10
00:00:26,822 --> 00:00:32,080
data set, and a private data set that
is not revealed to the competitors.

11
00:00:32,080 --> 00:00:35,610
Here Kaggle shows you the performance
of the top competitors

12
00:00:35,610 --> 00:00:38,020
when measured on the private test sets.

13
00:00:38,020 --> 00:00:42,070
The green and red arrows show
how different the ranking was

14
00:00:42,070 --> 00:00:45,090
compared to the ranking
on the public set.

15
00:00:45,090 --> 00:00:46,970
Let's look at the rankings.

16
00:00:46,970 --> 00:00:51,190
The top competitors were doing well
on the public validation data and

17
00:00:51,190 --> 00:00:54,100
they remain at the top once
their private data was revealed.

18
00:00:55,250 --> 00:00:59,260
If you go further down the leaderboard,
however, it's a real bloodshed.

19
00:00:59,260 --> 00:01:01,920
Many competitors who thought
they were doing well,

20
00:01:01,920 --> 00:01:05,019
were not doing well at all
in the private data sets.

21
00:01:05,019 --> 00:01:05,990
As a result,

22
00:01:05,990 --> 00:01:10,970
their ranks went down dramatically once
the private data set was revealed.

23
00:01:10,970 --> 00:01:12,440
Why is that?

24
00:01:12,440 --> 00:01:15,945
Maybe they had a good model that did
well in validation just by chance.

25
00:01:15,945 --> 00:01:21,670
What's more likely however, is that
by validating themselves over and

26
00:01:21,670 --> 00:01:26,150
over dozens of times on the validation
set, they ended up over fitting

27
00:01:26,150 --> 00:01:29,005
to the validation set and
failing to generalize.

28
00:01:29,005 --> 00:01:33,760
The top competitors] however,
had good experimental design.

29
00:01:33,760 --> 00:01:36,850
They were not misled into thinking
that they were doing well.

30
00:01:36,850 --> 00:01:39,990
They probably took out some of
the training sets to validate their

31
00:01:39,990 --> 00:01:44,480
algorithm or used more sophisticated
methods like cross validation and

32
00:01:44,480 --> 00:01:47,870
didn't make many decisions based
on the public data set scores.
