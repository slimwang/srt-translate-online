1
00:00:00,000 --> 00:00:06,000
In this question, you were asked to limit the depth the crawler searches to.

2
00:00:06,000 --> 00:00:11,000
In order to understand this question and what we need to do,

3
00:00:11,000 --> 00:00:19,000
I'm going to draw a graph of the links, so I'm going to start off with a seed page A,

4
00:00:19,000 --> 00:00:29,000
and from A I have links to another page B and another page C.

5
00:00:29,000 --> 00:00:35,000
Now B links to D, and C links to E.

6
00:00:35,000 --> 00:00:41,000
Now, just to make things a little tricky, E is going to link to D.

7
00:00:41,000 --> 00:00:45,000
Finally, D links to F.

8
00:00:45,000 --> 00:00:47,000
What do we actually need to do?

9
00:00:47,000 --> 00:00:55,000
Well, we need to keep track of what pages we have left to crawl, just like before.

10
00:00:55,000 --> 00:00:58,000
We need to keep track of the pages we've already crawled,

11
00:00:58,000 --> 00:01:02,000
because once we've crawled them we don't really want to have to crawl them again.

12
00:01:02,000 --> 00:01:10,000
I'm going to introduce a new list where we keep track of the pages at the next depth.

13
00:01:10,000 --> 00:01:14,000
I'm just going to call that next_depth.

14
00:01:14,000 --> 00:01:19,000
We start off at tocrawl, and we have just the seed page.

15
00:01:19,000 --> 00:01:23,000
Now, we haven't crawled anything yet, and we don't know what there is to come either.

16
00:01:23,000 --> 00:01:29,000
We start off with just tocrawl with the seed page in it.

17
00:01:29,000 --> 00:01:37,000
Now, from there we look at A, and we see that there are two pages that A links to.

18
00:01:37,000 --> 00:01:45,000
We're at A, and the next depth--the next pages at depth 1--are B and C.

19
00:01:45,000 --> 00:01:51,000
We're actually going to move A from tocrawl to crawled,

20
00:01:51,000 --> 00:01:53,000
because we've actually looked at it now.

21
00:01:53,000 --> 00:02:00,000
We've seen that the next pages are C and B.

22
00:02:00,000 --> 00:02:07,000
Those are going to be the ones that we crawl next. I'm going to put those into tocrawl.

23
00:02:07,000 --> 00:02:10,000
We'll look at those next.

24
00:02:10,000 --> 00:02:19,000
If we look at B, B has a neighbor D, and then C has a neighbor E.

25
00:02:19,000 --> 00:02:24,000
We do this in a couple of steps, but just to get the idea how the code is going to work.

26
00:02:24,000 --> 00:02:29,000
I'm just going to go through it looking at what's at each depth.

27
00:02:29,000 --> 00:02:37,000
B and C we've now crawled, so those two both go in to crawled.

28
00:02:37,000 --> 00:02:42,000
Now the ones at the next depth are D and E, so those are the ones we want to look at.

29
00:02:42,000 --> 00:02:45,000
We're going to move D and E over here,

30
00:02:45,000 --> 00:02:50,000
and then we're going to look at the next pages of D and E.

31
00:02:50,000 --> 00:02:58,000
D has neighbor F, and E has a neighbor D. Those are at depth 3.

32
00:02:58,000 --> 00:03:03,000
Now, we might not want to add D in here, because actually we've already look at D,

33
00:03:03,000 --> 00:03:08,000
but we'll see when we get to the actual code how we're going to handle that.

34
00:03:08,000 --> 00:03:15,000
Now we take D and E and put them into crawled.

35
00:03:15,000 --> 00:03:19,000
Then we've got F and D over here,

36
00:03:19,000 --> 00:03:24,000
and we can carry on, depending on what depth we want to go to.

37
00:03:24,000 --> 00:03:27,000
That's the idea behind how the code is going to work.

38
00:03:27,000 --> 00:03:30,000
We're going to start off with a page.

39
00:03:30,000 --> 00:03:34,000
We're going to crawl it and everything else at the same depth,

40
00:03:34,000 --> 00:03:41,000
and then add those pages to next depth and switch the lists around,

41
00:03:41,000 --> 00:03:44,000
which hopefully you'll be able to see more clearly in the code.

42
00:03:44,000 --> 00:03:51,000
This is the supplied code for crawl_web, and we're going to make a few changes

43
00:03:51,000 --> 00:03:56,000
to this to take account of the depth. So what do we do first?

44
00:03:56,000 --> 00:03:59,000
Well, we're going to need another list next depth,

45
00:03:59,000 --> 00:04:05,000
which will keep track of the next level of links.

46
00:04:05,000 --> 00:04:10,000
When we start we have a depth of 0.

47
00:04:10,000 --> 00:04:15,000
We're going to carry on through the while loop while there's something in tocrawl,

48
00:04:15,000 --> 00:04:18,000
because if there's nothing to crawl, then we can't carry on.

49
00:04:18,000 --> 00:04:27,000
Also while our depth is at most the maximum depth that's supplied as an input to the function.

50
00:04:27,000 --> 00:04:32,000
Just as before, we're going to pop from tocrawl,

51
00:04:32,000 --> 00:04:37,000
and we're not going to make any changes to this line either,

52
00:04:37,000 --> 00:04:42,000
but instead of adding to tocrawl--because we don't want to muddle up

53
00:04:42,000 --> 00:04:48,000
the pages we're searching through now and the pages at the next level--

54
00:04:48,000 --> 00:04:53,000
we're going to actually add the new links to next depth instead.

55
00:04:53,000 --> 00:04:59,000
Then just as before, we're going to add our page that we've crawled

56
00:04:59,000 --> 00:05:02,000
to the crawled list so that we know we've already looked through it,

57
00:05:02,000 --> 00:05:04,000
and we're not going to look through it again.

58
00:05:04,000 --> 00:05:08,000
Now, the last bit of code that we need to add--

59
00:05:08,000 --> 00:05:12,000
when we've finished going through all the lists at a given level,

60
00:05:12,000 --> 00:05:15,000
just like I showed you in the example,

61
00:05:15,000 --> 00:05:21,000
we're then going to change our tocrawl list and our depth list over.

62
00:05:21,000 --> 00:05:25,000
So tocrawl is now empty.

63
00:05:25,000 --> 00:05:32,000
Next depth contains all the new links, and I'm going to use a multiple assignment

64
00:05:32,000 --> 00:05:37,000
where I swap tocrawl and next_depth.

65
00:05:37,000 --> 00:05:42,000
Tocrawl becomes equal to next_depth and next_depth is going to be empty again,

66
00:05:42,000 --> 00:05:49,000
so that we can start the while loop again with a clean list.

67
00:05:49,000 --> 00:05:53,000
Then depth we're going to increase by 1.

68
00:05:53,000 --> 00:05:55,000
Then we're going to return crawled.

69
00:05:55,000 --> 00:06:03,000
Just to quickly recap--we carry on through the while loop as long as we've got some page

70
00:06:03,000 --> 00:06:08,000
to crawl and our depth is less than or equal to max_depth.

71
00:06:08,000 --> 00:06:12,000
We take page, and if we haven't crawled the page before,

72
00:06:12,000 --> 00:06:16,000
we add all the links from the page to the next_depth.

73
00:06:16,000 --> 00:06:22,000
Then we take our page, and we put it in the crawled list.

74
00:06:22,000 --> 00:06:26,000
We keep doing this. If we haven't got any pages left to crawl, then we switch them around.

75
00:06:26,000 --> 00:06:31,000
But if we have got pages left to crawl, we just carry on with the while loop

76
00:06:31,000 --> 00:06:34,000
until tocrawl is empty.

77
00:06:34,000 --> 00:06:39,000
Once tocrawl is empty, that means that that level is finished, that depth is finished,

78
00:06:39,000 --> 00:06:42,000
and we want to go on to start the next depth, and that's it.
