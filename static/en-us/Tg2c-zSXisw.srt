1
00:00:00,210 --> 00:00:02,730
Okay Charles so we've been talking
about temporal difference learning.

2
00:00:02,730 --> 00:00:06,420
And in particular various flavors
of the TD lambda algorithm.

3
00:00:06,420 --> 00:00:08,970
Can you remember some of the important
things that we've touched on?

4
00:00:08,970 --> 00:00:10,950
>> Well I think you
just said the big one.

5
00:00:10,950 --> 00:00:15,300
That all this discussion has really
been about temporal difference learning.

6
00:00:15,300 --> 00:00:17,870
>> And in a nutshell what's
a temporal difference?

7
00:00:17,870 --> 00:00:22,240
>> Well it's the difference
between things you see on

8
00:00:22,240 --> 00:00:24,180
different subsequent time steps.

9
00:00:24,180 --> 00:00:26,640
>> What kind of things
in different time steps?

10
00:00:26,640 --> 00:00:27,500
>> Reward.

11
00:00:27,500 --> 00:00:31,230
>> So, sort of the difference in value
estimates as we go from one step to

12
00:00:31,230 --> 00:00:31,960
another, right?

13
00:00:31,960 --> 00:00:35,135
>> That's exactly right, because value
is just a nice summary statistic for

14
00:00:35,135 --> 00:00:35,960
long-term reward.

15
00:00:35,960 --> 00:00:36,900
>> Good, all right.

16
00:00:36,900 --> 00:00:41,410
>> But actually, I think it was
important that you started off talking

17
00:00:41,410 --> 00:00:45,020
about this is the thing that we wanted
to do, but first you put it in context.

18
00:00:45,020 --> 00:00:47,680
By talking about different

19
00:00:47,680 --> 00:00:49,920
ways that we could solve
the reinforcement learning problem.

20
00:00:51,750 --> 00:00:57,600
As I recall, there were three ways to
think about it: model based learning,

21
00:00:57,600 --> 00:01:01,730
value based learning, and
let's say policy based learning.

22
00:01:01,730 --> 00:01:03,980
>> That's right and
TD methods kind of fall where?

23
00:01:04,989 --> 00:01:06,470
>> In the value based.

24
00:01:06,470 --> 00:01:09,840
>> All right then, we dove into
the update rules themselves.

25
00:01:09,840 --> 00:01:13,160
So what do we talk about there?

26
00:01:13,160 --> 00:01:15,711
>> We actually deroved?

27
00:01:15,711 --> 00:01:17,780
>> [LAUGHS] Derived?

28
00:01:17,780 --> 00:01:19,830
>> Either one of those things.

29
00:01:19,830 --> 00:01:21,270
It's just semantics.

30
00:01:21,270 --> 00:01:23,060
This kind of incremental way.

31
00:01:23,060 --> 00:01:28,330
Of building up estimates of the values,
and I think you called it outcome based.

32
00:01:28,330 --> 00:01:32,270
And I suppose all that really means,
of course, is that you actually look at

33
00:01:32,270 --> 00:01:36,540
the outcomes of what you experience
from different trajectories.

34
00:01:36,540 --> 00:01:39,930
And episodes through the space,
and you use that to build up

35
00:01:39,930 --> 00:01:43,620
your sort of estimate of what
the values of various states are.

36
00:01:44,620 --> 00:01:47,780
>> Yeah, and in particular,
the idea of an outcome based says, well.

37
00:01:47,780 --> 00:01:50,070
You can kind of treat
reinforcement learning as a kind of

38
00:01:50,070 --> 00:01:51,080
supervised learning.

39
00:01:51,080 --> 00:01:54,250
Just wait for time to stop and
see what happened, and

40
00:01:54,250 --> 00:01:56,050
use that as a training example.

41
00:01:56,050 --> 00:01:57,790
But we can't really wait for
time to end,

42
00:01:57,790 --> 00:02:00,330
because it's kind of too late to
use any of the information there.

43
00:02:00,330 --> 00:02:02,140
So what we want are incremental methods,

44
00:02:02,140 --> 00:02:04,060
methods that can actually
do updates along the way.

45
00:02:04,060 --> 00:02:06,500
That would be equivalent to
waiting until the end, but

46
00:02:06,500 --> 00:02:08,101
actually gives us results sooner.

47
00:02:08,101 --> 00:02:10,858
>> Right, and there was a technical
point that you made that I guess is

48
00:02:10,858 --> 00:02:12,295
something that we learned about.

49
00:02:12,295 --> 00:02:14,581
Which we had talked about in
the old machine learning course but

50
00:02:14,581 --> 00:02:15,928
we didn't go into any detail about.

51
00:02:15,928 --> 00:02:19,978
Which is this sort of incremental update
by making it look like a supervised

52
00:02:19,978 --> 00:02:23,662
learning rule, just look like
the normal perceptron update rule.

53
00:02:23,662 --> 00:02:26,088
And it had a learning rate.

54
00:02:26,088 --> 00:02:29,028
And in order for us to be sure that
we're going to learn something that we

55
00:02:29,028 --> 00:02:32,380
wanted to learn, that learning rate
had to have certain properties.

56
00:02:32,380 --> 00:02:33,300
>> Great, that's exactly right.

57
00:02:33,300 --> 00:02:34,260
And in a nutshell,

58
00:02:34,260 --> 00:02:37,260
what were these properties that
the learning rate had to have?

59
00:02:37,260 --> 00:02:41,430
>> Well, let's see, that the sum over
all the learning rate values, so

60
00:02:41,430 --> 00:02:46,300
let's call that Alpha T for
time, needed to be infinite.

61
00:02:46,300 --> 00:02:53,385
And that the sum of the learning
rate squared needed to be finite.

62
00:02:55,350 --> 00:02:59,290
>> Yeah, so intuitively we want a
learning rate sequences that would allow

63
00:02:59,290 --> 00:03:02,665
us to move the value to wherever
it needed to go, to converge.

64
00:03:02,665 --> 00:03:05,198
But would dampen overtime, decay so

65
00:03:05,198 --> 00:03:11,350
that the estimates wouldn't keep bumping
around as a function of the noise.

66
00:03:11,350 --> 00:03:12,530
>> Right, I think that's great.

67
00:03:12,530 --> 00:03:16,160
Well, anyway I think that pretty
much brought us to the end.

68
00:03:16,160 --> 00:03:21,262
We took a lot of slides to get there,
but it turned out the algorithm that we

69
00:03:21,262 --> 00:03:26,520
derived looked a lot like something we
call TD(1) give it a nice little name.

70
00:03:26,520 --> 00:03:30,200
And TD(1) was very nice
except it was inefficient.

71
00:03:30,200 --> 00:03:31,420
In the way that it used data.

72
00:03:31,420 --> 00:03:34,700
>> Yeah, I don't know that everybody
would agree exactly with that

73
00:03:34,700 --> 00:03:36,600
way of characterizing it.

74
00:03:36,600 --> 00:03:40,910
But it is true that it had this
sort of weird property that it was,

75
00:03:40,910 --> 00:03:42,222
there's a lot of variability in it.

76
00:03:42,222 --> 00:03:44,740
There was a very high variance,
because if we had one run,

77
00:03:44,740 --> 00:03:49,930
we had a value that we're
estimating from one place and.

78
00:03:49,930 --> 00:03:55,030
The estimate was based
on a very noisy example,

79
00:03:55,030 --> 00:03:56,750
then we're going to have
a very noisy estimate.

80
00:03:56,750 --> 00:03:59,530
It wasn't going to be able to use
the rest of the examples that it's seen

81
00:03:59,530 --> 00:04:00,430
to kind of smooth this out.

82
00:04:01,510 --> 00:04:04,710
>> Right it didn't use
information along the way.

83
00:04:04,710 --> 00:04:07,170
It used information sort of
at the end of every episode.

84
00:04:07,170 --> 00:04:09,330
And that's what made it
sort of data inefficient.

85
00:04:09,330 --> 00:04:11,264
And I guess in the end that
gives you high variance.

86
00:04:11,264 --> 00:04:12,769
But we came to the rescue and

87
00:04:12,769 --> 00:04:15,860
when I say we I mean you came
to the rescue with TD(0).

88
00:04:15,860 --> 00:04:19,952
Which has the nice property that it
gives you the maximum likelihood

89
00:04:19,952 --> 00:04:20,680
estimate.

90
00:04:20,680 --> 00:04:23,360
>> I prefer to think of it as
the Micheal Litman estimate.

91
00:04:23,360 --> 00:04:24,006
>> Yeah, I know you do.

92
00:04:24,006 --> 00:04:28,510
I've see how you snuck that in and then
it got us to the very end which is good

93
00:04:28,510 --> 00:04:30,330
because you're running out
of space on the slide.

94
00:04:30,330 --> 00:04:36,071
We generalize TD1 and
TD0 into something we called TD lambda.

95
00:04:36,071 --> 00:04:39,057
>> That has the property that when
lambda is zero we get TD(0) and

96
00:04:39,057 --> 00:04:40,725
when lambda is one we get TD (1).

97
00:04:40,725 --> 00:04:42,931
What about the intermediate values?

98
00:04:42,931 --> 00:04:44,949
>> Well,
then that just gives you dubstep.

99
00:04:45,980 --> 00:04:47,040
>> Wait what?

100
00:04:47,040 --> 00:04:47,790
>> It gives you dub step.

101
00:04:47,790 --> 00:04:49,224
You get the dub step look ahead.

102
00:04:49,224 --> 00:04:55,197
[SOUND] [SOUND] [SOUND]
>> [LAUGH] I see that's right,

103
00:04:55,197 --> 00:04:57,390
multiple steps of updates.

104
00:04:57,390 --> 00:05:01,090
I see what you mean by dub step,
like double the number of steps.

105
00:05:01,090 --> 00:05:01,720
>> Oh yeah I like that.

106
00:05:01,720 --> 00:05:02,770
That sounds like I meant to do that.

107
00:05:02,770 --> 00:05:03,790
Yeah let's go with that.

108
00:05:03,790 --> 00:05:05,260
And there you go,
I think that's just about everything.

109
00:05:05,260 --> 00:05:10,109
Oh, one more thing, is that you talked
a little bit about different values of

110
00:05:10,109 --> 00:05:13,624
lambda and why you might
choose one value over another.

111
00:05:13,624 --> 00:05:17,666
And it turns out that values
of lambda between 0.3 and

112
00:05:17,666 --> 00:05:22,000
0.7 empirically seem
to work pretty well.

113
00:05:22,000 --> 00:05:25,495
Yeah, because it's somehow
making a nice trade-off

114
00:05:25,495 --> 00:05:28,990
between estimating the right
kind of quantity and

115
00:05:28,990 --> 00:05:33,499
estimating it using the values
that you get from looking ahead.

116
00:05:33,499 --> 00:05:34,272
>> Right, and

117
00:05:34,272 --> 00:05:39,460
first off that's neat because otherwise
you wouldn't need to have TD lambda.

118
00:05:39,460 --> 00:05:41,730
But it's in some ways
a little surprising.

119
00:05:41,730 --> 00:05:42,910
>> Why is that?

120
00:05:42,910 --> 00:05:44,810
>> Well because it kind of
just works out the way that

121
00:05:44,810 --> 00:05:45,720
you would want it work out.

122
00:05:45,720 --> 00:05:48,820
You went through all this trouble to
kind of generalize between things.

123
00:05:48,820 --> 00:05:51,810
You have this kind of highly
non-linear way of combining

124
00:05:51,810 --> 00:05:53,890
all the different k step look aheads.

125
00:05:53,890 --> 00:05:58,410
And it terms out that combining them so
it's not quite 0 it's not quite one

126
00:05:58,410 --> 00:06:03,140
is better than either 0 or 1, and
often that's not how it works.

127
00:06:03,140 --> 00:06:05,987
Right, I mean often it's like
a line from 0 to 1 is bad.

128
00:06:05,987 --> 00:06:08,310
It's like a line, or
it gets really better,

129
00:06:08,310 --> 00:06:12,180
then it sort of flattens out
towards the end, like you said.

130
00:06:12,180 --> 00:06:14,670
But, in this case it just gets better,
and better, and better, and

131
00:06:14,670 --> 00:06:18,310
then it starts to get worse when
you go from one end to the other.

132
00:06:18,310 --> 00:06:18,990
>> Yeah that's right.
So

133
00:06:18,990 --> 00:06:22,920
often when you combine two good things,
you get the worst of both.

134
00:06:22,920 --> 00:06:25,760
In this case, it does somehow
manage to get the best of both.

135
00:06:25,760 --> 00:06:26,720
>> Yeah, that's pretty cool.

136
00:06:26,720 --> 00:06:33,528
>> All right, so what I'd like to dive
into next is a deeper understanding of.

137
00:06:33,528 --> 00:06:37,026
How we could show that these kinds
of methods actually converge, and

138
00:06:37,026 --> 00:06:41,450
they actually produce optimal estimates
and optimal behavior in the limit.

139
00:06:41,450 --> 00:06:42,960
And so
we're going to take a step back and

140
00:06:42,960 --> 00:06:44,580
look at the Belmont
equation again to do that.

141
00:06:45,580 --> 00:06:46,460
>> Oh I like it.

142
00:06:46,460 --> 00:06:47,990
I love the Belmont equation.

143
00:06:47,990 --> 00:06:50,680
>> All right see you then then.

144
00:06:50,680 --> 00:06:51,840
>> All right see you, bye.
