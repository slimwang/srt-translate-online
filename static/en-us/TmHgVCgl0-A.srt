1
00:00:00,110 --> 00:00:03,100
So going back to our choices for

2
00:00:03,100 --> 00:00:06,970
reducing the AMAT, we have seen
that we can reduce the hit time.

3
00:00:06,970 --> 00:00:10,750
We have now seen that there are
techniques that reduce the miss rate.

4
00:00:10,750 --> 00:00:16,379
And the final set of techniques
are those that reduce the miss penalty.

5
00:00:16,379 --> 00:00:20,160
So when we miss,
we don't suffer as much as before.

6
00:00:20,160 --> 00:00:22,170
So the first technique for

7
00:00:22,170 --> 00:00:26,190
using the missed penalty is
to overlap multiple misses.

8
00:00:26,190 --> 00:00:28,260
If time goes in this direction,

9
00:00:28,260 --> 00:00:32,250
our processor does a lot of activity
multiple instructions per cycle.

10
00:00:32,250 --> 00:00:35,870
And at some point it does a load for
example.

11
00:00:35,870 --> 00:00:39,520
And now that load tries
to be found in the cache.

12
00:00:39,520 --> 00:00:44,170
And if not, we're going to go to
memory and it eventually comes back.

13
00:00:44,170 --> 00:00:46,830
Now if you have a fancy
out of order processor,

14
00:00:46,830 --> 00:00:48,900
it doesn't stop here and wait.

15
00:00:48,900 --> 00:00:51,768
What it does is it finds
other instructions to do.

16
00:00:51,768 --> 00:00:57,040
So even after we start
fetching the data from memory,

17
00:00:57,040 --> 00:00:58,830
the processor is continuing.

18
00:00:58,830 --> 00:01:02,860
But eventually it starts
running out of things to do.

19
00:01:02,860 --> 00:01:05,770
And probably before the load
comes back from memory,

20
00:01:05,770 --> 00:01:07,680
the processor runs out of resources.

21
00:01:07,680 --> 00:01:09,770
Remember that it cannot
commit this load.

22
00:01:09,770 --> 00:01:12,960
So eventually for example,
it will fill up the ROB or

23
00:01:12,960 --> 00:01:16,160
maybe even sooner than that it will
run out of reservation stations,

24
00:01:16,160 --> 00:01:17,990
if a lot of things depend on this load.

25
00:01:17,990 --> 00:01:21,960
So some part of this missed latency
is going to be directly added to

26
00:01:21,960 --> 00:01:23,340
the execution time, but

27
00:01:23,340 --> 00:01:27,660
some part of it is actually overlapping
with the processor activity.

28
00:01:27,660 --> 00:01:32,077
During the time, between,
trying to execute this load,

29
00:01:32,077 --> 00:01:34,509
and running out of things to do,

30
00:01:34,509 --> 00:01:40,296
this processor might actually issue
another load that will be a cache miss.

31
00:01:40,296 --> 00:01:43,801
So, if we have what is
called a blocking cache,

32
00:01:43,801 --> 00:01:48,536
then this load cannot be done until
the first load is finished and

33
00:01:48,536 --> 00:01:53,020
only at this point this load can
really be tried in the cache.

34
00:01:53,020 --> 00:01:56,332
We realize it's a miss,
we suffer the miss latency, and

35
00:01:56,332 --> 00:02:00,472
meanwhile because the processor can
commit these instructions here,

36
00:02:00,472 --> 00:02:05,240
the processor can overlap some of this
miss latency with some other activity.

37
00:02:05,240 --> 00:02:10,210
We can also have a non-blocking
cache and a non-blocking cache can

38
00:02:10,210 --> 00:02:15,410
support things like hit under a miss,
meaning while

39
00:02:15,410 --> 00:02:19,670
we are having a cache miss, hits to
other blocks in the cache that are sent

40
00:02:19,670 --> 00:02:24,412
by the processor will be serviced and
returned to the processor with data.

41
00:02:24,412 --> 00:02:29,270
And also we can have what is called
a miss under a miss, in which case

42
00:02:29,270 --> 00:02:33,070
while we are having a miss,
we can send another request to memory.

43
00:02:33,070 --> 00:02:35,280
So let's look at what that looks like.

44
00:02:35,280 --> 00:02:37,980
So our processor is
happily chugging along,

45
00:02:37,980 --> 00:02:40,510
it has this first load
that suffers a miss.

46
00:02:40,510 --> 00:02:42,560
We check in the cache, we wait for

47
00:02:42,560 --> 00:02:45,420
it to come back from memory,
we continue working,

48
00:02:45,420 --> 00:02:49,290
and eventually run out of things to do
because they depend on this first load.

49
00:02:49,290 --> 00:02:52,990
But the idea is that now this load here

50
00:02:52,990 --> 00:02:57,020
that also suffers a miss will have
its own check in the cache and

51
00:02:57,020 --> 00:02:59,910
when we realize it's a miss
we will send it to memory.

52
00:02:59,910 --> 00:03:01,780
So it will come back here.

53
00:03:01,780 --> 00:03:05,150
Now what we have is when the first
load comes back from memory,

54
00:03:05,150 --> 00:03:06,680
there is a burst of activity.

55
00:03:06,680 --> 00:03:11,600
It starts drying out because we
are still waiting for the second load.

56
00:03:11,600 --> 00:03:13,790
But then the second load comes back and

57
00:03:13,790 --> 00:03:16,880
we are very quickly back
to a normal operation.

58
00:03:16,880 --> 00:03:22,770
So now, as you can see, the inactivity
in the processor used to be this and

59
00:03:22,770 --> 00:03:27,580
this and now it's just this and
maybe a small amount here.

60
00:03:27,580 --> 00:03:31,980
So by overlapping the miss
time of the two loads,

61
00:03:31,980 --> 00:03:36,460
we have almost cut
the penalty to on performance

62
00:03:36,460 --> 00:03:41,530
to half of what it was because
before we had to wait twice this.

63
00:03:41,530 --> 00:03:45,470
Now we really wait once and
maybe some little more.

64
00:03:45,470 --> 00:03:49,560
If we manage to find three or
four rows that overlap.

65
00:03:49,560 --> 00:03:54,230
Then a blocking cacher will pay
the penalty three or four times.

66
00:03:54,230 --> 00:03:58,660
Here we might be paying one
penalty plus a little bit more.

67
00:03:58,660 --> 00:04:03,210
The property that the processor
is exploiting here is called

68
00:04:03,210 --> 00:04:05,700
memory level parallelism.

69
00:04:05,700 --> 00:04:09,970
Here, the memory never gets
more than one access at a time.

70
00:04:09,970 --> 00:04:13,070
Here, the memory gets accesses
to process them in parallel.

71
00:04:13,070 --> 00:04:15,640
So, of course,
our memory needs to be able to do this.

72
00:04:15,640 --> 00:04:19,920
But if it can't, then a non-blocking
cache that can do miss under miss,

73
00:04:19,920 --> 00:04:23,320
can dramatically cut down
on the cost of misses,

74
00:04:23,320 --> 00:04:28,260
instead of seeing the full miss latency
being added to the memory access time.

75
00:04:28,260 --> 00:04:31,780
We are really seeing that the penalties
of the two misses overlap.

76
00:04:31,780 --> 00:04:37,720
So really we paid a penalty once and
we get 1, 2, 15 misses in exchange.
