1
00:00:00,230 --> 00:00:03,180
>> Alright, so now we're ready to dive in and actually work out some

2
00:00:03,180 --> 00:00:07,030
math that turned out not to be so bad, but it was, it ends

3
00:00:07,030 --> 00:00:09,410
up being okay specifically because we were

4
00:00:09,410 --> 00:00:10,860
very carefully about setting up all the

5
00:00:10,860 --> 00:00:14,950
definitions to lead us to this moment in time. It is our destiny, Charles.

6
00:00:14,950 --> 00:00:17,320
>> Oh good. I love destiny. Is this destiny's theorem?

7
00:00:17,320 --> 00:00:19,350
>> No, actually turns out it's Haussler's Theorem.

8
00:00:19,350 --> 00:00:22,570
>> Is Haussler like German for destiny? It

9
00:00:22,570 --> 00:00:25,340
might be, but I don't think it is. So,

10
00:00:25,340 --> 00:00:26,830
Hausser is the name of a person in this

11
00:00:26,830 --> 00:00:29,660
particular case. And what he worked out is a

12
00:00:29,660 --> 00:00:32,509
way of bounding the true error as a function

13
00:00:32,509 --> 00:00:34,610
of the number of training examples that are drawn.

14
00:00:34,610 --> 00:00:35,420
>> Oh. Nice.

15
00:00:35,420 --> 00:00:38,440
>> So, let's consider. From the hypothesis

16
00:00:38,440 --> 00:00:40,620
set that we have, all the hypotheses can

17
00:00:40,620 --> 00:00:44,240
be categorized as to whether or not they have high true error or low true error.

18
00:00:44,240 --> 00:00:45,130
>> Sure.

19
00:00:45,130 --> 00:00:47,730
>> Right so, let's let H1 through HK be

20
00:00:47,730 --> 00:00:50,610
the ones that have a priority of high true error.

21
00:00:50,610 --> 00:00:52,750
What we'd like to do is make sure that as we're

22
00:00:52,750 --> 00:00:55,840
drawing data sets that we have knocked all these guys out.

23
00:00:55,840 --> 00:01:02,450
We've gotten enough examples that actually allow us to verify that

24
00:01:02,450 --> 00:01:03,670
they have high error. So they have high error on the, on

25
00:01:03,670 --> 00:01:06,840
the training set. So they have high training error. Alright, so

26
00:01:06,840 --> 00:01:09,280
how many, how much data do we need to establish that

27
00:01:09,280 --> 00:01:11,900
these guys are actually bad? Alright, so let's take a look

28
00:01:11,900 --> 00:01:15,830
at the probability that if we draw an X, an input from

29
00:01:15,830 --> 00:01:19,740
this distribution D. That, for any of these hypotheses.

30
00:01:19,740 --> 00:01:23,090
Hi, and this set of bad hypotheses. That it will

31
00:01:23,090 --> 00:01:30,120
match the true concept, right? So that H sub I of X is equal to C of X. And

32
00:01:30,120 --> 00:01:31,418
we know that that's less than or equal to

33
00:01:31,418 --> 00:01:35,180
one minus epsilon. It's unlikely that they match. Because it's

34
00:01:35,180 --> 00:01:38,220
likely, or relatively likely, that they mismatch. Right? That's what

35
00:01:38,220 --> 00:01:41,980
this exactly means. This, this error being greater than epsilon.

36
00:01:41,980 --> 00:01:43,670
>> Oh, I see, so if I have an error of

37
00:01:43,670 --> 00:01:48,400
greater than epsilon, that means that the probability that I'm wrong is

38
00:01:48,400 --> 00:01:52,790
greater than epsilon, which means the probability that I'm right is

39
00:01:52,790 --> 00:01:55,630
one minus epsilon, less than one minus epsilon. Okay, that makes sense.

40
00:01:55,630 --> 00:02:00,360
>> Yeah, so it's sort of a relatively low probability of match.

41
00:02:00,360 --> 00:02:02,430
>> Well, if epsilon is high.

42
00:02:02,430 --> 00:02:04,790
>> Low relative to one minus epsilon.

43
00:02:04,790 --> 00:02:05,320
>> Right, okay.

44
00:02:05,320 --> 00:02:06,820
>> So this is,

45
00:02:06,820 --> 00:02:10,550
this is a fact about the hypothesis in, in

46
00:02:10,550 --> 00:02:13,690
the abstract. For any given sample set we've got a

47
00:02:13,690 --> 00:02:16,270
set of m examples, and what we'd like to know

48
00:02:16,270 --> 00:02:17,810
is, since we're trying to knock it out, what's the

49
00:02:17,810 --> 00:02:21,920
probability that even after we've drawn m examples that this

50
00:02:21,920 --> 00:02:26,240
hypothesis, h of i, remains consistent with c. Right, even

51
00:02:26,240 --> 00:02:29,250
though the data doesn't really match all that well, we've

52
00:02:29,250 --> 00:02:31,870
drawn M examples, and it still looks like it matches.

53
00:02:31,870 --> 00:02:34,970
It's still in the version space. So the probability that

54
00:02:34,970 --> 00:02:39,750
that happens if it were the case that everything was independent

55
00:02:39,750 --> 00:02:42,040
is going to be one minus epsilon raised to the M

56
00:02:42,040 --> 00:02:45,070
power. Right. Because it's less than one minus epsilon to be

57
00:02:45,070 --> 00:02:48,740
wrong once Well, which is to say, that your consistent

58
00:02:48,740 --> 00:02:51,580
ones. To continue to be consistent, we keep having to have

59
00:02:51,580 --> 00:02:54,660
this probability come true. So, it's going to be, we're going

60
00:02:54,660 --> 00:02:56,900
to just keep multiplying it in again and again and again.

61
00:02:56,900 --> 00:02:58,720
So one minus epsilon raised the the m power.

62
00:02:58,720 --> 00:03:01,280
>> Right. So that makes sense because

63
00:03:01,280 --> 00:03:04,140
you're basically saying it's Consistent with the

64
00:03:04,140 --> 00:03:08,750
first example and the second example and the third example and dot dot dot

65
00:03:08,750 --> 00:03:12,240
nth example. So, and with independent variables

66
00:03:12,240 --> 00:03:14,810
is just multiplication so it's one minus

67
00:03:14,810 --> 00:03:16,780
epsilon times, one minus epsilon times, one

68
00:03:16,780 --> 00:03:21,190
minus epsilon n times. Okay, I see that.

69
00:03:21,190 --> 00:03:22,200
>> Great.

70
00:03:22,200 --> 00:03:23,790
Alright, so can we use that to figure out

71
00:03:23,790 --> 00:03:27,650
what's the probability that at least one of these h1

72
00:03:27,650 --> 00:03:31,770
through hk's is consistent with c on m examples.

73
00:03:31,770 --> 00:03:35,840
We have to knock them all out. That's...that's really what

74
00:03:35,840 --> 00:03:38,230
the goal is. To knock out all the ones

75
00:03:38,230 --> 00:03:42,320
that have high true error We failed at that. If

76
00:03:42,320 --> 00:03:44,030
one of them still slips through, one of them still

77
00:03:44,030 --> 00:03:47,310
looks consistent and remains in the version space. So what's

78
00:03:47,310 --> 00:03:48,680
the probability that at least one of

79
00:03:48,680 --> 00:03:51,620
these remains consistent. That this still has happened.

80
00:03:51,620 --> 00:03:55,562
>> I think I know that. So just like you did add before and did

81
00:03:55,562 --> 00:04:01,620
multiplication... Another way of writing at least one of is to say or.

82
00:04:01,620 --> 00:04:06,800
So h1 or h2 or h3 or h4... or hk

83
00:04:06,800 --> 00:04:12,120
is consistent. And just like and is multiplication, or is addition. That's true.

84
00:04:12,120 --> 00:04:12,840
>> And

85
00:04:12,840 --> 00:04:16,089
there are k different ones of these, so I have to

86
00:04:16,089 --> 00:04:20,620
say this one minus epsilon to the m plus one minus epsilon

87
00:04:20,620 --> 00:04:23,710
to the m plus one minus epsilon to the mk times

88
00:04:23,710 --> 00:04:27,290
So that would be one minus epsilon to the m times k.

89
00:04:27,290 --> 00:04:33,740
>> Great. So that is a bound on the probability that at least one of these

90
00:04:33,740 --> 00:04:38,350
bad hypothesis is going to remain in the version space even after m examples.

91
00:04:38,350 --> 00:04:40,310
But how many bad examples, how many bad

92
00:04:40,310 --> 00:04:43,180
hypothesis are there? What's an upper bound on that?

93
00:04:43,180 --> 00:04:45,610
>> Well, there might be one or there might be two. There's actually k of

94
00:04:45,610 --> 00:04:49,480
them, but we know that k itself is bound by the total number of hypotheses.

95
00:04:49,480 --> 00:04:51,800
>> Yeah, that's right. So it has to be, you know, the number of

96
00:04:51,800 --> 00:04:55,600
bad hypotheses. We, we assume if c is equal to h anyway that there's

97
00:04:55,600 --> 00:05:00,440
at least one that is not bad, but, you know, almost h of them

98
00:05:00,440 --> 00:05:03,600
can be bad. So that, that should give us a bound. Alright, so now

99
00:05:03,600 --> 00:05:06,050
we're going to to work on this expression a little

100
00:05:06,050 --> 00:05:07,860
bit more to put it in a more convenient form.

101
00:05:07,860 --> 00:05:08,530
>> Okay.
