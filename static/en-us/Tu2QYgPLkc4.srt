1
00:00:00,370 --> 00:00:02,520
Now we know what is the information in one random

2
00:00:02,520 --> 00:00:05,300
variable. Now assume that I told you to predict if you're

3
00:00:05,300 --> 00:00:08,310
going to hear thunder or not. Well, that's very difficult. But

4
00:00:08,310 --> 00:00:10,490
what if I tell you if it is raining or not.

5
00:00:10,490 --> 00:00:15,030
Your guess regarding the thunder is going to be significantly better.

6
00:00:15,030 --> 00:00:17,570
So there is some information in this variable that tells you

7
00:00:17,570 --> 00:00:20,660
something about this variable. We can measure that in two different

8
00:00:20,660 --> 00:00:25,550
ways. The first one is called as joint entropy. Joint entropy

9
00:00:25,550 --> 00:00:29,440
is the randomness contained in two variables together as given

10
00:00:29,440 --> 00:00:32,210
by H of X comma Y. And, as you can

11
00:00:32,210 --> 00:00:35,660
predict, it is given by this particular formula, which is

12
00:00:35,660 --> 00:00:38,980
the joint probability distribution between X and Y. And, as you

13
00:00:38,980 --> 00:00:41,780
predicted, it is given by this particular formula where P

14
00:00:41,780 --> 00:00:44,540
of X comma Y is the joint probability of X and

15
00:00:44,540 --> 00:00:47,970
Y. The other measure is called as conditional entropy. Conditional

16
00:00:47,970 --> 00:00:51,380
entropy is a measure of the randomness of one variable given

17
00:00:51,380 --> 00:00:54,018
the other variable. And it is generated by H

18
00:00:54,018 --> 00:00:57,770
of Y given X. Now, to understand these two concepts,

19
00:00:57,770 --> 00:01:00,460
you have to imagine what happens when X and Y

20
00:01:00,460 --> 00:01:03,660
are independent. If X and Y are independent, then the

21
00:01:03,660 --> 00:01:06,790
conditional probability of Y given X is just the

22
00:01:06,790 --> 00:01:09,800
conditional probability of Y. It's quite obvious, right. If two

23
00:01:09,800 --> 00:01:13,020
variables are independent of each other, Y variable doesn't get

24
00:01:13,020 --> 00:01:16,790
any information from X at all. The joint entropy between

25
00:01:16,790 --> 00:01:20,560
X and Y, if X and Y are independent, is the sum of information

26
00:01:20,560 --> 00:01:24,050
of both X and Y. That is why the entropies have been added here.
