1
00:00:00,180 --> 00:00:02,550
So, you remember how we did panoramas?

2
00:00:02,550 --> 00:00:06,470
We basically found interesting points
in an image, and we described them.

3
00:00:06,470 --> 00:00:08,990
We built these descriptors,
these sift descriptors.

4
00:00:08,990 --> 00:00:13,080
We found punitive matches, possible
matches between them, and then we did

5
00:00:13,080 --> 00:00:15,660
something like RANSAC, or something like
that, in order to find the alignment.

6
00:00:16,850 --> 00:00:19,909
Well, we can actually do something for
recognition that's related.

7
00:00:21,290 --> 00:00:22,060
Now essentially

8
00:00:22,060 --> 00:00:23,880
what we we're going to do is to say
we're going to do the same thing.

9
00:00:23,880 --> 00:00:26,420
We're going to find interesting
points where these are sort of

10
00:00:26,420 --> 00:00:28,980
highly repeatable points
that you'll find.

11
00:00:28,980 --> 00:00:33,360
And what we're going to talk about
is describing the patches around

12
00:00:33,360 --> 00:00:34,500
those points.

13
00:00:34,500 --> 00:00:38,540
And then we're going to use the
collection of those described patches

14
00:00:38,540 --> 00:00:43,710
as a way of generally characterizing
the image for recognition.

15
00:00:43,710 --> 00:00:47,940
So, as we've talked about before,
each location in an image.

16
00:00:47,940 --> 00:00:51,630
So here we have different
locations in an image, all right.

17
00:00:51,630 --> 00:00:54,230
And a bunch of different
training images.

18
00:00:54,230 --> 00:00:58,860
And each one of these
has a descriptor that,

19
00:00:58,860 --> 00:01:02,410
in this what these curves are meant
to show is that they land somewhere

20
00:01:02,410 --> 00:01:06,360
in feature space, so this they land
in the descriptor's feature space.

21
00:01:06,360 --> 00:01:08,680
And you can imagine that you
get a collection of these, so

22
00:01:08,680 --> 00:01:11,520
that there's a whole bunch of
descriptors in a given image.

23
00:01:11,520 --> 00:01:13,870
And then we're going to make
the following assumption, and

24
00:01:13,870 --> 00:01:15,610
we actually made this
assumption before right?

25
00:01:15,610 --> 00:01:20,660
We assume that when we have
points that are sort of similar

26
00:01:20,660 --> 00:01:25,274
in the descriptor space that they are or
nearby in the descriptor space,

27
00:01:25,274 --> 00:01:31,010
that those patches are either the same
patch in a new image a similar patch.

28
00:01:31,010 --> 00:01:34,760
So one way of thinking about this is
it's a way of just giving a description

29
00:01:34,760 --> 00:01:38,240
of a little bit of local
content in the image.

30
00:01:38,240 --> 00:01:43,000
And so now if I get a new
picture that comes in, right?

31
00:01:43,000 --> 00:01:45,600
I can say let me find the patches in it.

32
00:01:45,600 --> 00:01:50,580
And see if I find lots of
the same local patches that

33
00:01:50,580 --> 00:01:55,810
were in my first image, and say,
okay, if I find a lot of those,

34
00:01:55,810 --> 00:02:00,130
then I would know that maybe this
new picture is the same thing,

35
00:02:00,130 --> 00:02:03,580
maybe same category or
same object, as the first picture.

36
00:02:03,580 --> 00:02:04,830
The problem, of course,

37
00:02:04,830 --> 00:02:08,810
is that any given picture can
have thousands of patches.

38
00:02:08,810 --> 00:02:11,330
So a collection of different objects,
and different images.

39
00:02:11,330 --> 00:02:13,550
There could be millions of these things.

40
00:02:13,550 --> 00:02:17,060
And the problem is, if I come up with
a new image, and I find a patch in it.

41
00:02:17,060 --> 00:02:18,070
I have this problem,

42
00:02:18,070 --> 00:02:21,970
of possibly having to search over
millions of different patches.

43
00:02:21,970 --> 00:02:23,460
I have to do something
a little more clever.
