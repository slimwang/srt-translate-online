1
00:00:00,250 --> 00:00:01,370
So that was very astute of you.

2
00:00:01,370 --> 00:00:04,950
And in fact there's a very general
property that could be stated about

3
00:00:04,950 --> 00:00:06,700
learning algorithms of this type.

4
00:00:06,700 --> 00:00:10,870
So if we have a learning update rule
like the one that we have, which is that

5
00:00:10,870 --> 00:00:16,340
the value at time T is the value at
Time T minus one, plus some learning

6
00:00:16,340 --> 00:00:20,170
rate times the difference between the
new estimate and the previous estimate.

7
00:00:20,170 --> 00:00:24,663
Or the new observed value and
the previous estimate, then

8
00:00:24,663 --> 00:00:30,000
this Vt will actually go
to the true expectations,

9
00:00:30,000 --> 00:00:34,720
the actual average value of
the Rs once T is big enough.

10
00:00:36,150 --> 00:00:37,150
Okay, so that's good.

11
00:00:37,150 --> 00:00:38,980
That means it's actually
learning the right thing, but

12
00:00:38,980 --> 00:00:42,310
there are conditions that we have to
put on the learning rate sequence.

13
00:00:43,430 --> 00:00:46,300
And the learning rate sequence has to
have the property that if you sum up all

14
00:00:46,300 --> 00:00:49,320
the learning rates,
it actually sums up to infinity.

15
00:00:49,320 --> 00:00:50,176
It's unbounded.

16
00:00:50,176 --> 00:00:51,240
it diverges.

17
00:00:51,240 --> 00:00:52,930
>> As opposed to greater than infinity.

18
00:00:52,930 --> 00:00:54,270
>> Oh, good point.

19
00:00:54,270 --> 00:00:55,560
Yes, that it's equal to infinity.

20
00:00:55,560 --> 00:00:59,630
Very good, it's actually
greater than any finite number.

21
00:00:59,630 --> 00:01:04,300
Anyway, yeah it diverges, but
the square of the learning rates,

22
00:01:04,300 --> 00:01:06,910
if you sum those up,
it's less than infinity.

23
00:01:06,910 --> 00:01:09,790
And we're not going to get in
the details about why these

24
00:01:09,790 --> 00:01:13,580
are the standard properties for
making this kind of learning rule work.

25
00:01:13,580 --> 00:01:18,430
But the first approximation, the
learning rates have to be big enough so

26
00:01:18,430 --> 00:01:22,320
that you can move to what the true
value is, no matter where you start.

27
00:01:22,320 --> 00:01:25,750
But they can't be so big that
they don't damp out the noise and

28
00:01:25,750 --> 00:01:28,920
actually do a proper job of averaging.

29
00:01:28,920 --> 00:01:32,080
So, if you just accept that these
are the two conditions that make sure

30
00:01:32,080 --> 00:01:34,760
that that's true,
that gives us a way of choosing

31
00:01:34,760 --> 00:01:37,090
different kinds of
learning rate sequences.

32
00:01:37,090 --> 00:01:42,580
>> Okay so what's some alphas that tease
to satisfy that and some that don't.
