1
00:00:00,590 --> 00:00:03,880
Now you've thought about that let me show you why this matters.

2
00:00:03,880 --> 00:00:06,340
Let me take the example of the middle line first.

3
00:00:06,340 --> 00:00:08,650
My errors are going to look something like this.

4
00:00:08,650 --> 00:00:09,710
I'm sketching them in right now.

5
00:00:09,710 --> 00:00:13,070
It's just the distance from each point to sort of the predicted value for

6
00:00:13,070 --> 00:00:15,790
that point on the regression line.

7
00:00:15,790 --> 00:00:19,370
And if you were just summing the absolute value of the errors you

8
00:00:19,370 --> 00:00:22,930
would just sum up all of these orange distances and you would have your answer.

9
00:00:22,930 --> 00:00:24,740
But now let's look at the top line.

10
00:00:24,740 --> 00:00:26,870
We can do the same thing here too.

11
00:00:26,870 --> 00:00:28,920
So now we start drawing in the distances.

12
00:00:28,920 --> 00:00:30,970
So while we'll be closer to all the points above it.

13
00:00:30,970 --> 00:00:33,820
It would be further away from all of the points below it.

14
00:00:33,820 --> 00:00:37,130
So taking these two points as concrete examples.

15
00:00:37,130 --> 00:00:38,610
It'll be a little bit closer to the top point and

16
00:00:38,610 --> 00:00:40,660
a little bit further away from the bottom point.

17
00:00:40,660 --> 00:00:42,150
But overall the total error for

18
00:00:42,150 --> 00:00:45,640
these two example points would be exactly the same.

19
00:00:45,640 --> 00:00:48,520
As a total error for these two points to the middle line.

20
00:00:48,520 --> 00:00:50,030
And, in fact, the same thing would be true for

21
00:00:50,030 --> 00:00:52,030
the bottom regression line as well.

22
00:00:52,030 --> 00:00:54,200
And if you have equal number of points above and

23
00:00:54,200 --> 00:00:57,510
below each of these lines then, in general, that's always going to be true.

24
00:00:57,510 --> 00:01:01,910
There's a fundamental ambiguity when you use the absolute value of

25
00:01:01,910 --> 00:01:06,290
the errors in terms of exactly where the regression can fall.

26
00:01:06,290 --> 00:01:07,470
It could be anywhere in this range.

27
00:01:08,850 --> 00:01:13,830
In other words, there can be multiple lines that minimize the absolute errors.

28
00:01:13,830 --> 00:01:18,310
There's only going to be one line that minimizes the error squared.

29
00:01:18,310 --> 00:01:22,890
In other words, this ambiguity does not exist when the metric that we use is not

30
00:01:22,890 --> 00:01:26,300
the absolute value of the distance, the absolute value of the distance squared.

31
00:01:27,300 --> 00:01:29,590
There's one more thing that I should add as well.

32
00:01:29,590 --> 00:01:32,150
This is a little bit more of a practical concern.

33
00:01:32,150 --> 00:01:35,340
And that is this using the sum of the squared error as a way of

34
00:01:35,340 --> 00:01:38,630
finding the regression also makes the implementation underneath the hood of

35
00:01:38,630 --> 00:01:40,420
the regression much easier.

36
00:01:40,420 --> 00:01:43,100
It's much easier to find this line when what you're trying to do is minimize

37
00:01:43,100 --> 00:01:46,680
the sum of the squared errors instead of the sum of the absolute errors.

38
00:01:46,680 --> 00:01:50,180
And this is something we have the luxury of not worrying about too much

39
00:01:50,180 --> 00:01:54,370
when we're using sklearn to do most of the computational heavy lifting for us.

40
00:01:54,370 --> 00:01:57,450
But of course, if you're actually writing the code that goes in and

41
00:01:57,450 --> 00:02:00,400
does the linear algebra to find the regression, or

42
00:02:00,400 --> 00:02:05,530
maybe the calculus to find the results of what your regression answer should be,

43
00:02:05,530 --> 00:02:07,450
then this is a big concern to you.

44
00:02:07,450 --> 00:02:10,258
This is another reason why traditionally regressions are going to

45
00:02:10,258 --> 00:02:11,342
be minimizing the sum of the squared error.

46
00:02:11,884 --> 00:02:14,263
Is because that computationally it's just much easier to do that.
