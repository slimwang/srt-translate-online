1
00:00:00,140 --> 00:00:02,640
Let's look at the solution
to our parity quiz.

2
00:00:02,640 --> 00:00:06,830
We're adding parity to our memory,
which consists of eight arrays.

3
00:00:06,830 --> 00:00:10,980
So now the question is should we
add parity to each row of data?

4
00:00:10,980 --> 00:00:14,860
And then when we read out the row into
the row buffer, we check for parity and

5
00:00:14,860 --> 00:00:16,140
report an error?

6
00:00:16,140 --> 00:00:19,950
Or should we have the modules as is and

7
00:00:19,950 --> 00:00:22,600
just add two more that
will hold a parity?

8
00:00:22,600 --> 00:00:27,460
Both approaches have exactly the same
number of extra bits we're adding, but

9
00:00:27,460 --> 00:00:31,940
the approach of having extra modules
as opposed to changing the modules

10
00:00:31,940 --> 00:00:34,170
is preferable for two reasons.

11
00:00:34,170 --> 00:00:37,940
One is that this way,
we can design these memory arrays.

12
00:00:37,940 --> 00:00:42,170
Both are unprotected and
for protective memories.

13
00:00:42,170 --> 00:00:46,070
The protected memories will just
have additional arrays, but

14
00:00:46,070 --> 00:00:48,030
the design of each array is the same.

15
00:00:48,030 --> 00:00:52,670
If we decided to add parity bits here,
then depending on exactly

16
00:00:52,670 --> 00:00:56,290
how many parity bits we are adding for
how many data bits, and whether we

17
00:00:56,290 --> 00:01:00,250
are having parity at all, we would need
to redesign our memory array every time.

18
00:01:01,310 --> 00:01:07,230
The second reason why using redundancy
at the coarser grain rather then

19
00:01:07,230 --> 00:01:12,530
finer grain tends to work better,
is because lets say the ten array fails

20
00:01:12,530 --> 00:01:17,170
by reading zeroes for the entire row,
this is not such a far fetched error.

21
00:01:18,180 --> 00:01:23,160
In that case what we have is if we have
the parity bits added to each row,

22
00:01:23,160 --> 00:01:25,370
the row still has correct parity.

23
00:01:25,370 --> 00:01:28,520
So a single failure, for
example, in the row decoder

24
00:01:28,520 --> 00:01:31,050
will just read the wrong row,
it still has the right parity.

25
00:01:32,670 --> 00:01:36,920
If however, we separate the modules
with a parity, then each of the modules

26
00:01:38,220 --> 00:01:43,030
will operate on its own so
the error of our row decoder here

27
00:01:43,030 --> 00:01:48,080
will cause this bit now to be wrong
in every eight bit word we read.

28
00:01:48,080 --> 00:01:50,700
But if this other arrays
are still working correctly,

29
00:01:50,700 --> 00:01:54,340
then we can still detect that because
the parity is somewhere else.

30
00:01:54,340 --> 00:01:57,820
So this is kind of similar to
doing geographical distribution

31
00:01:57,820 --> 00:02:00,520
of our computers, for redundancy.

32
00:02:00,520 --> 00:02:06,200
Pretty much the further away
the protection bits are, the more

33
00:02:06,200 --> 00:02:10,610
likely they will protect us against
large failures in any single component.
