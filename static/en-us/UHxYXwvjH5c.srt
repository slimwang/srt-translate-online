1
00:00:00,330 --> 00:00:03,380
Okay Michael. So here's a quick quiz. So we just tried

2
00:00:03,380 --> 00:00:06,990
to argue that boosting has this annoying habbit of not always

3
00:00:06,990 --> 00:00:10,920
over fitting, but of course something can always over fit. Because

4
00:00:10,920 --> 00:00:13,400
otherwise we just do boosting and we're done, then neither of us

5
00:00:13,400 --> 00:00:16,580
would have jobs. And we don't want that to happen. So

6
00:00:16,580 --> 00:00:18,190
here's a little quiz to see if we can figure out the

7
00:00:18,190 --> 00:00:21,480
circumstances under which boosting might over fit, or tends to over

8
00:00:21,480 --> 00:00:25,180
fit. So here are five possiblities. Let me read them to you.

9
00:00:25,180 --> 00:00:27,410
Tell me if they actually make sense. So

10
00:00:27,410 --> 00:00:30,860
here's possiblity number one, boosting will tend to over

11
00:00:30,860 --> 00:00:34,270
fit if The weak learner that it's boosting

12
00:00:34,270 --> 00:00:38,520
over, always chooses the weakest output that is, it,

13
00:00:38,520 --> 00:00:41,930
among all the hypothesis that it finds that

14
00:00:41,930 --> 00:00:45,190
do better than chance over the training with whatever

15
00:00:45,190 --> 00:00:47,540
given distribution. It always pick the one that is

16
00:00:47,540 --> 00:00:50,350
still nonetheless closest to chance, while still being better.

17
00:00:50,350 --> 00:00:51,720
>> Well, why would it do that?

18
00:00:52,810 --> 00:00:53,700
>> Just to be difficult.

19
00:00:53,700 --> 00:00:56,150
>> Alright, and so you want to know, whether that makes

20
00:00:56,150 --> 00:01:00,552
it over, would [INAUDIBLE] make it over fit? [UNKNOWN] boosting overfit.

21
00:01:00,552 --> 00:01:01,680
>> Okay. Alright.

22
00:01:01,680 --> 00:01:05,280
>> The second one is weak learner actually ends up using...or

23
00:01:05,280 --> 00:01:08,930
the weak leaner itself that boosting is using is in fact

24
00:01:08,930 --> 00:01:12,730
a neural network learner. And just for a little specificity, let's

25
00:01:12,730 --> 00:01:15,570
say this is a neural network that has many many layers and

26
00:01:15,570 --> 00:01:18,550
many many nodes. So, you know, it's a big

27
00:01:18,550 --> 00:01:22,270
powerful neural network, alright? the other option is... Boosting has

28
00:01:22,270 --> 00:01:24,650
a lot of data. So you're trying to learn, your

29
00:01:24,650 --> 00:01:26,810
training data is actually very, very, very large. You have

30
00:01:26,810 --> 00:01:30,490
lots and lots of examples. The fourth case, is that,

31
00:01:30,490 --> 00:01:35,350
the true underlying hypothesis,the true underlying concept, is in fact

32
00:01:35,350 --> 00:01:37,560
non linear. So you can't just draw a line. And

33
00:01:37,560 --> 00:01:41,020
then the fifth case is that we let boosting train

34
00:01:41,020 --> 00:01:44,130
much too long. Whatever that means. Let's just say we let it

35
00:01:44,130 --> 00:01:48,770
train a lot. Not just a thousand iterations but a hundred billion iterations.

36
00:01:48,770 --> 00:01:49,480
>> Okay.

37
00:01:49,480 --> 00:01:53,590
>> Okay. Billions and billions of itterations. Okay. You got it?

38
00:01:53,590 --> 00:01:54,620
>> Yeah.

39
00:01:54,620 --> 00:01:55,630
>> Alright. Go.
