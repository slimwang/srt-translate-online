1
00:00:00,320 --> 00:00:04,620
So this brings up the issue of what neural nets

2
00:00:04,620 --> 00:00:08,420
are more or less appropriate for. What is the restriction bias,

3
00:00:08,420 --> 00:00:14,420
and the inductive bias of this class of classifiers, and

4
00:00:14,420 --> 00:00:18,800
regression algorithms? So Charles, can you remind us what restriction bias is?

5
00:00:18,800 --> 00:00:22,290
>> Well, restriction bias tells you something

6
00:00:22,290 --> 00:00:25,450
about the representational power of whatever data structure

7
00:00:25,450 --> 00:00:29,490
it is that you're using. So in this case the network of neurons.

8
00:00:29,490 --> 00:00:33,350
And it tells you the set of hypotheses that you're willing to consider.

9
00:00:33,350 --> 00:00:38,750
>> Right, so if, if the, if there's a great deal of restriction,

10
00:00:38,750 --> 00:00:41,530
then there's lots and lots of different kinds of models that we're just

11
00:00:41,530 --> 00:00:44,680
not even considering. We're, we're restricting our view to just a subset of

12
00:00:44,680 --> 00:00:49,400
those. So In the case of neural nets, what restrictions are we putting?

13
00:00:49,400 --> 00:00:51,150
>> Well,

14
00:00:51,150 --> 00:00:54,320
we started out with a simple perceptron unit, and that

15
00:00:54,320 --> 00:00:59,040
we decided was linear. So we were only considering planes. Then

16
00:00:59,040 --> 00:01:00,940
we move to networks, so that we could do things

17
00:01:00,940 --> 00:01:04,280
like Xor, and that allowed us to do more. Then we

18
00:01:04,280 --> 00:01:09,450
started sticking Sigmoids and other arbitrary functions and to nodes

19
00:01:09,450 --> 00:01:12,380
so that we could represent more and more, and you mention

20
00:01:12,380 --> 00:01:14,100
that if you let weights get big and we have

21
00:01:14,100 --> 00:01:16,700
lots of layers and lots of nodes, we can be really,

22
00:01:16,700 --> 00:01:21,410
really complex. So, it seems to me that we

23
00:01:21,410 --> 00:01:23,270
are actually not doing much of a restriction at

24
00:01:23,270 --> 00:01:27,320
all. So let me ask you this then Michael.

25
00:01:27,320 --> 00:01:30,260
What kind of functions can we represent, clearly we can

26
00:01:30,260 --> 00:01:33,060
represent boolean functions, cause we did that. Can we

27
00:01:33,060 --> 00:01:37,140
represent continuous functions? That's, that's a great question to

28
00:01:37,140 --> 00:01:38,470
ask, that's what we should try to figure that

29
00:01:38,470 --> 00:01:41,970
out. So, in the case, as you said, Boolean functions,

30
00:01:41,970 --> 00:01:45,110
we can. If we give ourselves a complex enough

31
00:01:45,110 --> 00:01:48,500
network with enough units, we can basically map all the

32
00:01:48,500 --> 00:01:51,890
different sub components of any Boolean expression to threshold

33
00:01:51,890 --> 00:01:54,910
like units and basically build a circuit that can compute

34
00:01:54,910 --> 00:01:58,000
whatever Boolean function we want. So that one definitely

35
00:01:58,000 --> 00:02:01,270
can happen. So what about continuous functions? So what is

36
00:02:01,270 --> 00:02:04,000
it? What is a continuous function? A continuous function

37
00:02:04,000 --> 00:02:07,510
is one where, as the input changes the output changes

38
00:02:08,520 --> 00:02:11,620
somewhat smoothly, right? There's no jumps in the function like that.

39
00:02:11,620 --> 00:02:15,380
>> Well, there's no discon, there's no discontinuities, that's for sure.

40
00:02:15,380 --> 00:02:16,610
>> Alright, now if we've got a continuous

41
00:02:16,610 --> 00:02:19,050
function that we're trying to model with a neural

42
00:02:19,050 --> 00:02:22,500
network. As long as it's connected, it has

43
00:02:22,500 --> 00:02:24,970
no, no discontinuous jumps to any place in the

44
00:02:24,970 --> 00:02:29,640
space, we can do this with just a single hidden layer. As long as we have enough

45
00:02:29,640 --> 00:02:31,400
hidden units, as long as there's enough units in

46
00:02:31,400 --> 00:02:33,650
that layer. And, essentially one way to think about

47
00:02:33,650 --> 00:02:37,530
that is, if we have enough hidden units, each hidden

48
00:02:37,530 --> 00:02:39,980
unit can worry about one little patch of the function

49
00:02:39,980 --> 00:02:45,070
that, that it needs to model. And they, the patches

50
00:02:45,070 --> 00:02:47,380
get set at the hidden. And at the output layer

51
00:02:47,380 --> 00:02:49,410
they get stitched together. And if you just have that

52
00:02:49,410 --> 00:02:51,690
one layer you can make any function as long as

53
00:02:51,690 --> 00:02:55,710
it's continuous. If it's Arbitrary. We can still represent that

54
00:02:55,710 --> 00:02:58,720
in our neural network. Any mapping from inputs to outputs we

55
00:02:58,720 --> 00:03:01,910
can represent, even if it's discontinuous, just by adding

56
00:03:01,910 --> 00:03:04,970
one more hidden layer, so two total hidden layers.

57
00:03:04,970 --> 00:03:06,960
And that gives us the ability to not just

58
00:03:06,960 --> 00:03:10,240
stitch these patches at their seams, but also to have

59
00:03:10,240 --> 00:03:13,340
big jumps between the patches. So in fact, neural

60
00:03:13,340 --> 00:03:16,170
networks are not very restrictive in terms of their bias

61
00:03:16,170 --> 00:03:20,100
as long as you have a sufficiently complex network

62
00:03:20,100 --> 00:03:23,600
structure, right, so maybe multiple hidden layers and multiple units.

63
00:03:23,600 --> 00:03:23,910
>> So

64
00:03:23,910 --> 00:03:26,680
that worries me a little bit Michael, because it means that we're almost

65
00:03:26,680 --> 00:03:28,720
certainly going to overfit, right? We're

66
00:03:28,720 --> 00:03:31,140
going to have arbitrarily complicated neural networks and

67
00:03:31,140 --> 00:03:34,800
we can represent anything we want to. Including all of the noise that's

68
00:03:34,800 --> 00:03:38,210
represented in our training set. So, how are we going to avoid doing that?

69
00:03:38,210 --> 00:03:41,030
>> Excellent question. So, it seems like there's, there

70
00:03:41,030 --> 00:03:44,130
is exactly that worry. But, it is the case

71
00:03:44,130 --> 00:03:46,200
though, that when we train neural networks, we typically

72
00:03:46,200 --> 00:03:49,400
give them some bounded number of hidden units and

73
00:03:49,400 --> 00:03:52,520
we give them some bounded number of layers. And so, it's

74
00:03:52,520 --> 00:03:56,180
not like any fixed network can actually capture any arbitrary function. So

75
00:03:56,180 --> 00:03:58,700
any fixed network can only capture whatever it can capture, which

76
00:03:58,700 --> 00:04:02,320
is a smaller set. So going to neural nets in general doesn't

77
00:04:02,320 --> 00:04:05,730
have much restriction. but any given network architecture actually does have

78
00:04:05,730 --> 00:04:08,540
a bit more restriction. So that's one thing, the other is hey,

79
00:04:08,540 --> 00:04:11,510
well we can do with overfitting what we've done the other

80
00:04:11,510 --> 00:04:14,610
times we've had to deal with overfitting. And that's to use ideas

81
00:04:14,610 --> 00:04:19,140
like, cross validation. And we used cross validation to

82
00:04:19,140 --> 00:04:22,720
decide. How many hidden layers to use. We can

83
00:04:22,720 --> 00:04:27,790
use it to decide how many nodes to put in each layer. And we can also use it

84
00:04:27,790 --> 00:04:30,820
to decide when to stop training because the weights

85
00:04:30,820 --> 00:04:33,400
have gotten too large. So, and this is, it's

86
00:04:33,400 --> 00:04:35,210
probably worth pointing this out that this is kind

87
00:04:35,210 --> 00:04:39,970
of a different, different property from the other classes

88
00:04:39,970 --> 00:04:42,700
of supervised learning algorithms we've looked at so far. So

89
00:04:42,700 --> 00:04:44,980
in a decision tree, you build up the decision tree, an

90
00:04:44,980 --> 00:04:50,140
you may have over fit, but it is what it is.

91
00:04:50,140 --> 00:04:52,730
In regression, you know, you solve the regression problem, and again

92
00:04:52,730 --> 00:04:55,900
that may have over fit. What's interesting about neural network training

93
00:04:55,900 --> 00:04:58,730
is it's this iterative process that you started out running, and

94
00:04:58,730 --> 00:05:01,980
as it's running, it's actually Errors going down and down and

95
00:05:01,980 --> 00:05:05,650
down. So, in this standard kind of graph, we get the

96
00:05:05,650 --> 00:05:09,220
error on the training set dropping as we increase iterations. It's

97
00:05:09,220 --> 00:05:12,130
doing a better and better job of modeling the training data.

98
00:05:12,130 --> 00:05:15,250
But, in classic style, if you look at the error in

99
00:05:15,250 --> 00:05:18,290
the, in some kind of held-out test set, or maybe in a

100
00:05:18,290 --> 00:05:21,260
cross validation set, you see the error starting out kind of

101
00:05:21,260 --> 00:05:24,430
high and maybe dropping along with this, and at some point

102
00:05:24,430 --> 00:05:27,720
It actually turns around and goes the other way. So here,

103
00:05:27,720 --> 00:05:28,990
even though we're not changing the

104
00:05:28,990 --> 00:05:30,910
network structure itself, we're just continuing

105
00:05:30,910 --> 00:05:34,560
to improve our fit, we actually get this, this pattern that

106
00:05:34,560 --> 00:05:38,950
we've seen before, that the cross validation error can turn around and,

107
00:05:38,950 --> 00:05:41,270
and at this, you know, at this low point, you might have,

108
00:05:41,270 --> 00:05:43,780
you might want to just stop training your network there. The more

109
00:05:43,780 --> 00:05:47,860
you train it, possibly the worse you'll do. And again, that, it's

110
00:05:47,860 --> 00:05:50,570
reflecting this idea that the complexity of the network is not just

111
00:05:50,570 --> 00:05:53,280
in the nodes and the layers, but also in the magnetude of

112
00:05:53,280 --> 00:05:56,040
the weights. Typically what happens in this turnaround point is that some

113
00:05:56,040 --> 00:05:57,910
weights are actually getting larger and larger

114
00:05:57,910 --> 00:06:00,920
and larger. So, just wanted to highlight that

115
00:06:00,920 --> 00:06:03,320
difference between neural net function approximation of what

116
00:06:03,320 --> 00:06:04,760
we see in some of the other algorithms
