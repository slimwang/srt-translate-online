1
00:00:00,600 --> 00:00:02,420
What can you do with RNN?

2
00:00:02,420 --> 00:00:03,550
Lots of things.

3
00:00:03,550 --> 00:00:05,500
Imagine that you have a model
that predicts the next step

4
00:00:05,500 --> 00:00:06,580
of your sequence, for example.

5
00:00:06,580 --> 00:00:10,090
You can use that to generate sequences.

6
00:00:10,090 --> 00:00:14,940
For example, text, either one word at
a time or one character at a time.

7
00:00:14,940 --> 00:00:18,780
For that, you take your sequence at
time, t, you generate the predictions

8
00:00:18,780 --> 00:00:22,120
from your RNN, and then you sample
from the predicted distribution.

9
00:00:22,120 --> 00:00:25,530
And then you pick one element
based on its probability.

10
00:00:25,530 --> 00:00:28,830
And feed that sample to
the next step and go on.

11
00:00:28,830 --> 00:00:33,700
Do this repeatedly, predict,
sample, predict, sample, and

12
00:00:33,700 --> 00:00:37,580
you can generate a pretty good
sequence of whatever your RNN models.

13
00:00:37,580 --> 00:00:40,910
There is a more sophisticated way to
do this, that gives better results.

14
00:00:40,910 --> 00:00:45,300
Because just sampling the next
prediction every time, is very greedy.

15
00:00:45,300 --> 00:00:47,730
Instead of just sampling
once at each step,

16
00:00:47,730 --> 00:00:49,415
you could imagine
sampling multiple times.

17
00:00:49,415 --> 00:00:52,425
Here we pick O but also A for example.

18
00:00:53,455 --> 00:00:56,795
When we have multiple sequences,
often call that hypotheses,

19
00:00:56,795 --> 00:00:59,915
that you could continue
predicting from at every step.

20
00:00:59,915 --> 00:01:03,175
You can choose the best sequence out
of those, by computing the total

21
00:01:03,175 --> 00:01:07,025
probability of all the characters
that you generated so far.

22
00:01:07,025 --> 00:01:10,800
By looking at the total probability
of the multiple time steps at a time.

23
00:01:10,800 --> 00:01:14,690
You prevent the sampling from
accidentally making one by choice, and

24
00:01:14,690 --> 00:01:17,950
being stuck with that one
bad decision forever.

25
00:01:17,950 --> 00:01:20,950
For example, we could have just
picked A by chance here, and

26
00:01:20,950 --> 00:01:26,170
never explored the hypothesis that O
could lead to a very sensible next word.

27
00:01:26,170 --> 00:01:29,890
Of course, if you do this the number
of sequences that you need to consider

28
00:01:29,890 --> 00:01:30,950
grows exponentially.

29
00:01:32,000 --> 00:01:35,620
There is a smarter way to do that, which
is to do what's called a Beam Search.

30
00:01:35,620 --> 00:01:39,760
You only keep, say, the most likely few
candidate sequences at every time step,

31
00:01:39,760 --> 00:01:41,600
and then simply prune the rest.

32
00:01:41,600 --> 00:01:43,620
In practice, this works very well for

33
00:01:43,620 --> 00:01:45,610
generating very good
sequences from your model.
