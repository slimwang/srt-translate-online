1
00:00:01,210 --> 00:00:04,860
The first way we prevent over fitting
is by looking at the performance under

2
00:00:04,860 --> 00:00:09,590
validation set, and stopping to
train as soon as we stop improving.

3
00:00:09,590 --> 00:00:12,790
It's called early termination, and
it's still the best way to prevent your

4
00:00:12,790 --> 00:00:15,170
network from over-optimizing
on the training set.

5
00:00:16,420 --> 00:00:19,260
Another way is to apply regularization.

6
00:00:19,260 --> 00:00:23,480
Regularizing means applying artificial
constraints on your network

7
00:00:23,480 --> 00:00:26,700
that implicitly reduce
the number of free parameters

8
00:00:26,700 --> 00:00:28,700
while not making it more
difficult to optimize.

9
00:00:29,930 --> 00:00:33,220
In the skinny jeans analogy,
think stretch pants.

10
00:00:33,220 --> 00:00:34,680
They fit just as well, but

11
00:00:34,680 --> 00:00:38,620
because they're flexible,
they don't make things harder to fit in.

12
00:00:38,620 --> 00:00:42,620
The stretch pants of
are called L2 Regularization.

13
00:00:42,620 --> 00:00:48,200
The idea is to add another term to
the loss, which penalizes large weights.

14
00:00:48,200 --> 00:00:52,430
It's typically achieved by adding the L2
norm of your weights to the loss,

15
00:00:52,430 --> 00:00:54,620
multiplied by a small constant.

16
00:00:54,620 --> 00:00:57,530
And yes, yet another hyper-parameter to tune.

17
00:00:57,530 --> 00:00:58,230
Sorry about that.
