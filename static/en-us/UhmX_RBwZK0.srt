1
00:00:00,038 --> 00:00:02,735
I find it helpful to think of the memory system as a pipe.

2
00:00:02,735 --> 00:00:07,090
Threads issuing requests stuff memory transactions into that pipe.

3
00:00:07,090 --> 00:00:11,396
For example, these could be load instructions to read in a certain address in memory.

4
00:00:11,396 --> 00:00:17,106
And the result of that transaction eventually falls out the bottom of the pipe, to head back to those threads.

5
00:00:17,106 --> 00:00:22,940
Now, the pipe is really deep. It takes 200 or 300 clock cycles for a transaction to move through this pipe.

6
00:00:22,940 --> 00:00:26,091
And the pipe is also really thick and wide.

7
00:00:26,091 --> 00:00:29,762
Okay, it's designed to be filled with many transactions at the same time

8
00:00:29,762 --> 00:00:32,726
from lots of SM's running lots of threads.

9
00:00:32,726 --> 00:00:38,157
So when you only have a few threads issuing transactions, the pipe is mostly empty.

10
00:00:38,157 --> 00:00:44,396
This could happen, for example, if you don't have all of your SM's, actively filled with threads that

11
00:00:44,396 --> 00:00:52,191
are issuing transactions, or if the latency between the transactions coming from each thread is too high.

12
00:00:52,191 --> 00:00:55,563
So if we only have a few threads issuing transactions,

13
00:00:55,563 --> 00:00:59,198
the pipe is mostly empty and not many bytes are being delivered.

14
00:00:59,198 --> 00:01:02,960
And Little's law tells us that if not many bytes are being delivered,

15
00:01:02,960 --> 00:01:05,698
then our total bandwidth is going to suffer.

16
00:01:05,698 --> 00:01:08,701
So you can make this better by having more threads issuing memory transactions,

17
00:01:08,701 --> 00:01:11,560
or by having more memory in flight per thread.

18
00:01:11,560 --> 00:01:14,343
So here's one of those strategies.

19
00:01:14,343 --> 00:01:19,261
This is measured data from taking a GPU and copying a bunch of data

20
00:01:19,261 --> 00:01:22,834
from one matrix to another, just like we're doing.

21
00:01:22,834 --> 00:01:28,379
And the important thing to notice is that we're doing it in 3 different styles.

22
00:01:28,379 --> 00:01:31,152
We're doing a 4 byte word--so this is the equivalent

23
00:01:31,152 --> 00:01:34,093
of a single precision floating point, which is what our code is doing--

24
00:01:34,093 --> 00:01:37,998
an 8 byte word--for example, if we were moving double precision floating point around,

25
00:01:37,998 --> 00:01:41,481
that's what we would be getting--or a 16 byte word.

26
00:01:41,481 --> 00:01:47,391
And there exist in CUDA data types in the 4, 8, and 16 byte, variety.

27
00:01:47,391 --> 00:01:50,104
For example, float, which is what we've been using so far,

28
00:01:50,104 --> 00:01:55,295
float 2, which is 2 adjacent floating point numbers, float 4, which is 4 adjacent floating point numbers.

29
00:01:55,295 --> 00:01:57,425
And so one option that we could do

30
00:01:57,425 --> 00:02:02,506
is we could try to restructure our code around the idea of having a single

31
00:02:02,506 --> 00:02:07,581
thread pull in 4 floating point numbers at a time and do its transpose on that.

32
00:02:07,581 --> 00:02:11,083
That would move us up from this line.

33
00:02:11,083 --> 00:02:19,037
In terms which is a percent of achieved bandwidth, up to this line, that's a healthy increase.

34
00:02:19,037 --> 00:02:25,129
On the other hand, the sort of torquing the code around to do something that's less natural--

35
00:02:25,129 --> 00:02:29,118
like manipulate 4 single floating point values at a time--

36
00:02:29,118 --> 00:02:32,341
in this case, it's not a particularly natural thing to do.

37
00:02:32,341 --> 00:02:37,934
What we have coming in--an array of floating point numbers or a matrix of floating point numbers.

38
00:02:37,934 --> 00:02:42,091
And to read them 4 at a time, in little bitty rows, and transpose those into little

39
00:02:42,091 --> 00:02:45,703
bitty columns, we can certainly do it, but I think it borders on a Ninja topic.

40
00:02:45,703 --> 00:02:50,723
While this would help, this is the kind of Ninja optimization that will prove useful.

41
00:02:50,723 --> 00:02:54,862
I don't think that it's vital. Let's talk about other things that we can do.

42
00:02:54,862 --> 00:02:59,226
So if we can't make all of our memory transactions wider very easily,

43
00:02:59,226 --> 00:03:02,315
then we can try to have more transactions.

44
00:03:02,315 --> 00:03:04,292
Let's have a quiz.

45
00:03:04,292 --> 00:03:08,187
Which of our transpose kernels, so far, probably suffer from too few transactions?

46
00:03:08,187 --> 00:03:11,500
Our first version had a single thread for the entire matrix.

47
00:03:11,500 --> 00:03:15,479
Our next version had a single thread for every row of the matrix.

48
00:03:15,479 --> 00:03:18,886
Our third version had a single thread for every element,

49
00:03:18,886 --> 00:03:21,398
and our fourth version was tiled.

50
00:03:21,398 --> 00:03:24,509
Check those which probably suffered from too few transactions.
