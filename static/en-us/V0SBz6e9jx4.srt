1
00:00:00,090 --> 00:00:02,100
So, what we're going to get out of feature transformation is

2
00:00:02,100 --> 00:00:06,820
the ability to combine these features, these words, somehow, into

3
00:00:06,820 --> 00:00:09,260
some new space where hopefully we will do a better

4
00:00:09,260 --> 00:00:12,820
job of eliminating our false positives and false negatives by combining

5
00:00:12,820 --> 00:00:16,059
words together. So, we're going to leave this example for now,

6
00:00:16,059 --> 00:00:18,460
but just to give you an intuition about how a proper

7
00:00:18,460 --> 00:00:20,930
kind of feature transformation might help you just let me

8
00:00:20,930 --> 00:00:25,600
point out that if I type in a word like car,

9
00:00:25,600 --> 00:00:30,450
you would expect, since I know what car kind of means. Let's say automobiles in

10
00:00:30,450 --> 00:00:32,810
this case, I should pick up documents or

11
00:00:32,810 --> 00:00:36,290
score documents higher if they don't contain the

12
00:00:36,290 --> 00:00:38,500
word car, but they do contain the word

13
00:00:38,500 --> 00:00:41,300
automobile. Or perhaps they contain a word like

14
00:00:41,300 --> 00:00:46,780
motor or race track. Or Tesla, right, that

15
00:00:46,780 --> 00:00:50,540
somehow words like Tesla, and automobile, and motorway,

16
00:00:50,540 --> 00:00:52,410
and anything else you can think of that has

17
00:00:52,410 --> 00:00:55,760
to do with cars probably are highly correlated or highly

18
00:00:55,760 --> 00:00:58,770
related to cars in some way. And so, it would

19
00:00:58,770 --> 00:01:01,950
make sense to combine words like automobile and car together

20
00:01:01,950 --> 00:01:04,430
into new features to pick up documents that are

21
00:01:04,430 --> 00:01:06,680
somehow related together that way. Does that make sense? So

22
00:01:06,680 --> 00:01:08,910
that's the intuition I want you to think of for

23
00:01:08,910 --> 00:01:12,200
the three algorithms that we're going to go over next. Okay?

24
00:01:12,200 --> 00:01:13,960
>> Yeah, I could definitely see how synonymy is

25
00:01:13,960 --> 00:01:15,840
going to be a win, when we index the documents,

26
00:01:15,840 --> 00:01:17,830
if we include, well, if we map them down to a

27
00:01:17,830 --> 00:01:22,332
lower dimensional feature space where one feature's used for anything sort of

28
00:01:22,332 --> 00:01:24,410
car related. I'm not sure how that's going to help with polysemy, but

29
00:01:24,410 --> 00:01:27,940
I definitely see how this feature transformation idea could be a win

30
00:01:27,940 --> 00:01:30,675
with synonymy. The way, so that's right, and I think it's, it's

31
00:01:30,675 --> 00:01:33,410
much easier to see how it works with synonomy then with polysemy.

32
00:01:33,410 --> 00:01:36,680
The way it will, it could in principle help you with polysemy

33
00:01:36,680 --> 00:01:41,140
is that it will combine a bunch of features that together eliminate

34
00:01:42,200 --> 00:01:45,430
the ambiguity of any particular word. So as you type in

35
00:01:45,430 --> 00:01:48,050
more words together it'll start to pick those thing up while also

36
00:01:48,050 --> 00:01:52,350
eliminating synonomy. So we'll see. The way it's going to work in practice

37
00:01:52,350 --> 00:01:54,240
actually is that if you think of it. Now we are talking

38
00:01:54,240 --> 00:01:56,400
about unsupervised learning. But if you think of this as a

39
00:01:56,400 --> 00:02:00,350
supervised learning problem, then you can imagine how you can ask yourself

40
00:02:00,350 --> 00:02:04,850
how to combine sets of these features, these words together such that

41
00:02:04,850 --> 00:02:07,322
they can still give you correct labels. And if you can solve

42
00:02:07,322 --> 00:02:08,919
that problem you will end up solving both

43
00:02:08,919 --> 00:02:12,210
polysemy and synonomy. Or at least minimizing their impact.

44
00:02:12,210 --> 00:02:12,860
>> Cool.

45
00:02:12,860 --> 00:02:15,572
>> OK. So lets go over this specific example to

46
00:02:15,572 --> 00:02:18,900
far more abstract examples and talk about three specific algorithms. .
