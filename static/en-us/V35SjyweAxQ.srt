1
00:00:00,450 --> 00:00:04,080
So let's look at the system
that is not even unusual,

2
00:00:04,080 --> 00:00:07,560
that combines all of these
three forms of parallelism.

3
00:00:07,560 --> 00:00:12,650
So we can have a dual socket motherboard
that has two chips with four cores on

4
00:00:12,650 --> 00:00:17,160
each chip, and each core can
simultaneously run two threads,

5
00:00:17,160 --> 00:00:19,260
using simultaneous multi-threading.

6
00:00:19,260 --> 00:00:23,690
So as far as the programmer is concerned
and as far as the operating system is

7
00:00:23,690 --> 00:00:29,560
concerned, what we really have
is two times four times two.

8
00:00:29,560 --> 00:00:33,120
Which means that 16 threads,
or for that matter,

9
00:00:33,120 --> 00:00:37,010
16 different single-threaded
programs can run simultaneously.

10
00:00:37,010 --> 00:00:41,460
And a relatively simple operating
system would just treat these

11
00:00:41,460 --> 00:00:44,900
as 16 largely equivalent cores.

12
00:00:44,900 --> 00:00:47,570
This can be a big problem.

13
00:00:47,570 --> 00:00:51,920
Because let's say that we actually have
three threads, and the operating system

14
00:00:51,920 --> 00:00:57,850
not knowing any better, just maps them
to the first three threads that it has.

15
00:00:57,850 --> 00:01:01,700
But it turns out that these two
are really threads on the same

16
00:01:01,700 --> 00:01:03,840
core that compete with each other for

17
00:01:03,840 --> 00:01:07,150
issue slots and other things
in an out of order processor.

18
00:01:07,150 --> 00:01:11,270
And then,
really the first half of these threads,

19
00:01:11,270 --> 00:01:14,245
eight of them,
are really on the same chip.

20
00:01:14,245 --> 00:01:19,015
So we're using only half of the overall
cache capacity that the system has.

21
00:01:19,015 --> 00:01:23,255
Pretty much, we have a very large cache
in the other chip that we are not using.

22
00:01:23,255 --> 00:01:28,012
So, a much smarter policy for
using three threads would be

23
00:01:28,012 --> 00:01:31,982
to put them on different
chips if we can and

24
00:01:31,982 --> 00:01:36,832
definitely on different cores if we can,
so that they don't compete.

25
00:01:36,832 --> 00:01:40,126
So the idea would be that
you would first run threads

26
00:01:40,126 --> 00:01:41,906
such that they're
distributed among the chips,

27
00:01:41,906 --> 00:01:45,996
so that you maximize the cache
capacity that you're benefiting from.

28
00:01:45,996 --> 00:01:50,816
Then we also want to make sure that
the first map a thread to each core.

29
00:01:50,816 --> 00:01:53,926
And only then do we want to actually map

30
00:01:53,926 --> 00:01:56,920
threads to the second
thread in each core.

31
00:01:56,920 --> 00:01:58,810
So that if there are fewer threads,

32
00:01:58,810 --> 00:02:01,330
they don't need to share
the resources of the same core.

33
00:02:01,330 --> 00:02:03,110
Each gets its own core.

34
00:02:03,110 --> 00:02:07,100
And that requires the operating system
to actually know what's going on.

35
00:02:07,100 --> 00:02:10,889
So most of the well known operating
systems like Windows, and Linux, and

36
00:02:10,889 --> 00:02:14,320
so on, will actually be
able to figure this out.

37
00:02:14,320 --> 00:02:18,970
But that means that as we have this
fancy hardware that has pluralism

38
00:02:18,970 --> 00:02:21,210
at different levels of granularity,

39
00:02:21,210 --> 00:02:24,590
we also have to make our
operating systems aware of that.

40
00:02:24,590 --> 00:02:27,380
So it's not just expose

41
00:02:27,380 --> 00:02:30,760
more thread contexts that
the operating system can use.

42
00:02:30,760 --> 00:02:34,250
It actually matters how you
use this thread context.
