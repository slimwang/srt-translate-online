1
00:00:00,240 --> 00:00:01,819
>> This is where LSTMs come in.

2
00:00:02,910 --> 00:00:06,309
LSTM stands for long short-term memory.

3
00:00:06,309 --> 00:00:10,220
Now, conceptually, a recurrent neural
network consists of a repetition of

4
00:00:10,220 --> 00:00:14,325
simple little units like this,
which take as an input the past,

5
00:00:14,325 --> 00:00:20,560
a new inputs, and produces a new
prediction and connects to the future.

6
00:00:20,560 --> 00:00:24,140
Now, what's in the middle of that is
typically a simple set of layers.

7
00:00:24,140 --> 00:00:27,550
Some weights and some linearities.A
typical neural network.

8
00:00:27,550 --> 00:00:28,750
With LSTMs,

9
00:00:28,750 --> 00:00:32,870
we're going to replace this little
module with a strange little machine.

10
00:00:32,870 --> 00:00:36,030
It will look complicated, but
it's still functionally a neural net.

11
00:00:36,030 --> 00:00:39,200
And you can drop it into your RNN and
not worry about it.

12
00:00:39,200 --> 00:00:42,010
Everything else about the architecture
remains the same, and

13
00:00:42,010 --> 00:00:44,710
it greatly reduces
the vanishing gradient problem.
