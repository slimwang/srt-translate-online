1
00:00:00,230 --> 00:00:02,420
Alright, so this is now the Q in the equation, again.

2
00:00:02,420 --> 00:00:06,130
Which again is one of these alphabased things. And, just to

3
00:00:06,130 --> 00:00:08,860
be clear, I really do mean, you know, alpha sub t.

4
00:00:08,860 --> 00:00:10,750
That we're, that were doing this over time. We're updating our

5
00:00:10,750 --> 00:00:13,160
learning rates as we go. It's just, sometimes it's a little

6
00:00:13,160 --> 00:00:17,370
irritating to put that, that there. But but yeah. So let's

7
00:00:17,370 --> 00:00:19,880
imagine that we're doing that. We're, we're, we're using that same

8
00:00:19,880 --> 00:00:25,770
kind of weighted process. Bumping the values around. So, what would,

9
00:00:25,770 --> 00:00:26,750
based on what we just talked about,

10
00:00:26,750 --> 00:00:28,610
Charles, what would this actually be computing?

11
00:00:28,610 --> 00:00:36,460
>> Well, it would be computing the average value that you would get for

12
00:00:36,460 --> 00:00:39,930
following, you know, kind of optimal policy

13
00:00:39,930 --> 00:00:42,260
after you take this particular action. Yeah, that

14
00:00:42,260 --> 00:00:46,340
it's, it's trying to go to this expected value and, and what is that

15
00:00:46,340 --> 00:00:48,980
expected value. So the linearity of expectation

16
00:00:48,980 --> 00:00:51,740
says that we can actually move the expectation

17
00:00:51,740 --> 00:00:55,110
to break up, break up the sum using the expectation. So this,

18
00:00:55,110 --> 00:00:59,503
the expected value of the reward is actually r of s. Mm-hm.

19
00:00:59,503 --> 00:01:02,570
>> The gamma's going to come out because of linearity of expectation. ANd

20
00:01:02,570 --> 00:01:07,110
then what we're left with is the expectation over all next dates of

21
00:01:07,110 --> 00:01:12,180
the maximum estimated value of the estimated Q value of the next state.

22
00:01:12,180 --> 00:01:16,760
But what is this distribution over S prime? It's the distribution that is

23
00:01:16,760 --> 00:01:22,900
determeined by the transtiion function. SO it's this. Which is you know this.

24
00:01:22,900 --> 00:01:27,310
So that's good. It's I'm kind of cheating though. Do you see how I'm cheating.

25
00:01:27,310 --> 00:01:29,020
>> I think I know how you are cheating but tell me.

26
00:01:29,020 --> 00:01:34,400
>> Well so when I told the story about this alpa arrow updating thing. I

27
00:01:34,400 --> 00:01:37,860
said that it convergence to the expected value

28
00:01:37,860 --> 00:01:41,670
of this quantity here. But this quantity here...

29
00:01:41,670 --> 00:01:44,590
Since it's "Q hat", it's actually changing over time.

30
00:01:44,590 --> 00:01:45,400
>> Right.

31
00:01:45,400 --> 00:01:48,640
>> So this target is actually a moving target. It's changing over time. So we

32
00:01:48,640 --> 00:01:52,440
can't quite do this analysis because the

33
00:01:52,440 --> 00:01:54,770
first step is a little bit questionable. But

34
00:01:54,770 --> 00:01:59,915
it turns out that there really is a theorem that this simple update rule, this

35
00:01:59,915 --> 00:02:02,420
Q-learning update rule, this tiny little one

36
00:02:02,420 --> 00:02:05,540
line of code, actually solves Markov decision processes.

37
00:02:05,540 --> 00:02:06,770
>> Yeah.
