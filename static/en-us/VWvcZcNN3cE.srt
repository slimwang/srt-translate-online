1
00:00:00,000 --> 00:00:02,922
So today we're going to cover the following broad topics.

2
00:00:02,922 --> 00:00:06,530
We'll start by returning to the topic of optimizing GPU programs.

3
00:00:06,530 --> 00:00:09,822
Now in unit 5, we gave some pretty specific, detailed advice.

4
00:00:09,822 --> 00:00:13,318
And unit 6 explored some examples of how to think parallel.

5
00:00:13,318 --> 00:00:15,464
Here we're going to back off and talk more generally

6
00:00:15,464 --> 00:00:18,853
about strategies that parallel programmers use to optimize their programs.

7
00:00:18,853 --> 00:00:22,588
Now the best kind of programming, as we mentioned briefly in unit 5,

8
00:00:22,588 --> 00:00:25,930
is the kind of programming that you don't do because somebody else has already programmed

9
00:00:25,930 --> 00:00:28,594
it for you and packaged into a library you can use.

10
00:00:28,594 --> 00:00:31,944
So we'll talk about some important and useful CUDA libraries that are out there.

11
00:00:31,944 --> 00:00:37,568
Some of these libraries are less about packaging up code to solve a particular problem and more about

12
00:00:37,568 --> 00:00:41,929
providing what I call programming power tools to help code up your own solutions.

13
00:00:41,929 --> 00:00:46,180
So examples familiar to C++ programmers would be the standard template library,

14
00:00:46,180 --> 00:00:48,365
or STL, or the boost library.

15
00:00:48,365 --> 00:00:51,737
We'll discuss a few such power tools for CUDA C++ programmers.

16
00:00:51,737 --> 00:00:55,615
Now to write this class we focused on the CUDA C++ language,

17
00:00:55,615 --> 00:00:58,735
but there are many other platforms for parallel programming on GPUs.

18
00:00:58,735 --> 00:01:04,411
We'll talk about CUDA platforms that support other languages from Fortran to Python to Matlab.

19
00:01:04,411 --> 00:01:08,972
And we'll also discuss cross platform accelerator solutions like Open CL,

20
00:01:08,972 --> 00:01:11,285
Open ACC and Open GL compute.

21
00:01:11,285 --> 00:01:14,169
Now GPU computing is a young field and part of what

22
00:01:14,169 --> 00:01:17,710
makes it exciting is that the hardware and software are improving each year,

23
00:01:17,710 --> 00:01:21,463
not just getting incrementally faster but also adding fundamentally new capabilities.

24
00:01:21,463 --> 00:01:24,565
So we're going to finish the unit and the course with a fantastic

25
00:01:24,565 --> 00:01:28,785
guest lecture, inviting Dr. Stephen Jones from NVIDIA to come and teach us about

26
00:01:28,785 --> 00:01:31,955
the latest advance in CUDA called Dynamic Parllelism.
