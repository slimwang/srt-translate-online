1
00:00:00,380 --> 00:00:03,650
You are now ready to connect the graph
Laplacian, its eigenvalues and

2
00:00:03,650 --> 00:00:05,840
eigenvectors and graph partitioning.

3
00:00:05,840 --> 00:00:07,950
But first some handy facts.

4
00:00:07,950 --> 00:00:09,930
I'll just state these without proof.

5
00:00:09,930 --> 00:00:13,080
If you need that,
please see the paper by Pothen et al,

6
00:00:13,080 --> 00:00:15,050
posted in the instructors notes.

7
00:00:15,050 --> 00:00:18,090
The first key fact is that
the Laplacian is symmetric.

8
00:00:18,090 --> 00:00:20,040
This follows from its definition.

9
00:00:20,040 --> 00:00:23,260
An important consequence of
this fact is the second fact.

10
00:00:23,260 --> 00:00:27,462
The eigen values of the Laplacian
will be real-valued and non-negative.

11
00:00:27,462 --> 00:00:30,820
I say real-valued as
opposed to complex-valued.

12
00:00:30,820 --> 00:00:33,530
The Laplacian's eigen vectors
will also be real-valued.

13
00:00:33,530 --> 00:00:35,620
And we can define them to be orthogonal.

14
00:00:35,620 --> 00:00:38,190
If you've forgotten what
an eigenvalue and eigonvector are,

15
00:00:38,190 --> 00:00:39,920
here's a quick reminder.

16
00:00:39,920 --> 00:00:42,520
Eigenvalues and eigenvectors are paired.

17
00:00:42,520 --> 00:00:45,570
Let's let land to i and
Q sub i be an eigenvalue and

18
00:00:45,570 --> 00:00:48,720
an eigenvector respectively of L of G.

19
00:00:48,720 --> 00:00:52,900
And the following holds,
multiply L of G by it's eigenvector

20
00:00:52,900 --> 00:00:55,540
gives you a scaled version
of the eigenvector.

21
00:00:55,540 --> 00:00:57,900
The scaling factor is the eigenvalue.

22
00:00:57,900 --> 00:01:04,708
You can also write this differently in
matrix form, L(G) Q = Q times lambda.

23
00:01:04,708 --> 00:01:07,180
Q and lambda are matrices.

24
00:01:07,180 --> 00:01:11,530
In particular, the columns of Q
are the eigenvectors of L(G).

25
00:01:11,530 --> 00:01:16,530
Lamb dot is a diagonal matrix where all
the eigen values sit on the diagonal.

26
00:01:16,530 --> 00:01:18,710
What about this orthogonal bit?

27
00:01:18,710 --> 00:01:21,750
Orthogonality tells us that
the dot product between any

28
00:01:21,750 --> 00:01:24,650
pair of eigen vectors will be
zero if they're different.

29
00:01:24,650 --> 00:01:26,520
Or one if they're the same.

30
00:01:26,520 --> 00:01:28,030
In more compact matrix notation,

31
00:01:28,030 --> 00:01:31,590
Q transposed times Q is
the identity matrix.

32
00:01:31,590 --> 00:01:34,400
Now what about this
non-negative eigen values bit?

33
00:01:34,400 --> 00:01:38,040
That tells us that all the eigen
values are at least zero.

34
00:01:38,040 --> 00:01:42,500
Now a common convention is to assume
that we can sort the eigen values So

35
00:01:42,500 --> 00:01:43,890
let's do that too.

36
00:01:43,890 --> 00:01:46,110
We'll let the eigenvalues
with the smaller indices,

37
00:01:46,110 --> 00:01:48,240
be the smaller eigenvalues.

38
00:01:48,240 --> 00:01:51,010
So in this case,
lambda0 is the smallest eigenvalue, and

39
00:01:51,010 --> 00:01:53,210
lambda1 is the second smallest.

40
00:01:53,210 --> 00:01:56,490
Now here's a third fact which
I find especially interesting.

41
00:01:56,490 --> 00:01:59,200
It says that the graph G has
K connected components if and

42
00:01:59,200 --> 00:02:03,431
only if the K smallest
values are identically zero.

43
00:02:03,431 --> 00:02:05,700
This is a fact due to Fiedler.

44
00:02:05,700 --> 00:02:08,610
And when I first learned this fact,
I was like, wow..

45
00:02:08,610 --> 00:02:14,550
I mean omg wow, like seriously,
as in srsly seriously.

46
00:02:15,560 --> 00:02:17,050
It's a cool fact because it says,

47
00:02:17,050 --> 00:02:20,580
the graph laplacian spectrum,
meaning it's eigen values,

48
00:02:20,580 --> 00:02:24,500
tells us something about the underlying
connectivity of the graph.

49
00:02:24,500 --> 00:02:26,560
Here's one more important fact.

50
00:02:26,560 --> 00:02:29,730
Let's say you have a graph that's
been partitioned into two pieces,

51
00:02:29,730 --> 00:02:33,520
one piece is the plus piece, and
the other piece is the minus piece.

52
00:02:33,520 --> 00:02:36,852
Let's let v plus be the set of
vertices in the plus piece and

53
00:02:36,852 --> 00:02:41,348
v minus the set of vertices in the minus
piece, now this is all graph language.

54
00:02:41,348 --> 00:02:43,510
What about the linear algebra side?

55
00:02:43,510 --> 00:02:47,190
Let’s define a vector x with
respect to this partition.

56
00:02:47,190 --> 00:02:50,790
Each entry of x will correspond
to a particular vertex.

57
00:02:50,790 --> 00:02:55,260
We’ll assign that entry a positive
one if the vertex lies in V plus and

58
00:02:55,260 --> 00:02:58,020
a minus one if the vertex is in V minus.

59
00:02:58,020 --> 00:03:01,030
Now, suppose you want to count
the number of edges that are being

60
00:03:01,030 --> 00:03:02,850
cut in the graph.

61
00:03:02,850 --> 00:03:05,330
This leads us to our fourth fact.

62
00:03:05,330 --> 00:03:07,986
The number of cut edges is this product.

63
00:03:07,986 --> 00:03:11,570
One-fourth x transpose L times x.

64
00:03:11,570 --> 00:03:13,500
This fact is also very nice.

65
00:03:13,500 --> 00:03:16,050
It says that if you want
to minimize edge cuts,

66
00:03:16,050 --> 00:03:18,860
you should try minimizing this product.

67
00:03:18,860 --> 00:03:21,780
Okay, so
let's quickly summarize the four facts.

68
00:03:21,780 --> 00:03:23,740
Take a moment to digest them.

69
00:03:23,740 --> 00:03:26,868
Once you've done so, you're ready for
the spectral partitioning algorithm.
