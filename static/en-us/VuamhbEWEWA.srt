1
00:00:00,440 --> 00:00:03,220
These problems are often
called the exploding and

2
00:00:03,220 --> 00:00:04,330
vanishing gradient problems.

3
00:00:04,330 --> 00:00:09,720
And we're going to fix them,
surprisingly in very different ways.

4
00:00:09,720 --> 00:00:13,940
One using a very simple hack, and
the other one with a very elegant but

5
00:00:13,940 --> 00:00:16,300
slightly complicated
change to the model.

6
00:00:16,300 --> 00:00:18,990
The simple hack is called
gradient clipping.

7
00:00:18,990 --> 00:00:21,830
In order to prevent the gradients
from growing inbounded,

8
00:00:21,830 --> 00:00:26,600
you can compute their norm and shrink
their step, when the norm grows too big.

9
00:00:26,600 --> 00:00:29,310
It's hacky, but
it's cheap and effective.

10
00:00:29,310 --> 00:00:32,549
The more difficult thing to
fix is gradient vanishing.

11
00:00:32,549 --> 00:00:36,200
Vanishing gradients make your model
only remember recent events and

12
00:00:36,200 --> 00:00:38,250
forget the more distant past,

13
00:00:38,250 --> 00:00:42,580
which means recurrent neural nets tend
to not work well past a few time steps.
