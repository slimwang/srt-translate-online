1
00:00:00,290 --> 00:00:01,400
Oh, hey Charles!

2
00:00:01,400 --> 00:00:02,180
>> Hi Michael!

3
00:00:02,180 --> 00:00:03,130
How are you doing today?

4
00:00:03,130 --> 00:00:04,019
>> I'm doing well, thank you.

5
00:00:04,019 --> 00:00:04,640
How are you?

6
00:00:04,640 --> 00:00:06,790
>> I'm doing just fine,
thank you so much for asking.

7
00:00:06,790 --> 00:00:08,460
But I do have a question.

8
00:00:08,460 --> 00:00:08,960
>> And what is that?

9
00:00:10,260 --> 00:00:11,210
>> Why are we here?

10
00:00:11,210 --> 00:00:12,270
>> Why are we here?

11
00:00:12,270 --> 00:00:14,439
>> Because we're teaching a class
on reinforcement learning.

12
00:00:14,439 --> 00:00:16,760
>> But we've already taught
a class on reinforcement learning.

13
00:00:16,760 --> 00:00:21,022
>> [SOUND] We taught a class on machine
learning, and there was a section of

14
00:00:21,022 --> 00:00:25,375
that class on reinforcement learning,
so it was kind of a mini course.

15
00:00:25,375 --> 00:00:29,100
>> Okay, but we've done this already,
so why would we be doing this again?

16
00:00:29,100 --> 00:00:31,800
>> The idea was that when we were
doing the mini course last time,

17
00:00:31,800 --> 00:00:34,980
there was all sorts of topics that
we didn't get to go into depth on.

18
00:00:34,980 --> 00:00:37,930
And it's a topic that's
really interesting to us, so

19
00:00:37,930 --> 00:00:40,700
we thought we could just kind of
expand it out and do a whole class.

20
00:00:40,700 --> 00:00:43,432
>> Okay, that seems reasonable,
but you haven't convinced me yet

21
00:00:43,432 --> 00:00:46,930
that this is really a whole class, and
that we're doing something different.

22
00:00:46,930 --> 00:00:47,950
So what are we doing that's different?

23
00:00:47,950 --> 00:00:51,298
>> So, well mostly we're just going to
be going more deeply into the theory and

24
00:00:51,298 --> 00:00:53,140
practice of reinforcement learning.

25
00:00:53,140 --> 00:00:54,880
>> Hm, okay well,
half of that sounds interesting.

26
00:00:54,880 --> 00:00:56,404
>> Oh, good.
So on the theory side,

27
00:00:56,404 --> 00:00:59,649
we're going to be studying things
like the convergence of algorithms.

28
00:00:59,649 --> 00:01:01,690
How long it takes them to converge.

29
00:01:01,690 --> 00:01:04,730
Whether you can put bounds on how many
mistakes it's going to make before it

30
00:01:04,730 --> 00:01:05,470
converges.

31
00:01:05,470 --> 00:01:07,860
Situations in which you
don't have convergence.

32
00:01:07,860 --> 00:01:12,107
These are all sorts of really, you
don't want to hear so much about this.

33
00:01:12,107 --> 00:01:12,629
>> Oh, were you talking?

34
00:01:12,629 --> 00:01:14,490
I'm sorry.
What about the practice side?

35
00:01:14,490 --> 00:01:16,620
>> All right, so on the practice side
we're going to be doing a lot more

36
00:01:16,620 --> 00:01:19,370
in terms of implementations of
reinforcement learning algorithms.

37
00:01:19,370 --> 00:01:21,160
And we're going to do, for each lesson,

38
00:01:21,160 --> 00:01:24,370
some kind of exercise where you use
a system that we built called Burlap.

39
00:01:24,370 --> 00:01:26,970
The sorts of topics we're going to
talk about include things like

40
00:01:26,970 --> 00:01:29,370
temporal difference learning,
like the TD lambda algorithm.

41
00:01:29,370 --> 00:01:30,239
>> Oh, I like TD lambda.

42
00:01:30,239 --> 00:01:31,817
I actually did a thesis on that once.

43
00:01:31,817 --> 00:01:33,872
>> Oh nice, okay good, so you should-
>> Yes, exactly once.

44
00:01:33,872 --> 00:01:36,279
>> Okay, [LAUGH] so you should be
interested in hearing about that or

45
00:01:36,279 --> 00:01:37,540
maybe even talking about that.

46
00:01:37,540 --> 00:01:41,110
We're also going to be talking about
how you can express things using

47
00:01:41,110 --> 00:01:42,230
reward functions.

48
00:01:42,230 --> 00:01:45,010
Which is a really important topic
in reinforcement learning because

49
00:01:45,010 --> 00:01:47,810
all of the way that we communicate
with the algorithms is in terms of

50
00:01:47,810 --> 00:01:49,750
their incentives in the form of rewards.

51
00:01:49,750 --> 00:01:51,080
>> I like that,
that makes a lot of sense.

52
00:01:51,080 --> 00:01:53,688
So that's sort of the practical
things that matter about making

53
00:01:53,688 --> 00:01:56,700
reinforcement learning actually
work in the, wow, let's do that.

54
00:01:56,700 --> 00:01:57,640
I like that.
What else?

55
00:01:57,640 --> 00:01:59,380
>> We should be talking
about generalization and

56
00:01:59,380 --> 00:02:00,780
scaling to some degree.

57
00:02:00,780 --> 00:02:02,450
>> Oh, I like that.
I want to talk about generalization and

58
00:02:02,450 --> 00:02:05,500
scaling, and
I want to tie it back in to abstraction.

59
00:02:05,500 --> 00:02:07,140
>> Oh, that's a really important topic,
great.

60
00:02:07,140 --> 00:02:07,920
>> Okay, cool.

61
00:02:07,920 --> 00:02:08,860
Can I do that?

62
00:02:08,860 --> 00:02:09,990
>> Sure, why don't you do that piece.

63
00:02:09,990 --> 00:02:13,120
And I'll talk about partially
observable Markov decision processes,

64
00:02:13,120 --> 00:02:15,880
which is decision-making when you
don't have complete knowledge about

65
00:02:15,880 --> 00:02:17,450
the current state of the environment.

66
00:02:17,450 --> 00:02:19,433
>> Okay, that's seems reasonable.

67
00:02:19,433 --> 00:02:20,037
I'll trade you.

68
00:02:20,037 --> 00:02:22,312
You can do POMDPs,
if you let me do options.

69
00:02:22,312 --> 00:02:23,856
>> Okay, all right.

70
00:02:23,856 --> 00:02:25,179
>> And, Monte Carlo methods.

71
00:02:25,179 --> 00:02:26,376
>> Okay, it's a deal.

72
00:02:26,376 --> 00:02:28,931
But we're also going to talk about
game theory which is awesome,

73
00:02:28,931 --> 00:02:30,752
because it has theory
right there in the name.

74
00:02:30,752 --> 00:02:33,269
>> Okay that's true, but it also has
game right there in the name, and

75
00:02:33,269 --> 00:02:34,470
that's what makes it cool.

76
00:02:34,470 --> 00:02:35,361
>> It should be fun for everyone.

77
00:02:35,361 --> 00:02:36,476
>> Yes, fun for the whole family.

78
00:02:36,476 --> 00:02:39,792
>> So, in the game theory section, we're
actually going to hearken back to things

79
00:02:39,792 --> 00:02:41,820
that we did in the previous mini course.

80
00:02:41,820 --> 00:02:44,490
But then we're going to update
it by giving more details about

81
00:02:44,490 --> 00:02:46,340
different solution concepts.

82
00:02:46,340 --> 00:02:49,654
>> Oh right, that's right, last time we
did this class we talked about all these

83
00:02:49,654 --> 00:02:52,386
things besides Nash equilibria
that were really interesting.

84
00:02:52,386 --> 00:02:53,663
And so we're going to have
a chance to talk about that now?

85
00:02:53,663 --> 00:02:54,348
>> Yep.

86
00:02:54,348 --> 00:02:57,120
>> Good, so
can we do correlated equilibrium?

87
00:02:57,120 --> 00:02:58,438
>> You can do correlated equilibrium.

88
00:02:58,438 --> 00:03:00,180
>> I'll do correlated equilibrium,
sounds good.

89
00:03:00,180 --> 00:03:01,740
>> All right, so are you ready to go?

90
00:03:01,740 --> 00:03:04,800
>> I think I'm ready to go, so let's go.
