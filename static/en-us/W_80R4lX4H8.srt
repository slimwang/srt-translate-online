1
00:00:00,640 --> 00:00:05,840
So the question then becomes, how might
an error agent learn from its mistakes?

2
00:00:05,840 --> 00:00:07,290
Learning by correcting mistakes, or

3
00:00:07,290 --> 00:00:11,950
learning from failures, really entails
answering three separate questions.

4
00:00:13,140 --> 00:00:17,740
The first question is, how can the agent
isolate the error in its former model?

5
00:00:17,740 --> 00:00:20,220
So the agent had some
model of the world.

6
00:00:20,220 --> 00:00:22,260
It made a mistake based on its model,

7
00:00:22,260 --> 00:00:25,490
how can it identify
the error in its model?

8
00:00:25,490 --> 00:00:28,410
Note that this particular problem
is very closely connected

9
00:00:28,410 --> 00:00:31,550
to the equation of diagnosis
that we discussed earlier.

10
00:00:31,550 --> 00:00:36,220
The second question is how can the agent
explain to itself that default it has

11
00:00:36,220 --> 00:00:39,840
identified the error in fact led
it to the problem to the failure.

12
00:00:40,910 --> 00:00:43,070
Having identified the fault and

13
00:00:43,070 --> 00:00:45,550
explained how the fault
led to the failure.

14
00:00:45,550 --> 00:00:48,400
The third question is how can
the agent repair the fault

15
00:00:48,400 --> 00:00:50,940
in order to prevent the error,
the failure from recurring.

16
00:00:51,950 --> 00:00:53,145
You may have noticed that earlier,

17
00:00:53,145 --> 00:00:56,775
we had related learning by correcting
mistakes with matter cognition.

18
00:00:56,775 --> 00:00:59,605
You can see how that
relationship occurs.

19
00:00:59,605 --> 00:01:01,685
The agent has some
knowledge of the world.

20
00:01:01,685 --> 00:01:03,515
That knowledge leads it to failure.

21
00:01:03,515 --> 00:01:07,215
The agent is using that failure
to repair it's own knowledge.

22
00:01:07,215 --> 00:01:13,065
It is as if the agent is looking into
itself, looking into it's own reasoning,

23
00:01:13,065 --> 00:01:15,085
into it's own knowledge and
correcting itself.

24
00:01:16,250 --> 00:01:19,300
Note once again that the learning
here is incremental.

25
00:01:19,300 --> 00:01:22,260
We are learning from
one example at a time.

26
00:01:22,260 --> 00:01:25,010
However instead of simply
learning from an example

27
00:01:25,010 --> 00:01:27,405
we are also using
explanation-based learning.

28
00:01:27,405 --> 00:01:32,215
We're trying to explain why a particular
fault led to a particular failure.

29
00:01:32,215 --> 00:01:35,300
An explanation connects them with
explanation that's learning,

30
00:01:35,300 --> 00:01:38,080
not just with the notion
of incremental learning.

31
00:01:38,080 --> 00:01:42,190
Let us see how these three questions
occur in the example of the pail.

32
00:01:42,190 --> 00:01:45,440
The first question is how
can the agent identify that

33
00:01:45,440 --> 00:01:48,870
the fact that this particular
pail has a moveable handle,

34
00:01:48,870 --> 00:01:52,830
not a fixed handle, is why this
is not a good example of a cup.

35
00:01:52,830 --> 00:01:57,610
The second question is how can the agent
with an explanation that proves why

36
00:01:57,610 --> 00:02:01,240
having that moveable handle makes
this the poor example of a cup.

37
00:02:01,240 --> 00:02:03,260
Why does that lead to a failure?

38
00:02:03,260 --> 00:02:06,435
The third question is how can
the agent change its model of a cup so

39
00:02:06,435 --> 00:02:11,935
that it never again picks an object with
a movable handle as an example of a cup.

40
00:02:11,935 --> 00:02:13,935
>> So when we talked about
explanation-based learning,

41
00:02:13,935 --> 00:02:15,445
we used another example of this as well.

42
00:02:15,445 --> 00:02:18,645
We imagined a desktop assistant
that I can just say hey,

43
00:02:18,645 --> 00:02:21,230
fetch me that important
file from last Tuesday.

44
00:02:21,230 --> 00:02:23,830
And it can construct it's own
understanding of what file I might

45
00:02:23,830 --> 00:02:25,080
be talking about.

46
00:02:25,080 --> 00:02:27,380
It has a notion of what files have
been important in the past and

47
00:02:27,380 --> 00:02:29,360
certain criteria of those files.

48
00:02:29,360 --> 00:02:31,810
So constructs an understanding
of what important is and

49
00:02:31,810 --> 00:02:34,690
tries to transfer that onto
files from last Tuesday.

50
00:02:34,690 --> 00:02:36,430
Now imagine that I told this agent hey,

51
00:02:36,430 --> 00:02:38,100
fetch me that important
file from last Tuesday.

52
00:02:38,100 --> 00:02:41,230
And it returns me a file that
actually wasn't important.

53
00:02:41,230 --> 00:02:43,700
And I say hey, that file isn't
actually important at all.

54
00:02:43,700 --> 00:02:47,350
The agent would first try to isolate
what error it made in diagnosing

55
00:02:47,350 --> 00:02:49,940
that particular document as important.

56
00:02:49,940 --> 00:02:52,900
You might, for example, notice that
every other document I ever labeled as

57
00:02:52,900 --> 00:02:56,360
important was very recent
whereas this one was really old.

58
00:02:56,360 --> 00:02:59,040
So even though it met all the criteria
for an important document

59
00:02:59,040 --> 00:03:01,460
there might be more criteria
that it didn't consider yet, and

60
00:03:01,460 --> 00:03:04,090
one of those might be that only
new documents are very important.

61
00:03:04,090 --> 00:03:07,320
It would then explain that the problem
came from the assumption that

62
00:03:07,320 --> 00:03:08,830
an old document could be important, and

63
00:03:08,830 --> 00:03:12,995
it would It would then repair its model
to say that in the future old documents

64
00:03:12,995 --> 00:03:16,815
can't be important, even if they meet
the other criteria for importance.

65
00:03:16,815 --> 00:03:20,951
>> This problem identifying the error in
one's knowledge that led to a failure

66
00:03:20,951 --> 00:03:23,435
was called credit assignment.

67
00:03:23,435 --> 00:03:25,315
Blame assignment might be a better term.

68
00:03:25,315 --> 00:03:26,725
A failure has occurred.

69
00:03:26,725 --> 00:03:31,205
What fault of gap in one's knowledge
was responsible for the failure?

70
00:03:31,205 --> 00:03:32,700
That's blame assignment.

71
00:03:32,700 --> 00:03:37,410
In this lesson, we'll be focusing on
gaps or errors in one's knowledge.

72
00:03:37,410 --> 00:03:41,960
In general, the error could be in one's
reasoning or in one's architecture.

73
00:03:41,960 --> 00:03:45,120
Credit assignment applies to all
of those different kind of errors.

74
00:03:45,120 --> 00:03:47,380
Several Herculists, Marvin Minsky for

75
00:03:47,380 --> 00:03:52,660
example, consider credit assignment to
be the central problem in learning.

76
00:03:52,660 --> 00:03:56,630
This is because error agents
live in dynamic worlds.

77
00:03:56,630 --> 00:04:00,390
Therefore, we'll never be able to
create an error agent which is perfect.

78
00:04:00,390 --> 00:04:04,057
Even if you were to create an error
agent which had complete knowledge and

79
00:04:04,057 --> 00:04:06,342
perfect reasoning that
lives in some world,

80
00:04:06,342 --> 00:04:09,190
the world around it
would changed over time.

81
00:04:09,190 --> 00:04:11,720
As it changes,
the agent will start failing.

82
00:04:11,720 --> 00:04:15,590
Once it starts failing, it must have
the ability of correcting itself,

83
00:04:15,590 --> 00:04:18,350
of correcting its known knowledge,
correcting its own reasoning,

84
00:04:18,350 --> 00:04:20,050
correcting its own architecture.

85
00:04:20,050 --> 00:04:23,545
You can see again how this
a record of meta cognition.

86
00:04:23,545 --> 00:04:27,005
The agent is not diagnosing
some electrical circuit or

87
00:04:27,005 --> 00:04:30,115
a car or software program outside.

88
00:04:30,115 --> 00:04:33,145
Instead, it is self diagnosing,
self repairing.
