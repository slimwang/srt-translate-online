1
00:00:00,310 --> 00:00:03,510
The second source of latency, as I mentioned,

2
00:00:03,510 --> 00:00:06,689
is the kernel preemption latency. Now how do

3
00:00:06,689 --> 00:00:10,240
we reduce the kernel preemption latency? Timer goes

4
00:00:10,240 --> 00:00:12,110
off when the kernel is in the middle of

5
00:00:12,110 --> 00:00:16,660
something and so we have to wait for the kernel to be ready to take that

6
00:00:16,660 --> 00:00:21,950
interrupt. The first approach is the explicitly insert

7
00:00:21,950 --> 00:00:25,330
preemption points in the kernel. So that it can

8
00:00:25,330 --> 00:00:28,470
actually look for events that may have gone off

9
00:00:28,470 --> 00:00:31,000
and take action. So that is the first approach.

10
00:00:31,000 --> 00:00:34,840
The second approach is to allow preemption of the

11
00:00:34,840 --> 00:00:40,130
kernel anytime the kernel is not manipulating shared data structures.

12
00:00:40,130 --> 00:00:42,020
Now the reason why you don't want the kernel

13
00:00:42,020 --> 00:00:44,820
to be preempted is because it is manipulating some

14
00:00:44,820 --> 00:00:47,330
shared data structures and if you preempt that that

15
00:00:47,330 --> 00:00:50,500
can result in some race conditions in the kernel. Bad

16
00:00:50,500 --> 00:00:54,800
news. So what we want to do is, we want to make sure that the kernel is

17
00:00:54,800 --> 00:00:57,800
not preempted while it is manipulating shared data

18
00:00:57,800 --> 00:01:00,270
structures. But if it is not, then we

19
00:01:00,270 --> 00:01:05,050
want to allow preemption. So these are two different approaches that can be

20
00:01:05,050 --> 00:01:09,870
combined in order to reduce the latency associated with kernel

21
00:01:09,870 --> 00:01:15,586
preemption. A technique, due to Robert Love, called the lock-breaking

22
00:01:15,586 --> 00:01:19,940
preemptible kernel combines these two ideas. And by

23
00:01:19,940 --> 00:01:23,710
combining these two ideas it reduces the spin lock

24
00:01:23,710 --> 00:01:27,020
holding time in the kernel. The idea is

25
00:01:27,020 --> 00:01:29,960
the following. So there will be a long critical

26
00:01:29,960 --> 00:01:32,530
section in the kernel where in it is

27
00:01:32,530 --> 00:01:35,910
manipulating some shared data structures, but it also doing

28
00:01:35,910 --> 00:01:38,330
other things within this critical section. So what

29
00:01:38,330 --> 00:01:41,120
we're going to do is apply these two principles and

30
00:01:41,120 --> 00:01:44,480
break this long critical section as follows.

31
00:01:44,480 --> 00:01:48,030
So we're going to change this critical section code

32
00:01:48,030 --> 00:01:51,740
into two sections: you acquire the lock, you

33
00:01:51,740 --> 00:01:54,560
manipulate the shared data structures, and you release

34
00:01:54,560 --> 00:01:56,720
the lock. And the reason you're releasing the

35
00:01:56,720 --> 00:01:58,820
lock is because you're done with manipulating the

36
00:01:58,820 --> 00:02:02,500
shared data structures, and then we will reacquire

37
00:02:02,500 --> 00:02:06,300
the lock, and finally release it. So essentially

38
00:02:06,300 --> 00:02:09,710
we replaced this long critical section by two

39
00:02:09,710 --> 00:02:14,200
shorter critical sections, one manipulating shared data and the

40
00:02:14,200 --> 00:02:16,690
rest of the code here. What is the advantage

41
00:02:16,690 --> 00:02:19,320
of doing this? What we can do is, at

42
00:02:19,320 --> 00:02:22,450
this point, once we are done with manipulating

43
00:02:22,450 --> 00:02:25,300
shared data, we release the lock and at this

44
00:02:25,300 --> 00:02:29,360
point we can preempt the kernel safetly. And since

45
00:02:29,360 --> 00:02:31,780
we can preempt the kernel at this point it's

46
00:02:31,780 --> 00:02:35,270
a great opportunity to check for expired timers

47
00:02:35,270 --> 00:02:37,920
at this point. If there are expired timers,

48
00:02:37,920 --> 00:02:41,450
dispatch them, reprogram one-shot timers if need be.

49
00:02:41,450 --> 00:02:43,730
All of that stuff can be done at this

50
00:02:43,730 --> 00:02:46,500
point, and then you can come back, reacquire

51
00:02:46,500 --> 00:02:49,560
the lock, and continue with the critical section.

52
00:02:49,560 --> 00:02:51,950
So that's the idea of the lock breaking

53
00:02:51,950 --> 00:02:57,460
preemptible kernel that combines these two ideas of explicit

54
00:02:57,460 --> 00:03:02,240
insertion of preemption points and allowing preemption any time

55
00:03:02,240 --> 00:03:05,840
the kernel is not manipulating shared data structures. The

56
00:03:05,840 --> 00:03:10,040
third source of time latency I mentioned is the

57
00:03:10,040 --> 00:03:14,320
scheduling latency, that is, the timer event goes off,

58
00:03:14,320 --> 00:03:16,040
and we want to schedule the app that is

59
00:03:16,040 --> 00:03:18,060
going to deal with the timer event as quickly

60
00:03:18,060 --> 00:03:20,790
as possible, but the scheduler is in the way.

61
00:03:20,790 --> 00:03:22,590
How do we quickly make sure that that app

62
00:03:22,590 --> 00:03:29,485
gets scheduled? Firm timer implementation in TS Linux uses a combination of two

63
00:03:29,485 --> 00:03:34,080
principles. One is called Proportional Period Scheduling

64
00:03:34,080 --> 00:03:36,430
and in Proportional Period Scheduling what is

65
00:03:36,430 --> 00:03:42,140
being done is that, when a task, so these things represent tasks T1

66
00:03:42,140 --> 00:03:48,370
and T2, when a task starts up it is going to say that it needs

67
00:03:48,370 --> 00:03:53,090
a certain proportion of the CPU time to be allocated to it in every

68
00:03:53,090 --> 00:03:57,840
time quantum. So there are two parameters associated with proportional period

69
00:03:57,840 --> 00:04:03,420
scheduling, Q and T. T is a time window, a time

70
00:04:03,420 --> 00:04:09,450
quantum, and this is the time quantum that is exposed to an application.

71
00:04:09,450 --> 00:04:14,160
And the application can say, within the time quantum T, I

72
00:04:14,160 --> 00:04:19,230
need a certain proportion of the time for my task. So T1

73
00:04:19,230 --> 00:04:24,240
might say that, in any time T, I need two thirds of the time to be

74
00:04:24,240 --> 00:04:29,810
devoted to my task and if another task, T2, says in any time

75
00:04:29,810 --> 00:04:35,180
period T, I need one-third of the CPU to be devoted to my task,

76
00:04:35,180 --> 00:04:39,845
then these two requests can be obviously satisfied by

77
00:04:39,845 --> 00:04:46,330
TS Linux because the two add to the periodicity of scheduling T. And this is

78
00:04:46,330 --> 00:04:50,200
the idea behind proportional period scheduling, in which

79
00:04:50,200 --> 00:04:53,020
what the scheduler does is admission control at

80
00:04:53,020 --> 00:04:58,640
the time that a process starts up. If it asks for a certain proportion of time

81
00:04:58,640 --> 00:05:01,460
it sees whether it is possible to satisfy

82
00:05:01,460 --> 00:05:05,290
that request. So, for example, if already the

83
00:05:05,290 --> 00:05:11,700
scheduler has promised T1 two-thirds of the time, and T2 comes along and says I

84
00:05:11,700 --> 00:05:13,290
also need two-thirds of the time in

85
00:05:13,290 --> 00:05:16,480
every period TS Linux is going to say uh-huh,

86
00:05:16,480 --> 00:05:19,220
no, cannot do that, because it doesn't

87
00:05:19,220 --> 00:05:22,800
have the capacity to accommodate both these requests

88
00:05:22,800 --> 00:05:26,050
simultaneously. So that's the idea behind Proportional

89
00:05:26,050 --> 00:05:30,640
Period Scheduling. So it provides temporal protection by

90
00:05:30,640 --> 00:05:37,490
allocating each task a fixed proportion of the CPU during each task

91
00:05:37,490 --> 00:05:43,210
period T. And both this Q and T parameters are adjustable parameters using a

92
00:05:43,210 --> 00:05:46,060
feedback control mechanism. And so this,

93
00:05:46,060 --> 00:05:49,560
in essence, improves the accuracy of scheduling

94
00:05:49,560 --> 00:05:51,800
analysis that you can do on behalf

95
00:05:51,800 --> 00:05:55,990
of processes that are time-sensitive. The second

96
00:05:55,990 --> 00:06:02,630
technique that is used in TS Linux for reducing scheduling latency is to use

97
00:06:02,630 --> 00:06:09,290
priority scheduling. And let me motivate that by introducing a problem

98
00:06:09,290 --> 00:06:15,750
that plagues real time tasks and that is what is called priority inversion.

99
00:06:15,750 --> 00:06:21,240
So here is a high priority task C1 and it needs some service.

100
00:06:21,240 --> 00:06:26,240
And it gets that service by calling a server. And this call is a blocking

101
00:06:26,240 --> 00:06:31,730
call to the server. And the server itself may be a low priority server.

102
00:06:31,730 --> 00:06:36,800
For example, this client may contact a window manager to

103
00:06:36,800 --> 00:06:39,730
say, I need a portion of the window and this

104
00:06:39,730 --> 00:06:41,360
is what I want you to do in terms of

105
00:06:41,360 --> 00:06:43,616
painting that portion of the window. That might be a

106
00:06:43,616 --> 00:06:46,244
call that a high priority task is making to a

107
00:06:46,244 --> 00:06:49,091
low priority server and this is a blocking call

108
00:06:49,091 --> 00:06:51,500
and till the server is done with its work

109
00:06:51,500 --> 00:06:55,216
the high priority task cannot continue execution. So if

110
00:06:55,216 --> 00:06:58,472
you look at the timeline, C1 is running and

111
00:06:58,472 --> 00:07:01,464
it makes a blocking call at this point and

112
00:07:01,464 --> 00:07:04,986
the server takes over. And this is the service

113
00:07:04,986 --> 00:07:08,626
time for the server to execute the blocking call

114
00:07:08,626 --> 00:07:11,574
made by C1. So at the end of this service

115
00:07:11,574 --> 00:07:17,300
time, C1 is ready to run again. But not so fast. It could be

116
00:07:17,300 --> 00:07:22,380
that during the service time of this low priority server, on behalf of

117
00:07:22,380 --> 00:07:29,695
C1, some of the higher priority tasks C2, which perhaps was waiting for some

118
00:07:29,695 --> 00:07:32,850
I/O completion, becomes runnable again. If it

119
00:07:32,850 --> 00:07:36,960
becomes runnable again, then this higher priority

120
00:07:36,960 --> 00:07:40,370
task, compared to this server, is going to

121
00:07:40,370 --> 00:07:43,310
preempt the server and take over the CPU.

122
00:07:43,310 --> 00:07:46,120
So what happens at this point is that

123
00:07:46,120 --> 00:07:49,300
this medium priority task, because it is higher

124
00:07:49,300 --> 00:07:52,860
than the servers priority, it's going to take over,

125
00:07:52,860 --> 00:07:56,150
preempting the server, and that is essentially a

126
00:07:56,150 --> 00:07:59,260
priority inversion as far as C1 is concerned

127
00:07:59,260 --> 00:08:02,500
because C1 is higher in priority than C2.

128
00:08:02,500 --> 00:08:06,570
But unfortunately, at this point of time, the

129
00:08:06,570 --> 00:08:09,360
server is the one that is scheduled and

130
00:08:09,360 --> 00:08:12,320
that is lower priority than either of these

131
00:08:12,320 --> 00:08:16,530
two guys. And therefore, C2 happily preempts the

132
00:08:16,530 --> 00:08:21,300
server and takes over the CPU. But from the point of view of C1, that's a

133
00:08:21,300 --> 00:08:24,954
priority inversion. And it is a time-sensitive task

134
00:08:24,954 --> 00:08:29,970
that affects the sensitivity of the time-sensitive tasks.

135
00:08:29,970 --> 00:08:35,990
This is where the priority-based scheduling of TSL saves the day for us.

136
00:08:35,990 --> 00:08:42,640
Basically, the idea is that when C1 makes a request to the server, the server

137
00:08:42,640 --> 00:08:48,710
is going to assume the priority of C1. Even though normally, it has a static

138
00:08:48,710 --> 00:08:55,020
priority which is lower than the priority of C1. Because C1 is making this call,

139
00:08:55,020 --> 00:08:59,520
the server's priority is going to be boosted to be the priority of C1

140
00:08:59,520 --> 00:09:04,560
itself. So for the duration of the service time of the server the

141
00:09:04,560 --> 00:09:09,890
priority of the server task is going to be the same as the priority

142
00:09:09,890 --> 00:09:14,910
of C1, which is higher. Now, C2, when it becomes

143
00:09:14,910 --> 00:09:20,400
runnable, it cannot preempt the server because the server is now running

144
00:09:20,400 --> 00:09:26,780
at the priority of C1. And therefore, we avoid the priority inversion that

145
00:09:26,780 --> 00:09:29,115
would have normally happened because of the

146
00:09:29,115 --> 00:09:32,720
priority-based scheduling in TS linux, so the

147
00:09:32,720 --> 00:09:34,640
upside is that there will be no

148
00:09:34,640 --> 00:09:38,710
priority inversion with this priority-based scheduling mechanism

149
00:09:38,710 --> 00:09:46,170
that is there in TS linux. So to recap, TS linux avoids scheduling latency

150
00:09:46,170 --> 00:09:52,740
to shrink the distance between the event happening and event activation by doing

151
00:09:52,740 --> 00:09:55,915
this admission control through the proportion period

152
00:09:55,915 --> 00:10:00,530
scheduler and also it avoids priority inversion

153
00:10:00,530 --> 00:10:03,070
by using this priority-based scheduling. So

154
00:10:03,070 --> 00:10:05,900
these two mechanisms allow shrinking that distance

155
00:10:05,900 --> 00:10:08,690
as well. The other advantage of the

156
00:10:08,690 --> 00:10:12,990
Proportion Period Scheduling is that TS Linux

157
00:10:12,990 --> 00:10:19,560
can have a control over how much of the CPU time is devoted to time-sensitive

158
00:10:19,560 --> 00:10:21,580
tasks so that it can reserve a

159
00:10:21,580 --> 00:10:25,580
portion of the time for throughput-oriented tasks. So

160
00:10:25,580 --> 00:10:31,420
for instance, it could say, within any period T, I'm going to reserve a third of

161
00:10:31,420 --> 00:10:35,330
the time for throughput-oriented tasks, so that

162
00:10:35,330 --> 00:10:38,790
even if there are time-sensitive tasks running throughput-oriented

163
00:10:38,790 --> 00:10:43,260
tasks are going to get their dibs for running on the CPU. And that way, we can

164
00:10:43,260 --> 00:10:50,320
make sure that while supporting the timeliness of time sensitive tasks, TS-Linux

165
00:10:50,320 --> 00:10:54,820
can also ensure that throughput oriented tasks

166
00:10:54,820 --> 00:10:57,050
are able to make forward progress. So

167
00:10:57,050 --> 00:11:00,795
the three ideas that are enshrined in

168
00:11:00,795 --> 00:11:04,000
TS Linux, for dealing with time-sensitive tasks are,

169
00:11:04,000 --> 00:11:10,030
first of all, coming up with the firm timer design that increases the

170
00:11:10,030 --> 00:11:15,980
accuracy of the timer without exorbitant overhead in dealing with

171
00:11:15,980 --> 00:11:21,830
one shock timer interrupts. Second, using a preemptible kernel to reduce

172
00:11:21,830 --> 00:11:25,140
the kernel preemption latency and third,

173
00:11:25,140 --> 00:11:29,160
using priority-based scheduling to avoid priority

174
00:11:29,160 --> 00:11:35,282
inversion and guaranteeing a portion of the CPU time to be allowed for

175
00:11:35,282 --> 00:11:38,760
time-sensitive tasks. Those are the ways

176
00:11:38,760 --> 00:11:42,600
by which the distance between event happening

177
00:11:42,600 --> 00:11:48,055
and event activation can be deduced and we can get good performance for

178
00:11:48,055 --> 00:11:51,290
time-sensitive applications even though the operating system

179
00:11:51,290 --> 00:11:53,425
is a commodity operating system like Linux.
