1
00:00:00,000 --> 00:00:05,000
There was a very interesting and wide-reaching question on the forums

2
00:00:05,000 --> 00:00:09,000
about the uses of lexical analysis and regular expressions

3
00:00:09,000 --> 00:00:14,000
and this general sort of string processing outside of what we're doing in this class.

4
00:00:14,000 --> 00:00:19,000
The essence of the question was, what else is this good for in the real world?

5
00:00:19,000 --> 00:00:21,000
Is it used to do anything interesting?

6
00:00:21,000 --> 00:00:24,000
And the question is actually a resounding yes.

7
00:00:24,000 --> 00:00:30,000
There are a large number of great uses of regular expressions and lexical analysis

8
00:00:30,000 --> 00:00:34,000
beyond just building web browsers or writing language interpreters.

9
00:00:34,000 --> 00:00:37,000
One of the most basic is in electronic commerce.

10
00:00:37,000 --> 00:00:39,000
I hinted at this in the beginning of Unit 1,

11
00:00:39,000 --> 00:00:43,000
but things like phone numbers and credit card numbers are recognized

12
00:00:43,000 --> 00:00:46,000
and processed using regular expressions--

13
00:00:46,000 --> 00:00:48,000
essentially using lexical analysis.

14
00:00:48,000 --> 00:00:53,000
Break down a big input string from the user into something that you want to deal with.

15
00:00:53,000 --> 00:00:56,000
For example, users often enter their phone numbers or credit card numbers

16
00:00:56,000 --> 00:00:59,000
with hyphens in the middle, but maybe you want to get rid of all of the hyphens

17
00:00:59,000 --> 00:01:05,000
so that you can just treat it as one big number and ask about it in your database.

18
00:01:05,000 --> 00:01:10,000
However, quite a few other things make use of lexical analysis.

19
00:01:10,000 --> 00:01:14,000
Let me give you one you probably haven't thought of: virus detection.

20
00:01:14,000 --> 00:01:18,000
These days there are a number of adversaries out there in the world of computing--

21
00:01:18,000 --> 00:01:21,000
people who are interested in taking over your machine remotely,

22
00:01:21,000 --> 00:01:23,000
perhaps for economic reasons.

23
00:01:23,000 --> 00:01:26,000
Maybe they'd like to take over your machine and use it to send spam

24
00:01:26,000 --> 00:01:30,000
or do something else malicious--delete your files, something fun like that.

25
00:01:30,000 --> 00:01:35,000
It turns out that the virus software you probably have running on your computer now--

26
00:01:35,000 --> 00:01:41,000
that is, the antivirus software, the good stuff designed to keep away the evil attackers--

27
00:01:41,000 --> 00:01:44,000
is based on regular expressions and lexing.

28
00:01:44,000 --> 00:01:47,000
In essence, in order to be relatively interesting,

29
00:01:47,000 --> 00:01:51,000
a virus typically has to have some sort of important payload,

30
00:01:51,000 --> 00:01:55,000
a part where it replicates and takes over another program,

31
00:01:55,000 --> 00:01:59,000
writing out something to the disk or changing data structures in memory.

32
00:01:59,000 --> 00:02:04,000
A virus definition file, which you may have seen your antivirus software talk about updating,

33
00:02:04,000 --> 00:02:08,000
is actually just a big list of tokens.

34
00:02:08,000 --> 00:02:11,000
For every virus out there, we write a regular expression

35
00:02:11,000 --> 00:02:14,000
corresponding to the machine code,

36
00:02:14,000 --> 00:02:20,000
corresponding to this payload that it uses to take over or infect other files.

37
00:02:20,000 --> 00:02:22,000
And a virus scanner is actually just like a lexer

38
00:02:22,000 --> 00:02:26,000
except that if you match any of those tokens, it's a bad scene.

39
00:02:26,000 --> 00:02:31,000
A virus scanner runs all of those finite state machines over executable programs

40
00:02:31,000 --> 00:02:34,000
before you run them or attachments you download from the Internet.

41
00:02:34,000 --> 00:02:37,000
And if they find any of those patterns, they stop and say,

42
00:02:37,000 --> 00:02:42,000
"Oh, I think this file may be malicious. Let's do something special with it."

43
00:02:42,000 --> 00:02:47,000
So virus scanning or virus checking also uses regular expressions.

44
00:02:47,000 --> 00:02:49,000
Here's a third use.

45
00:02:49,000 --> 00:02:55,000
String matching in general is commonly used for dealing with computational biology

46
00:02:55,000 --> 00:02:59,000
like DNA sequencing or protein folding.

47
00:02:59,000 --> 00:03:02,000
When we get pieces of DNA from biological laboratories,

48
00:03:02,000 --> 00:03:05,000
you can view human DNA or animal DNA in the world

49
00:03:05,000 --> 00:03:11,000
as basically just a long string over a very simple alphabet of 4 characters--G, C, A, T--

50
00:03:11,000 --> 00:03:15,000
corresponding to special objects or special structures

51
00:03:15,000 --> 00:03:18,000
in the world of biology and biochemistry.

52
00:03:18,000 --> 00:03:21,000
And everything about us, we believe, in the physical world

53
00:03:21,000 --> 00:03:27,000
is based on the interactions or the unrolling or instructions gleaned from DNA,

54
00:03:27,000 --> 00:03:30,000
perhaps through DNA or RNA transcription.

55
00:03:30,000 --> 00:03:34,000
I'm not much of a biologist, but I know that these days a lot of biology research

56
00:03:34,000 --> 00:03:38,000
is done with the aid of or enabled by computers.

57
00:03:38,000 --> 00:03:43,000
So for example, if I want to check to see if 2 particular DNA sources

58
00:03:43,000 --> 00:03:47,000
have a relatively high overlap but when they come out of the biology lab

59
00:03:47,000 --> 00:03:51,000
these strings are a bit scrambled because they've gone through a physical process,

60
00:03:51,000 --> 00:03:54,000
what I essentially want to do is say, "Oh, could this blob of text over here

61
00:03:54,000 --> 00:03:58,000
"match up with this DNA text from some other person?"

62
00:03:58,000 --> 00:04:01,000
"Can I find the best matching or the best alignment?"

63
00:04:01,000 --> 00:04:06,000
And that task uses exactly the same sort of lexical analysis string matching

64
00:04:06,000 --> 00:04:08,000
that we've talked about here.

65
00:04:08,000 --> 00:04:10,000
But I can take it 1 step further.

66
00:04:10,000 --> 00:04:14,000
You might have wondered at some point, "How do we come up with new drugs?"

67
00:04:14,000 --> 00:04:18,000
"How do I make a better aspirin? How does that sort of thing go?"

68
00:04:18,000 --> 00:04:21,000
And while there are definitely in-lab scientific techniques involved--

69
00:04:21,000 --> 00:04:25,000
you might imagine, "Oh, maybe I take some sort of test subject,

70
00:04:25,000 --> 00:04:27,000
"I inject them with my candidate medicine and see if they get better"--

71
00:04:27,000 --> 00:04:29,000
that sort of trial is very expensive.

72
00:04:29,000 --> 00:04:32,000
It would be really nice if we could use mathematical models

73
00:04:32,000 --> 00:04:34,000
to help narrow down the search space

74
00:04:34,000 --> 00:04:39,000
to help get an idea for how drugs might interact in your body, how they might behave.

75
00:04:39,000 --> 00:04:42,000
And it turns out that a lot of chemical interactions at that level

76
00:04:42,000 --> 00:04:45,000
are governed by protein folding--

77
00:04:45,000 --> 00:04:49,000
how various proteins will sort of rub against each other and interact or not.

78
00:04:49,000 --> 00:04:54,000
We can use computers to simulate protein folding

79
00:04:54,000 --> 00:04:57,000
and thus get a better handle on candidate drugs we're designing.

80
00:04:57,000 --> 00:05:00,000
Maybe we can use the computers to simulate a bunch of possibilities,

81
00:05:00,000 --> 00:05:03,000
come up with the best 10, present those to the scientists,

82
00:05:03,000 --> 00:05:05,000
and then those are the ones that we try on live subjects.

83
00:05:05,000 --> 00:05:11,000
That sort of protein folding experiment involves a number of the same sort of techniques

84
00:05:11,000 --> 00:05:13,000
that we're learning about here in lexing--

85
00:05:13,000 --> 00:05:17,000
go over strings, do comparisons, do big simulations.

86
00:05:17,000 --> 00:05:21,000
And in fact, if you're curious about this sort of genome sequence alignment,

87
00:05:21,000 --> 00:05:23,000
string matching, protein folding sort of world,

88
00:05:23,000 --> 00:05:27,000
you might want to check out BLAST--B-L-A-S-T, all capital letters--

89
00:05:27,000 --> 00:05:31,000
which is a common software project for doing just that.

90
00:05:31,000 --> 00:05:35,000
It's not necessarily the fastest, but it is one of the most popular.

91
00:05:35,000 --> 00:05:40,000
And then the final thing I would mention is there are a lot of very high level

92
00:05:40,000 --> 00:05:45,000
readability metrics, both for software and also for natural language text,

93
00:05:45,000 --> 00:05:50,000
that are based on the sort of lexing or lexical analysis or token definition principles

94
00:05:50,000 --> 00:05:52,000
that we've come to know.

95
00:05:52,000 --> 00:05:56,000
For example, one of the least expensive ways of figuring out

96
00:05:56,000 --> 00:05:59,000
the appropriate grade level for a book

97
00:05:59,000 --> 00:06:03,000
is to measure very simple things like the number of words in a sentence

98
00:06:03,000 --> 00:06:06,000
and the number of syllables per word.

99
00:06:06,000 --> 00:06:09,000
Then through some division and multiplication you can arrive at

100
00:06:09,000 --> 00:06:13,000
whether that should be given to a 10-year-old child or a 14-year-old child or whatnot

101
00:06:13,000 --> 00:06:16,000
to read based on expected reading norms.

102
00:06:16,000 --> 00:06:19,000
That sort of readability metric can be easily computed

103
00:06:19,000 --> 00:06:23,000
using exactly the sort of break a string up into words kind of lexical analysis

104
00:06:23,000 --> 00:06:25,000
that we've been focusing on.

105
00:06:25,000 --> 00:06:28,000
And that segues into the final part of this question.

106
00:06:28,000 --> 00:06:32,000
Students were asking about natural language processing.

107
00:06:32,000 --> 00:06:36,000
With the lexing and parsing, the lexical analysis and syntactical analysis

108
00:06:36,000 --> 00:06:40,000
that we'll cover in this class, do they carry over to parsing real-world languages

109
00:06:40,000 --> 00:06:43,000
like Japanese or English or German?

110
00:06:43,000 --> 00:06:47,000
And the answer is, to some degree, yes.

111
00:06:47,000 --> 00:06:50,000
Linguists or computation linguists are often interested

112
00:06:50,000 --> 00:06:52,000
in modeling real-world natural languages

113
00:06:52,000 --> 00:06:56,000
using the same sorts of formal grammars that we're going to introduce in Unit 3.

114
00:06:56,000 --> 00:06:58,000
Stay tuned. Fun stuff.

115
00:06:58,000 --> 00:07:02,000
And this question of breaking a sentence up into words is legitimately tricky

116
00:07:02,000 --> 00:07:04,000
in languages other than English.

117
00:07:04,000 --> 00:07:08,000
I hinted at this in some of the lecture material.

118
00:07:08,000 --> 00:07:13,000
But in languages like Japanese or in Latin where spaces might not be written explicitly,

119
00:07:13,000 --> 00:07:18,000
we really have to think hard about how to break down an utterance into its substructures.

120
00:07:18,000 --> 00:07:23,000
However, natural language processing is actually, in some sense, significantly harder

121
00:07:23,000 --> 00:07:27,000
than the unnatural language processing that we do here.

122
00:07:27,000 --> 00:07:32,000
Real-world languages like English are not well behaved compared to JavaScript.

123
00:07:32,000 --> 00:07:34,000
JavaScript is very artificial, it's very regular.

124
00:07:34,000 --> 00:07:38,000
We know just how it will go, what the nouns are and what the verbs are.

125
00:07:38,000 --> 00:07:41,000
In a language like English, that's very hard to figure out.

126
00:07:41,000 --> 00:07:47,000
So natural language processing is currently, to some degree, still in its infancy.

127
00:07:47,000 --> 00:07:51,000
There are still a lot of tests or a lot of tasks that we would like to be able to carry out

128
00:07:51,000 --> 00:07:54,000
on natural language that are hard to do.

129
00:07:54,000 --> 00:07:58,000
If you're interested in natural language processing, writing computer programs

130
00:07:58,000 --> 00:08:01,000
that look at human written documents and do something with them,

131
00:08:01,000 --> 00:08:05,000
a common task to start with is document summarization.

132
00:08:05,000 --> 00:08:09,000
For example, you might go to a news aggregator site on the Web

133
00:08:09,000 --> 00:08:11,000
and there's a really long article.

134
00:08:11,000 --> 00:08:14,000
Wouldn't it be nice if you could say something like, "Summarize this for me."

135
00:08:14,000 --> 00:08:18,000
"What's the gist of this? Boil it down to just a few sentences."

136
00:08:18,000 --> 00:08:23,000
That's one of the common tasks that natural language processing researchers try to solve.

137
00:08:23,000 --> 00:08:27,000
And it turns out that that task is very difficult.

138
00:08:27,000 --> 00:08:30,000
Sort of a straw algorithm, a cheap way to do it

139
00:08:30,000 --> 00:08:34,000
might be to just take the topic sentence from each paragraph

140
00:08:34,000 --> 00:08:37,000
and put them together, counting on the human who wrote it

141
00:08:37,000 --> 00:08:39,000
to have used a special structure where each paragraph

142
00:08:39,000 --> 00:08:42,000
is introduced by a declarative topic sentence.

143
00:08:42,000 --> 00:08:47,000
It turns out that that simple approach is actually relatively hard to beat

144
00:08:47,000 --> 00:08:52,000
because understanding natural language requires your computer program to have,

145
00:08:52,000 --> 00:08:55,000
in some sense, a model of semantic meaning or the rest of the world.

146
00:08:55,000 --> 00:08:59,000
People will sometimes jokingly say that natural language processing

147
00:08:59,000 --> 00:09:03,000
is AI-complete where complete has a special mathematical meaning.

148
00:09:03,000 --> 00:09:08,000
We may get to deal with jokes and puns like that in a later class on theory.

149
00:09:08,000 --> 00:09:11,000
Suffice it to say for now, to some degree, I'm actually glad

150
00:09:11,000 --> 00:09:14,000
that I work in the easy world of unnatural language processing.

151
00:09:14,000 --> 00:09:18,000
It's significantly harder to deal with the utterances we use day to day

152
00:09:18,000 --> 99:59:59,999
when talking to other people.
