1
00:00:00,210 --> 00:00:04,490
Instead of just making a one
variable kernel density estimate,

2
00:00:04,490 --> 00:00:07,720
we could use multivariate
density estimates.

3
00:00:07,720 --> 00:00:12,590
Multivariate kernel density estimates
will give us joint probability density

4
00:00:12,590 --> 00:00:17,020
functions from which we can
draw conditional probabilities.

5
00:00:17,020 --> 00:00:22,940
In this case, we can use another
second variable denoted as

6
00:00:22,940 --> 00:00:28,530
x bar which could be the difference
between f0 and f2 divided by f0.

7
00:00:29,560 --> 00:00:35,010
This will then be the difference
of the submitted charge amount and

8
00:00:35,010 --> 00:00:39,270
the Medicare allowed amount,
divided by the submitted charge amount.

9
00:00:40,280 --> 00:00:44,020
Let's take a look at what
the kernel density estimate for

10
00:00:44,020 --> 00:00:49,020
the joint distribution of x and
x bar looks like.

11
00:00:49,020 --> 00:00:51,190
Back to your iPython notebook.

12
00:00:51,190 --> 00:00:56,178
We have defined a new variable called
xbar which is the absolute value

13
00:00:56,178 --> 00:01:01,030
of the difference between f0 and
f2 divided by f0.

14
00:01:01,030 --> 00:01:06,653
We can use a very convenient
joint plot function included in

15
00:01:08,330 --> 00:01:14,340
to make the joint kernel density
estimate of the variables x and x bar.

16
00:01:14,340 --> 00:01:16,920
Let's run this piece of code.

17
00:01:16,920 --> 00:01:19,910
This might take a while to run.

18
00:01:19,910 --> 00:01:24,400
The star here indicates that the
computer is still working on your plot.

19
00:01:24,400 --> 00:01:27,860
Have patience and
you will get a nice looking figure.

20
00:01:29,310 --> 00:01:31,410
Once that code finishes running,

21
00:01:31,410 --> 00:01:35,760
you will get a two-dimensional
plot looking like this.

22
00:01:35,760 --> 00:01:40,230
On each axis we have plotted
the one dimensional PDFs.

23
00:01:41,920 --> 00:01:46,690
The joint distribution is
given as this heat map here.

24
00:01:46,690 --> 00:01:48,611
Looking at the joint PDF,

25
00:01:48,611 --> 00:01:52,811
you can see that there is
a cluster around the point 00.

26
00:01:52,811 --> 00:01:56,563
There is also another one around 0.8.

27
00:01:56,563 --> 00:02:00,110
There could be another
cluster somewhere around 0.6.

28
00:02:00,110 --> 00:02:07,000
You can ask several different
questions after you have a joint KDE.

29
00:02:07,000 --> 00:02:12,170
Notice that the multivariate kernel
density estimation was made behind

30
00:02:12,170 --> 00:02:17,910
the scene, and all you see is
the plot of the probability density.

31
00:02:17,910 --> 00:02:22,930
You can use multivariate
KDEs to do regressions.

32
00:02:22,930 --> 00:02:27,929
You can also use them to do pattern
classification using Bayes' decision

33
00:02:27,929 --> 00:02:29,020
rules.

34
00:02:29,020 --> 00:02:34,250
The usefulness of KDEs is also to
determine the shape of the data

35
00:02:34,250 --> 00:02:37,800
very quickly without having
to collect a lot of data and

36
00:02:37,800 --> 00:02:41,860
can be used for online models and
high velocity data.

37
00:02:41,860 --> 00:02:45,350
In the next lessons,
we're going to look at some ways

38
00:02:45,350 --> 00:02:49,050
to determine if we have
outliers in the data.

39
00:02:49,050 --> 00:02:52,860
We covered a little bit of kernel
density estimates as part of

40
00:02:52,860 --> 00:02:56,030
nonparametric methods in statistics.

41
00:02:56,030 --> 00:02:59,110
However, the subject is really vast.

42
00:02:59,110 --> 00:03:02,740
We recommend the book All of
Nonparametric Statistics by

43
00:03:02,740 --> 00:03:07,190
Larry Wasserman if you want to
learn more about the subject.

44
00:03:07,190 --> 00:03:11,080
A link to the book is given
in the instructors notes.

45
00:03:11,080 --> 00:03:12,360
In the next lesson,

46
00:03:12,360 --> 00:03:17,490
we're going to look at a simple way to
see if there are outliers in our data.
