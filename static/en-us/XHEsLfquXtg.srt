1
00:00:00,510 --> 00:00:04,370
You can also write at those gates as
multiplying the inputs either by 0,

2
00:00:04,370 --> 00:00:08,990
when the gate is closed, or
by 1, when the gate is open.

3
00:00:08,990 --> 00:00:12,400
That's all fine, but what does it
have to do with neural networks?

4
00:00:12,400 --> 00:00:15,750
Imagine that instead of having
binary decisions at each gate,

5
00:00:15,750 --> 00:00:17,990
you had continuous decisions.

6
00:00:17,990 --> 00:00:22,820
For example, instead of yes-no gate
at the input, you take the input and

7
00:00:22,820 --> 00:00:25,700
multiply it by a value
that's between 0 and 1.

8
00:00:25,700 --> 00:00:28,880
If it's exactly 0, no input comes in.

9
00:00:28,880 --> 00:00:32,060
If it's exactly 1,
you write it entirely to memory.

10
00:00:32,060 --> 00:00:35,610
Anything in between can be
added partially to the memory.

11
00:00:37,028 --> 00:00:38,770
Now that becomes very interesting for

12
00:00:38,770 --> 00:00:43,190
us, because if that multiplicative
factor is a continuous function

13
00:00:43,190 --> 00:00:47,180
that's also differentiable, that
means we can take derivatives of it.

14
00:00:47,180 --> 00:00:49,280
And that also means we can
back propagate through it.

15
00:00:50,530 --> 00:00:52,930
That's exactly what an LSTM is.

16
00:00:52,930 --> 00:00:54,950
We take this simple model of memory,

17
00:00:54,950 --> 00:00:57,820
we replace everything with
continuous functions.

18
00:00:57,820 --> 00:01:00,550
And make that the new Lowell
machine that's at the heart

19
00:01:00,550 --> 00:01:01,850
of a recurrent neural network.
