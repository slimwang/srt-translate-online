1
00:00:00,450 --> 00:00:02,270
All right, so here's a summary of
what we're just talking about.

2
00:00:02,270 --> 00:00:04,950
We have, actually have these two
different forms of the bellman equation.

3
00:00:04,950 --> 00:00:07,970
One V which we can think
of as standing for value.

4
00:00:07,970 --> 00:00:11,260
And one Q which we can think
of as standing for quality.

5
00:00:11,260 --> 00:00:13,940
But mostly they're just letters from
the end of the alphabet that we

6
00:00:13,940 --> 00:00:14,880
made up names for.

7
00:00:14,880 --> 00:00:16,950
So you had U for utility.

8
00:00:16,950 --> 00:00:18,510
This is V for value and Q for quality.

9
00:00:18,510 --> 00:00:20,900
But anyway, the point is that
these are just expressions,

10
00:00:20,900 --> 00:00:25,410
that represent different pieces of
this overall sequence of states and

11
00:00:25,410 --> 00:00:26,530
rewards and actions.

12
00:00:26,530 --> 00:00:29,740
>> Okay, so I got two questions.

13
00:00:29,740 --> 00:00:31,120
One is, why?

14
00:00:31,120 --> 00:00:34,560
I mean, why are we bothering to
create this quality function,

15
00:00:34,560 --> 00:00:38,060
when the value function worked
just fine, thank you very much?

16
00:00:38,060 --> 00:00:42,450
Well, I'll ask the second question
after you give me a satisfactory

17
00:00:42,450 --> 00:00:43,730
answer to the first one.

18
00:00:43,730 --> 00:00:44,425
>> So why?

19
00:00:44,425 --> 00:00:46,980
So, in some sense because
they're both equally good.

20
00:00:46,980 --> 00:00:49,400
But in fact what's going to
turn out to be the case,

21
00:00:49,400 --> 00:00:51,760
is that this Q form of
the Bellman equation.

22
00:00:51,760 --> 00:00:55,050
Is going to be much more useful in
the context of reinforcement learning.

23
00:00:55,050 --> 00:00:57,690
And the reason for
that is we're going to be able to take

24
00:00:57,690 --> 00:01:02,370
expectations of this quantity
using just experienced data.

25
00:01:02,370 --> 00:01:06,470
And you don't need to actually to have
direct access to the reward function or

26
00:01:06,470 --> 00:01:07,290
the transition function.

27
00:01:07,290 --> 00:01:08,000
To do that.

28
00:01:08,000 --> 00:01:10,580
Whereas if we're going to
try to learn V values.

29
00:01:10,580 --> 00:01:14,410
The only way to connect one
V value to the next V value

30
00:01:14,410 --> 00:01:16,860
is by knowing the transitions and
rewards.

31
00:01:16,860 --> 00:01:19,380
So this is going to be really helpful in
the context of reinforcement learning.

32
00:01:19,380 --> 00:01:21,350
Where we don't know the transitions and
rewards in advance.

33
00:01:22,440 --> 00:01:23,560
>> Okay, I'll buy that.

34
00:01:23,560 --> 00:01:26,180
So here's my second question then.

35
00:01:26,180 --> 00:01:31,350
You've got V function for
what happens when you're at a state.

36
00:01:31,350 --> 00:01:34,900
You've got a Q function would
happen when you're at an action.

37
00:01:34,900 --> 00:01:37,680
There's at least one more thing
when you're at that infinite

38
00:01:37,680 --> 00:01:40,510
jar jar jar jar jar, and that's R.

39
00:01:41,980 --> 00:01:42,680
>> Yes.

40
00:01:42,680 --> 00:01:47,060
You're saying we can group things this
way, or we can group things this way.

41
00:01:47,060 --> 00:01:48,680
So why can't we group things this way?

42
00:01:50,490 --> 00:01:52,770
>> Sure, that's what I'm asking.

43
00:01:52,770 --> 00:01:54,600
>> because if we go one step more
than that then we're back to

44
00:01:54,600 --> 00:01:56,080
the max again and that was our V.

45
00:01:56,080 --> 00:01:56,600
>> Right.

46
00:01:56,600 --> 00:02:00,150
>> So yeah, that seems to be the third
member of our little trio that's

47
00:02:00,150 --> 00:02:00,880
been left out.

48
00:02:00,880 --> 00:02:02,960
So I think maybe we should
have a quiz about that.

49
00:02:02,960 --> 00:02:03,580
>> Of course we should.
