1
00:00:00,420 --> 00:00:04,019
In the case where you're using lots of features to get the most tightly fitting

2
00:00:04,019 --> 00:00:05,050
regression or,

3
00:00:05,050 --> 00:00:09,180
or a classifier that you can, that's a classic high variance situation.

4
00:00:09,180 --> 00:00:12,240
You want to be careful that you're not overfitting to

5
00:00:12,240 --> 00:00:14,140
the data when you're doing this.

6
00:00:14,140 --> 00:00:17,300
And so, another way to frame this whole discussion about the number of features

7
00:00:17,300 --> 00:00:22,460
that you should be using can be phrased in terms of the bias variance dilemma,

8
00:00:22,460 --> 00:00:24,400
or how many features should you be using so

9
00:00:24,400 --> 00:00:26,580
that you're balancing these two concerns.

10
00:00:26,580 --> 00:00:29,620
You want to have the accurate description that comes with having enough

11
00:00:29,620 --> 00:00:33,110
variance to your model, you want it to be able to fit your data in a,

12
00:00:33,110 --> 00:00:36,140
in an accurate and true way.

13
00:00:36,140 --> 00:00:36,760
But you want to do it

14
00:00:36,760 --> 00:00:39,620
with the minimum number of features that's needed to do that.

15
00:00:41,040 --> 00:00:44,940
So there's this tradeoff between sort of the goodness of the fit and

16
00:00:44,940 --> 00:00:46,150
the simplicity of the fit,

17
00:00:46,150 --> 00:00:49,730
the number of features that you have to use to achieve that goodness of fit.

18
00:00:49,730 --> 00:00:53,440
And so what that means is you want to fit an algorithm with few features.

19
00:00:53,440 --> 00:00:57,820
But using the case of a regression as a large r squared or

20
00:00:57,820 --> 00:01:01,310
conversely a low sum of the squared residual errors.

21
00:01:01,310 --> 00:01:03,127
This is the sweet spot that you want to find.
