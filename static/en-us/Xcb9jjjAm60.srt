1
00:00:00,420 --> 00:00:03,110
Now that you know all about K-fold cross validation for

2
00:00:03,110 --> 00:00:05,530
validating the performance of your algorithm,

3
00:00:05,530 --> 00:00:07,790
I want to show you a different use of it that I really love.

4
00:00:08,900 --> 00:00:12,170
Remember back to very early in the class we were talking about

5
00:00:12,170 --> 00:00:16,239
machine learning algorithms like naive bayse support vector machines and

6
00:00:16,239 --> 00:00:19,640
decision trees and especially the parameters that you have tune for

7
00:00:19,640 --> 00:00:21,940
these to get the best performance out.

8
00:00:21,940 --> 00:00:24,530
The way we were tuning these parameters was kind of with a guess and

9
00:00:24,530 --> 00:00:25,610
check method.

10
00:00:25,610 --> 00:00:27,450
But as it turns out this is really clunky.

11
00:00:27,450 --> 00:00:28,025
It takes a long time.

12
00:00:28,025 --> 00:00:29,025
Nobody likes to do it.

13
00:00:29,025 --> 00:00:30,235
It's just the worst.

14
00:00:30,235 --> 00:00:32,856
Cross validation can actually help you out here.

15
00:00:32,856 --> 00:00:34,994
It can automate a lot of that testing.

16
00:00:34,994 --> 00:00:37,452
And pick our the parameter tune that's going to be best.

17
00:00:37,452 --> 00:00:38,960
So let me show you how to do that.
