1
00:00:00,300 --> 00:00:04,059
Here's the big picture at a high
level of how we train a q learner.

2
00:00:05,310 --> 00:00:09,700
We have our data here and we select
which data we want to train on,

3
00:00:09,700 --> 00:00:15,790
of course this data in the case of
the stock market is time series.

4
00:00:15,790 --> 00:00:21,040
And so it's arranged from oldest
to newest vertically here.

5
00:00:21,040 --> 00:00:22,640
So we select the day
we want to train on.

6
00:00:22,640 --> 00:00:27,350
And then we iterate over
this data over time.

7
00:00:27,350 --> 00:00:31,870
So we evaluate the situation there and
for

8
00:00:31,870 --> 00:00:36,250
a particular stock that
gives us s our state.

9
00:00:36,250 --> 00:00:40,330
We consult our policy and
that gives us an action.

10
00:00:40,330 --> 00:00:43,950
So we take that action,
plug it into our system here,

11
00:00:45,210 --> 00:00:51,110
evaluate the next state, and
we get our s prime and our reward.

12
00:00:52,140 --> 00:00:59,090
So after one iteration here we've got
an s, an action, an s prime, and an r.

13
00:01:00,140 --> 00:01:06,280
Or an experience tuple, and we use that
experience tuple to update our Q table.

14
00:01:06,280 --> 00:01:11,380
Once we get all the way through the
training data, we test our policy and

15
00:01:11,380 --> 00:01:13,380
we see how well it
performs in a back test.

16
00:01:14,810 --> 00:01:19,340
If it's converged or it's not getting
any better then we say we're done.

17
00:01:19,340 --> 00:01:23,200
If not, we repeat this whole process
all the way through the training data.

18
00:01:24,430 --> 00:01:26,320
So what do we mean by converge?

19
00:01:26,320 --> 00:01:31,474
Well, each time we cycle through
the data training our Q table and

20
00:01:31,474 --> 00:01:36,828
then testing back across that same data,
we get some performance.

21
00:01:36,828 --> 00:01:41,130
And we expect that each time
we complete an iteration here,

22
00:01:41,130 --> 00:01:44,838
our performance is going
to get better and better.

23
00:01:44,838 --> 00:01:50,580
But after a point it finally stops
getting better and it converges.

24
00:01:50,580 --> 00:01:53,388
So overall the chart's going to
look something like this.

25
00:01:53,388 --> 00:01:57,632
Eventually we reach this regime
where more iterations doesn't

26
00:01:57,632 --> 00:02:01,178
make it better and
we call it converged at that point.

27
00:02:01,178 --> 00:02:05,728
Let's consider now in more
detail what happens here when

28
00:02:05,728 --> 00:02:08,389
we're iterating over the data.

29
00:02:09,789 --> 00:02:12,960
So here are the details as we
iterate over our training data.

30
00:02:14,190 --> 00:02:16,190
We start by setting our start time,

31
00:02:16,190 --> 00:02:21,280
which is right here at the beginning and
we initialize our Q table.

32
00:02:21,280 --> 00:02:25,830
The usual way to initialize a Q
table is with small random numbers,

33
00:02:25,830 --> 00:02:28,430
but variations of that are fine.

34
00:02:28,430 --> 00:02:35,590
Now we're here in time and
we observe the features of our stock or

35
00:02:35,590 --> 00:02:41,340
stocks and from those build
up together our state s.

36
00:02:41,340 --> 00:02:43,380
We consult our policy or

37
00:02:43,380 --> 00:02:48,790
in other words we consult Q to find
the best action in the current state.

38
00:02:48,790 --> 00:02:49,870
That gives us a.

39
00:02:50,910 --> 00:02:57,390
Then we step forward and we see what
reward we get and what's our new state.

40
00:02:57,390 --> 00:03:02,630
We now have a complete experience tuple
that we can use to update our Q table.

41
00:03:02,630 --> 00:03:05,880
So we take this information
that we just learned and

42
00:03:05,880 --> 00:03:10,120
we improve Q based on that information.

43
00:03:10,120 --> 00:03:12,650
Then we step to the next
point in time and

44
00:03:12,650 --> 00:03:15,080
the next point time and
the next next one time and so on.

45
00:03:16,350 --> 00:03:21,930
So these are all the details of what
happens in this step of the big picture.
