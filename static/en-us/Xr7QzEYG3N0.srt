1
00:00:00,000 --> 00:00:04,000
And here we have the result of some simulations of the exploratory agent.

2
00:00:04,000 --> 00:00:09,000
We see it's doing much better than the passive agent or than the Greedy agent.

3
00:00:09,000 --> 00:00:13,000
So I'm graphing here; and we only had to go through 100 trials.

4
00:00:13,000 --> 00:00:16,000
We didn't have to go through 500--so it's converging much faster.

5
00:00:16,000 --> 00:00:19,000
And it's converging to much better results.

6
00:00:19,000 --> 00:00:22,000
So the policy loss and the dotted lines

7
00:00:22,000 --> 00:00:25,000
started off high; but after only 20 trials,

8
00:00:25,000 --> 00:00:27,000
it's come down to perfect.

9
00:00:27,000 --> 00:00:31,000
So it learned the exact, correct policy after 20 trials.

10
00:00:31,000 --> 00:00:35,000
The error in the utilities--so you can have the perfect policy,

11
00:00:35,000 --> 00:00:39,000
while not quite having the right utilities for each state--

12
00:00:39,000 --> 00:00:42,000
and the errors in utility comes down,

13
00:00:42,000 --> 00:00:47,000
and that, too, comes down to a level that's lower than the previous agent's--

14
00:00:47,000 --> 00:00:49,000
but still, not quite perfect.

15
00:00:49,000 --> 99:59:59,999
And we see here that it, in fact, learns the correct policy.
