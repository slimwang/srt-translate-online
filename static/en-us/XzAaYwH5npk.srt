1
00:00:00,000 --> 00:00:03,000
Now, we've seen probabilistic word models before.

2
00:00:03,000 --> 00:00:06,000
If you remember when we were doing machine learning,

3
00:00:06,000 --> 00:00:09,000
we talked about the bad-of-words model.

4
00:00:09,000 --> 00:00:13,000
What I'm showing you now is a copy of a bumper sticker that my friend

5
00:00:13,000 --> 00:00:18,000
Othar Hansson, who is one of the main engineers on the Google search team came up with,

6
00:00:18,000 --> 00:00:23,000
the bumper sticker, of course, says "Honk if you love the bag-of-words model,"

7
00:00:23,000 --> 00:00:28,000
but it says that in a way where the words are a bag rather than a sequence.

8
00:00:28,000 --> 00:00:31,000
This just kind of indicates the power of the model--

9
00:00:31,000 --> 00:00:35,000
that it gets the idea across while loosing all notion of sequence,

10
00:00:35,000 --> 00:00:39,000
and thus making the probabilistic model simpler to deal with.

11
00:00:39,000 --> 00:00:45,000
But we can move on from a bag-of-words model, which we can think of as a unigram--

12
00:00:45,000 --> 00:00:48,000
sometimes also called a naive Bayes model,

13
00:00:48,000 --> 00:00:54,000
because every individual word is treated as a separate factor that's unrelated

14
00:00:54,000 --> 00:01:01,000
or unconditionally independent of all the other words.

15
00:01:01,000 --> 99:59:59,999
We can move beyond those types of models to ones where we do take sequence into account.
