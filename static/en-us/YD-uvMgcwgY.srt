1
00:00:00,570 --> 00:00:02,610
Okay Michael. So let's see if we can jump

2
00:00:02,610 --> 00:00:05,500
on that notion of useless and some other word

3
00:00:05,500 --> 00:00:07,700
you used. I'm going to define what it was you

4
00:00:07,700 --> 00:00:11,250
were trying to say [LAUGH] by calling it relevance, okay?

5
00:00:11,250 --> 00:00:12,570
>> Okay.

6
00:00:12,570 --> 00:00:15,110
>> So it turns out this thing that you were hanging onto where you

7
00:00:15,110 --> 00:00:18,320
were saying that well, C which you thought was useless and then it turned

8
00:00:18,320 --> 00:00:21,470
out to be useful and you were. You were coming up with reasons why

9
00:00:21,470 --> 00:00:23,150
doesn't change anything, it doesn't provide any

10
00:00:23,150 --> 00:00:25,840
information. We can actually make very precise informal.

11
00:00:25,840 --> 00:00:27,520
So here's a formal definition of the notion

12
00:00:27,520 --> 00:00:30,570
of relevance and i'm just going to go through

13
00:00:30,570 --> 00:00:35,190
this. Let me know if it makes sense. So we have a set of features and let's just

14
00:00:35,190 --> 00:00:40,940
say they're XI, so there is X1, X2, X3. The X end features

15
00:00:40,940 --> 00:00:46,310
and any particular feature X and I. Is going to be strongly relevant,

16
00:00:47,310 --> 00:00:51,800
exactly in the case if removing that feature degrades

17
00:00:51,800 --> 00:00:56,280
the Baze optimal classifier, that is what BOC stands for, does that make sense.

18
00:00:57,910 --> 00:01:02,490
>> Did we talk about what A BOC is? Yes, way back

19
00:01:02,490 --> 00:01:08,080
in the supervised learning. Lesson on bashe learning. We ended up talking about

20
00:01:08,080 --> 00:01:11,870
the bayes optimal classifier, the one that takes the weighted average of

21
00:01:11,870 --> 00:01:13,910
all of the hypotheses, based on

22
00:01:13,910 --> 00:01:16,190
their probability of being the correct hypothesis.

23
00:01:16,190 --> 00:01:16,610
>> Okay.

24
00:01:16,610 --> 00:01:17,700
>> Remember that?

25
00:01:17,700 --> 00:01:18,340
>> Yeah.

26
00:01:18,340 --> 00:01:22,310
>> And we, and what we actually said is that the bayes optimal classifier is

27
00:01:22,310 --> 00:01:26,520
the best that you could possibly do on average, if you could actually find it.

28
00:01:26,520 --> 00:01:27,350
>> Right.

29
00:01:27,350 --> 00:01:28,410
>> That coming back to you?

30
00:01:29,430 --> 00:01:30,910
>> Yeah i mean we need a notion of

31
00:01:30,910 --> 00:01:34,770
priors and stuff to be able to define strongly relevant.

32
00:01:34,770 --> 00:01:37,790
>> No it really just says that there is a base optimal classifier which

33
00:01:37,790 --> 00:01:43,120
is to say the best you could do. That X of i is strongly

34
00:01:43,120 --> 00:01:46,560
relevant in the case if you didnt have that feature

35
00:01:46,560 --> 00:01:49,420
you couldnt do as well as the base optimal classifier

36
00:01:49,420 --> 00:01:52,080
that had access to all the features. In the quiz

37
00:01:52,080 --> 00:01:54,370
that we did before we know that the actual function

38
00:01:54,370 --> 00:01:57,410
that we're looking for really was a and b. So

39
00:01:57,410 --> 00:02:00,030
if I remove a, I can't actually compute a and

40
00:02:00,030 --> 00:02:03,660
b. Similarly if I remove b, I can't actually compute

41
00:02:03,660 --> 00:02:06,920
a and b. So both a and b are strongly relevant.

42
00:02:06,920 --> 00:02:08,280
>> Okay, I mean in this case

43
00:02:08,280 --> 00:02:11,220
is uses that fact that not only that it's a and b but there is

44
00:02:11,220 --> 00:02:15,710
nothing else that has the same information as a and the same information as b.

45
00:02:15,710 --> 00:02:18,580
>> Right, exactly. And that little, that, that difference you just

46
00:02:18,580 --> 00:02:20,180
noted is the difference between being

47
00:02:20,180 --> 00:02:22,580
strongly relevant and being weakly relevant.

48
00:02:22,580 --> 00:02:22,960
>> Oh.

49
00:02:22,960 --> 00:02:25,620
>> So, a variable, a feature is called

50
00:02:25,620 --> 00:02:27,690
weakly relevant if it's the case that it's

51
00:02:27,690 --> 00:02:30,750
not strongly relevant. So, the order of these

52
00:02:30,750 --> 00:02:33,310
definitions matter. And it turns out that there

53
00:02:33,310 --> 00:02:39,140
exists some subset of your features, let's call that subset s, such that if I

54
00:02:39,140 --> 00:02:41,970
added the feature to that subset s, it

55
00:02:41,970 --> 00:02:45,020
would in fact improve the Bayes Optimal Classifier.

56
00:02:45,020 --> 00:02:46,530
>> So in this case, we're talking about

57
00:02:46,530 --> 00:02:48,440
the Bayes Optimal Classifier on. Oh, I guess

58
00:02:48,440 --> 00:02:50,380
in both cases, we're talking about. The Bayes

59
00:02:50,380 --> 00:02:52,620
Optimal Classifier on the reduced set of features.

60
00:02:52,620 --> 00:02:53,200
>> Right.

61
00:02:53,200 --> 00:02:55,660
>> Comparing it to the Bayes Optimal Classifier on

62
00:02:55,660 --> 00:02:57,030
the full set of features in the first case.

63
00:02:57,030 --> 00:02:58,330
>> Right, or just on

64
00:02:58,330 --> 00:03:00,550
any subset of features. So in particular, in the strongly relevant

65
00:03:00,550 --> 00:03:02,770
case, that's the case. We're saying, well, what would the Bayes

66
00:03:02,770 --> 00:03:05,760
Optimal Classifier be on all of the features, versus the Bayes

67
00:03:05,760 --> 00:03:09,250
Optimal Classifier on all of the features except. X sub i,

68
00:03:10,390 --> 00:03:13,240
and if removing X sub i degrades the performance, then we

69
00:03:13,240 --> 00:03:16,330
say it's strongly relevant. On the other hand, if X sub

70
00:03:16,330 --> 00:03:19,960
i is not strongly relevant, that is, removing it doesn't actually

71
00:03:19,960 --> 00:03:23,970
hurt the base optimal classifier. It can still be weakly relevant

72
00:03:23,970 --> 00:03:25,850
in the case where there is some subset of

73
00:03:25,850 --> 00:03:28,500
the features. Such that if I added x of

74
00:03:28,500 --> 00:03:31,020
i to that subset it would improve the Bayes

75
00:03:31,020 --> 00:03:33,980
optimal classifier on just that subset. So I can

76
00:03:33,980 --> 00:03:36,650
make that a little more concrete: Imagine that we

77
00:03:36,650 --> 00:03:40,630
had another variable, let's call it e, ok? Which

78
00:03:40,630 --> 00:03:43,370
had these values. So, if you look carefully, you'll

79
00:03:43,370 --> 00:03:48,160
notice that e is in fact not a. Mhm.

80
00:03:48,160 --> 00:03:48,970
>> So that means

81
00:03:48,970 --> 00:03:53,600
neither A nor E is strongly relevant, because I can remove

82
00:03:53,600 --> 00:03:57,100
A and still learn B and A by basically making it

83
00:03:57,100 --> 00:04:00,610
B and not E, or I could move E and still

84
00:04:00,610 --> 00:04:03,530
learn A and B by simply using A and B. Agreed?

85
00:04:03,530 --> 00:04:07,270
>> Yea, so its so then B would be strongly relevant but the other two not.

86
00:04:07,270 --> 00:04:10,410
>> Right. However both A and E are

87
00:04:10,410 --> 00:04:14,090
weakly relevent. Because there exists a subset such that

88
00:04:14,090 --> 00:04:16,269
adding it back in gives you better performance.

89
00:04:16,269 --> 00:04:19,709
In particular, A is weakly relevant for any

90
00:04:19,709 --> 00:04:22,290
subset that doesn't include E. And E is

91
00:04:22,290 --> 00:04:25,530
weakly relevant for any subset that doesn't include A.

92
00:04:25,530 --> 00:04:28,010
>> Huh? And that includes the null set in this case?

93
00:04:28,010 --> 00:04:31,600
>> Yes, exactly. If you have a particular feature

94
00:04:31,600 --> 00:04:33,700
which is not strongly relevant, and is not weakly

95
00:04:33,700 --> 00:04:36,660
relevant, then we call it irrelevant. So, in this

96
00:04:36,660 --> 00:04:39,330
case, when, what you were calling C as being useless.

97
00:04:39,330 --> 00:04:42,580
What it actually is is irrelevant. Because it provides no

98
00:04:42,580 --> 00:04:45,980
information to the classifier. You might as well have just simply

99
00:04:45,980 --> 00:04:50,770
had the normal subset and just always output no and you

100
00:04:50,770 --> 00:04:53,400
would do just as well as having the value of c.

101
00:04:53,400 --> 00:04:58,830
>> And yet it somehow turned out to be helpful for the perceptron case.

102
00:04:58,830 --> 00:05:01,790
>> Right. And that's because there's another notion that

103
00:05:01,790 --> 00:05:06,130
we could think about. Which is not relevance but usefulness.

104
00:05:06,130 --> 00:05:08,340
So, let me define that for you, okay? So.
