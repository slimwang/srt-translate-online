1
00:00:00,000 --> 00:00:03,572
Now, just as on a CPU, there are different levels of optimization for the GPU.

2
00:00:03,572 --> 00:00:06,377
There's picking good algorithms in the first place.

3
00:00:06,377 --> 00:00:10,081
There's some basic principles for writing efficient code.

4
00:00:10,081 --> 00:00:13,380
There's architecture-specific detailed optimizations,

5
00:00:13,380 --> 00:00:17,484
and there's sort of bit twiddling micro-optimization at the instructions level.

6
00:00:17,484 --> 00:00:24,060
So, let's use a quiz to explore a few CPU examples. There's the use of vector registers such as SSE and AVX.

7
00:00:24,060 --> 00:00:30,130
There's the use of something like mergesort which runs an order n log n time versus insertion sort

8
00:00:30,130 --> 00:00:33,971
which runs an order n squared time. Next is writing cache-aware code,

9
00:00:33,971 --> 00:00:37,539
meaning write code that is likely to make efficient use of the cache.

10
00:00:37,539 --> 00:00:44,346
So an example would be it's generally faster to traverse across the rows of a large 2D array than down the columns,

11
00:00:44,346 --> 00:00:49,085
assuming that array is laid out in row order. You'll get better cache performance in that case.

12
00:00:49,085 --> 00:00:52,323
Or you can do explicit cache blocking, for example,

13
00:00:52,323 --> 00:00:54,356
for the L1 cache and a CPU core.

14
00:00:54,356 --> 00:00:58,827
Or finally, you could approximate the inverse square root of a floating point number

15
00:00:58,827 --> 00:01:04,534
by shifting it right one bit and subtracting it from the integer 0x5f3759df.
