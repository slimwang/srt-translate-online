1
00:00:00,000 --> 00:00:05,000
This problem involves the Q-learning agent who is currently situated at this square

2
00:00:05,000 --> 00:00:11,000
called (3,3), and executes the NORTH action trying to go up,

3
00:00:11,000 --> 00:00:17,000
but because the environment is stochastic, it actually ends up arriving at this terminal state

4
00:00:17,000 --> 00:00:19,000
with value 100.

5
00:00:19,000 --> 00:00:26,000
And what I want you to answer is how should the Q-values be updated for this state,

6
00:00:26,000 --> 00:00:31,000
and I want you to enter the Q-values over here because we don't want you to

7
00:00:31,000 --> 00:00:37,000
mess up the original, and we'll use the formula below which I should point out is

8
00:00:37,000 --> 00:00:41,000
from the Sarsa version of Q-learning,

9
00:00:41,000 --> 00:00:47,000
and in this formula, the parameter alpha--the learning rate--will take on the value of 1/2, and

10
00:00:47,000 --> 00:00:52,000
gamma--the discount rate--will be 0.9,

11
00:00:52,000 --> 00:00:57,000
and all the rewards for moving from one state to the next are 0

12
00:00:57,000 --> 00:01:01,000
with the exception of moving into the terminal state,

13
00:01:01,000 --> 00:01:06,000
and this Q of S prime, A prime--that means what goes on in the next state,

14
00:01:06,000 --> 00:01:12,000
so here we were in this S, and we took the action of going NORTH,

15
00:01:12,000 --> 00:01:18,000
and we transferred into this state, and in that state, no matter what action you take

16
00:01:18,000 --> 99:59:59,999
the Q value is always 100, so this value here will always be 100.
