1
00:00:00,000 --> 00:00:03,000
[Narrator] Let me give a much simpler example

2
00:00:03,000 --> 00:00:06,000
often called grid world for MDPs,

3
00:00:06,000 --> 00:00:08,000
and I'll be using this insanely simple

4
00:00:08,000 --> 00:00:11,000
example over here throughout this class.

5
00:00:11,000 --> 00:00:13,000
Let's assume we have a starting state over here,

6
00:00:13,000 --> 00:00:16,000
and there's 2 goal states who

7
00:00:16,000 --> 00:00:18,000
are often called absorbing states

8
00:00:18,000 --> 00:00:21,000
with very different reward or payout.

9
00:00:21,000 --> 00:00:23,000
Plus 100 for the state over here,

10
00:00:23,000 --> 00:00:25,000
minus 100 for the state over here,

11
00:00:25,000 --> 00:00:28,000
and our agent is able to move about the environment,

12
00:00:28,000 --> 00:00:30,000
and when it reaches one of those 2 states,

13
00:00:30,000 --> 00:00:33,000
the game is over and the task is done.

14
00:00:33,000 --> 00:00:37,000
Obviously the top state is much more attractive than the bottom state with minus 100.

15
00:00:37,000 --> 00:00:41,000
Now to turn this into an MDP, let's assume

16
00:00:41,000 --> 00:00:43,000
actions are somewhat stochastic.

17
00:00:43,000 --> 00:00:46,000
So suppose we had a grid cell, and we attempt to go north.

18
00:00:46,000 --> 00:00:49,000
The deterministic agent would always succeed

19
00:00:49,000 --> 00:00:53,000
to go to the north square if it's available,

20
00:00:53,000 --> 00:00:55,000
but let's assume that we only have an 80% chance

21
00:00:55,000 --> 00:00:57,000
to make it to the cell in the north.

22
00:00:57,000 --> 00:00:59,000
If there's no cell at all,

23
00:00:59,000 --> 00:01:01,000
there's a wall like over here,

24
00:01:01,000 --> 00:01:04,000
we assume with 80% chance, we just bounce back to the same cell,

25
00:01:04,000 --> 00:01:08,000
but with 10% chance, we instead go left.

26
00:01:08,000 --> 00:01:10,000
Another 10% chance, we go right.

27
00:01:10,000 --> 00:01:13,000
So if an agent is over here and wishes to go north,

28
00:01:13,000 --> 00:01:15,000
then with 80% chance, it finds itself over here,

29
00:01:15,000 --> 00:01:17,000
10% over here, 10% over here.

30
00:01:17,000 --> 00:01:19,000
If it goes north from here,

31
00:01:19,000 --> 00:01:21,000
because there's no north cell,

32
00:01:21,000 --> 00:01:23,000
it'll bounce back with 80% probability,

33
00:01:23,000 --> 00:01:25,000
10% left, 10% right.

34
00:01:25,000 --> 00:01:27,000
In a cell like this one over here,

35
00:01:27,000 --> 00:01:30,000
it'll bounce back with 90% probability,

36
00:01:30,000 --> 00:01:32,000
80 from the top and 10 from the left,

37
00:01:32,000 --> 00:01:35,000
but it still has a 10% chance of going right.

38
00:01:35,000 --> 00:01:37,000
This is a stochastic state transition which

39
00:01:37,000 --> 00:01:39,000
we can equally define for actions,

40
00:01:39,000 --> 00:01:41,000
south, west and east, and

41
00:01:41,000 --> 00:01:43,000
now we can see a situation like this

42
00:01:43,000 --> 00:01:45,000
conventional planning is insufficient.

43
00:01:45,000 --> 00:01:48,000
So for example if you're plan a sequence of actions starting over here,

44
00:01:48,000 --> 00:01:51,000
you might go north, north, east, east, east

45
00:01:51,000 --> 00:01:55,000
to reach our plus 100 absorbing or final state,

46
00:01:55,000 --> 00:01:57,000
but with this state transition model over here,

47
00:01:57,000 --> 00:02:02,000
even with the first step it might happen with 10% chance do you find yourselves over here,

48
00:02:02,000 --> 00:02:05,000
in which case conventional planning would not give us an answer.

49
00:02:05,000 --> 00:02:09,000
So we wish to have a planning method that provides an answer no matter where we are

50
00:02:09,000 --> 00:02:12,000
and that's called a policy.

51
00:02:12,000 --> 00:02:15,000
A policy assigns actions to any state.

52
00:02:15,000 --> 00:02:17,000
So for example a policy might look as follows:

53
00:02:17,000 --> 00:02:24,000
for this state, we wish to go north, north, east, east, east,

54
00:02:24,000 --> 00:02:28,000
but for this state over here, we wish to go north, maybe east over here,

55
00:02:28,000 --> 00:02:31,000
and maybe west over here.

56
00:02:31,000 --> 00:02:33,000
So each state, except for the absorbing states,

57
00:02:33,000 --> 00:02:36,000
we have to define an action to define a policy.

58
00:02:36,000 --> 99:59:59,999
The planning problem we have becomes one of finding the optimal policy
