1
00:00:00,230 --> 00:00:03,060
So lets talk about cache affinity and modern

2
00:00:03,060 --> 00:00:06,750
multicore processors, in modern multicore processors you have

3
00:00:06,750 --> 00:00:09,960
multiple cores on a single processor, and in

4
00:00:09,960 --> 00:00:12,100
addition to the multiple cores that are in a

5
00:00:12,100 --> 00:00:15,700
single processor, the processors the,selves are also hardware

6
00:00:15,700 --> 00:00:19,990
multithreaded. What hardware multithreading means is that. If

7
00:00:19,990 --> 00:00:22,150
a, if a thread that is currently running

8
00:00:22,150 --> 00:00:25,650
on a processor, on a core C1, is experiencing

9
00:00:25,650 --> 00:00:28,820
a long latency operation, for instance it misses in the

10
00:00:28,820 --> 00:00:30,880
cache and therefore has to go out in order to

11
00:00:30,880 --> 00:00:34,930
fetch the contents from memory, that's a long latency operation.

12
00:00:34,930 --> 00:00:38,430
In that case. The hardware may switch to one of the

13
00:00:38,430 --> 00:00:42,110
other threads and run those. So, in other words, it

14
00:00:42,110 --> 00:00:44,670
wants to keep this core busy. There's only one execution

15
00:00:44,670 --> 00:00:47,970
engine in this core, but it has four threads that

16
00:00:47,970 --> 00:00:50,710
it can run on this core. Depending on what these threads

17
00:00:50,710 --> 00:00:53,480
are doing, if they are involved in long latency operations,

18
00:00:53,480 --> 00:00:55,910
meaning they are going out, they're not switched out of

19
00:00:55,910 --> 00:00:59,540
the processor, in terms of operating system scheduling. It is

20
00:00:59,540 --> 00:01:03,130
just that these are the, the threads that have been scheduled

21
00:01:03,130 --> 00:01:06,660
to run on this core and the hardware is switching

22
00:01:06,660 --> 00:01:10,000
among these threads by itself without the intervention of the operating

23
00:01:10,000 --> 00:01:12,920
system. It is automatically switching among these threads depending on

24
00:01:12,920 --> 00:01:15,903
what these threads are doing. If this thread, does the memory

25
00:01:15,903 --> 00:01:19,207
access which is going outside the processor, then the hardware is

26
00:01:19,207 --> 00:01:22,440
going to say, well, you know this guy is going to be. Waiting

27
00:01:22,440 --> 00:01:24,940
for a while, so let me switch to this guy, and

28
00:01:24,940 --> 00:01:27,960
let him do its, do its work because it's possible that

29
00:01:27,960 --> 00:01:30,670
what he needs is available in the cache. And if this

30
00:01:30,670 --> 00:01:35,110
guy also makes a long latency operation, like a memory access,

31
00:01:35,110 --> 00:01:37,820
then the hardware can switch to this guy. And to this

32
00:01:37,820 --> 00:01:40,820
guy. So if all of these guys are waiting on memory,

33
00:01:40,820 --> 00:01:42,140
then of course the core is not able to

34
00:01:42,140 --> 00:01:44,520
going to be, going to be able to do anything useful until

35
00:01:44,520 --> 00:01:47,450
at least one of these memory accesses are complete. So

36
00:01:47,450 --> 00:01:51,510
that's the idea behind. Hardware multithreading. So it is not

37
00:01:51,510 --> 00:01:55,870
very unusual for modern multicore processors to employ hardware

38
00:01:55,870 --> 00:01:58,930
multithreading. So in this example I'm showing you, there are

39
00:01:58,930 --> 00:02:02,610
four cores and in each core I have four hardware

40
00:02:02,610 --> 00:02:07,190
threads. So it is a four way hardware multithreaded core.

41
00:02:07,190 --> 00:02:13,510
And I'm showing you two levels of caches, L1 and L2 cache. L1 cache is specific

42
00:02:13,510 --> 00:02:16,040
to this particular core, C1, shared by these

43
00:02:16,040 --> 00:02:20,290
threads. Similarly, L1 cache here is. Is specific to

44
00:02:20,290 --> 00:02:25,450
this core C2, shared by the threads that are on it. On the other hand this

45
00:02:25,450 --> 00:02:28,140
L2 cache, is common for all the cores.

46
00:02:28,140 --> 00:02:32,210
So [INAUDIBLE] and any, anyone of these L1 caches,

47
00:02:32,210 --> 00:02:34,740
the hope is that we were able to find it

48
00:02:34,740 --> 00:02:37,900
in the L2 cache. If the processor. Has only these

49
00:02:37,900 --> 00:02:40,980
two levels of caches, L1 cache and L2 cache. This

50
00:02:40,980 --> 00:02:43,330
thing in L2 cache is really bad news [LAUGH] because

51
00:02:43,330 --> 00:02:46,170
then, you're going all to, all to the chip. It's

52
00:02:46,170 --> 00:02:50,640
a long latency memory operation. And modern multiprocessors may in

53
00:02:50,640 --> 00:02:55,770
fact even employ even more levels of caching. In addition

54
00:02:55,770 --> 00:02:58,080
to L1 and L2, there may be an L3 cache.

55
00:02:58,080 --> 00:03:01,090
It's normal to have, modern processors having at least

56
00:03:01,090 --> 00:03:04,170
three levels of caches on the chip. And L1

57
00:03:04,170 --> 00:03:07,750
cache associated with core, and a shared L2 cache,

58
00:03:07,750 --> 00:03:10,860
and a shared L3 cache. So that's the structure

59
00:03:10,860 --> 00:03:13,120
that you might see in modern mulitprocessors. So what

60
00:03:13,120 --> 00:03:15,710
we have to think about now is. Eh, thinking

61
00:03:15,710 --> 00:03:19,440
about this cache affinity and the modern multi core

62
00:03:19,440 --> 00:03:23,180
processes and how the operating system should make its

63
00:03:23,180 --> 00:03:26,010
scheduling decisions. So here again, there's a

64
00:03:26,010 --> 00:03:30,210
partnership between the operating system and the hardware.

65
00:03:30,210 --> 00:03:34,400
The hardware is providing these hardware threads inside

66
00:03:34,400 --> 00:03:37,180
each core. And what the operating system is

67
00:03:37,180 --> 00:03:43,240
doing is picking which threads that it has in its pool of vulnerable threads and

68
00:03:43,240 --> 00:03:45,370
map them on to the threads that are

69
00:03:45,370 --> 00:03:48,400
available in the hardware. And clearly, the scheduling

70
00:03:48,400 --> 00:03:51,820
decision, what it tries to do is to make sure

71
00:03:51,820 --> 00:03:55,840
that. Most of the threads that are scheduled a particular core

72
00:03:55,840 --> 00:04:00,150
may find their working set in the L1 cache if possible,

73
00:04:00,150 --> 00:04:02,880
and similarly the threads that are scheduled on this may find

74
00:04:02,880 --> 00:04:05,946
its working set of [UNKNOWN] cache of C2 if possible, and

75
00:04:05,946 --> 00:04:08,540
so on. And also the other thing that the operating system

76
00:04:08,540 --> 00:04:10,612
may try to do is, if you just take the universal,

77
00:04:10,612 --> 00:04:13,700
all the threads That are currently scheduled by the operating system

78
00:04:13,700 --> 00:04:16,320
to run on all these four cores. You want to make

79
00:04:16,320 --> 00:04:19,990
sure that working set of all these threads are likely to

80
00:04:19,990 --> 00:04:23,110
be found in the L2 cache, because if you're missing the

81
00:04:23,110 --> 00:04:26,960
L2 cache bad news because then you're going outside the chip

82
00:04:26,960 --> 00:04:29,760
and that's a very long latency memory operation. And of course

83
00:04:29,760 --> 00:04:32,040
you can extend this idea if, if there is a third

84
00:04:32,040 --> 00:04:34,950
level of cache but to make things concrete let's just stick

85
00:04:34,950 --> 00:04:39,110
to two levels of caches L1 cache and L2 cache and.

86
00:04:39,110 --> 00:04:41,740
The criterion for operating system is to

87
00:04:41,740 --> 00:04:43,760
make sure that the threads that are currently

88
00:04:43,760 --> 00:04:46,810
scheduled on the processors that's available, all the

89
00:04:46,810 --> 00:04:48,800
cores that are available, what it wants to

90
00:04:48,800 --> 00:04:52,630
try to do is make sure that all the threads will find the contents in the

91
00:04:52,630 --> 00:04:54,500
L2 cache. Because nothing in the L2 cache

92
00:04:54,500 --> 00:04:56,760
is going to be elongating the memory operation.
