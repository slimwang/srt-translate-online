1
00:00:00,290 --> 00:00:00,790
We have so

2
00:00:00,790 --> 00:00:07,100
far built multivariate non-parametric
models using kernel density estimation.

3
00:00:07,100 --> 00:00:12,700
We can also ask the question,
are there outliers in the data?

4
00:00:12,700 --> 00:00:18,190
The Mahalanobis distance for a single
variable x, can be defined this way.

5
00:00:18,190 --> 00:00:22,510
Where Mu x is the mean value for x.

6
00:00:22,510 --> 00:00:26,290
And S is the covariance matrix.

7
00:00:26,290 --> 00:00:31,370
We use the inverse of S here
to get this product matrix.

8
00:00:31,370 --> 00:00:35,880
Similarly for two identically
distributed variables x and

9
00:00:35,880 --> 00:00:39,550
y, we take the difference transposed.

10
00:00:39,550 --> 00:00:44,160
And use the inverse of
the covariance matrix and

11
00:00:44,160 --> 00:00:47,180
then the difference and
multiply them together.

12
00:00:47,180 --> 00:00:51,550
In both cases the values
are always positive definite.

13
00:00:51,550 --> 00:00:54,230
Since it's a true distance function.

14
00:00:54,230 --> 00:00:57,940
The Mahalanobis distance gives
us a simple way to look for

15
00:00:57,940 --> 00:00:59,760
outliers in the data.

16
00:00:59,760 --> 00:01:03,350
The square of the Mahalanobis
distance in two dimensions or

17
00:01:03,350 --> 00:01:09,910
with two variables is equal to the
chi-square with two degrees of freedom.

18
00:01:09,910 --> 00:01:14,820
Thus that distance directly give
us how far these values of data

19
00:01:14,820 --> 00:01:19,410
are from their central tendencies
in the probability distributions.

20
00:01:19,410 --> 00:01:24,836
In our case,
we have two distributions of F1 and F2.

21
00:01:24,836 --> 00:01:28,501
F1 was the average Medicare
payment amount and

22
00:01:28,501 --> 00:01:32,565
F2 was the average
Medicare Allowed amount.

23
00:01:32,565 --> 00:01:35,260
We had inspected these
two distributions and

24
00:01:35,260 --> 00:01:38,740
they seemed to be quite
identically distributed.

25
00:01:38,740 --> 00:01:43,152
Assuming they were identically
distributed, given that F1 and

26
00:01:43,152 --> 00:01:46,420
F2 were quite identically distributed.

27
00:01:46,420 --> 00:01:52,200
We had the derived variables x and
x bar identically distributed.

28
00:01:52,200 --> 00:01:58,120
Can we look for outliers in a joint
distribution of x and x bar?

29
00:01:58,120 --> 00:02:02,600
Moreover if we do find outliers
using the Mahalanobis distance

30
00:02:02,600 --> 00:02:06,960
we can still ask the question
what do these outliers mean?

31
00:02:06,960 --> 00:02:09,389
Well, we will leave it up to you

32
00:02:09,389 --> 00:02:13,050
to find what the meaning
of these outliers could be.

33
00:02:13,050 --> 00:02:15,690
Let's go back to the i
Python notebook and

34
00:02:15,690 --> 00:02:19,080
implement the code for
the Mahalanobis distance and

35
00:02:19,080 --> 00:02:23,480
see if we can find some outliers
in our variables x and x bar.
