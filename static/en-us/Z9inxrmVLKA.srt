1
00:00:00,275 --> 00:00:01,350
We're really interested in solving this

2
00:00:01,350 --> 00:00:03,300
temporal credit assignment problem as we will

3
00:00:03,300 --> 00:00:06,220
see as we talk later on in the course. This sort of issue

4
00:00:06,220 --> 00:00:09,190
comes up again and again and again. But, I didn't really want to talk

5
00:00:09,190 --> 00:00:12,040
about that in any great detail, I just wanted to make certain that,

6
00:00:12,040 --> 00:00:13,690
that people saw the difference between

7
00:00:13,690 --> 00:00:15,840
supervised learning and then sort of problems

8
00:00:15,840 --> 00:00:19,200
we're trying to solve now. That really has to do with sequences and

9
00:00:19,200 --> 00:00:21,170
time. What I want to just is focus a little bit more on

10
00:00:21,170 --> 00:00:25,280
rewards. And in particular, I want to look at the little grid world

11
00:00:25,280 --> 00:00:28,710
that we've been playing with so far, and think about how

12
00:00:28,710 --> 00:00:32,540
we would learn how to get from our start state. We

13
00:00:32,540 --> 00:00:35,880
always start it over here, over to one of the plus

14
00:00:35,880 --> 00:00:38,550
one or the minus 1, depending upon the kind of rewards

15
00:00:38,550 --> 00:00:40,970
that we see. So, let's push on this rewards notion for

16
00:00:40,970 --> 00:00:43,180
a minute, if we think about how MDP's are made up.

17
00:00:43,180 --> 00:00:44,920
And I'm just going to write down for you what the

18
00:00:44,920 --> 00:00:47,850
reward is, for the states in this particular grid world, okay?

19
00:00:47,850 --> 00:00:48,748
>> Yeah.

20
00:00:48,748 --> 00:00:51,043
>> So, let's say

21
00:00:51,043 --> 00:00:55,768
I'm going to set the rewards to be equal to,

22
00:00:55,768 --> 00:00:58,767
minus 0.04. So, the first thing I want to do, is

23
00:00:58,767 --> 00:01:00,015
I want to make certain you understand what I mean

24
00:01:00,015 --> 00:01:01,608
by this. So, what I mean when I write this

25
00:01:01,608 --> 00:01:04,540
down, for this particular example. Is that the reward

26
00:01:04,540 --> 00:01:08,009
that I'm going to receive in every single state is minus

27
00:01:08,009 --> 00:01:11,550
0.04, except of course for these two states which

28
00:01:11,550 --> 00:01:14,470
I've already labelled as plus 1 and minus 1. Okay?

29
00:01:14,470 --> 00:01:16,100
>> Alright, so,

30
00:01:16,100 --> 00:01:19,570
it seems like you should just never ever move then, right?

31
00:01:19,570 --> 00:01:23,180
>> no, I don't think that's true. Let's think

32
00:01:23,180 --> 00:01:24,680
about that for a little while. So, if I

33
00:01:24,680 --> 00:01:29,280
set this reward to be minus 0.04 and you're

34
00:01:29,280 --> 00:01:31,620
starting out here in this state, and you have

35
00:01:31,620 --> 00:01:33,620
to keep living, you always have to take an

36
00:01:33,620 --> 00:01:35,940
action, right? That's the way I've set this up.

37
00:01:35,940 --> 00:01:38,900
And you don't get to stop until you reach

38
00:01:38,900 --> 00:01:41,220
either plus 1 or negative 1, we'll call these

39
00:01:41,220 --> 00:01:43,360
terminating or absorbing states. Once you reach

40
00:01:43,360 --> 00:01:44,480
into one of these things that ends

41
00:01:44,480 --> 00:01:47,530
the game. What would having a small

42
00:01:47,530 --> 00:01:50,310
negative reward like, this encourage you to do?

43
00:01:50,310 --> 00:01:56,420
>> It's, it's making me think about kind of walking across hot beach, because

44
00:01:56,420 --> 00:01:59,290
like each time you take a step you're going to get a little bit of punishment

45
00:01:59,290 --> 00:02:01,780
and the only thing that ends it is if you can get into that nice

46
00:02:01,780 --> 00:02:06,937
cool ocean. And then you get a plus 1 for that and the minus 0.04s,

47
00:02:06,937 --> 00:02:08,848
minus 0.04s, stop.

48
00:02:08,848 --> 00:02:11,610
>> Right, so, that means, so wait how would

49
00:02:11,610 --> 00:02:12,880
you put that another way. Sounds to me like

50
00:02:12,880 --> 00:02:15,200
what you're saying is, by having a small negative

51
00:02:15,200 --> 00:02:17,700
reward everywhere, it encourages you to end the game.

52
00:02:17,700 --> 00:02:20,990
>> Yeah, that's [UNKNOWN]. Yeah, I agree with that.

53
00:02:20,990 --> 00:02:22,917
>> Okay, yeah and, and I think that's exactly what I like

54
00:02:22,917 --> 00:02:25,660
that analogy a lot. Your in a, well, it's not a very

55
00:02:25,660 --> 00:02:30,210
hot beach it's just a you know, a slightly unpleasantly warm beach

56
00:02:30,210 --> 00:02:32,340
and you really want to hurry up and get into the ocean.

57
00:02:32,340 --> 00:02:33,800
In fact, you want to hurry up and get into the ocean

58
00:02:33,800 --> 00:02:36,820
even though on your way to getting to the ocean. You might step

59
00:02:36,820 --> 00:02:40,430
on some glass, that the minus 1 is. So, let's go back

60
00:02:40,430 --> 00:02:43,220
to this notion of policy of mapping states to action and let's think

61
00:02:43,220 --> 00:02:46,120
about as being in this world where you have a reward of

62
00:02:46,120 --> 00:02:50,640
minus 0.04 every where except in the absorbing states. What do you think

63
00:02:50,640 --> 00:02:54,410
the best set of actions are to take in these different states?

64
00:02:54,410 --> 00:02:57,620
What do you think the best policy is? Actually before you tell me,

65
00:02:57,620 --> 00:02:59,740
let me write down what the actual policy is. I'm not

66
00:02:59,740 --> 00:03:01,890
going to tell you, how we get here but I'm about to

67
00:03:01,890 --> 00:03:03,680
write down the policy that's sort of the best one to

68
00:03:03,680 --> 00:03:06,350
take in a world where you have these sets of rewards. Okay.

69
00:03:06,350 --> 00:03:07,040
>> Sure.

70
00:03:07,040 --> 00:03:08,310
>> Okay, you see this Michael.

71
00:03:08,310 --> 00:03:10,310
>> Yeah, and it makes a lot of sense. Like if

72
00:03:10,310 --> 00:03:13,879
you're basically heading towards the, the rewarding state, the green state.

73
00:03:14,880 --> 00:03:18,520
>> Right, so if I start out here in the leftmost bottom state, basically,

74
00:03:18,520 --> 00:03:22,900
it says, go up and then go to the right which, coincidentally is the policy

75
00:03:22,900 --> 00:03:24,130
that we had found before.

76
00:03:24,130 --> 00:03:27,780
>> Cool. There's a couple spots that are a little bit strange, though.

77
00:03:27,780 --> 00:03:29,770
>> Like what?

78
00:03:29,770 --> 00:03:32,330
>> Well, I'm thinking about the one, not directly under the minus 1,

79
00:03:32,330 --> 00:03:34,930
that makes sense to me, but the one, it takes you now to a

80
00:03:34,930 --> 00:03:39,780
position. Where it seems like you want to go straight up to the goal.

81
00:03:39,780 --> 00:03:42,790
But yet, it's going the long way around. It just didn't see the goal.

82
00:03:43,890 --> 00:03:45,600
>> No that's not it at all because of the way we

83
00:03:45,600 --> 00:03:48,470
are going to learn this, you're going to. So this is a global policy and

84
00:03:48,470 --> 00:03:51,260
I'm just telling you it's the optimal policy given the

85
00:03:51,260 --> 00:03:53,450
rewards. So let's kind of work out why going to

86
00:03:53,450 --> 00:03:55,900
the left makes sense here. Well, so here's my argument.

87
00:03:55,900 --> 00:03:59,180
What this basically says is take the long way around. Yes?

88
00:03:59,180 --> 00:03:59,660
>> Yeah?

89
00:03:59,660 --> 00:04:03,860
>> But, by taking the long way around, what's happening? Well, on

90
00:04:03,860 --> 00:04:07,260
the downside I'm going to pick up a little bit of negative reward,

91
00:04:07,260 --> 00:04:10,500
for a while, you know, one, two, three, four, five, six or

92
00:04:10,500 --> 00:04:12,422
so which is something like negative

93
00:04:12,422 --> 00:04:15,630
0.2, negative 0.3, something like that, right?

94
00:04:15,630 --> 00:04:18,970
But, by doing this, I avoid every getting into

95
00:04:18,970 --> 00:04:22,180
a state where it might fall into the negative 1.

96
00:04:22,180 --> 00:04:25,133
>> Because it's, it's stochastic.

97
00:04:25,133 --> 00:04:27,299
>> Right, it's exactly, so stochastic means

98
00:04:27,299 --> 00:04:28,952
if I'm in this state here no matter

99
00:04:28,952 --> 00:04:31,289
what I do I have som, if I go up I have some chance of

100
00:04:31,289 --> 00:04:33,113
moving to the right and following into

101
00:04:33,113 --> 00:04:36,468
the negative 1. It's a relatively low chance,

102
00:04:36,468 --> 00:04:40,915
it's only 10%. But, it works out that moving from here to here, the probability

103
00:04:40,915 --> 00:04:47,210
of me falling into here. Is to high compared to the sort of near impossibility

104
00:04:47,210 --> 00:04:53,110
of me ending up falling into the minus 1, if I can follow this path.

105
00:04:53,110 --> 00:04:55,300
>> Interesting, okay, I guess that makes sense, it's cool.

106
00:04:56,370 --> 00:04:57,800
>> Which actually suggests sort of the point that

107
00:04:57,800 --> 00:05:00,530
I want to make here which is, that minor changes to

108
00:05:00,530 --> 00:05:02,990
your reward function actually matter. So, if we had

109
00:05:02,990 --> 00:05:06,001
a slightly different reward here, say. Not minus 0.04 but

110
00:05:06,001 --> 00:05:08,479
something else, you might find that some of these

111
00:05:08,479 --> 00:05:11,830
decisions would be different. I think you can see that?

112
00:05:11,830 --> 00:05:16,945
>> Hm. I mean at the level that I'm understanding it, it seems like if

113
00:05:16,945 --> 00:05:21,885
it's zero, that then it's in no particular hurry. If

114
00:05:21,885 --> 00:05:26,170
it's minus something big, then maybe it's in a bigger hurry. But,

115
00:05:26,170 --> 00:05:28,530
I don't, yeah I don't see exactly what the difference is going to be.

116
00:05:28,530 --> 00:05:31,050
>> Okay, so let's see if we can help you see the difference by

117
00:05:31,050 --> 00:05:32,680
making you take a quiz.

118
00:05:32,680 --> 00:05:33,350
>> Aha!
