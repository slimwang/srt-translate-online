1
00:00:00,014 --> 00:00:07,599
So here is my code for this. I'll start by setting k equal to 32 and here is the actual code.

2
00:00:07,599 --> 00:00:12,127
I begin by figuring out the locations of the tile corners.

3
00:00:12,127 --> 00:00:16,782
This is going to tell me where I need to start writing in the output and start reading from the input--

4
00:00:16,782 --> 00:00:22,522
so just a little book keeping and giving things variable names that mean something to me.

5
00:00:22,522 --> 00:00:25,001
But as you can see the, the place where I start

6
00:00:25,001 --> 00:00:32,037
reading the i value is a function of which block we're in times the width of the tile

7
00:00:32,037 --> 00:00:36,688
because each tile is responsible for 1 block and that's the case.

8
00:00:36,688 --> 00:00:42,814
And the j value is the same but in y, in y instead of x, and the output simply inverts y and x.

9
00:00:42,814 --> 00:00:47,400
Okay, so now that I know where I need to read to write my tile,

10
00:00:47,400 --> 00:00:51,236
I'm going to want to know which element of the tile to read and write from,

11
00:00:51,236 --> 00:00:54,057
and just a shorthand to make the code a little more readable.

12
00:00:54,057 --> 00:00:58,109
I'm going to set x to thread index .x, and y to threat index y.

13
00:00:58,109 --> 00:01:01,147
So now, the code itself is really simple.

14
00:01:01,147 --> 00:01:07,456
I declare my floating point array in shared memory k by k array of tiles,

15
00:01:07,456 --> 00:01:13,361
and I read from global memory, and write the result into shared memory.

16
00:01:13,361 --> 00:01:19,905
So here's my read from global memory and its function of where the tile starts

17
00:01:19,905 --> 00:01:25,103
plus which thread I'm responsible, or which element this thread is responsible for in the tile.

18
00:01:25,103 --> 00:01:28,015
To avoid an extra sync threads, I'm going to go

19
00:01:28,015 --> 00:01:31,151
ahead and write this into shared memory in transposed fashion.

20
00:01:31,151 --> 00:01:36,915
So it's not tile x y, it's tile y x. Okay, that saves one of these sync thread's barriers.

21
00:01:36,915 --> 00:01:39,481
Now I've got the transpose tile sitting in shared memory.

22
00:01:39,481 --> 00:01:42,892
It's already been transposed, and I want to write it out to global memory.

23
00:01:42,892 --> 00:01:47,558
And I want to write it out in coalesced fashion, so adjacent threads write adjacent locations and memory.

24
00:01:47,558 --> 00:01:52,649
In other words, adjacent threads are varying by x the way I've set this out.

25
00:01:52,649 --> 00:01:59,556
So here's my write to global memory after my read to from a shared memory.

26
00:01:59,556 --> 00:02:01,370
You could have done this in 2 sync threads.

27
00:02:01,370 --> 00:02:04,086
You could have read this in the shared memory, performed the transpose,

28
00:02:04,086 --> 00:02:05,955
and written it out to global memory.

29
00:02:05,955 --> 00:02:08,902
And you have needed a sync threads after reading it in to shared memory

30
00:02:08,902 --> 00:02:11,018
and again after performing the transpose.

31
00:02:11,018 --> 00:02:13,118
So, if you did that I encourage you to go back

32
00:02:13,118 --> 00:02:18,276
and convert it to this single version, and see how much faster it goes.

33
00:02:18,276 --> 00:02:26,697
Let's go ahead and run this on my laptop. Okay, there's 2 interesting things to note here.

34
00:02:26,697 --> 00:02:31,555
One is that the amount of time taken by the parallel per element code--

35
00:02:31,555 --> 00:02:34,604
the kernel that we had before--actually went up.

36
00:02:34,604 --> 00:02:36,853
It's almost twice as slow now as it was before.

37
00:02:36,853 --> 00:02:42,429
And if you think about it this code didn't change at all, except that we changed the value of k.

38
00:02:42,429 --> 00:02:46,257
We changed the size of the thread block that's being used in this code.

39
00:02:46,272 --> 00:02:49,495
We're going to come back to that. That's going to give us a hint as to a further optimization.

40
00:02:49,495 --> 00:02:54,961
In the meantime, transpose parallel per element tiled, our new version,

41
00:02:54,961 --> 00:02:59,464
is a little bit faster--not a lot faster and that's kind of disturbing.

42
00:02:59,464 --> 00:03:04,436
We should have gone to perfectly coalesced loads and stores which should have made a difference.

43
00:03:04,436 --> 00:03:08,001
Let's go ahead and fire up NVVP again and see what happened.
