1
00:00:00,000 --> 00:00:05,000
Now, I wanted to show you how powerful n-gram models of language are.

2
00:00:05,000 --> 00:00:08,000
That is, if we're only looking at word sequences,

3
00:00:08,000 --> 00:00:11,000
what is it that we're giving up and what are we getting?

4
00:00:11,000 --> 00:00:16,000
So I read in the complete works of Shakespeare into a small computer program,

5
00:00:16,000 --> 00:00:20,000
and then built n-gram models and sampled from that model.

6
00:00:20,000 --> 00:00:27,000
That is, generated random sentences that come from the probability distribution defined by that model.

7
00:00:27,000 --> 00:00:30,000
And here are samples from the unigram model.

8
00:00:30,000 --> 00:00:37,000
That is sampling from words according to frequency in the corpus of Shakespeare text,

9
00:00:37,000 --> 00:00:41,000
but not taking into account any relationship between adjacent words.

10
00:00:41,000 --> 00:00:44,000
And looking at this, it doesn't make much sense.

11
00:00:44,000 --> 00:00:46,000
It does seem like real sentences.

12
00:00:46,000 --> 00:00:49,000
You can tell the vocabulary is somewhat archaic.

13
00:00:49,000 --> 00:00:54,000
You have words like "o'erthrown" and "thou" and "'tis" and so on,

14
00:00:54,000 --> 99:59:59,999
but you aren't really getting very much from this.
