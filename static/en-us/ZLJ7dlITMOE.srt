1
00:00:00,380 --> 00:00:04,220
In the reduce phase, there is more work to be done in pulling the data that

2
00:00:04,220 --> 00:00:07,730
is needed for each one of these reducers.

3
00:00:07,730 --> 00:00:12,050
Because, we know that the mappers have been executing

4
00:00:12,050 --> 00:00:14,780
on different nodes of the computational cluster, and

5
00:00:14,780 --> 00:00:18,008
they produce their intermediate results as files on

6
00:00:18,008 --> 00:00:21,880
their local disk. And this worker that is

7
00:00:21,880 --> 00:00:26,350
carrying out a particular split of the reduce operation

8
00:00:26,350 --> 00:00:33,610
has to reach out and pull the data from all of the m mappers that have

9
00:00:33,610 --> 00:00:36,690
stored their intermediate results on their respective local

10
00:00:36,690 --> 00:00:39,070
disks. So there is remote read that is

11
00:00:39,070 --> 00:00:44,512
involved. As the first thing in the reduce phase is to pull the data. This is

12
00:00:44,512 --> 00:00:47,088
part of what I mean by the plumbing

13
00:00:47,088 --> 00:00:51,412
that the runtime system provides is to recognize that,

14
00:00:51,412 --> 00:00:57,392
for this reduce operation to work, it needs the mapping results from all the m

15
00:00:57,392 --> 00:01:03,289
nodes that carried out the map function. And so it is going to do RPC in

16
00:01:03,289 --> 00:01:09,596
order to get all this data from all the local disks of the nodes on

17
00:01:09,596 --> 00:01:16,340
which the map was executed. And once it has all the data, it can sort it,

18
00:01:16,340 --> 00:01:20,583
and then call the reduce function. The reduce function is

19
00:01:20,583 --> 00:01:23,510
the one that has been written by the domain expert.

20
00:01:23,510 --> 00:01:26,690
And this is the point at which the domain expertise

21
00:01:26,690 --> 00:01:29,680
comes in, in saying, well, I've got the data now,

22
00:01:29,680 --> 00:01:31,620
let me do the processing that I want to do

23
00:01:31,620 --> 00:01:34,810
for the reduce operation. The sorting that is being done

24
00:01:34,810 --> 00:01:37,980
as part of the programming framework may be to sort

25
00:01:37,980 --> 00:01:41,410
the input that is coming in from all of these different

26
00:01:41,410 --> 00:01:48,440
mappers, so that all the same keys are together in the input data set that is

27
00:01:48,440 --> 00:01:53,480
going to be given to the reduce function. And once such sorting has been done,

28
00:01:53,480 --> 00:01:56,422
the programming framework will call the user-supplied

29
00:01:56,422 --> 00:01:59,860
reduce function for each key with the set

30
00:01:59,860 --> 00:02:02,940
of intermediate values so that the reduce function

31
00:02:02,940 --> 00:02:06,730
can do its thing, which is domain specific.

32
00:02:06,730 --> 00:02:11,860
Each reduce function will then write to the final output file

33
00:02:11,860 --> 00:02:16,340
specific to the partition that it is dealing with. So, if you think about the

34
00:02:16,340 --> 00:02:21,680
original example that we started with, if, let's say, this guy is

35
00:02:21,680 --> 00:02:26,930
accumulating all the instances of the name Kishore, then it

36
00:02:26,930 --> 00:02:32,230
will write the output file that says oh, I've found so many instances of

37
00:02:32,230 --> 00:02:35,870
the name Kishore in the input corpus of

38
00:02:35,870 --> 00:02:39,250
data. And similarly, this guy may be doing

39
00:02:39,250 --> 00:02:41,990
it for another name like Drew, or Arun,

40
00:02:41,990 --> 00:02:45,644
and so on. And once each worker has completed

41
00:02:45,644 --> 00:02:49,970
its work by writing its final output file

42
00:02:49,970 --> 00:02:52,400
for this partition that it is responsible for,

43
00:02:52,400 --> 00:02:54,860
then it informs the master, that, yes, I'm

44
00:02:54,860 --> 00:02:56,510
done with the work that was assigned to me.

45
00:02:57,620 --> 00:03:00,920
The user program can be woken up when all

46
00:03:00,920 --> 00:03:03,210
the reducers have indicated to the master that they

47
00:03:03,210 --> 00:03:05,630
have done the work, and at that point the

48
00:03:05,630 --> 00:03:08,710
map reduce computation that was initiated by the user

49
00:03:08,710 --> 00:03:11,800
program is complete. We said that there could be

50
00:03:11,800 --> 00:03:16,285
m splits of the input dataset, meaning the input

51
00:03:16,285 --> 00:03:20,230
key-value pairs, and there could be R splits of

52
00:03:20,230 --> 00:03:23,230
the output that has to be generated by the application

53
00:03:23,230 --> 00:03:26,290
as a whole. Now, the computational resources that

54
00:03:26,290 --> 00:03:29,460
are available in the data center, N, may

55
00:03:29,460 --> 00:03:35,954
be less than the sum m plus R. So there may not be a unique machine to

56
00:03:35,954 --> 00:03:38,900
assign for each one of the m splits

57
00:03:38,900 --> 00:03:41,590
that have been made. It is the responsibility

58
00:03:41,590 --> 00:03:44,670
of the master to manage the machines and

59
00:03:44,670 --> 00:03:48,890
assign the machines that are available to handing the

60
00:03:48,890 --> 00:03:52,392
m input data sets as well as the R

61
00:03:52,392 --> 00:03:55,430
reduce splits that need to be generated. So this is

62
00:03:55,430 --> 00:03:57,580
part of the heavy lifting that has to be

63
00:03:57,580 --> 00:04:00,540
done by the runtime. So, for instance, let's say that

64
00:04:00,540 --> 00:04:05,040
I have only 100 worker nodes available as mappers.

65
00:04:05,040 --> 00:04:08,600
And I have an input split of 1000, then what

66
00:04:08,600 --> 00:04:11,610
I'm going to do is, I'm going to assign one split to

67
00:04:11,610 --> 00:04:14,008
this worker. And when the guy says I'm done with

68
00:04:14,008 --> 00:04:16,392
it, then I'd say, oh, you're done? Okay, take the next

69
00:04:16,392 --> 00:04:19,002
split and work on it. Take the next split and work on

70
00:04:19,002 --> 00:04:24,240
it. So that's how we're going to manage the available resources for carrying

71
00:04:24,240 --> 00:04:27,334
out the work that needs to be done. So remember that this

72
00:04:27,334 --> 00:04:30,002
is all done as part of the heavy lifting by the runtime,

73
00:04:30,002 --> 00:04:33,009
the user doesn't have to worry about it. All that the user

74
00:04:33,009 --> 00:04:36,749
did was write the map function and write the reduce function, and

75
00:04:36,749 --> 00:04:39,265
the rest of it is magic so far as the map reduce

76
00:04:39,265 --> 00:04:42,918
framework is concerned. And similarly, the number of

77
00:04:42,918 --> 00:04:45,408
R splits specified by the user may be

78
00:04:45,408 --> 00:04:48,147
more than the number of workers that are

79
00:04:48,147 --> 00:04:51,433
available to carry that out. And in that

80
00:04:51,433 --> 00:04:54,565
case, again, the master is going to assign a

81
00:04:54,565 --> 00:04:57,610
particular split to this worker so that he

82
00:04:57,610 --> 00:05:01,699
can compute and generate the output file corresponding

83
00:05:01,699 --> 00:05:04,320
to that split. Once he's done with that and

84
00:05:04,320 --> 00:05:06,999
notifies the master, then the master will say, okay,

85
00:05:06,999 --> 00:05:09,336
now that you're done with that, work on the next

86
00:05:09,336 --> 00:05:11,770
split. And it'll do all the work that is

87
00:05:11,770 --> 00:05:16,136
associated with plumbing, meaning, bringing the data for the split

88
00:05:16,136 --> 00:05:19,244
that a particular worker is working on right now,

89
00:05:19,244 --> 00:05:22,944
sorting it to put all the corresponding keys together, and

90
00:05:22,944 --> 00:05:26,348
then finally calling the reduce function that has been

91
00:05:26,348 --> 00:05:29,577
written by the user. That's the kind of work that

92
00:05:29,577 --> 00:05:34,030
goes on under the covers, in the programming framework of map reduce.
