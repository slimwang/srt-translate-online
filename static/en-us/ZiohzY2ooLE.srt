1
00:00:00,510 --> 00:00:04,900
You're now ready to think about really
big computations, computations where,

2
00:00:04,900 --> 00:00:08,980
for instance, the problem can't fit
in the memory of a single computer.

3
00:00:08,980 --> 00:00:11,960
Or calculations that need so
many operations, it

4
00:00:11,960 --> 00:00:14,690
would take hundreds of years to finish
them using only a single computer.

5
00:00:14,690 --> 00:00:18,860
Now when the computation is
really that big, you might ask,

6
00:00:18,860 --> 00:00:21,589
can I harness the collective
power of many computers?

7
00:00:22,760 --> 00:00:24,660
But to design an efficient algorithm for

8
00:00:24,660 --> 00:00:27,840
such a machine, you need
a suitable abstract machine model.

9
00:00:27,840 --> 00:00:31,070
And that's the goal for this lesson,
for you to develop just such a model.

10
00:00:32,159 --> 00:00:34,930
This particular model and
its variants go by many names,

11
00:00:34,930 --> 00:00:38,190
which you are bound to encounter as
you read the literature on your own.

12
00:00:38,190 --> 00:00:41,840
These names include the Network Model,
the Distributed Memory Model,

13
00:00:41,840 --> 00:00:45,850
the Message-Passing Model, the
Communicating Sequential Processes Model

14
00:00:45,850 --> 00:00:48,450
and probably a zillion
others that I've forgotten.

15
00:00:48,450 --> 00:00:50,960
Now personally,
I like message-passing and for

16
00:00:50,960 --> 00:00:52,770
reasons you'll soon see, alpha beta.

17
00:00:54,030 --> 00:00:55,280
Now if any of that seems confusing,

18
00:00:55,280 --> 00:00:58,250
don't worry the basic
abstraction is really simple.

19
00:00:58,250 --> 00:01:02,050
You have a bunch of computers
collectively carrying out a computation.

20
00:01:02,050 --> 00:01:02,890
To coordinate,

21
00:01:02,890 --> 00:01:07,400
they communicate with one another by
sending mass messages back and forth.

22
00:01:07,400 --> 00:01:11,830
This message-passing style stands
in stark contrast to shared memory.

23
00:01:11,830 --> 00:01:15,660
There, processes or more likely threads,
communicate by reading and

24
00:01:15,660 --> 00:01:17,790
writing shared variables.

25
00:01:17,790 --> 00:01:21,440
Now before you start, I think it's
really important to think big.

26
00:01:21,440 --> 00:01:24,550
That's because distributed memory
computations are all about solving

27
00:01:24,550 --> 00:01:26,440
problems that are really, really huge.

28
00:01:27,530 --> 00:01:28,740
How huge, you ask?

29
00:01:28,740 --> 00:01:31,460
Well, let's start with a quick
warm up exercise to get you

30
00:01:31,460 --> 00:01:34,080
thinking about computations
at the right scale.
