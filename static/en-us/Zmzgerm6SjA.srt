1
00:00:00,470 --> 00:00:03,209
But first,
I want to introduce you to another idea.

2
00:00:03,209 --> 00:00:05,000
It’s the idea of 1 x 1 convolutions.

3
00:00:06,030 --> 00:00:10,560
You might wonder, why one would ever
want to use 1 x 1 convolutions?

4
00:00:10,560 --> 00:00:13,900
They’re not really looking at a patch
of the image just that one pixel.

5
00:00:15,119 --> 00:00:19,859
Look at the classic convolution setting,
it’s basically a small classifier for

6
00:00:19,859 --> 00:00:23,429
a patch of the image but
it's only a linear classifier.

7
00:00:24,879 --> 00:00:29,160
But if you add a 1 x 1 convolution in
the middle, suddenly you have a mini

8
00:00:29,160 --> 00:00:33,920
neural network running over the patch
instead of a linear classifier.

9
00:00:33,920 --> 00:00:38,832
Interspersing your convolutions with 1
x 1 convolutions is a very inexpensive

10
00:00:38,832 --> 00:00:42,296
way to make your models deeper and
have more parameters,

11
00:00:42,296 --> 00:00:45,272
without completely
changing their structure.

12
00:00:45,273 --> 00:00:48,362
They're also very cheap because
if you go through their math,

13
00:00:48,362 --> 00:00:50,462
they're not really convolutions at all,

14
00:00:50,462 --> 00:00:54,563
they're really just matrix multiplies
and they have relatively few parameters.

15
00:00:54,563 --> 00:00:57,190
I mentioned all of these,
average pulling and

16
00:00:57,189 --> 00:01:01,500
1 x 1 convolutions because I want to
talk about the general strategy that has

17
00:01:01,500 --> 00:01:05,337
been very successful at creating
cognates that are both smaller and

18
00:01:05,337 --> 00:01:09,070
better than cognates that simply
use a pyramid of convolutions.

