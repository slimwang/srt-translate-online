1
00:00:00,300 --> 00:00:02,400
So that's really all I wanted to say about genetic algorithms.

2
00:00:02,400 --> 00:00:04,750
I mean, there's lots of tweaky things that you need to

3
00:00:04,750 --> 00:00:07,650
do to get this to work very effectively. You have some

4
00:00:07,650 --> 00:00:09,780
choice about how to represent the input, and you have some

5
00:00:09,780 --> 00:00:12,600
choice about how you can do your selection, and your fit

6
00:00:12,600 --> 00:00:15,620
to finding your fitness function. But, at a generic level, this

7
00:00:15,620 --> 00:00:17,460
is, this is, it's a, it's a useful thing. Some people

8
00:00:17,460 --> 00:00:20,750
call genetic algorithms the second best solution to any given problem.

9
00:00:20,750 --> 00:00:21,070
>> Hmm.

10
00:00:21,070 --> 00:00:23,340
>> So, it's a good thing to have in your toolbox. But

11
00:00:23,340 --> 00:00:25,420
I think that's, that's really it. That's all I wanted to say about

12
00:00:25,420 --> 00:00:28,600
randomized optimization. So what have we learned?

13
00:00:28,600 --> 00:00:34,030
>> Well, we learned about random optimization period. That that

14
00:00:34,030 --> 00:00:36,570
there is a notion of optimization in the first place.

15
00:00:36,570 --> 00:00:39,180
>> So, we talked about optimization in general

16
00:00:39,180 --> 00:00:40,840
and then we, what was the randomized part?

17
00:00:40,840 --> 00:00:43,310
>> Well, we'd take random steps where we

18
00:00:43,310 --> 00:00:46,360
start off in random places. Or, we'd do random

19
00:00:46,360 --> 00:00:48,060
kind, well actually that's really it. You take

20
00:00:48,060 --> 00:00:51,360
random steps, you start off in random places and

21
00:00:51,360 --> 00:00:55,960
it's a way to overcome when you can't take a natural gradient step.

22
00:00:55,960 --> 00:00:58,230
>> That's right. So did we talk, and we

23
00:00:58,230 --> 00:01:00,150
talked about some particular randomization.

24
00:01:00,150 --> 00:01:03,170
Er, sorry, randomized optimization algorithms.

25
00:01:03,170 --> 00:01:05,230
>> Let's see. There was randomized hill climbing.

26
00:01:05,230 --> 00:01:07,040
>> And we had 2 flavors of that.

27
00:01:07,040 --> 00:01:12,680
>> Right. We did simulated annealing. And, we did genetic algorithms.

28
00:01:12,680 --> 00:01:16,520
>> And don' t forget, that we talked a little bit about how this all connects

29
00:01:16,520 --> 00:01:17,990
back up with learning, because in many

30
00:01:17,990 --> 00:01:20,660
cases, we're searching some parameter space to

31
00:01:20,660 --> 00:01:23,940
find a good classifier, a good regression

32
00:01:23,940 --> 00:01:28,480
function. A later in this particular sub-unit

33
00:01:28,480 --> 00:01:30,470
we're going to be talking about, finding good

34
00:01:30,470 --> 00:01:33,250
clusterings. And so, this notion of finding

35
00:01:33,250 --> 00:01:35,060
something that's good, finding a way to,

36
00:01:35,060 --> 00:01:38,630
to be optimal is pervasive through apache learning.

37
00:01:38,630 --> 00:01:41,650
>> Oh that make sense. Well, there, well, if we're trying to remember all

38
00:01:41,650 --> 00:01:42,960
these other things we learned. We

39
00:01:42,960 --> 00:01:46,220
also learned that AI researchers like analogies.

40
00:01:46,220 --> 00:01:47,883
>> [LAUGH]

41
00:01:47,883 --> 00:01:49,260
>> Both simulating annealing and generic

42
00:01:49,260 --> 00:01:51,410
algorithms are analogies. And they don't

43
00:01:51,410 --> 00:01:52,850
just like analogies, they like taking

44
00:01:52,850 --> 00:01:55,770
analogies and pushing them until they break.

45
00:01:55,770 --> 00:01:57,670
>> Indeed, actually hill climbing [LAUGH] is an analogy too.

46
00:01:57,670 --> 00:01:59,350
>> Yeah, actually, right? Every single thing that we

47
00:01:59,350 --> 00:02:02,390
did is an analogy to something. Okay, that's good. The

48
00:02:02,390 --> 00:02:03,790
other thing that we learned, which I think is

49
00:02:03,790 --> 00:02:06,680
important, is that there's no way to talk about cross,

50
00:02:06,680 --> 00:02:12,244
crossover without getting a bunch of people in the studio to giggle. [LAUGH]

51
00:02:12,244 --> 00:02:15,150
>> Yeah, genetic algorithms make people blush.

52
00:02:15,150 --> 00:02:16,230
>> Okay, that's pretty good, but Michael you

53
00:02:16,230 --> 00:02:17,610
know, I'm, I'm looking at all this and

54
00:02:17,610 --> 00:02:18,780
now that you put all these words together

55
00:02:18,780 --> 00:02:21,275
on one slide, I have 2 observations I want to

56
00:02:21,275 --> 00:02:23,890
make. That that are kind of bothering me. So,

57
00:02:23,890 --> 00:02:26,145
one is, I'm looking at hill climbing that makes

58
00:02:26,145 --> 00:02:29,280
sense, hill climbing restarts makes sense, simulated annealing

59
00:02:29,280 --> 00:02:31,865
makes sense, [INAUDIBLE] but you know what, they don't

60
00:02:31,865 --> 00:02:34,283
really remember a lot. So, what do I mean by that?

61
00:02:34,283 --> 00:02:36,263
So you do all this hill climbing and you go 8

62
00:02:36,263 --> 00:02:39,020
billion steps, and then what happens? You end up with the

63
00:02:39,020 --> 00:02:43,020
point. You do simulated annealing. You do all this fancy stuff

64
00:02:43,020 --> 00:02:48,160
with slowly changing your temperature, and creating swords with black smiths,

65
00:02:48,160 --> 00:02:50,290
and all that other stuff you talked about and in the

66
00:02:50,290 --> 00:02:53,280
end, at every step, the only thing you remember is, where

67
00:02:53,280 --> 00:02:56,810
you are and maybe where you last were. And with genetic algorithms,

68
00:02:56,810 --> 00:02:59,410
it's a little more complicated that because you keep a population,

69
00:02:59,410 --> 00:03:02,500
but really you're just keeping track of where you are, not

70
00:03:02,500 --> 00:03:05,672
where you've been. So in some sense, the only difference between

71
00:03:05,672 --> 00:03:09,110
the 1 millionth iteration, and the 1st iteration is that you might

72
00:03:09,110 --> 00:03:11,800
be at a better point. And it just feels like, if

73
00:03:11,800 --> 00:03:15,250
you're going to go through all this trouble of going through what is

74
00:03:15,250 --> 00:03:18,390
some complicated space that hopefully has some structure, there should be

75
00:03:18,390 --> 00:03:21,840
some way to communicate information about that structure as you go along.

76
00:03:21,840 --> 00:03:23,838
And that just, that sort of bothers. So that's

77
00:03:23,838 --> 00:03:26,140
one thing. The second thing is, what I really

78
00:03:26,140 --> 00:03:28,710
liked about simulating annealing, other than, you know, the

79
00:03:28,710 --> 00:03:33,310
analogy and hearing you talk about strong swords, is that

80
00:03:33,310 --> 00:03:35,700
it came out at the end with a really

81
00:03:35,700 --> 00:03:38,640
nice result, which is this Boltzmann distribution, that there's

82
00:03:38,640 --> 00:03:42,540
some probability distribution that we can understand, that is

83
00:03:42,540 --> 00:03:46,930
actually trying to model. So, here are my questions then.

84
00:03:46,930 --> 00:03:48,022
It's the long way of asking a real

85
00:03:48,022 --> 00:03:50,820
simple question. Is there something out there, something

86
00:03:50,820 --> 00:03:53,550
we can say more about? Not just keeping

87
00:03:53,550 --> 00:03:55,480
track of points, but keeping track of structure

88
00:03:55,480 --> 00:03:58,110
and information. And is there some way that

89
00:03:58,110 --> 00:04:00,080
we can take advantage of the fact that,

90
00:04:00,080 --> 00:04:03,540
all of these things are somehow tracking probability

91
00:04:03,540 --> 00:04:05,920
distributions just by their very nature of being randomized.

92
00:04:05,920 --> 00:04:09,280
>> That's, that's outstanding. Yes, very good, very good question.

93
00:04:09,280 --> 00:04:11,830
It is true that these are all kind of amnesic,

94
00:04:11,830 --> 00:04:14,400
right? They kind of just wander around, and forget

95
00:04:14,400 --> 00:04:16,550
everything about what they learned. They don't really learn about

96
00:04:16,550 --> 00:04:19,810
the optimization space itself. And use that information to be

97
00:04:19,810 --> 00:04:23,325
more effective. There are some other algorithms that these are,

98
00:04:23,325 --> 00:04:26,405
these are kind of the simplest algorithms, but you can re-combine

99
00:04:26,405 --> 00:04:29,485
these ideas you know, sort of cross-over style, to get

100
00:04:29,485 --> 00:04:33,690
other more powerful algorithms. There's one that's called taboo search,

101
00:04:33,690 --> 00:04:37,130
that specifically tries to remember where you've been, and you're

102
00:04:37,130 --> 00:04:39,660
supposed to avoid it. Right, they become taboo regions.

103
00:04:40,690 --> 00:04:42,920
To stay, with the idea that you should stay away

104
00:04:42,920 --> 00:04:45,480
from regions where you've already done a lot of evaluations,

105
00:04:45,480 --> 00:04:48,090
so you cover the space better. And then there's other

106
00:04:48,090 --> 00:04:50,170
methods that are, have been popular for a while

107
00:04:50,170 --> 00:04:53,120
that are gaining in popularity, where they explicitly model the

108
00:04:53,120 --> 00:04:56,590
probability distribution over where good solutions might be. So they

109
00:04:56,590 --> 00:04:58,690
might be worth actually talking a little more about that.

110
00:04:58,690 --> 00:05:03,040
>> Okay, so go ahead. Well, so I kind of added time.

111
00:05:03,040 --> 00:05:05,030
Maybe maybe you could look into this, I can

112
00:05:05,030 --> 00:05:07,430
give you some references and maybe you can report back.

113
00:05:07,430 --> 00:05:08,280
>> Fine.

114
00:05:08,280 --> 00:05:10,380
>> [LAUGH]

115
00:05:10,380 --> 00:05:12,290
You'll learn, you'll learn very well by doing that.

116
00:05:12,290 --> 00:05:12,938
>> Yes sir.

117
00:05:12,938 --> 00:05:15,610
>> [LAUGH] Alright, we'll see you then, then.

118
00:05:15,610 --> 00:05:18,330
>> Okay, I'll see you then, then too. Bye, Michael.

119
00:05:18,330 --> 00:05:19,140
>> Bye Charles.
