1
00:00:00,000 --> 00:00:05,008
So Thrust gives a host-side interface that can replace writing CUDA kernels in many cases.

2
00:00:05,008 --> 00:00:07,390
But for those times when you are writing CUDA kernels

3
00:00:07,390 --> 00:00:10,248
you'd still like to achieve software reuse.

4
00:00:10,248 --> 00:00:12,764
You'd like to identify the building blocks in your kernel

5
00:00:12,764 --> 00:00:16,486
and use somebody else's highly optimized implementation of those building blocks

6
00:00:16,486 --> 00:00:19,600
and then stitch them together with any custom code of your own.

7
00:00:19,600 --> 00:00:23,444
But software reuse in CUDA kernel code faces some kind of special challenges.

8
00:00:23,444 --> 00:00:29,044
So suppose, for example, that you are calling a library implementation of, say, radix sort,

9
00:00:29,044 --> 00:00:33,078
and this is somebody else's implementation; you're going to call it from inside your kernel.

10
00:00:33,078 --> 00:00:37,636
And so the question is, how does that implementation know how many threads it is running in?

11
00:00:37,636 --> 00:00:40,488
How much shared memory is it allowed to use?

12
00:00:40,488 --> 00:00:43,872
Can it use all of the shared memory, or have you reserved some for other purposes?

13
00:00:43,872 --> 00:00:48,121
Should it use a work-efficient or a step-efficient algorithm for this piece of the problem?

14
00:00:48,121 --> 00:00:51,130
The answer probably depends on what you're sorting, how much you're sorting,

15
00:00:51,130 --> 00:00:53,698
and exactly what's important to you right now.

16
00:00:53,698 --> 00:00:57,808
The CUB library, by Duane Merrill, provides a neat solution

17
00:00:57,808 --> 00:01:02,309
to these challenges of software reuse for CUDA kernel code at high performance.

18
00:01:02,309 --> 00:01:06,841
CUB stands for CUDA Unbound, and the reason it's called that

19
00:01:06,841 --> 00:01:09,788
is because the implementation in CUB leaves these decisions,

20
00:01:09,788 --> 00:01:13,195
like the number of threads and the amount of shared memory, unbound.

21
00:01:13,195 --> 00:01:16,929
You the programmer bind the necessary parameters when you write your program

22
00:01:16,929 --> 00:01:21,770
using templates and a novel binding approach that's embedded in the C++ type system.

23
00:01:21,770 --> 00:01:26,933
The building blocks that CUB provides are really high-performance implementations.

24
00:01:26,933 --> 00:01:30,738
So by stitching them together you can create a high-performance CUDA kernel.

25
00:01:30,738 --> 00:01:34,454
And finally, because CUB doesn't dictate things like thread block size

26
00:01:34,454 --> 00:01:37,614
or shared memory usage, it allows you to experiment with these values

27
00:01:37,614 --> 00:01:40,083
as you optimize or auto-tune your code.

28
00:01:40,083 --> 00:01:42,785
And one thing you'll find if you get into optimizing CUDA code

29
00:01:42,785 --> 00:01:45,723
is that this kind of auto-tuning where you experiment with

30
00:01:45,723 --> 00:01:50,340
or even automatically sweep the parameters of your code,

31
00:01:50,340 --> 00:01:52,356
things like the block size and the shared memory size,

32
00:01:52,356 --> 00:01:55,491
that's a really powerful tool for extracting the maximum amount of performance

33
00:01:55,491 --> 00:01:59,198
from your code without investing a whole lot of personal effort.
