1
00:00:00,000 --> 00:00:05,000
So, let's now generalize k-means into expectation maximization.

2
00:00:05,000 --> 00:00:10,000
Expectation maximization is an algorithm that uses actual probability distributions

3
00:00:10,000 --> 00:00:14,000
to describe what we're doing, and it's in many ways more general,

4
00:00:14,000 --> 00:00:17,000
and it's also nice in that it really has a probabilistic basis.

5
00:00:17,000 --> 00:00:21,000
To get there, I have to take the discourse and tell you all about Gaussians,

6
00:00:21,000 --> 00:00:24,000
or the normal distribution, and the reason is so far,

7
00:00:24,000 --> 00:00:26,000
we've just encountered discrete distributions,

8
00:00:26,000 --> 00:00:30,000
and Gaussians will be the first example of a continuous distribution.

9
00:00:30,000 --> 00:00:34,000
Many of you know that a Gaussian is described by an identity that looks as follows,

10
00:00:34,000 --> 00:00:41,000
where the mean is called mu, and the variance is called sigma or sigma squared.

11
00:00:41,000 --> 00:00:47,000
And for any X along the horizontal access, the density is given by the following function:

12
00:00:47,000 --> 00:00:52,000
1 over square root of 2 pi times sigma, and then an exponential function

13
00:00:52,000 --> 00:00:56,000
of minus Â½ of x - mu squared over sigma squared.

14
00:00:56,000 --> 00:01:01,000
This function might look complex, but it's also very, very beautiful.

15
00:01:01,000 --> 00:01:07,000
It peaks at X = mu where the value in the exponent becomes 0.

16
00:01:07,000 --> 00:01:11,000
And towards plus or minus infinity, it goes to 0 quickly.

17
00:01:11,000 --> 00:01:14,000
In fact, exponentially fast.

18
00:01:14,000 --> 00:01:16,000
The argument inside is a quadratic function.

19
00:01:16,000 --> 00:01:20,000
The exponential function makes it exponential.

20
00:01:20,000 --> 00:01:23,000
And this over here is a normalizer to make sure that the area underneath

21
00:01:23,000 --> 00:01:29,000
sums up to one, which is characteristic of any probability density function.

22
00:01:29,000 --> 00:01:32,000
If you map this back to our discrete random variables,

23
00:01:32,000 --> 00:01:37,000
for each possible X, we can now assign a density value,

24
00:01:37,000 --> 00:01:41,000
which is the function of this, and that's effectively

25
00:01:41,000 --> 00:01:43,000
the probability that this X might be drawn.

26
00:01:43,000 --> 00:01:48,000
Now, the space itself is infinite, so any individual value will have a probability of 0,

27
00:01:48,000 --> 00:01:52,000
but what you can do is you can make an interval, A and B,

28
00:01:52,000 --> 00:01:56,000
and the area underneath this function is the total probability

29
00:01:56,000 --> 00:02:00,000
that an experiment will come up between A and B.

30
00:02:00,000 --> 00:02:03,000
Clearly, it's more likely to generate values around mu

31
00:02:03,000 --> 00:02:07,000
then it is to generate values in the periphery summary over here.

32
00:02:07,000 --> 00:02:09,000
And just for completeness, I'm going to give you the formula

33
00:02:09,000 --> 00:02:12,000
for what's called the multi-variate Gaussian

34
00:02:12,000 --> 00:02:17,000
where multi-variate means nothing else but we have more than one input variable.

35
00:02:17,000 --> 00:02:21,000
You might have a Gaussian over a 2-dimensional space or a 3-dimensional space.

36
00:02:21,000 --> 00:02:24,000
Often, these Gaussians are drawn by what's called level sets,

37
00:02:24,000 --> 00:02:26,000
sets of equal probability.

38
00:02:26,000 --> 00:02:30,000
Here's one in a 2-dimensional space, X1 and X2.

39
00:02:30,000 --> 00:02:35,000
The Gaussian itself can be thought of as coming out of the paper towards me

40
00:02:35,000 --> 00:02:39,000
where the most likely or highest point of probability is the center over here.

41
00:02:39,000 --> 00:02:43,000
And these rings measure areas of equal probability.

42
00:02:43,000 --> 00:02:49,000
The formula for a multi-variate Gaussian looks as follows:

43
00:02:49,000 --> 00:02:53,000
N is the number of dimensions in the input space.

44
00:02:53,000 --> 00:02:57,000
Sigma is a covariance matrix that generalizes the value over here.

45
00:02:57,000 --> 00:03:02,000
And the inner product inside the exponential

46
00:03:02,000 --> 00:03:08,000
is now done using linear algebra where this is the difference between

47
00:03:08,000 --> 00:03:12,000
a probe point and the mean vector mu

48
00:03:12,000 --> 00:03:16,000
transposed sigma to the minus 1 times X - mu.

49
00:03:16,000 --> 00:03:21,000
You can find this formula in any textbook or web page

50
00:03:21,000 --> 00:03:25,000
on Gaussians or multi-variate normal distributions.

51
00:03:25,000 --> 00:03:29,000
It looks cryptic at first, but the key thing to remember is

52
00:03:29,000 --> 00:03:33,000
it's just a generalization of the 1-dimensional case.

53
00:03:33,000 --> 00:03:36,000
We have a quadratic area over here as manifested by the product

54
00:03:36,000 --> 00:03:38,000
of this guy and this guy.

55
00:03:38,000 --> 00:03:42,000
We have a normalization by a variance or covariance

56
00:03:42,000 --> 00:03:48,000
as shown by this number over here or the inverse matrix over here.

57
00:03:48,000 --> 00:03:51,000
And then this entire thing is an exponential form in both cases,

58
00:03:51,000 --> 00:03:55,000
and the normalizer looks a little more different in the multi-variate case,

59
00:03:55,000 --> 00:03:59,000
but all it does is make sure that the volume underneath adds up to 1

60
00:03:59,000 --> 00:04:02,000
to make it a legitimate probability density function.

61
00:04:02,000 --> 00:04:07,000
For most of this explanation, I will stick with 1-dimensional Gaussians,

62
00:04:07,000 --> 00:04:10,000
so all you have to do is to worry about this formula over here,

63
00:04:10,000 --> 99:59:59,999
but this is given just for completeness.
