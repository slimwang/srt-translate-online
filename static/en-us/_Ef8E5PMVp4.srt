1
00:00:00,047 --> 00:00:04,104
Okay, so, for more efficient partitioning, that is actually not true.

2
00:00:04,104 --> 00:00:07,050
We have not been touching the partition function, the partition function does not have

3
00:00:07,050 --> 00:00:10,685
anything to do with the dynamic launches that I can do the recursive parallelism.

4
00:00:10,685 --> 00:00:13,183
Launching on the fly, however, yes.

5
00:00:13,183 --> 00:00:20,323
That does substantially contribute because I don't have to keep returning back to the CPU to do my launch forming.

6
00:00:20,323 --> 00:00:23,494
That means I'm communicating less data and it means that my

7
00:00:23,494 --> 00:00:25,862
launch occurs immediately when I need it,

8
00:00:25,862 --> 00:00:29,599
instead of waiting around until that particular wave of launched is finished.

9
00:00:29,599 --> 00:00:33,675
Simple code, while convenient and I can probably maintain it faster,

10
00:00:33,675 --> 00:00:36,506
is not the reason why it actually runs any faster.

11
00:00:36,506 --> 00:00:42,412
And finally, greater GPU utilization is probably the cause for the greatest of speedups.

12
00:00:42,412 --> 00:00:48,551
By launching on the fly, I'm making sure my GPU is always busy, so when one partial sort finishes,

13
00:00:48,551 --> 00:00:54,090
it creates 2 more immediately, keeping my GPU fully stacked up and busy with work.

14
00:00:54,090 --> 00:00:58,628
It streams more work for my GPU at one time, and my sort ends up faster end-to-end.

15
00:00:58,628 --> 00:01:04,634
In fact, when I've written this program in dynamic parallel form and then host launched form,

16
00:01:04,634 --> 00:01:08,634
I see a pretty much exactly factor of 2 speed up between the two.
