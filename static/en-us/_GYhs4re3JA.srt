1
00:00:00,490 --> 00:00:02,469
Okay, Michael so we talked a little bit about the curse of dimensionality,

2
00:00:02,469 --> 00:00:05,500
but I think it's worthwhile to talk about some other stuff that comes

3
00:00:05,500 --> 00:00:07,600
up. We've been sort of skirting around this and you know bring it

4
00:00:07,600 --> 00:00:12,360
up in various ways throughout our discussion so far. But I think it's worthwhile

5
00:00:12,360 --> 00:00:15,060
kind of writing them all down on a slide and trying to think

6
00:00:15,060 --> 00:00:18,100
through them for a little bit. So ,uh, the other stuff that comes

7
00:00:18,100 --> 00:00:21,232
up in [UNKNOWN] mainly comes up in these sort of assumptions we make

8
00:00:21,232 --> 00:00:25,671
about parameters to the algorithm. So the one we talked about ,uh, probably the

9
00:00:25,671 --> 00:00:28,155
most is our distance measure, you know our

10
00:00:28,155 --> 00:00:30,777
distance between some X and some query point Q

11
00:00:30,777 --> 00:00:33,480
and we've explored a couple. We looked at

12
00:00:33,480 --> 00:00:37,780
Eucudean and we looked at Manhattan. And we even

13
00:00:37,780 --> 00:00:40,680
looked at weighted versions of those. And this

14
00:00:40,680 --> 00:00:42,860
really matters, I've said this before but I really

15
00:00:42,860 --> 00:00:45,340
think it bears repeating that your choice of

16
00:00:45,340 --> 00:00:50,910
distance function Really matters. If you pick the wrong

17
00:00:50,910 --> 00:00:53,700
kind of distance function, you're just going to get very poor behavior.

18
00:00:53,700 --> 00:00:57,760
>> So I, so I have a question about these these distance functions.

19
00:00:57,760 --> 00:00:59,670
So you mentioned Euclidean and Manhattan,

20
00:00:59,670 --> 00:01:01,040
are there other distance functions that the

21
00:01:01,040 --> 00:01:04,510
students should know? Like, things that they, that might come up, or things

22
00:01:04,510 --> 00:01:06,920
that they should think of first if they have a particular kind of data?

23
00:01:08,060 --> 00:01:11,150
>> yeah, there's a, there's a ton of them. I think Well, first off,

24
00:01:11,150 --> 00:01:13,260
it, it's probably worth pointing out that

25
00:01:13,260 --> 00:01:15,980
this, this notion of weighted distance is one

26
00:01:15,980 --> 00:01:18,580
way to deal with the curse of dimensionality. You can weight

27
00:01:18,580 --> 00:01:21,610
different dimensions differently. And that would be one, and you might

28
00:01:21,610 --> 00:01:24,100
come up with sort of automatic ways of doing that. That,

29
00:01:24,100 --> 00:01:27,260
that's sort of worth mentioning. But you will notice that both Euclidean

30
00:01:27,260 --> 00:01:30,560
and Manhattan distance at least as we have talked about them,

31
00:01:30,560 --> 00:01:34,640
are really useful for things like regression. Their kind of assuming

32
00:01:34,640 --> 00:01:38,110
that you have numbers in that subtraction kind of makes sense.

33
00:01:38,110 --> 00:01:41,340
But there are other functions, distance functions that you might do if

34
00:01:41,340 --> 00:01:46,504
you are dealing with cases like, I don't know Discrete

35
00:01:46,504 --> 00:01:50,760
data, right? Where instead of it all being numbers, it's colors,

36
00:01:50,760 --> 00:01:54,730
or something like that. Alright so, your distance might be mismatches.

37
00:01:54,730 --> 00:01:56,870
For example, or it might be a mixture of those. In

38
00:01:56,870 --> 00:01:58,800
fact, one of the nice things about KNN, is that

39
00:01:58,800 --> 00:02:00,670
we've been talking about it with points, because it's sort of

40
00:02:00,670 --> 00:02:02,950
easy to think about it that way. But this distance function

41
00:02:02,950 --> 00:02:07,160
is just a black box. You can take Arbitrary things and

42
00:02:07,160 --> 00:02:09,539
try to decide how similar they are based on whatever you

43
00:02:09,539 --> 00:02:12,125
know about the domain and that could be very useful. So

44
00:02:12,125 --> 00:02:14,960
,you could talk about images right, where you take pictures of

45
00:02:14,960 --> 00:02:18,160
people and you know rather than doing something like a pixel

46
00:02:18,160 --> 00:02:21,960
by pixel comparison, you try to line up their eyes. And

47
00:02:21,960 --> 00:02:24,750
look at their mouths, and try to see if they're the

48
00:02:24,750 --> 00:02:27,740
same shape you know things like that, that might be more

49
00:02:27,740 --> 00:02:29,690
complicated and and perhaps even arbitrarily

50
00:02:29,690 --> 00:02:32,570
computational to determine notions of similarity

51
00:02:32,570 --> 00:02:35,620
so really this idea of distance in similarity tells

52
00:02:35,620 --> 00:02:36,830
you a lot about your domain and what you

53
00:02:36,830 --> 00:02:39,190
believe about it. ,another thing that's worth what what's

54
00:02:39,190 --> 00:02:41,730
pushing on a little bit is how you pick k.

55
00:02:41,730 --> 00:02:43,440
Well there's no good was to pick k you

56
00:02:43,440 --> 00:02:46,580
just You just have to know something about it, but

57
00:02:46,580 --> 00:02:48,590
I want to think about a particular case. Well,

58
00:02:48,590 --> 00:02:51,915
what if we end up in a world where K=N.

59
00:02:51,915 --> 00:02:52,760
>> Well, that would be silly.

60
00:02:52,760 --> 00:02:53,800
>> Why would it be silly?

61
00:02:53,800 --> 00:02:57,740
>> Well, so if K=N, then what you're doing is you're taking, so in the case

62
00:02:57,740 --> 00:02:59,540
of regression for example, you're taking all

63
00:02:59,540 --> 00:03:02,930
of the data points and averaging the.

64
00:03:02,930 --> 00:03:06,080
Y values together. Basically ignoring the query.

65
00:03:06,080 --> 00:03:08,530
So, you end up with a constant function.

66
00:03:08,530 --> 00:03:10,310
>> But that's only if you do a

67
00:03:10,310 --> 00:03:12,480
simple average. What if you do a weighted average?

68
00:03:16,100 --> 00:03:19,040
>> A weighted average. So the near, the points that are

69
00:03:19,040 --> 00:03:21,520
the query are going to get more weight in the average, so

70
00:03:21,520 --> 00:03:25,200
that acually will be diffrent. Even though k equals n, it

71
00:03:25,200 --> 00:03:28,170
will be different depending on where you actually put your query down.

72
00:03:28,170 --> 00:03:32,180
>> Exactly. That's exactly right so, for example, if I have

73
00:03:32,180 --> 00:03:35,820
a little bunch of points like this say. Where you notice it

74
00:03:35,820 --> 00:03:38,480
kind of looks like I have two different lines here and

75
00:03:38,480 --> 00:03:41,300
I can pick a query point way over here, all of these

76
00:03:41,300 --> 00:03:44,670
points are going to influence me as oppose to these points and

77
00:03:44,670 --> 00:03:48,240
so I'm going to end up. Estimating with something that looks more

78
00:03:48,240 --> 00:03:51,990
like this because these points over here won't have much to

79
00:03:51,990 --> 00:03:54,540
say. But if I have a query point that's way over here

80
00:03:54,540 --> 00:03:57,010
somewhere these points are going to matter and I'm going to end up

81
00:03:57,010 --> 00:03:59,760
looking something looks a little bit more like this than like

82
00:03:59,760 --> 00:04:02,880
that. Now I'm drawing these as lines. They won't exactly look

83
00:04:02,880 --> 00:04:06,300
like lines because these points will have some influence. They'll be more

84
00:04:06,300 --> 00:04:09,170
curvy than that. But the point is that near, near

85
00:04:09,170 --> 00:04:10,870
the place we want to do the query it will

86
00:04:10,870 --> 00:04:13,880
look To be more strongly influenced by these points over

87
00:04:13,880 --> 00:04:16,880
here or these points over here depending on where you are.

88
00:04:16,880 --> 00:04:18,450
>> Well that gives me an idea.

89
00:04:18,450 --> 00:04:19,890
>> Oh, what kind of idea does it give you?

90
00:04:19,890 --> 00:04:21,920
>> Well, what about instead of just taking

91
00:04:21,920 --> 00:04:24,900
a weighted average, what about using the distance matrix

92
00:04:24,900 --> 00:04:26,930
to pick up some of the points? And

93
00:04:26,930 --> 00:04:30,220
then do a different regression on that substantive point.

94
00:04:30,220 --> 00:04:31,380
>> Right, I like that.

95
00:04:31,380 --> 00:04:32,920
So we can replace this whole notion of

96
00:04:32,920 --> 00:04:36,270
average with a more kind of, regression-y thing.

97
00:04:36,270 --> 00:04:39,080
>> So it actually, instead of using the same value for

98
00:04:39,080 --> 00:04:42,250
the whole patch. Actually, it still continues to use the input values.

99
00:04:42,250 --> 00:04:45,160
>> Yeah. So, in fact, average is just

100
00:04:45,160 --> 00:04:46,890
a special case of a kind of regression, right?

101
00:04:46,890 --> 00:04:47,520
>> Mm hm, mm hm.

102
00:04:47,520 --> 00:04:49,430
>> Right? So this actually has a name,

103
00:04:49,430 --> 00:04:52,130
believe it or not. It's actually called locally

104
00:04:52,130 --> 00:04:54,850
weighted regression. Yeah, so this actually works pretty

105
00:04:54,850 --> 00:04:56,770
well and in place of sort of averaging

106
00:04:56,770 --> 00:04:59,750
function, you can do just about anything you want to.

107
00:04:59,750 --> 00:05:02,460
You could throw in a decision tree, you could throw in

108
00:05:02,460 --> 00:05:06,810
a neural network, you could throw in lines do linear

109
00:05:06,810 --> 00:05:11,270
regression. You can do, almost anything that you can imagine doing.

110
00:05:11,270 --> 00:05:11,440
>> Neat,

111
00:05:11,440 --> 00:05:14,314
>> Yeah. Add that works out very well. And again, it

112
00:05:14,314 --> 00:05:16,680
gives you a little bit of power. So here's something I don't

113
00:05:16,680 --> 00:05:18,980
think is very obvious until it's pointed out to you. Which is

114
00:05:18,980 --> 00:05:21,980
this notion of replacing the average with a more general regression or

115
00:05:21,980 --> 00:05:24,930
even classification function. It actually allows

116
00:05:24,930 --> 00:05:26,570
you to do something more powerful

117
00:05:26,570 --> 00:05:29,470
than it seems. So let's imagine that we were going to do locally

118
00:05:29,470 --> 00:05:33,250
weighted regression and we were going to do, in fact, linear regression. So,

119
00:05:33,250 --> 00:05:36,650
what would locally- weighted linear regression look like? Well, if we go

120
00:05:36,650 --> 00:05:39,360
back to this example over here on the left basically, you take

121
00:05:39,360 --> 00:05:41,250
all the points that are nearby and you try to fit a

122
00:05:41,250 --> 00:05:43,680
line to it. And, so you would end up with stuff that

123
00:05:43,680 --> 00:05:47,200
looked, pretty much, like this. While you're over here, you would get

124
00:05:47,200 --> 00:05:49,030
the line like this, but while you were over here you'd

125
00:05:49,030 --> 00:05:51,590
get a line like this. Then, somewhere in the middle you

126
00:05:51,590 --> 00:05:54,630
would get lines that started to look like this and And

127
00:05:54,630 --> 00:05:57,400
you would end up with something that kind of ended up looking

128
00:05:57,400 --> 00:06:00,720
a lot like a curve. So that's kind of cool because

129
00:06:00,720 --> 00:06:04,640
you notice that we start with a hypothesis state of lines

130
00:06:04,640 --> 00:06:07,990
and this locally weighted linear regression. But then we end up

131
00:06:07,990 --> 00:06:14,050
actually being able to represent a hypothesis space that is strictly bigger.

132
00:06:14,050 --> 00:06:16,570
than the set of lines. Hm. So we can

133
00:06:16,570 --> 00:06:19,310
use a very simple kind of hypothesis space but by

134
00:06:19,310 --> 00:06:21,800
using this locally weighted regression we end up with

135
00:06:21,800 --> 00:06:26,010
a more complicated space that is complicated, that's made more

136
00:06:26,010 --> 00:06:29,310
complicated depending upon the complications that are represented by

137
00:06:29,310 --> 00:06:33,110
your data points. So this results, this sort of reveals

138
00:06:33,110 --> 00:06:36,460
another bit of power with kNN. Which is, it

139
00:06:36,460 --> 00:06:39,650
allows you to take local information and build functions or

140
00:06:39,650 --> 00:06:42,860
build concepts around the local things that are similar to

141
00:06:42,860 --> 00:06:45,920
you. And that allows you to make arbitrarily complicated functions.

142
00:06:45,920 --> 00:06:46,770
>> Neat.

143
00:06:46,770 --> 00:06:49,800
>> At least in principle. Okay, cool. Alright so you got all that?

144
00:06:49,800 --> 00:06:51,090
>> I, think so yeah.

145
00:06:51,090 --> 00:06:53,220
>> Okay, cool. So then, I think we should wrap up.

146
00:06:53,220 --> 00:06:54,000
>> Nice.

147
00:06:54,000 --> 00:06:54,810
>> Nice.
