1
00:00:00,190 --> 00:00:03,680
Behind the scenes deployments
manage replica sets.

2
00:00:03,680 --> 00:00:06,090
Each deployment is mapped
to one active replica set.

3
00:00:07,240 --> 00:00:11,710
Use kubectl get replicasets command
to view the current set of replicas.

4
00:00:12,835 --> 00:00:16,110
Replicasets are scaled to
the deployment for each service and

5
00:00:16,110 --> 00:00:18,450
can be scaled independently.

6
00:00:18,450 --> 00:00:22,010
As mentioned before, the real strength
of kubernetes come when you work in

7
00:00:22,010 --> 00:00:25,690
a declarative way, instead of using
imperative scale and expose commands.

8
00:00:26,760 --> 00:00:29,470
Let me show you how to scale
the front end deployment using

9
00:00:29,470 --> 00:00:32,259
an existing deployment
configuration file.

10
00:00:32,259 --> 00:00:34,934
First, let's see how many of
our hello pods are running.

11
00:00:34,934 --> 00:00:36,727
[BLANK_AUDIO]

12
00:00:36,727 --> 00:00:38,710
Currently we only have one.

13
00:00:38,710 --> 00:00:41,300
We'll change that by
updating the replicas field

14
00:00:41,300 --> 00:00:42,400
of our deployment manifest.

15
00:00:44,440 --> 00:00:45,610
Then we'll apply the change.

16
00:00:47,370 --> 00:00:51,290
Now we look at our replicasets, we can
see information about what's happening.

17
00:00:51,290 --> 00:00:54,200
That desired number of
replicas was updated.

18
00:00:54,200 --> 00:00:57,610
I can use to get pods command
to watch the pods come online.

19
00:00:57,610 --> 00:01:00,280
We'll see the correct
number of pods here.

20
00:01:00,280 --> 00:01:02,860
Now we can check that the deployment
of that it is the correct

21
00:01:02,860 --> 00:01:03,680
number of replicas.

22
00:01:05,660 --> 00:01:09,340
Yep, looks like we now have three
replicas that the deployment

23
00:01:09,340 --> 00:01:09,860
is managing.

24
00:01:10,910 --> 00:01:13,150
And we can still hit our
end point like before.

25
00:01:14,340 --> 00:01:18,390
At this point we now have multiple
copies of our Hello service

26
00:01:18,390 --> 00:01:20,000
running in kubernetes and

27
00:01:20,000 --> 00:01:25,110
we have a single front in service that's
proxying traffic to all three pods.

28
00:01:25,110 --> 00:01:29,330
This allows us to share the load and
scale our containers and kubernetes.
