1
00:00:00,210 --> 00:00:02,900
All right,
welcome back to Computer Vision again.

2
00:00:02,900 --> 00:00:04,750
We're going back to recognition now.

3
00:00:04,750 --> 00:00:07,850
Remember, before we were talking
about supervised classification?

4
00:00:07,850 --> 00:00:11,870
Supervised classification is I give you
a bunch of labeled examples, right?

5
00:00:11,870 --> 00:00:14,230
Typically referred to
as training examples.

6
00:00:14,230 --> 00:00:16,560
And you're supposed to come up with
a function that'll label some new

7
00:00:16,560 --> 00:00:17,320
examples.

8
00:00:17,320 --> 00:00:21,510
And we showed this one where, we have a
bunch of hand-drawn fours and hand-drawn

9
00:00:21,510 --> 00:00:26,330
nines, and the idea was, given some new
input, is it a four or is it a nine?

10
00:00:26,330 --> 00:00:29,120
The idea was,
since we know what the desired labels

11
00:00:29,120 --> 00:00:32,159
are from the training data,
we're going to try to build a classifier

12
00:00:32,159 --> 00:00:35,874
that's going to minimize the expected
misclassification, or minimize our risk.

13
00:00:35,874 --> 00:00:39,760
All right, and we talked about
that there were two strategies.

14
00:00:39,760 --> 00:00:41,290
One was called generative,

15
00:00:41,290 --> 00:00:44,240
where we probabilistically model
each of the classes and then,

16
00:00:44,240 --> 00:00:47,460
somehow, picked them, we'll talk
about those again just for a second.

17
00:00:47,460 --> 00:00:51,710
And then there is this discriminative,
and discriminative says, I don't care so

18
00:00:51,710 --> 00:00:53,900
much about modeling the classes,

19
00:00:53,900 --> 00:00:57,330
what I care about is modelling some
sort of boundary between the classes.

20
00:00:57,330 --> 00:01:00,840
And that makes it easier
to make some decisions.

21
00:01:00,840 --> 00:01:03,900
So, we talked about the risk of
a particular decision as being,

22
00:01:03,900 --> 00:01:06,420
if you're making a particular decision,
what's the probability that that

23
00:01:06,420 --> 00:01:11,880
decision's wrong, and what's the cost
of making that wrong decision?

24
00:01:11,880 --> 00:01:16,750
So, for example, let's suppose that I'm
going to label something as a four.

25
00:01:16,750 --> 00:01:18,230
Well, if I'm going to
label it as a four,

26
00:01:18,230 --> 00:01:20,630
what's the risk associated
with it being a four?

27
00:01:20,630 --> 00:01:21,800
Well, that risk would be,

28
00:01:21,800 --> 00:01:27,300
well, you know, what's the probability
that it's actually a nine, okay?

29
00:01:27,300 --> 00:01:31,470
And what's the cost, to me,
of calling a nine a four, all right?

30
00:01:31,470 --> 00:01:34,865
So that's the risk of labeling
something a four, right?

31
00:01:34,865 --> 00:01:38,010
because it actually might have
been a nine, and what's the cost.

32
00:01:38,010 --> 00:01:40,910
Likewise, suppose I'm going to
label something a nine.

33
00:01:40,910 --> 00:01:42,950
What's the risk of
labeling something a nine?

34
00:01:42,950 --> 00:01:46,855
Well the risk of labeling something
a nine is the probability that it was

35
00:01:46,855 --> 00:01:50,779
actually a four, times the cost of
labeling something a four as a nine.

36
00:01:50,779 --> 00:01:51,325
Right?

37
00:01:51,325 --> 00:01:55,248
And, where should my
decision boundary be?

38
00:01:55,248 --> 00:02:00,165
Well my decision boundary should be
that the risk of either decision would

39
00:02:00,165 --> 00:02:01,720
be the same.

40
00:02:01,720 --> 00:02:03,650
Okay, and that was the idea
of the generative model.

41
00:02:03,650 --> 00:02:07,150
And so that's what's being shown here
is, so here are my, all my real fours so

42
00:02:07,150 --> 00:02:09,699
I'm pretty sure that they're four,
so I'm going to call them fours.

43
00:02:09,699 --> 00:02:13,340
But as I'm getting over to
the other side, it might be a nine.

44
00:02:13,340 --> 00:02:16,280
Here are all my nines, and
as I'm getting closer over here,

45
00:02:16,280 --> 00:02:17,590
it might be a four.

46
00:02:17,590 --> 00:02:20,680
And this boundary in the middle,
that's where the risk is the same.
