1
00:00:00,450 --> 00:00:03,469
>> In theory you take a DAG and
you scheduled it on a PRAM.

2
00:00:03,469 --> 00:00:07,260
But here a question
where do DAGs come from?

3
00:00:07,260 --> 00:00:09,290
The answer is from you.

4
00:00:09,290 --> 00:00:12,870
But you need a programming
model to generate the DAGs.

5
00:00:12,870 --> 00:00:16,615
Let me show you one that,
I think is especially clean and elegant.

6
00:00:16,615 --> 00:00:19,860
Here's the DAG for a divide and
conquer algorithm to computer reduction.

7
00:00:19,860 --> 00:00:24,060
You have argued that it's work
optimal and it has logarithmic span.

8
00:00:24,060 --> 00:00:28,120
Now, I want to give you an algorithm
that produces this DAG.

9
00:00:28,120 --> 00:00:31,200
Let's start with just a sequential
algorithm that implements the divide and

10
00:00:31,200 --> 00:00:31,890
conquer scheme.

11
00:00:32,960 --> 00:00:35,360
So here is the pseudo code for
such an algorithm.

12
00:00:35,360 --> 00:00:38,110
It takes an array A of length n, and

13
00:00:38,110 --> 00:00:40,870
if there are at least two elements
it does divide and conquer.

14
00:00:40,870 --> 00:00:46,440
So recursively calls itself on each
half of the array, and returns the sum.

15
00:00:46,440 --> 00:00:50,410
Otherwise the array is just of size
one and it returns that element.

16
00:00:50,410 --> 00:00:53,220
Now what you can observe
is that the two recursive

17
00:00:53,220 --> 00:00:56,110
calls are independent of one another.

18
00:00:56,110 --> 00:00:59,270
Now since the two recursive
function calls are independent.

19
00:00:59,270 --> 00:01:02,920
I'm going to give you a special
keyword to mark that fact.

20
00:01:02,920 --> 00:01:04,720
That keyword is called a spawn.

21
00:01:05,830 --> 00:01:10,140
The target of a spawn is always either
a function call or a procedure call.

22
00:01:10,140 --> 00:01:12,650
In this case I'm showing
a function call.

23
00:01:12,650 --> 00:01:16,050
Now the spawn keyword is a signal
to either the compiler or

24
00:01:16,050 --> 00:01:20,000
the runtime system that the target
is an independent unit of work.

25
00:01:21,010 --> 00:01:23,750
By inserting the spawn you're
effectively saying that the target

26
00:01:23,750 --> 00:01:27,080
may be executed asynchronously
from the collar

27
00:01:27,080 --> 00:01:29,570
any time a processor is available.

28
00:01:29,570 --> 00:01:32,740
Now even though these two calls
are independent units of work,

29
00:01:32,740 --> 00:01:37,210
notice that they produce intermediate
results that then have to be combined.

30
00:01:37,210 --> 00:01:42,590
In other words there's a dependence
from a and b and the return statement.

31
00:01:42,590 --> 00:01:44,550
That means in addition to spawns,

32
00:01:44,550 --> 00:01:48,100
we also need a way to indicate
these kinds of dependences.

33
00:01:48,100 --> 00:01:51,940
For that, I will give you a second
special keyword called sync.

34
00:01:51,940 --> 00:01:52,980
So, one question.

35
00:01:52,980 --> 00:01:56,880
To which spawns does a given sync apply?

36
00:01:56,880 --> 00:02:00,900
So we will use a particular convention
which is that the sync waits for

37
00:02:00,900 --> 00:02:04,520
any spawn that has occurred so
far within the same stack frame.

38
00:02:04,520 --> 00:02:07,790
If you need a refresher on
call stacks and stack frames,

39
00:02:07,790 --> 00:02:09,119
please see the instructor's notes.

40
00:02:10,320 --> 00:02:13,670
Let me make one final remark
about spawn and sync.

41
00:02:13,670 --> 00:02:16,600
Now, suppose you leave the sync out.

42
00:02:16,600 --> 00:02:21,485
Even if you leave sync out, there will
always be an implicit sync at the return

43
00:02:21,485 --> 00:02:24,430
immediately before going
back to the caller.

44
00:02:24,430 --> 00:02:28,196
Let me show you what that means by
transforming this code to match

45
00:02:28,196 --> 00:02:29,800
the note.

46
00:02:29,800 --> 00:02:35,020
So instead of return of a+b,
we'll generate three statements.

47
00:02:35,020 --> 00:02:37,550
The first will evaluate the operand a+b.

48
00:02:37,550 --> 00:02:40,950
The second will perform the sync.

49
00:02:40,950 --> 00:02:43,840
And then finally will return
the value to the caller.

50
00:02:43,840 --> 00:02:46,200
And notice where the sync appears, and

51
00:02:46,200 --> 00:02:49,050
you transform the other
return in the same way.

52
00:02:49,050 --> 00:02:50,440
Now, an important point.

53
00:02:50,440 --> 00:02:54,460
Even with this transformation,
the program is still wrong.

54
00:02:54,460 --> 00:02:56,210
Can you see why?

55
00:02:56,210 --> 00:02:58,500
Notice that the sync
appears after the sum.

56
00:02:59,540 --> 00:03:03,981
Now the two spawned calls are only
guaranteed to be complete at the sync.

57
00:03:03,981 --> 00:03:08,940
Therefore the values of a and
b might not yet be valid at that point.

58
00:03:08,940 --> 00:03:13,070
Now the fact of Implicit syncs will
constrain the kinds of DAGs that this

59
00:03:13,070 --> 00:03:15,480
programming model can produce.

60
00:03:15,480 --> 00:03:20,490
The style of parallelism in such DAGs
is sometimes called nested parallelism.

61
00:03:20,490 --> 00:03:24,140
Now once you see how this pseudo code
gives rise to a DAG you'll understand

62
00:03:24,140 --> 00:03:27,140
where the term nested
parallelism comes from.

63
00:03:27,140 --> 00:03:29,540
Let's go back to the correct code.

64
00:03:29,540 --> 00:03:32,910
Now what I want you to do is try
simulating the algorithm to see how

65
00:03:32,910 --> 00:03:34,920
the DAG unfolds.

66
00:03:34,920 --> 00:03:38,480
So in particular let's say we start
to execute a reduce on an array of

67
00:03:38,480 --> 00:03:39,900
size four.

68
00:03:39,900 --> 00:03:42,740
The first step is to
enter the reduce call.

69
00:03:42,740 --> 00:03:46,320
That creates the first unit
of work within the DAG.

70
00:03:46,320 --> 00:03:48,030
Then there's the conditional test.

71
00:03:48,030 --> 00:03:49,270
Then you encounter the spawn.

72
00:03:50,310 --> 00:03:52,330
Here's where things get interesting.

73
00:03:52,330 --> 00:03:56,060
The spawn creates a new
branch in the DAG.

74
00:03:56,060 --> 00:03:59,300
Essentially the spawn signals that
there's a new independent of work

75
00:03:59,300 --> 00:04:00,690
that's ready to go.

76
00:04:00,690 --> 00:04:03,110
And it creates a new path as well.

77
00:04:03,110 --> 00:04:06,870
Now in the mean time,
the current path will continue.

78
00:04:06,870 --> 00:04:10,230
So this is an extremely important
point to that the spawn creates

79
00:04:10,230 --> 00:04:12,970
essentially two independent paths.

80
00:04:12,970 --> 00:04:16,750
One path carries the new work and
the other path is the path

81
00:04:16,750 --> 00:04:20,290
that continues executing
immediately after the spawn.

82
00:04:20,290 --> 00:04:23,490
Now while the main path is
happily going about its business,

83
00:04:23,490 --> 00:04:26,730
the newly spawned path
is a recursive call.

84
00:04:26,730 --> 00:04:30,020
And therefore it has to
generate its own subgraph.

85
00:04:30,020 --> 00:04:32,100
Now you are travelling
along the main path and

86
00:04:32,100 --> 00:04:34,500
so you'll encounter the second spawn.

87
00:04:34,500 --> 00:04:36,550
This branching will happen again.

88
00:04:36,550 --> 00:04:38,760
Next, you'll reach the sync.

89
00:04:38,760 --> 00:04:41,340
The sync waits for
the previous spawns to complete.

90
00:04:42,360 --> 00:04:45,625
In terms of the DAG, that should create
some dependence edges between these

91
00:04:45,625 --> 00:04:47,990
subgraphs and the sync point.

92
00:04:47,990 --> 00:04:50,960
Control then goes to
the return statement, and

93
00:04:50,960 --> 00:04:54,030
you'll evaluate a+b and
then finally return that value.
