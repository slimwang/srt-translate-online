1
00:00:00,270 --> 00:00:04,340
So this is called the first component, or the principle component, and this is

2
00:00:04,340 --> 00:00:07,100
called the second, or second principle component

3
00:00:07,100 --> 00:00:09,500
of this space. Okay, does that make sense?

4
00:00:09,500 --> 00:00:09,970
>> Yep.

5
00:00:09,970 --> 00:00:11,800
>> Now, here's what's interesting about principle

6
00:00:11,800 --> 00:00:13,440
components analysis. You might ask me exactly

7
00:00:13,440 --> 00:00:17,940
how you do this. There are several, several mechanisms for doing it. For those

8
00:00:17,940 --> 00:00:19,590
of you who have dealt with linear

9
00:00:19,590 --> 00:00:21,580
algebra before, something like this singular value

10
00:00:21,580 --> 00:00:24,070
decomposition might be familiar to you. It's

11
00:00:24,070 --> 00:00:25,720
one way of finding the principal component.

12
00:00:25,720 --> 00:00:28,860
But principal components analysis basically has a lot of really

13
00:00:28,860 --> 00:00:30,970
neat properties, so let me just describe some of those

14
00:00:30,970 --> 00:00:33,600
properties to you. The first property is, well, the two

15
00:00:33,600 --> 00:00:37,074
that I've written here. It finds directions that maximize variants and

16
00:00:37,074 --> 00:00:41,640
it finds directions that are mutually orthogonal. Mutually orthogonal means

17
00:00:41,640 --> 00:00:44,430
it's a global algorithm. And by global here I mean

18
00:00:44,430 --> 00:00:47,100
that all the directions, all the new features that they

19
00:00:47,100 --> 00:00:51,016
find have a big global constraint, namely that they must be

20
00:00:51,016 --> 00:00:55,720
mutually orthogonal. It also turns out that you can prove. Which

21
00:00:55,720 --> 00:00:57,994
I will try to give you a little bit of evidence

22
00:00:57,994 --> 00:01:02,960
for. But I'm going to prove formally, that the PCA actually gives

23
00:01:02,960 --> 00:01:05,410
you the best reconstruction. Now what do I mean by best

24
00:01:05,410 --> 00:01:08,120
reconstruction? What I mean is, if you think of each of

25
00:01:08,120 --> 00:01:10,860
these directions that it found, in this case. You found this

26
00:01:10,860 --> 00:01:13,650
one first and found this one second. The first thing you,

27
00:01:13,650 --> 00:01:16,170
I, I hope you see is that if I return these two

28
00:01:16,170 --> 00:01:20,250
dimensions, I have actually lost no information. That is, this

29
00:01:20,250 --> 00:01:25,550
is just a linear rotation of the original dimensions. So if

30
00:01:25,550 --> 00:01:28,630
I were to give you back these two different features,

31
00:01:28,630 --> 00:01:31,080
you could reconstruct all of your original data. You see that?

32
00:01:31,080 --> 00:01:33,110
>> Wait. When you say features you mean, what we're

33
00:01:33,110 --> 00:01:35,590
going to give it is, for each of those little black

34
00:01:35,590 --> 00:01:38,650
data points, we're going to say how far along the red

35
00:01:38,650 --> 00:01:41,190
axis is it and how far along the orange axis

36
00:01:41,190 --> 00:01:41,490
is it.

37
00:01:41,490 --> 00:01:41,970
>> Right.

38
00:01:41,970 --> 00:01:43,590
>> So it really is just a, a, a kind

39
00:01:43,590 --> 00:01:47,980
of a relabeling of the. of the dimensions. Right, so just

40
00:01:47,980 --> 00:01:50,330
like when I think about these points in x and

41
00:01:50,330 --> 00:01:53,740
y space, the original feature space, whenever I give you value

42
00:01:53,740 --> 00:01:56,090
here for this feature I'm just describing how far along

43
00:01:56,090 --> 00:01:59,390
a black dot is on this dimension or this projection. Now

44
00:01:59,390 --> 00:02:02,280
the second dimension or the second feature just tells me how

45
00:02:02,280 --> 00:02:06,550
far along a dot is. On this particular dimension or axis,

46
00:02:06,550 --> 00:02:08,600
and similarly about projecting on the read and on

47
00:02:08,600 --> 00:02:10,788
the orange, I'm telling how far along a point is

48
00:02:10,788 --> 00:02:15,190
along this axis and along that axis. So if I were to return, if I were to take X

49
00:02:15,190 --> 00:02:17,370
and Y and transform them into this new one

50
00:02:17,370 --> 00:02:19,660
and two, I would have given you different values than

51
00:02:19,660 --> 00:02:21,690
I did from X or Y, but they're actually just

52
00:02:21,690 --> 00:02:24,320
the same point. And so I've thrown away no information.

53
00:02:24,320 --> 00:02:26,140
>> So that's a pretty good reconstruction.

54
00:02:26,140 --> 00:02:26,800
>> That's a pretty good

55
00:02:26,800 --> 00:02:28,920
reconstruction. But what principle components analysis

56
00:02:28,920 --> 00:02:31,810
does for you. Is if I take just on of these dimensions,

57
00:02:31,810 --> 00:02:36,300
in particular, the first one, the principle component, I am

58
00:02:36,300 --> 00:02:39,630
guaranteed that if I project only in to this space

59
00:02:39,630 --> 00:02:42,800
and then try to reproject into the original space. I

60
00:02:42,800 --> 00:02:47,070
will minimize what's called L2 error. So do you understand that?

61
00:02:47,070 --> 00:02:52,230
>> I'm not sure let me check. So, you're saying if we instead of,

62
00:02:52,230 --> 00:02:56,860
all right we take the little black dots, they now have a red dimension and

63
00:02:56,860 --> 00:03:01,470
orange dimension one two, and now if we reconstruct using only the first

64
00:03:01,470 --> 00:03:04,910
dimension, I guess it just puts the black dots on the red line.

65
00:03:04,910 --> 00:03:09,550
>> Yep. And so there is no, they have no existance in that second dimension.

66
00:03:09,550 --> 00:03:10,490
>> Correct.

67
00:03:10,490 --> 00:03:13,380
>> And now you're saying of all the different ways

68
00:03:13,380 --> 00:03:15,370
that I could do that to kind of project it to

69
00:03:15,370 --> 00:03:17,660
a, to a linear sequence. This is the one that's

70
00:03:17,660 --> 00:03:22,880
going to have the smallest. L2 error which if im not mistaken

71
00:03:22,880 --> 00:03:24,940
is the same kind of reconstruction error we talked about

72
00:03:24,940 --> 00:03:28,580
in all the other times we talked about reconstrucion. So it's

73
00:03:28,580 --> 00:03:31,510
like squared error. Thats exactly right, its squared error. This

74
00:03:31,510 --> 00:03:34,230
particular notion is called a [UNKNOWN] norm but thats really just

75
00:03:34,230 --> 00:03:36,460
talking about distance. What this means is that if I

76
00:03:36,460 --> 00:03:39,820
project onto this single axis here, and then I compare it

77
00:03:39,820 --> 00:03:43,870
to where it was in the original space, the distance,

78
00:03:43,870 --> 00:03:48,130
the sum of all the distances between those points will actually

79
00:03:48,130 --> 00:03:51,250
be the minimal that I could get for any other projection.

80
00:03:51,250 --> 00:03:51,730
>> Cool.

81
00:03:51,730 --> 00:03:54,650
>> And you can, you can sort of prove this. It kind of makes sense if you

82
00:03:54,650 --> 00:03:56,840
just think about the fact that points. Always

83
00:03:56,840 --> 00:03:59,650
start out in some orthogonal space. And, I'm basically

84
00:03:59,650 --> 00:04:02,290
finding in scaling and a rotation such that

85
00:04:02,290 --> 00:04:05,530
I don't lose any information. And I maximize variance

86
00:04:05,530 --> 00:04:08,280
along the way. By maximizing variance, it turns out

87
00:04:08,280 --> 00:04:13,400
I'm maximizing or maintaining distances as best I can

88
00:04:13,400 --> 00:04:16,560
in any given dimension. And, so, that gives me the

89
00:04:16,560 --> 00:04:19,839
best reconstruction that I can imagine. Now, you might ask

90
00:04:19,839 --> 00:04:23,070
yourself, Is there anything else nice about principal components analysis

91
00:04:23,070 --> 00:04:26,720
given this reconstruction error? And there's another property of PCA that

92
00:04:26,720 --> 00:04:29,000
is very, very useful. And it boils down to the

93
00:04:29,000 --> 00:04:31,500
fact that it's an eigenproblem. What happens when you do

94
00:04:31,500 --> 00:04:34,370
principal components analysis is you get all of these axes

95
00:04:34,370 --> 00:04:38,360
back, and in fact, if you start out with. N dimensions,

96
00:04:38,360 --> 00:04:42,110
you get back N dimensions again, and the job

97
00:04:42,110 --> 00:04:44,960
here for a future transformation as you might recall, is

98
00:04:44,960 --> 00:04:48,300
you want to pick a subset M of them hopefully

99
00:04:48,300 --> 00:04:51,870
much smaller than N. Well, it turns out that associated

100
00:04:51,870 --> 00:04:53,870
with each one of these new dimensions that we

101
00:04:53,870 --> 00:04:57,815
get. Is its eigenvalue. That eigenvalue is guaranteed to be

102
00:04:57,815 --> 00:05:00,270
non-negative, it has a lot of other neat properties.

103
00:05:00,270 --> 00:05:04,120
But what matters to us here is that the eigenvalues

104
00:05:04,120 --> 00:05:08,640
monotonically non-increase, that is, they tend to get smaller

105
00:05:08,640 --> 00:05:10,860
as you move from the principal to the second

106
00:05:10,860 --> 00:05:12,400
principal, to the third, to the fourth, to the

107
00:05:12,400 --> 00:05:14,540
fifth, to the sixth, and so on to the nth

108
00:05:14,540 --> 00:05:18,490
dimension. And so, you can throw away the ones

109
00:05:18,490 --> 00:05:21,040
with the least eigenvalue. And that's a way of

110
00:05:21,040 --> 00:05:23,930
saying that you're throw awaying these projections, or the

111
00:05:23,930 --> 00:05:26,800
directions, or the features, with the least amount of variance.
