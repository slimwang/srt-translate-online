1
00:00:00,320 --> 00:00:04,120
Typically, the virtual address space of a process

2
00:00:04,120 --> 00:00:07,210
running on a processor, let's say your desktop, your

3
00:00:07,210 --> 00:00:09,750
PC, your laptop, and so on, is much

4
00:00:09,750 --> 00:00:13,200
larger than the physical memory that is allocated for

5
00:00:13,200 --> 00:00:15,980
a particular process. So the role of the

6
00:00:15,980 --> 00:00:19,480
virtual memory manager in the operating system is to

7
00:00:19,480 --> 00:00:22,760
give the illusion to the process that all of

8
00:00:22,760 --> 00:00:25,930
its virtual address space is contained in physical memory.

9
00:00:25,930 --> 00:00:28,982
But, in fact, only a portion of the virtual

10
00:00:28,982 --> 00:00:31,810
address space is really in the physical memory. And that's

11
00:00:31,810 --> 00:00:34,470
called the working set of the process. And the

12
00:00:34,470 --> 00:00:39,590
virtual memory manager supports this illusion for the process by

13
00:00:39,590 --> 00:00:42,970
paging in and out from the disk the pages

14
00:00:42,970 --> 00:00:45,266
that are being accessed by a process at any particular

15
00:00:45,266 --> 00:00:48,300
point of time so that the process is happy

16
00:00:48,300 --> 00:00:50,960
in terns of having its working set contained in the

17
00:00:50,960 --> 00:00:54,070
physical memory. Now when a node, a desktop,

18
00:00:54,070 --> 00:00:57,220
is connected on a local area network to

19
00:00:57,220 --> 00:00:59,840
other peer nodes that are also on the

20
00:00:59,840 --> 00:01:03,468
same local area network it is conceivable that

21
00:01:03,468 --> 00:01:05,852
at any point of time, the memory pressure,

22
00:01:05,852 --> 00:01:10,190
meaning the amount of memory that is required

23
00:01:10,190 --> 00:01:12,830
to keep all the processes running on this

24
00:01:12,830 --> 00:01:16,290
node happy, may be different from the memory pressure

25
00:01:16,290 --> 00:01:19,500
on another nodes. In other words this particular node

26
00:01:19,500 --> 00:01:23,140
may have a much higher memory pressure because the work

27
00:01:23,140 --> 00:01:25,320
load on this load consumes a lot of the physical

28
00:01:25,320 --> 00:01:29,220
memory whereas this work station may be idle and therefore

29
00:01:29,220 --> 00:01:31,460
all of this memory is not being utilized at

30
00:01:31,460 --> 00:01:35,920
this point for running anything useful because no applications are

31
00:01:35,920 --> 00:01:39,280
running on these nodes. So this opens up a new

32
00:01:39,280 --> 00:01:41,890
line of thought. Given that all these nodes are connected

33
00:01:41,890 --> 00:01:47,074
on a local area network and some nodes may be busy while other nodes may be

34
00:01:47,074 --> 00:01:51,060
idle. Is it possible if a particular node

35
00:01:51,060 --> 00:01:55,190
experiences memory pressure, can we use the idle

36
00:01:55,190 --> 00:01:58,370
cluster memory, and in particular, can we use

37
00:01:58,370 --> 00:02:01,790
the cluster memory that's available for paging in

38
00:02:01,790 --> 00:02:06,980
and out the working set of the processes on this node? Rather than going to the

39
00:02:06,980 --> 00:02:12,085
disk, can we page in and out to the cluster memories that are idle at this point

40
00:02:12,085 --> 00:02:17,825
of time? It turns out that with advances in local area networking gear, it's

41
00:02:17,825 --> 00:02:21,790
already made possible for gigabit ethernet connectivity

42
00:02:21,790 --> 00:02:24,850
to be available for desktops. And pretty soon,

43
00:02:24,850 --> 00:02:28,450
10 gigabit links are going to be common

44
00:02:28,450 --> 00:02:32,190
in connecting desktops to the local area network.

45
00:02:32,190 --> 00:02:35,430
This makes sending a page to a remote memory

46
00:02:35,430 --> 00:02:39,350
or fetching a page from a remote memory faster than

47
00:02:39,350 --> 00:02:42,310
sending it to a local disk. Typically, the local

48
00:02:42,310 --> 00:02:46,460
disk access speeds are in the order of 200 megabytes

49
00:02:46,460 --> 00:02:48,290
per second in terms of transfer rate, but on

50
00:02:48,290 --> 00:02:51,040
top of that, you have to add things like seek

51
00:02:51,040 --> 00:02:54,080
latency and rotation latency for accessing the data that you

52
00:02:54,080 --> 00:02:57,650
want from the disk. So all of this augers well

53
00:02:57,650 --> 00:03:03,000
for saying that perhaps paging in and out through the local area network to peer

54
00:03:03,000 --> 00:03:04,940
memories that are idle may be a good

55
00:03:04,940 --> 00:03:09,070
solution when we have memory pressure being experienced

56
00:03:09,070 --> 00:03:12,860
at any particular node in the cluster. So

57
00:03:12,860 --> 00:03:16,120
the global memory system, or GMS for short,

58
00:03:16,120 --> 00:03:20,870
uses cluster memory for paging across the network.

59
00:03:20,870 --> 00:03:23,680
So in other words in normal memory management,

60
00:03:23,680 --> 00:03:27,570
if virtual address to physical address translation fails then

61
00:03:27,570 --> 00:03:30,748
the memory manager knows that it can find this

62
00:03:30,748 --> 00:03:33,840
virtual address on the disk, meaning the page that

63
00:03:33,840 --> 00:03:36,220
contains this virtual address on the disk, it pages

64
00:03:36,220 --> 00:03:38,910
in that particular page from the disk. Now in

65
00:03:38,910 --> 00:03:42,440
GMS, what we're going to do is, if there

66
00:03:42,440 --> 00:03:45,060
is a page fault, that is, this translation, virtual

67
00:03:45,060 --> 00:03:48,810
address to physical address, fails then that means that

68
00:03:48,810 --> 00:03:51,295
the page is not in the physical memory of this

69
00:03:51,295 --> 00:03:54,770
node. In that case GMS is going to say, well

70
00:03:54,770 --> 00:03:57,210
it might be there in the cluster memory, in one

71
00:03:57,210 --> 00:03:59,710
of the nodes of my peers, or it could be on

72
00:03:59,710 --> 00:04:02,180
the disk if it is not in the cluster memory

73
00:04:02,180 --> 00:04:06,120
of my peers. So that's the idea that GMS is

74
00:04:06,120 --> 00:04:10,840
sort of integrating the cluster memory into this memory hierarchy.

75
00:04:10,840 --> 00:04:14,160
Normally, when we think about memory hierarchy in a computer system,

76
00:04:14,160 --> 00:04:17,110
we say there's a processor, there's the caches, and there's the

77
00:04:17,110 --> 00:04:19,890
main memory, and then there is the virtual memory sitting on

78
00:04:19,890 --> 00:04:23,070
the disk. But now in GMS, we're sort of extending that

79
00:04:23,070 --> 00:04:27,370
by saying in addition to the normal memory hierarchy that exists

80
00:04:27,370 --> 00:04:30,980
in any computer system, that is processor, caches and memory, there

81
00:04:30,980 --> 00:04:34,780
is also the cluster memory. And only if it is not

82
00:04:34,780 --> 00:04:37,040
in the cluster memory, we have to think of going to

83
00:04:37,040 --> 00:04:39,550
the disk in order to get the page that we're looking for.

84
00:04:39,550 --> 00:04:41,230
That's sort of the idea. So in other

85
00:04:41,230 --> 00:04:46,259
words, GMS trades network communication for disk I/O. And

86
00:04:46,259 --> 00:04:48,580
we are doing this only for reads, for

87
00:04:48,580 --> 00:04:51,890
reading across a network. GMS does not get in

88
00:04:51,890 --> 00:04:58,730
the way of writing to the disk, that is the disk always has copy of all the

89
00:04:58,730 --> 00:05:01,330
pages. The only pages that can be in the

90
00:05:01,330 --> 00:05:04,590
cluster memories are pages that have been paged out

91
00:05:04,590 --> 00:05:07,520
that are not dirty. And if there are dirty

92
00:05:07,520 --> 00:05:09,260
copies of pages, they are to be written onto the

93
00:05:09,260 --> 00:05:11,800
disk just like it will happen in a regular

94
00:05:11,800 --> 00:05:16,220
computer system. In other words, GMS does not add any

95
00:05:16,220 --> 00:05:20,130
new causes for worrying about failures because even if

96
00:05:20,130 --> 00:05:23,470
a node crashes all that you lose is clean copies

97
00:05:23,470 --> 00:05:26,610
of pages belonging to a particular process that may have

98
00:05:26,610 --> 00:05:29,598
been in the memory of this node. But those pages

99
00:05:29,598 --> 00:05:34,560
are on the disk as well. So the disk always has all the copies of the pages.

100
00:05:34,560 --> 00:05:37,170
It is just that the remote memories of

101
00:05:37,170 --> 00:05:41,060
the cluster is serving as yet another level in

102
00:05:41,060 --> 00:05:44,230
the memory hierarchy. So just to recap the

103
00:05:44,230 --> 00:05:48,790
top level idea of GMS. Normally, in a computer

104
00:05:48,790 --> 00:05:52,750
system if a process page faults that means

105
00:05:52,750 --> 00:05:54,630
that the page it is looking for is not

106
00:05:54,630 --> 00:05:57,010
in physical memory, it'll go to the disk to get it.

107
00:05:57,010 --> 00:06:00,260
But in GMS, what we're going to do is if a process

108
00:06:00,260 --> 00:06:03,580
page faults, we know that it is not in physical memory but

109
00:06:03,580 --> 00:06:06,280
it could be in one of the peer memories as well. So

110
00:06:06,280 --> 00:06:10,430
GMS is a way of locating the faulting page from one

111
00:06:10,430 --> 00:06:13,280
of the peer memories, if it is in fact contained in one

112
00:06:13,280 --> 00:06:16,090
of the peer memories. If not, it's going to be found on

113
00:06:16,090 --> 00:06:19,920
the disk. So that's the idea behind that. So when the virtual

114
00:06:19,920 --> 00:06:23,480
memory manager, the virtual memory manager that is part of

115
00:06:23,480 --> 00:06:27,780
GMS, decides to evict a page from physical memory to

116
00:06:27,780 --> 00:06:31,400
make room for the current working set of processes running

117
00:06:31,400 --> 00:06:34,134
on this node, then what the virtual memory manager is going

118
00:06:34,134 --> 00:06:36,620
to do is, instead of swapping it out to the

119
00:06:36,620 --> 00:06:38,890
disk, it is going to go out on the network

120
00:06:38,890 --> 00:06:42,390
and find a peer memory that's idle, and put that

121
00:06:42,390 --> 00:06:45,210
page in the peer memory so that later on if that

122
00:06:45,210 --> 00:06:48,120
page is needed, it can fetch it back from the peer and memory.

123
00:06:48,120 --> 00:06:52,100
That's sort of the big picture of how the global memory system works.
