1
00:00:00,350 --> 00:00:04,390
I actually pulled out an older piece
of work focusing just on the gesture.

2
00:00:04,390 --> 00:00:10,730
This is by Nam and Wohn of recognizing
hand gestures using some HMMs.

3
00:00:10,730 --> 00:00:14,580
Just because it was very paradigmatic
of how people would do things.

4
00:00:14,580 --> 00:00:16,490
Well, what's kind of cool
about their system was,

5
00:00:16,490 --> 00:00:21,150
they had a bunch of different gestures
that they wanted to indicate.

6
00:00:21,150 --> 00:00:22,190
Like nouns.

7
00:00:22,190 --> 00:00:22,770
You know,

8
00:00:22,770 --> 00:00:27,500
chairs, vase, lamp, bulb because of
course that's a natural vocabulary.

9
00:00:27,500 --> 00:00:28,450
I have no idea.

10
00:00:28,450 --> 00:00:33,100
And then you want to do things like
put it down or bring it or put or

11
00:00:33,100 --> 00:00:35,180
discard or jump.

12
00:00:35,180 --> 00:00:38,630
I don't, don't even know why that was a
particular verb that they wanted to use.

13
00:00:38,630 --> 00:00:42,060
But the idea was that there
was a fixed vocabulary.

14
00:00:42,060 --> 00:00:43,760
The inputs that they
were going to use and

15
00:00:43,760 --> 00:00:47,172
they were going to control it with the
hand, was a couple of different kinds.

16
00:00:47,172 --> 00:00:50,770
Right?
They had something to do about hand

17
00:00:50,770 --> 00:00:55,060
configuration or posture, something to
do with how your fingers were shaped.

18
00:00:55,060 --> 00:00:58,140
Okay.
They also had how you're holding your

19
00:00:58,140 --> 00:01:05,940
palm and then they had a sequence
of hand position, a motion, okay?

20
00:01:05,940 --> 00:01:09,260
And they wanted to re,
use all of this information

21
00:01:09,260 --> 00:01:12,300
to make a decision about what
gesture you were indicating.

22
00:01:12,300 --> 00:01:16,420
So the first thing you should ask is
how are they getting this information?

23
00:01:16,420 --> 00:01:18,320
How are they getting this information?

24
00:01:18,320 --> 00:01:19,730
>> Boy, Meghan woke up.

25
00:01:19,730 --> 00:01:21,650
Well they were using something
called a data glove.

26
00:01:21,650 --> 00:01:22,390
Here's a picture.

27
00:01:22,390 --> 00:01:25,670
So a data glove is this thing that
you can wear on your hand that had

28
00:01:25,670 --> 00:01:29,700
both accelerometer and gyro information
about how it was being globally

29
00:01:29,700 --> 00:01:34,200
manipulated and also knew something
about how you were moving your fingers.

30
00:01:34,200 --> 00:01:37,990
So they built this kind
of interesting system.

31
00:01:37,990 --> 00:01:42,990
And the reason I liked it is,
they actually are using

32
00:01:42,990 --> 00:01:47,690
these three sensors the angles,
the orientation, the position, right?

33
00:01:47,690 --> 00:01:51,460
And what's interesting
is they use the HMMs

34
00:01:51,460 --> 00:01:55,790
just to get information about
that motion, trajectory.

35
00:01:55,790 --> 00:01:56,380
Right?

36
00:01:56,380 --> 00:02:00,050
They had other methods of
putting into their system

37
00:02:00,050 --> 00:02:03,690
how your orientation was changing and
what the angles were.

38
00:02:03,690 --> 00:02:06,100
Right?
So the idea is that you might have some

39
00:02:06,100 --> 00:02:10,940
basic overall probabilistic
integration system, and in order to

40
00:02:10,940 --> 00:02:15,570
make use of that, you needed things
that would output the probability or

41
00:02:15,570 --> 00:02:19,860
the likelihood of getting
a particular movement for

42
00:02:19,860 --> 00:02:23,530
a given gesture, and HMMs were exactly
the kind of thing to use for that.

43
00:02:24,570 --> 00:02:27,090
In fact, discriminative methods
might have been harder to use for

44
00:02:27,090 --> 00:02:29,360
that because they need to
combine these probabilities.

45
00:02:29,360 --> 00:02:32,440
So here was something very
simple that they did.

46
00:02:32,440 --> 00:02:34,430
They took the,
I shouldn't say very soon.

47
00:02:34,430 --> 00:02:37,040
Here's what they did,
it required some engineering.

48
00:02:37,040 --> 00:02:41,050
You take your raw data, which was
a trajectory that you would move in xyz,

49
00:02:41,050 --> 00:02:43,870
and the first thing they did was,
they said, well you know,

50
00:02:43,870 --> 00:02:46,910
your gesture is pretty much planar.

51
00:02:46,910 --> 00:02:47,660
Right?

52
00:02:47,660 --> 00:02:50,180
The plane could be this way,
could be this way,

53
00:02:50,180 --> 00:02:54,080
but the idea is that a single gesture
didn't move in more than a plane.

54
00:02:54,080 --> 00:02:55,100
I might even go like this, but

55
00:02:55,100 --> 00:02:58,000
you'll notice even this
figure 8 is in this plane.

56
00:02:59,000 --> 00:02:59,940
All right?

57
00:02:59,940 --> 00:03:05,550
So they find the plane that the gesture
fit best to, projected down.

58
00:03:05,550 --> 00:03:07,690
And then they did what's
called a chain code.

59
00:03:07,690 --> 00:03:11,430
We didn't actually talk about chain
codes, if you did some machine vision,

60
00:03:11,430 --> 00:03:12,500
you would learn about chain code.

61
00:03:12,500 --> 00:03:15,465
It was a way of encoding
a contour as being you know,

62
00:03:15,465 --> 00:03:17,155
maybe you can only take
one of eight steps.

63
00:03:17,155 --> 00:03:18,405
Right?
So you go up,

64
00:03:18,405 --> 00:03:21,675
you go you take a direction north.

65
00:03:21,675 --> 00:03:22,855
You go northeast.

66
00:03:22,855 --> 00:03:26,685
You go east, east, east.,
south, southeast, southwest and

67
00:03:26,685 --> 00:03:27,965
then eventually you come around.

68
00:03:27,965 --> 00:03:36,230
And you could think of the entire
contour as a sequence of discrete steps.

69
00:03:36,230 --> 00:03:36,820
Right.

70
00:03:36,820 --> 00:03:38,790
So north, east, south, right?

71
00:03:38,790 --> 00:03:42,230
So sequence of discrete symbols, ah-hah.

72
00:03:42,230 --> 00:03:43,410
HMMs.

73
00:03:43,410 --> 00:03:49,040
So they trained a very simple
left-to-right HMM, let-to-right

74
00:03:49,040 --> 00:03:53,750
meaning that you go through every state
and you, you don't skip states you have

75
00:03:53,750 --> 00:03:57,130
these loopback things because you may
stay in a state for a while where,

76
00:03:57,130 --> 00:04:01,950
essentially you are going kind of, lets
suppose you are going north, northeast.

77
00:04:01,950 --> 00:04:03,230
But you didn't have that as a direction.

78
00:04:03,230 --> 00:04:06,560
Well, north northeast, that state
you might sometimes put out north.

79
00:04:06,560 --> 00:04:08,540
Sometimes put out northeast,
etc, whatever.

80
00:04:08,540 --> 00:04:11,020
But you'd stay in that state for awhile.

81
00:04:11,020 --> 00:04:11,840
It would output,

82
00:04:11,840 --> 00:04:15,180
there would be two symbols it could
output with high probability.

83
00:04:15,180 --> 00:04:18,740
But that state for
example would never output south.

84
00:04:18,740 --> 00:04:21,519
Okay, so that would be,
you'd stay in the state for a while.

85
00:04:21,519 --> 00:04:26,540
So they trained up these HMMs and
they did one HMM for

86
00:04:26,540 --> 00:04:29,580
sort of each gesture, okay?

87
00:04:29,580 --> 00:04:33,820
They also did some stuff that had
to do with what they call junctures

88
00:04:33,820 --> 00:04:37,200
doing transitions between gestures so
that it, it could parse them.

89
00:04:37,200 --> 00:04:39,080
I don't think I'll,
I'll talk about that here.

90
00:04:39,080 --> 00:04:42,720
I just will say that people hook
up multiple HMM's together,

91
00:04:42,720 --> 00:04:47,130
to form sort of an HMM that can deal
with a sequence of gestures, and it's,

92
00:04:47,130 --> 00:04:50,950
it's not too much more complicated
than the original HMM.

93
00:04:50,950 --> 00:04:54,080
So they do that and then they test it.

94
00:04:54,080 --> 00:04:59,050
And because they're testing it on
a very controlled vocabulary, using

95
00:04:59,050 --> 00:05:04,790
features defined to work, because they
want their paper to be accepted, okay?

96
00:05:04,790 --> 00:05:08,370
You see results that look like this,
okay?

97
00:05:08,370 --> 00:05:09,080
So here are their hits.

98
00:05:09,080 --> 00:05:14,490
This is the percentage of, hits
are the percentage of the actual targets

99
00:05:14,490 --> 00:05:19,210
that were correctly labelled, misses are
when you don't label a thing correctly.

100
00:05:19,210 --> 00:05:20,360
And as you can see,

101
00:05:20,360 --> 00:05:25,290
they're doing tests of over a 100 of
each of these examples and almost

102
00:05:25,290 --> 00:05:30,760
all of their gestures are recognized
with a relatively high accuracy.

103
00:05:30,760 --> 00:05:35,130
You can see there are one or
two that was that were not so high and

104
00:05:35,130 --> 00:05:37,410
what and
showing a table like this is one thing,

105
00:05:37,410 --> 00:05:41,020
a slightly more interesting table
might be a confusion matrix.

106
00:05:41,020 --> 00:05:46,250
This is like we showed for the SVMs for
the bag of words where you go back and

107
00:05:46,250 --> 00:05:48,100
take a look at the slide
you'll see a table.

108
00:05:48,100 --> 00:05:50,706
Then it says how often is
an airplane confused as a face, or

109
00:05:50,706 --> 00:05:51,790
that kind of thing.

110
00:05:51,790 --> 00:05:54,930
Normally if you're going to report
results of that doing classification.

111
00:05:54,930 --> 00:05:58,535
Don't just provide hits and misses,
also provide a confusion matrix

112
00:05:58,535 --> 00:06:02,110
because sometimes the actual
source of the performance

113
00:06:02,110 --> 00:06:05,700
is that some of the categories were
much more confusable than the others.

114
00:06:05,700 --> 00:06:10,950
You have no way of knowing that if
you just report sort of performance

115
00:06:10,950 --> 00:06:15,230
rate overall, but you need to be
able to see the the, the confusion.

116
00:06:15,230 --> 00:06:17,490
What, which, which element, or

117
00:06:17,490 --> 00:06:19,530
which categories were confused
with which categories.
