1
00:00:00,100 --> 00:00:02,100
So in this section we're going to
take everything we've learned and

2
00:00:02,100 --> 00:00:04,720
we're going to build our first neural
network to train over the datasets that

3
00:00:04,719 --> 00:00:06,129
we just created.

4
00:00:06,129 --> 00:00:07,689
Now what I'd like for you to do for

5
00:00:07,689 --> 00:00:10,887
this project is to start with your
neural net form the last chapter.

6
00:00:10,887 --> 00:00:14,568
I guess the last module that you did
where you built a basic neural network

7
00:00:14,567 --> 00:00:16,929
for predicting on
a structured data dataset.

8
00:00:16,929 --> 00:00:19,367
Then I would like to take this
three layer neural network and

9
00:00:19,367 --> 00:00:21,375
remove the non-linearity
in the hidden layer.

10
00:00:21,375 --> 00:00:22,870
I'll show you why later.

11
00:00:22,870 --> 00:00:26,240
Then I would like for you to use
the functions that we created above

12
00:00:26,239 --> 00:00:29,199
to generate the trained data on the fly.

13
00:00:29,199 --> 00:00:32,390
So a review and a label goes in.

14
00:00:32,390 --> 00:00:36,939
It's converted into the two vectors that
we need for the input and output data.

15
00:00:36,939 --> 00:00:39,885
And then a forward pass and
a back prop pass happen, so

16
00:00:39,886 --> 00:00:42,185
that the data is being
trained on the fly.

17
00:00:42,185 --> 00:00:44,362
Next thing I would like for
you to do is create a function for

18
00:00:44,362 --> 00:00:45,570
pre-processing the data.

19
00:00:45,570 --> 00:00:49,039
So that all of these kind of
vocabulary variables, and

20
00:00:49,039 --> 00:00:53,140
word2index variables
are variables of this class, so

21
00:00:53,140 --> 00:00:55,579
everything is self-contained
in that class.

22
00:00:55,579 --> 00:00:59,530
And then modify the train variable to
actually train over the entire corpus,

23
00:00:59,530 --> 00:01:03,520
instead of just on one inputs and
targets list.

24
00:01:03,520 --> 00:01:06,147
So, that's kind of what I would like for
you to do.

25
00:01:06,147 --> 00:01:07,866
You can start with
either with this shell,

26
00:01:07,867 --> 00:01:10,543
that was presented at the beginning
of your last week's chapter or

27
00:01:10,543 --> 00:01:13,050
with the complete neural net
that you started with last time.

28
00:01:14,459 --> 00:01:17,913
Now if you do need help, obviously
the first thing to do is to go re-watch

29
00:01:17,914 --> 00:01:21,430
the previous week's Udacity lectures,
make you're familiar with that

30
00:01:21,430 --> 00:01:24,435
propagation and it's gradient ascent,
and the error measure

31
00:01:24,435 --> 00:01:28,280
that we're using and also how to modify
back prop to get rid of non-linearity.

32
00:01:28,280 --> 00:01:30,370
And if you still need more help,
go ahead and

33
00:01:30,370 --> 00:01:32,939
check out chapters three through
five of Grokking Deep Learning.

34
00:01:32,939 --> 00:01:35,689
I've included the 40% off code here.

35
00:01:35,689 --> 00:01:39,801
It does a comprehensive review of fore
prop, back prop and error gradients and

36
00:01:39,801 --> 00:01:41,495
stochastic gradient descent.

37
00:01:41,495 --> 00:01:43,903
In a moment I'll show you how I
put this network together, and

38
00:01:43,903 --> 00:01:47,210
then we'll kind of talk about
the different changes that we made.

39
00:01:47,209 --> 00:01:48,539
All right, I'll see you then.

