1
00:00:00,000 --> 00:00:03,000
An answer is that every one of these

2
00:00:03,000 --> 00:00:06,000
is a potential problem for passive reinforcement learning.

3
00:00:06,000 --> 00:00:09,000
So every problem won't show up in every possible domain.

4
00:00:09,000 --> 00:00:12,000
It'll depend on what the environment looks like.

5
00:00:12,000 --> 00:00:16,000
But it is a possibility that you could get bitten by any of these problems.

6
00:00:16,000 --> 00:00:19,000
And they all stem from the same cause,

7
00:00:19,000 --> 00:00:21,000
from the fact that passive learning

8
00:00:21,000 --> 00:00:25,000
stubbornly sticks to the same policy throughout.

9
00:00:25,000 --> 00:00:27,000
We have a policy, pi of S,

10
00:00:27,000 --> 00:00:29,000
and we always execute that policy.

11
00:00:29,000 --> 00:00:34,000
So if the policy here was to go up and then go right,

12
00:00:34,000 --> 00:00:36,000
then we would always stick to that;

13
00:00:36,000 --> 00:00:41,000
and the only time we would explore any other state is when those actions failed.

14
00:00:41,000 --> 00:00:43,000
If we tried to go up from this state--

15
00:00:43,000 --> 00:00:45,000
because that's what the policy said;

16
00:00:45,000 --> 00:00:48,000
but, stochastically, we slipped over to this state--

17
00:00:48,000 --> 00:00:51,000
then we wouldn't do something else, according to the policy

18
00:00:51,000 --> 00:00:53,000
and so we'd get a little bit of exploration,

19
00:00:53,000 --> 00:00:56,000
but we'd only vary from the chosen path

20
00:00:56,000 --> 00:00:58,000
because of that variation

21
00:00:58,000 --> 99:59:59,999
and we wouldn't intentionally explore enough of the space.
