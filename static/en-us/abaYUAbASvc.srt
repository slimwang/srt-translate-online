1
00:00:00,021 --> 00:00:04,553
Let's step back for a bit to remind us why we're focusing on memory the way we are.

2
00:00:04,553 --> 00:00:07,409
Our overarching goal, of course, is just to make code fast.

3
00:00:07,409 --> 00:00:12,953
So we say great, GPUs are fast, let's use those. But why are they fast?

4
00:00:12,953 --> 00:00:18,482
Gpus are fast, first, because they are massively parallel, with hundreds or thousands of

5
00:00:18,482 --> 00:00:23,054
processors on a single chip, working for you to solve your problem, but also because they have

6
00:00:23,054 --> 00:00:28,253
an extremely high bandwidth memory system to feed those massively parallel processors, okay?

7
00:00:28,253 --> 00:00:33,819
So if the memory system can't deliver data to all of these processors

8
00:00:33,819 --> 00:00:37,788
and store results from all those processors, then we're not going to get the full speed out of our GPU.

9
00:00:37,788 --> 00:00:40,935
And that's why, on a memory limited kernel like transpose,

10
00:00:40,935 --> 00:00:45,612
our subgoal is really to utilize all the available memory bandwidth.

11
00:00:45,612 --> 00:00:51,920
Hence our focus on global memory coalescing, DRAM utilization, and so on.

12
00:00:51,920 --> 00:00:54,692
Now I really want to ask a question a little bit more rigorously.

13
00:00:54,692 --> 00:00:58,172
What do we mean by utilizing all the available memory bandwidth?

14
00:00:58,172 --> 00:01:02,511
And this is going to bring us to a very important, very simple principle called Little's Law.

15
00:01:02,511 --> 00:01:05,268
Let's have the talented Kim Dilla illustrate this for us.

16
00:01:05,268 --> 00:01:08,997
Now John Little is a MIT professor who studies Marketing.

17
00:01:08,997 --> 00:01:13,642
He formulated his Eponymous Law, when writing about queuing theory in business processes.

18
00:01:13,642 --> 00:01:17,729
And Little's Law is usually used to reason about things like optimizing the number

19
00:01:17,729 --> 00:01:21,749
of customers in a line at Starbucks, or maybe the size of queues in a factory.

20
00:01:21,749 --> 00:01:24,574
But Little's Law is really very general and can be applied

21
00:01:24,574 --> 00:01:27,247
to many things including memory systems in computers.

22
00:01:27,247 --> 00:01:30,974
In that context, Little's Law states that the number of bytes delivered

23
00:01:30,974 --> 00:01:36,062
equals the average latency of each memory transaction times the bandwidth.

24
00:01:36,062 --> 00:01:38,829
Let's be a little more precise and emphasize

25
00:01:38,829 --> 00:01:42,769
that we care about the useful bytes delivered, and the problem with uncoalesced

26
00:01:42,769 --> 00:01:47,619
global memory accesses is that not all of the bytes in every memory transaction are actually being used.

27
00:01:47,619 --> 00:01:50,188
That's why coalescing global memory accesses helps

28
00:01:50,188 --> 00:01:53,844
ensure that every byte delivered in a memory transaction will be used.

29
00:01:53,844 --> 00:01:59,604
So given this definition, what can we do to improve our bandwidth? Let's check all that apply.

30
00:01:59,604 --> 00:02:05,678
We can increase the number of bytes delivered, we can increase the latency--

31
00:02:05,678 --> 00:02:10,068
meaning the time between memory transactions--we can decrease the

32
00:02:10,068 --> 00:02:15,747
number of bytes delivered, or we can decrease the latency or time between transactions.
