1
00:00:00,330 --> 00:00:02,220
So the next from that we
need to worry about is,

2
00:00:02,220 --> 00:00:04,140
the problem of state estimation.

3
00:00:04,140 --> 00:00:07,450
And as we were just talking
about a couple moments ago.

4
00:00:07,450 --> 00:00:12,250
We can make a POM D.P. into a M.D.P.,
by expanding out the state space.

5
00:00:12,250 --> 00:00:16,540
What we're going to do is,
consider what we call belief states.

6
00:00:16,540 --> 00:00:19,330
So, the state that the decision
maker is going to use in

7
00:00:19,330 --> 00:00:22,090
the context of a POM DP is,
going to be a belief state b.

8
00:00:22,090 --> 00:00:27,170
And what a belief state b gives you is,
that if you tell me a state like s.

9
00:00:27,170 --> 00:00:30,610
Belief state b(s) tells us the
probability that we're actually in state

10
00:00:30,610 --> 00:00:32,580
s at the current moment in time.

11
00:00:32,580 --> 00:00:35,470
>> So b is a sort of like a state,
well it's a belief state.

12
00:00:36,480 --> 00:00:38,280
So it's like what we did
in the last example.

13
00:00:38,280 --> 00:00:39,430
It's kind of a vector.

14
00:00:39,430 --> 00:00:42,690
It's a distribution over states.

15
00:00:42,690 --> 00:00:43,530
>> Yes, exactly.

16
00:00:43,530 --> 00:00:45,780
It's a probability
distribution over states.

17
00:00:45,780 --> 00:00:48,368
Yes, and so and that's going to
that's going to encode for us.

18
00:00:48,368 --> 00:00:51,830
It's going to retain for us
the information about the history that

19
00:00:51,830 --> 00:00:53,670
might be important for decision making.

20
00:00:53,670 --> 00:00:55,130
>> Okay, so gets us all
the way back to the beginning.

21
00:00:55,130 --> 00:01:00,060
Okay, so b(s) then, so b is just
a vector and it's indexed by s and s,

22
00:01:00,060 --> 00:01:02,320
instead of being an integer
is just a state.

23
00:01:02,320 --> 00:01:03,240
>> Right.
>> Okay, that works.

24
00:01:03,240 --> 00:01:04,129
It's a probability distribution.

25
00:01:04,129 --> 00:01:04,910
All right, that makes sense.

26
00:01:04,910 --> 00:01:07,420
Okay.
>> So now what we need to do is,

27
00:01:07,420 --> 00:01:12,380
figure out how this belief state gets
updated as we take actions in the world.

28
00:01:12,380 --> 00:01:13,550
>> Okay.
>> So the way we're going to think about

29
00:01:13,550 --> 00:01:16,910
that is, we're in some belief state,
we choose an action.

30
00:01:16,910 --> 00:01:19,680
The world gives us back
an observation and we need now,

31
00:01:19,680 --> 00:01:22,180
a new belief state to
be formed from that.

32
00:01:22,180 --> 00:01:26,694
>> Okay, so that's exactly parallel to
the, we're in a state we took an action,

33
00:01:26,694 --> 00:01:28,280
then we got into another state.

34
00:01:28,280 --> 00:01:30,910
>> Right, so
what we really are doing here is,

35
00:01:30,910 --> 00:01:32,929
we're building up
a notion of a belief MDP.

36
00:01:32,929 --> 00:01:37,820
So, we're going to turn
the POMDP into a kind of MDP,

37
00:01:37,820 --> 00:01:39,140
specifically the belief MDP.

38
00:01:39,140 --> 00:01:43,070
Where the states of the belief MDP are
probability distributions over states of

39
00:01:43,070 --> 00:01:45,630
the underlying MDP in the POMDP.

40
00:01:45,630 --> 00:01:46,650
>> Okay, sounds good.

41
00:01:46,650 --> 00:01:51,110
So, an aside Michael,
is if reward is a function of state.

42
00:01:51,110 --> 00:01:53,820
Which it sometimes is, so

43
00:01:53,820 --> 00:01:57,590
you should be able to use that
to update your belief state.

44
00:01:57,590 --> 00:01:58,170
>> Right, so

45
00:01:58,170 --> 00:02:02,050
in what we're going to be talking about,
the reward is essentially not observed.

46
00:02:03,640 --> 00:02:08,139
But of course in reality, if you have
a learner that's in some environment and

47
00:02:08,139 --> 00:02:10,110
it's making observations.

48
00:02:10,110 --> 00:02:12,150
And getting some reward back,
it can actually,

49
00:02:12,150 --> 00:02:14,890
and if it knows something about how
that reward is being generated.

50
00:02:14,890 --> 00:02:22,350
It can use that as a kind of observation
and so, we can essentially assume that

51
00:02:22,350 --> 00:02:27,250
the observation has whatever information
might be in the reward in it.

52
00:02:27,250 --> 00:02:28,050
>> Right, so in fact,

53
00:02:28,050 --> 00:02:32,210
the observation is whatever sub teachers
you get to see plus the reward.

54
00:02:32,210 --> 00:02:34,030
>> Yeah,
I've seen some people write it this way,

55
00:02:34,030 --> 00:02:37,710
where you say that the reward is
actually something about the observable.

56
00:02:37,710 --> 00:02:42,380
You know, all the reward you get,
is extracted from what you observe and

57
00:02:42,380 --> 00:02:44,750
so, anything that you
model in the POMDP.

58
00:02:44,750 --> 00:02:47,910
As being reward relevant, has to
actually show up in the observation.

59
00:02:47,910 --> 00:02:49,160
>> Right, that seems reasonable.

60
00:02:49,160 --> 00:02:51,640
>> Right, otherwise you can't
learn if you can't observe it.

61
00:02:51,640 --> 00:02:53,410
>> Right, that makes perfect sense.

62
00:02:53,410 --> 00:02:57,410
>> All right, so we need to do is,
derive how we get this probability

63
00:02:57,410 --> 00:03:02,140
distribution over states b prime
from the old belief state.

64
00:03:02,140 --> 00:03:03,680
The action, the observation, and

65
00:03:03,680 --> 00:03:06,122
whatever quantities in
the POMDP that we have.

66
00:03:06,122 --> 00:03:09,870
So, what we need to figure out is,
in this new belief state,

67
00:03:09,870 --> 00:03:11,910
after we've taken action a.

68
00:03:11,910 --> 00:03:16,520
And seen observation z, what is the
probability that we're in state S prime?

69
00:03:16,520 --> 00:03:18,890
And so, we're going to have to
do some kind of probabilistic,

70
00:03:19,920 --> 00:03:22,950
I guess inference or
derivation to work that out.

71
00:03:22,950 --> 00:03:26,270
>> Sure,
we ought to be able to do that right,

72
00:03:26,270 --> 00:03:30,140
because we have all the quantities
that we need in the POMDP, I think.

73
00:03:30,140 --> 00:03:33,470
>> Well that's the hope, so we want
the probability of the next state,

74
00:03:33,470 --> 00:03:34,860
given that we're in a belief state.

75
00:03:34,860 --> 00:03:38,140
Took an action and made an observation.

76
00:03:38,140 --> 00:03:42,590
So, can we manipulate this expression
using the laws of probability

77
00:03:42,590 --> 00:03:43,880
to make sense out of it?

78
00:03:43,880 --> 00:03:45,210
Sure.
The good answer.
