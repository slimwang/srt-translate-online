1
00:00:00,000 --> 00:00:02,832
(Sebastian) --to watch all the videos. (Peter) Right, so we'll try and do that better.

2
00:00:02,833 --> 00:00:05,966
So let's see. Here's one you touched on a little bit.

3
00:00:05,967 --> 00:00:10,766
"Many researchers believe we need a special, rather different kind of hardware to run human-like A.I.,

4
00:00:10,767 --> 00:00:17,266
"and that von-Neumann architecture computers just won't cut it. So which advancement in hardware do you think will benefit A.I. the most?"

5
00:00:17,267 --> 00:00:22,666
(Sebastian) That's a question out of New Delhi, India. That's a subject to a lot of ongoing debate,

6
00:00:22,667 --> 00:00:27,399
and depending who you ask, you'll get very different answers. There's the diehards who believe the only way to get A.I.

7
00:00:27,400 --> 00:00:32,598
is by emulating the human brain, or even more so, building a new computer structure that is analog

8
00:00:32,600 --> 00:00:36,466
and has similar connectivity to the human brain. It turns out it's hard to build.

9
00:00:36,467 --> 00:00:42,799
So building a device out of 10 to the 11 or so neurons and to the 14th interconnections is really difficult.

10
00:00:42,800 --> 00:00:46,832
I personally believe the power will come from something else, but of course I don't know.

11
00:00:46,833 --> 00:00:52,566
And future years of research will show us. I believe it will come from good and fast and interconnected processing

12
00:00:52,567 --> 00:00:58,732
the way we have it today. And by using massive amounts of data.

13
00:00:58,733 --> 00:01:03,499
People are able to sort through massive amounts of data. I have a small child, he's three years old;

14
00:01:03,500 --> 00:01:08,966
For most of his life, I think he sorted through data in ways that are not understandable to me,

15
00:01:08,967 --> 00:01:11,999
and if you have a child yourself, you know what I'm talking about.

16
00:01:12,000 --> 00:01:17,466
I think that's the way to go: Use existing computers and be really smart how to process massive amounts of data.

17
00:01:17,467 --> 00:01:23,266
That's my guess, and it's just a guess. (Peter) And I would agree with that. I think we've seen incredible progress

18
00:01:23,267 --> 00:01:30,199
in what computer hardware can do. And the hardware is very different from the brain, but I think we should focus on

19
00:01:30,200 --> 00:01:37,099
what the computers can do with us. We already know something about how brains work, and they can do their own thing.

20
00:01:37,100 --> 00:01:44,366
So we should figure out how to make the hardware we have work as best as it can, and then how to put people and computers together

21
00:01:44,367 --> 00:01:48,732
as a partnership. (Sebastian) Here's a question by Philip Scoco from Singapore:

22
00:01:48,733 --> 00:01:55,099
"Thanks for the fun and interesting course. A question: Many great A.I. methods, such as SVM and SVD,

23
00:01:55,100 --> 00:02:00,566
"are expensive and need a long time to train, especially in rich and large data sets.

24
00:02:00,567 --> 00:02:05,832
In a real world setting, how do people handle these computational issues?

25
00:02:05,833 --> 00:02:12,732
(Peter) That's a great question, and that's certainly true. First I want to say that a lot of these methods are not all that

26
00:02:12,733 --> 00:02:17,699
computationally challenging, and that there are many very interesting things you can do in--

27
00:02:17,700 --> 00:02:21,466
We mentioned image understanding and language understanding and so on.

28
00:02:21,467 --> 00:02:28,466
You can do a lot running just on your laptop. So your laptop of today was the supercomputer of a decade ago,

29
00:02:28,467 --> 00:02:34,666
and people got work done then, and they get work done now. So don't feel like you're locked out of the game

30
00:02:34,667 --> 00:02:38,366
because you don't have a whole data center at your disposal.

31
00:02:38,367 --> 00:02:40,266
So that's part of it, and the other question is--

32
00:02:40,267 --> 00:02:42,332
(Sebastian) Said the man with a data center at his disposal.

33
00:02:42,333 --> 00:02:47,966
(Peter) Right. If you do happen to have one, what can you do with it? And what types of approaches do you take

34
00:02:47,967 --> 00:02:56,866
for dealing with huge amounts of data? I guess one thing that is increasingly important is taking these existing algorithms,

35
00:02:56,867 --> 00:03:02,766
and most of the algorithms we've described in class for machine learning are batch algorithms,

36
00:03:02,767 --> 00:03:09,132
where you have a fixed set of data, you do some processing on that, and then you're able to answer queries about it.

37
00:03:09,133 --> 00:03:17,732
But you find with the online services today, you're much more likely to have to translate them into an online setting,

38
00:03:17,733 --> 00:03:22,199
where the data is not fixed, where every day and every second there's new data coming in,

39
00:03:22,200 --> 00:03:28,866
and that's updating what we know and updating the models, and then queries are intermixed with the new data,

40
00:03:28,867 --> 00:03:37,632
and we have to make that all work out. So we change the models so that they're online, and then we aren't so worried

41
00:03:37,633 --> 00:03:42,166
as much anymore about the complexity of the models.

42
00:03:42,167 --> 00:03:52,632
So, many of the hardest problems we've talked about are, "What are N squared or N cubed?" or some of them are nP complete.

43
00:03:52,633 --> 00:03:58,799
But that doesn't matter, because if you have to get to an online setting, it's got to be basically order one.

44
00:03:58,800 --> 00:04:03,632
You've got to be able to respond immediately. And so you make whatever approximations you have to make

45
00:04:03,633 --> 00:04:07,366
to get to that order one model, and then continue to update it as you go.

46
00:04:07,367 --> 00:04:10,799
(Sebastian) And I want to add to this, this is one of my frustrations with our scientific field.

47
00:04:10,800 --> 00:04:17,232
Our scientific field in the past has looked into lots of small data sets, and very few have looked at very large data sets.

48
00:04:17,233 --> 00:04:22,832
And as a result, the algorithms are mathematically beautiful and they're really elegant, but they don't scale.

49
00:04:22,833 --> 00:04:27,899
And sometimes you put mathematical beauty and rigor ahead of scaling,

50
00:04:27,900 --> 00:04:33,866
and it's much easier to publish a paper that has beautiful math in it than one that just uses large amounts of data.

51
00:04:33,867 --> 00:04:39,666
I think on the practical side, large data is becoming more and more important, and companies like Google live on large data,

52
00:04:39,667 --> 00:04:45,732
and Amazon and many others. And we lack the tools, we lack the rigorous research that goes and builds

53
00:04:45,733 --> 00:04:50,066
really fast and scalable methods that have good guarantees and good behavior.

54
00:04:50,067 --> 00:04:55,066
I think there's a huge opportunity for anyone in machine learning to step in and take this seriously.

55
00:04:55,067 --> 00:05:01,199
I can promise you, if you make a big impact and get something as good as, say, an SVM working in constant time

56
00:05:01,200 --> 00:05:07,232
per data item, then you're going to have a really really big impact on the field.

57
00:05:07,233 --> 00:05:13,466
(Peter) Okay. So speaking of impact, next question from Sydney, Australia, "What would you recommend as the next steps

58
00:05:13,467 --> 00:05:16,499
"after the course finishes for those of us who would like to progress further?

59
00:05:16,500 --> 00:05:21,332
"What steps should be taken to utilize this in the commercial domain?" It's from Greg.

60
00:05:21,333 --> 00:05:29,032
(Sebastian) Well, the very thing I would do is, there'll be more classes that we hope to post here, so please stay tuned.

61
00:05:29,033 --> 00:05:36,399
I think take any problem in your life right now, no matter how small, that annoys you, and fix it using A.I. technology,

62
00:05:36,400 --> 00:05:41,366
is a great way going forward. There's certainly many in my life that annoy me.

63
00:05:41,367 --> 00:05:46,799
And then if you really like what we're doing, Stanford is a great university. There's many fantastic universities.

64
00:05:46,800 --> 00:05:52,899
Enroll. Come and learn with us, and learn more with us. It's a lot of fun. And you'll be much more powerful if you learn

65
00:05:52,900 --> 00:05:56,699
all the skills that we're not even telling you today in this class.

66
00:05:56,700 --> 00:06:03,632
(Peter) So I think there is a lot more to learn, and you can learn that by going to school, you can learn that by reading on your own.

67
00:06:03,633 --> 00:06:09,199
And I think most important is by practice. So go out and find a problem and pitch in and do it.

68
00:06:09,200 --> 00:06:19,632
So as Sebastian said, something that's bothering you or go out and find an open source project and volunteer and start working.

69
00:06:19,633 --> 00:06:21,632
(Sebastian) Maybe the last question, Peter? (Peter) Okay. (Sebastian) Okay.

70
00:06:21,633 --> 00:06:26,799
"Apart from trial and error, are there any ways to find out which technique fits a problem?

71
00:06:26,800 --> 00:06:38,166
"For example, should we do dimensionality reduction or clustering or SVM?" This is by Vikovel from somewhere in this world.

72
00:06:38,167 --> 00:06:45,132
(Peter) I guess maybe trial and error is the best way to describe it, or another way to describe it is experience.

73
00:06:45,133 --> 00:06:53,332
Of saying, "What problems have I seen that are like this before?" And then experience comes in two forms:

74
00:06:53,333 --> 00:07:00,366
One is personal experience. "What have I seen and worked for me?" Another is experience of the literature:

75
00:07:00,367 --> 00:07:03,166
What have other people tried and what worked for them?

76
00:07:03,167 --> 00:07:09,266
(Sebastian) Yeah. When I work with students on things, the number one rule I give them is, "Before you pick a method,

77
00:07:09,267 --> 00:07:14,666
"look at the data. Just look at the data. Spend a week or so staring at the data, because you're going to discover things

78
00:07:14,667 --> 00:07:20,932
"that are very insightful that you will not see if you just toss a random, say, machine learning method at it."

79
00:07:20,933 --> 00:07:28,832
And that's really important. If you do this, you set yourself apart from I'd say 80% of the students I've worked with in my life.

80
00:07:28,833 --> 00:07:33,099
Which tend to just say, "Oh, here's a method called SVM; I believe it's the right one," or boosting, or what have you.

81
00:07:33,100 --> 00:07:39,899
"Let me just toss any data at it." Then as you do this, when you use a learning algorithm, I think it's really important

82
00:07:39,900 --> 00:07:44,066
to reflect on it, and have your expectations and see if your expectations are met.

83
00:07:44,067 --> 00:07:47,799
If they're met, that's great, and you're a really smart person, you understand what you're doing.

84
00:07:47,800 --> 00:07:51,932
If they're not met, you have an opportunity to learn more, and maybe you'll find it back in the code,

85
00:07:51,933 --> 00:07:55,299
or maybe there's some insight that you just missed so far.

86
00:07:55,300 --> 00:08:01,332
I'm pretty much entirely self-taught. I never took a class in A.I. or in robotics of any meaning.

87
00:08:01,333 --> 00:08:06,932
And most of what I've done is just look very carefully at data and methods and just learned from that process.

88
00:08:06,933 --> 00:08:12,099
It's been very gratifying to me. (Peter) Okay, great. Seems like this is a good place to stop,

89
00:08:12,100 --> 00:08:17,232
and let me thank everybody again for your hard work in the class and for your great and challenging questions.

90
00:08:17,233 --> 00:08:20,432
(Sebastian) Do you think we can promise one of these office hours next week?

91
00:08:20,433 --> 00:08:24,099
(Peter) Yea, we'll keep doing it. (Sebastian) Okay. We tried to do it online, and we had technical problems.

92
00:08:24,100 --> 00:08:29,499
So this is, of course, a re-recorded session. But maybe next week we'll do the same thing again. (Peter) Yeah.

93
00:08:29,500 --> 00:08:31,299
(Sebastian) It's a lot of fun. (Peter) We'll just do it this way--

94
00:08:31,300 --> 00:08:35,500
(Sebastian) Let us know via Twitter whether you like it or not, or G+. Bye.
