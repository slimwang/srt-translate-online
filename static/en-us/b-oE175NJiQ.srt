1
00:00:00,130 --> 00:00:03,610
So we learned a lot about outliers and outlier rejection.

2
00:00:03,610 --> 00:00:06,760
Rejection is what you're going to do to clean up your fit, but

3
00:00:06,760 --> 00:00:11,030
if you're into anomaly detection or under fraud detection,

4
00:00:11,030 --> 00:00:14,110
you reject the good data points, and you look at the outliers.

5
00:00:14,110 --> 00:00:17,140
Either way, a really great algorithm that can be looped on

6
00:00:17,140 --> 00:00:19,740
any machine algorithm is first train,

7
00:00:19,740 --> 00:00:24,780
then remove the points with the largest error, often called the residual error.

8
00:00:24,780 --> 00:00:27,410
And for the remaining points, you re-train.

9
00:00:27,410 --> 00:00:31,029
And if you so wish, you could repeat these removal and re-train procedure.

10
00:00:31,029 --> 00:00:33,480
But you normally converge to a much,

11
00:00:33,480 --> 00:00:38,120
much better solution than if you kept all the original data.

12
00:00:38,120 --> 00:00:42,980
And as I said before, the fraction of data you often remove is something in

13
00:00:42,980 --> 00:00:46,720
the order of 10%, but it might vary from application to application.

14
00:00:46,720 --> 00:00:48,980
So congratulations, you know about outlier rejection,

15
00:00:48,980 --> 00:00:53,160
a really important tool in the arsenal of machine learning algorithms.
