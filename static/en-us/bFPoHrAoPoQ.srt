1
00:00:00,230 --> 00:00:02,220
Alright, we're going to drill down and talk about a

2
00:00:02,220 --> 00:00:04,510
specific reinforcement learning algorithm in just a moment. But

3
00:00:04,510 --> 00:00:07,200
I wanted to remind everybody about, some of the

4
00:00:07,200 --> 00:00:10,090
quantities that Charles introduced in the lesson last time

5
00:00:10,090 --> 00:00:12,890
when he was talking about MVP's, and use them

6
00:00:12,890 --> 00:00:15,750
to describe three different approaches for solving reinforcement learning

7
00:00:15,750 --> 00:00:19,430
problems. So this first box here pi, maps states

8
00:00:19,430 --> 00:00:22,720
to actions, and what did you call this Charles?

9
00:00:22,720 --> 00:00:23,650
>> A policy.

10
00:00:23,650 --> 00:00:25,295
>> That's right. And reinforcement learning

11
00:00:25,295 --> 00:00:28,750
algorithms that work directly on trying to find the policy,

12
00:00:28,750 --> 00:00:32,119
are called policy search algorithms. So the good thing about policy

13
00:00:32,119 --> 00:00:36,420
search is, you're learning the quantity that you directly need

14
00:00:36,420 --> 00:00:38,530
to use. Right? You're learning the policy. That's supposed to be

15
00:00:38,530 --> 00:00:40,980
the output. So that seems like a really good thing.

16
00:00:40,980 --> 00:00:45,570
Unfortunately, learning this, this function is very indirect. We don't get

17
00:00:45,570 --> 00:00:48,570
direct access to, I was in this state, what actions should

18
00:00:48,570 --> 00:00:50,330
I have chosen? Right? So this is, what did you call

19
00:00:50,330 --> 00:00:54,140
this, Charles, the the temporal credit assignment problem, right?

20
00:00:54,140 --> 00:00:54,230
>> Right.

21
00:00:54,230 --> 00:00:57,340
>> So, the data doesn't tell you what action to actually choose.

22
00:00:57,340 --> 00:00:59,840
>> Right. And this is why it's not exactly

23
00:00:59,840 --> 00:01:02,560
like the way we've formulated supervised learning in the past.

24
00:01:02,560 --> 00:01:05,080
>> Well, at least if we're trying to do policy certs,

25
00:01:05,080 --> 00:01:08,450
that's right. But what we're going to do is now consider, well, maybe

26
00:01:08,450 --> 00:01:11,862
that's not the quantity that we want to learn. Let's, let's think about

27
00:01:11,862 --> 00:01:15,720
learning this function U, which you had said maps states to values.

28
00:01:15,720 --> 00:01:18,160
So, what was this guy?

29
00:01:18,160 --> 00:01:19,150
>> U is a utility.

30
00:01:19,150 --> 00:01:20,510
>> Yeah.

31
00:01:20,510 --> 00:01:23,080
>> Yeah the true utility of the state, sometimes I,

32
00:01:23,080 --> 00:01:25,010
I think I called it the, the value of the state.

33
00:01:25,010 --> 00:01:28,290
>> Yeah so, so, so sometimes it's referred to

34
00:01:28,290 --> 00:01:31,630
as a value function, and learning methods, reinforcement learning

35
00:01:31,630 --> 00:01:33,970
methods that target that as, as what they're trying

36
00:01:33,970 --> 00:01:36,630
to learn directly, are called value function based approaches.

37
00:01:36,630 --> 00:01:37,390
>> Mm-hm.

38
00:01:37,390 --> 00:01:40,860
>> And, let's say that we, that we try to learn this, we're trying to learn

39
00:01:40,860 --> 00:01:43,400
to map states to values, well the good news is, at

40
00:01:43,400 --> 00:01:47,260
least if we're acting in the world, we're getting to see, okay

41
00:01:47,260 --> 00:01:49,190
I was in some state, I took some action, duh, duh,

42
00:01:49,190 --> 00:01:51,890
duh, and I can, I can observe the values that actually result

43
00:01:51,890 --> 00:01:53,900
from that, and maybe use that to, to make updates. So

44
00:01:53,900 --> 00:01:57,680
you can kind of imagine learning this, so that the, the learning's

45
00:01:57,680 --> 00:02:00,450
not quite as indirect. How do we use this to decide how

46
00:02:00,450 --> 00:02:03,500
to behave? So we, we need to turn it into a policy.

47
00:02:03,500 --> 00:02:04,050
>> Mm hm.

48
00:02:04,050 --> 00:02:05,900
>> And we didn't quite, we sort of talked

49
00:02:05,900 --> 00:02:08,570
about how to do that, in terms of, of

50
00:02:08,570 --> 00:02:10,979
looking at the Bellman Equations. We're going to, we're going to, it

51
00:02:10,979 --> 00:02:12,600
turns out it's actually kind of hard to use

52
00:02:12,600 --> 00:02:16,130
U directly to do that. But we're going to talk about

53
00:02:16,130 --> 00:02:17,890
a different form of the value function that's going to

54
00:02:17,890 --> 00:02:20,000
make that easy. So this is actually, it turns out

55
00:02:20,000 --> 00:02:22,080
it'll be a relatively easy kind of arc max

56
00:02:22,080 --> 00:02:25,020
operation, once we have the right kind of value function.

57
00:02:25,020 --> 00:02:25,160
>> Okay.

58
00:02:25,160 --> 00:02:28,120
>> So, there's some computation we have to do, and

59
00:02:28,120 --> 00:02:31,240
there's some indirectness to the learning, but, you know, it's

60
00:02:31,240 --> 00:02:34,410
okay. Alright, so the other quantities that you

61
00:02:34,410 --> 00:02:36,880
told us about were T, which is, I

62
00:02:36,880 --> 00:02:39,920
think of it as the transition function, I think you had a different name for it.

63
00:02:39,920 --> 00:02:41,700
>> I just call it the transition model.

64
00:02:41,700 --> 00:02:44,210
>> The, the model, right, yeah. So this is the model,

65
00:02:44,210 --> 00:02:47,710
and and the, the R is the reinforcement function, the reward function.

66
00:02:47,710 --> 00:02:48,365
>> Mm-hm.

67
00:02:48,365 --> 00:02:50,730
>> And what do these things do, they take states and actions

68
00:02:50,730 --> 00:02:52,180
and the transition function, or the

69
00:02:52,180 --> 00:02:56,040
transition model, predicts next states, maybe probabilistically,

70
00:02:56,040 --> 00:03:01,370
and the reward function tells you, next rewards, but just rewards. And

71
00:03:01,370 --> 00:03:06,470
so this is a model, jointly, and I already mentioned this, but

72
00:03:06,470 --> 00:03:11,440
we call methods that learn

73
00:03:11,440 --> 00:03:13,880
this quantity, learn these, these functions and

74
00:03:13,880 --> 00:03:15,960
then use them to do decisions, model based

75
00:03:15,960 --> 00:03:20,260
reinforcement learners, so how do we go from T and R to something like U?

76
00:03:20,260 --> 00:03:21,070
>> Well

77
00:03:21,070 --> 00:03:25,640
if we had those, if we had T and we had R, and we could see the S's, and we,

78
00:03:25,640 --> 00:03:29,500
and the actions we took, then we could do you

79
00:03:29,500 --> 00:03:31,930
know, something like value iteration we did before the learned values.

80
00:03:31,930 --> 00:03:36,100
>> Right, which value iteration was used to solve the Bellman Equations.

81
00:03:36,100 --> 00:03:36,770
>> Right.

82
00:03:36,770 --> 00:03:42,460
>> So that is somewhat heavy weight computation to do, but it's doable.

83
00:03:42,460 --> 00:03:46,700
So, what would happens over here, is we actually have fairly direct learning.

84
00:03:46,700 --> 00:03:48,320
Why is that? Because when we're trying to learn the

85
00:03:48,320 --> 00:03:53,240
transition and reward function, we get state action pairs as input.

86
00:03:53,240 --> 00:03:56,060
And then when we receive next state reward pairs as

87
00:03:56,060 --> 00:03:58,955
output, so we can solve this as a supervised learning problem.

88
00:03:58,955 --> 00:03:59,955
>> Hm.

89
00:03:59,955 --> 00:04:03,350
>> So, so learning is rather direct, the usage of this

90
00:04:03,350 --> 00:04:06,080
is a little bit computationally complex because you actually have to do

91
00:04:06,080 --> 00:04:09,630
the planning and then the optimizing to actually develop what you're,

92
00:04:09,630 --> 00:04:12,150
you know the policy doesn't come directly out of that. But but

93
00:04:12,150 --> 00:04:13,590
this kind of gives you a sense of

94
00:04:13,590 --> 00:04:15,880
three different ways, three different ways, places that

95
00:04:15,880 --> 00:04:20,339
you can target the pieces of the MVP so that you can do reinforcement learning.

96
00:04:20,339 --> 00:04:21,519
>> I like that.

97
00:04:21,519 --> 00:04:25,320
>> Now, we're going to focus on this, this middle piece. Partly

98
00:04:25,320 --> 00:04:29,670
because, you know, it's kind of the Goldilocks situation. It's you know, the

99
00:04:29,670 --> 00:04:33,840
learning's not so indirect and the usage is not so indirect. There's

100
00:04:33,840 --> 00:04:37,220
just been a tremendous amount of work focused on, on value function

101
00:04:37,220 --> 00:04:41,700
based approaches. And it's, you know, remarkably simple it turns out,

102
00:04:41,700 --> 00:04:44,760
if you do it right. And also very powerful. That there's

103
00:04:44,760 --> 00:04:46,990
lots of ways to use these simple ideas to, to learn

104
00:04:46,990 --> 00:04:49,230
hard problems. So, I think this is a good place to focus.

105
00:04:49,230 --> 00:04:50,370
>> Okay. I buy that.
