1
00:00:00,360 --> 00:00:03,390
Charles I'm going to say that you
invented an algorithm just now

2
00:00:03,390 --> 00:00:05,270
that we're going to
call policy iteration.

3
00:00:05,270 --> 00:00:06,500
It's kind of like value iteration,

4
00:00:06,500 --> 00:00:08,170
except it's going to
iterate a policy space.

5
00:00:08,170 --> 00:00:09,450
>> I'm so smart
>> Yeah,

6
00:00:09,450 --> 00:00:11,540
though you invented it
a little bit to late,

7
00:00:11,540 --> 00:00:15,605
because it dates back to 1960 or
thereabout.

8
00:00:15,605 --> 00:00:16,195
>> Dijkstra?

9
00:00:16,195 --> 00:00:18,255
Dijkstra always has ideas before I do.

10
00:00:18,255 --> 00:00:19,335
>> Yes.
Dijkstra's pretty clever, but

11
00:00:19,335 --> 00:00:20,895
this is actually Ron Howard.

12
00:00:20,895 --> 00:00:21,795
>> You mean the Ron Howard?

13
00:00:21,795 --> 00:00:22,655
The director?

14
00:00:22,655 --> 00:00:23,175
>> Oh, sorry.

15
00:00:23,175 --> 00:00:24,795
No.
It's the Ron Howard.

16
00:00:24,795 --> 00:00:26,773
The algorithm designer.

17
00:00:26,773 --> 00:00:29,295
[LAUGH]
>> Oh, that one.

18
00:00:29,295 --> 00:00:30,505
I always get them confused.

19
00:00:30,505 --> 00:00:32,100
>> One played Opie.

20
00:00:32,100 --> 00:00:34,750
>> And the other one proved
contraction mappings.

21
00:00:34,750 --> 00:00:35,710
>> They're very similar things.

22
00:00:35,710 --> 00:00:37,690
>> So this is what I was
imagining you were explaining.

23
00:00:37,690 --> 00:00:39,080
Take it step by step.

24
00:00:39,080 --> 00:00:42,010
So what we're going to do is we're
going to start off picking an arbitrary

25
00:00:42,010 --> 00:00:43,810
Q function like we often do.

26
00:00:43,810 --> 00:00:46,170
We'll call that the initialization step.

27
00:00:46,170 --> 00:00:48,270
Then, we're going to
iterate the following.

28
00:00:48,270 --> 00:00:51,100
We're going to take the t-th Q function.

29
00:00:51,100 --> 00:00:53,260
And compute its greedy policy.

30
00:00:53,260 --> 00:00:56,200
Call that pi sub t, then, that policy,

31
00:00:56,200 --> 00:01:01,340
we're going to evaluate it to get
a new Q function, Q 2 plus 1, and

32
00:01:01,340 --> 00:01:04,519
then we're going to repeat, and iterate
this over and over and over again.

33
00:01:04,519 --> 00:01:08,383
So each time we go around this loop,
we're taking our previous Q function.

34
00:01:08,383 --> 00:01:11,550
>> Finding it's policy,
taking that policy,

35
00:01:11,550 --> 00:01:13,620
finding its value function and
repeating.

36
00:01:13,620 --> 00:01:14,730
>> Lather, rinse, repeat.

37
00:01:14,730 --> 00:01:16,050
>> Exactly.

38
00:01:16,050 --> 00:01:20,255
So unlike when I take a shower, we
actually get convergence in finite time.

39
00:01:20,255 --> 00:01:23,630
>> [LAUGH]
>> So

40
00:01:23,630 --> 00:01:28,540
in particular the sequence of Q
functions that we get converges to Q*.

41
00:01:28,540 --> 00:01:31,460
Which is good,
that's like how policy iteration works.

42
00:01:31,460 --> 00:01:35,360
But even better, convergence is
exact and complete in finite time.

43
00:01:35,360 --> 00:01:38,870
I guess that was kind of true
of value iteration as well.

44
00:01:38,870 --> 00:01:42,480
And it converges at least as
fast as value iterationm in that

45
00:01:42,480 --> 00:01:46,590
if at any point we sync up the Q
functions, we start value iteration and

46
00:01:46,590 --> 00:01:48,750
policy iteration from
the same Q function.

47
00:01:48,750 --> 00:01:52,970
Then each step that policy iteration
takes is moving us towards

48
00:01:52,970 --> 00:01:59,250
the optimal Q function, no more
slowly than valued iteration does.

49
00:01:59,250 --> 00:01:59,840
>> Okay.

50
00:01:59,840 --> 00:02:02,670
>> So that kind of suggests
that this is just way better.

51
00:02:02,670 --> 00:02:03,740
>> Yeah, isn't it way better?

52
00:02:03,740 --> 00:02:05,440
>> So, why is it not way better?

53
00:02:05,440 --> 00:02:09,508
There's kind of some excitement going
on in here that we need to take

54
00:02:09,508 --> 00:02:10,380
into consideration.

55
00:02:10,380 --> 00:02:12,356
There is a bit of trade-off
as you like to say.

56
00:02:12,356 --> 00:02:14,510
Mm-hm.
What's the trade-off?

57
00:02:14,510 --> 00:02:15,680
>> So where's the trade-off here?

58
00:02:15,680 --> 00:02:22,950
We're getting faster convergence at the
cost of greater computational expense.

59
00:02:22,950 --> 00:02:26,360
So in particular this step,
this policy evaluation step that says

60
00:02:26,360 --> 00:02:30,075
take the policy and then work out
the Q function for that policy.

61
00:02:30,075 --> 00:02:33,420
You can do that by say solving
a system of linear equations.

62
00:02:33,420 --> 00:02:36,080
Or perhaps more commonly,

63
00:02:36,080 --> 00:02:38,730
by writing something like
value iteration to completion.

64
00:02:40,270 --> 00:02:43,090
So in the inner loop of policy
iteration is something that's

65
00:02:43,090 --> 00:02:44,970
an awful lot like value iteration.

66
00:02:44,970 --> 00:02:46,040
And so maybe it's not so

67
00:02:46,040 --> 00:02:48,720
surprising that it goes at least
as fast as value iteration.

68
00:02:50,360 --> 00:02:52,600
It's doing a lot more work
than value iteration.

69
00:02:52,600 --> 00:02:55,390
Each iteration of policy iteration
is doing pretty much all the work

70
00:02:55,390 --> 00:02:55,960
of value iteration.

71
00:02:55,960 --> 00:02:58,450
Yeah, well, so, it just depends
upon what you're counting.

72
00:02:58,450 --> 00:03:00,050
I say we just count the outer loop.

73
00:03:00,050 --> 00:03:01,660
Then we win.

74
00:03:01,660 --> 00:03:03,250
Or at least we don't lose.

75
00:03:03,250 --> 00:03:06,070
In fact, this is kind of
an interesting outstanding question.

76
00:03:06,070 --> 00:03:09,710
So, we don't really know how many
iterations policy iteration takes, so

77
00:03:09,710 --> 00:03:12,430
an open question is what
the convergence time really is.

78
00:03:12,430 --> 00:03:15,640
We know a couple things about it,
but it turns out to be fairly weak.

79
00:03:15,640 --> 00:03:17,950
We know that there are some MDPs.

80
00:03:17,950 --> 00:03:22,470
Such that the number of iterations,
the policy iteration takes, is linear.

81
00:03:22,470 --> 00:03:27,330
It's at least as large as the number
of states in the MBP, though

82
00:03:27,330 --> 00:03:32,220
I don't think anybody's actually shown
like two times the number of states.

83
00:03:32,220 --> 00:03:35,100
So all we know is something really,
really basic which is that it

84
00:03:35,100 --> 00:03:38,690
takes at least the number of states
iterations, in theworst case.

85
00:03:38,690 --> 00:03:41,420
And we know it can't be
any worse than number of

86
00:03:41,420 --> 00:03:44,380
actions raised to the number
of states an exponential.

87
00:03:44,380 --> 00:03:47,450
But where is it's in between
we don't really know.

88
00:03:47,450 --> 00:03:50,950
And so if it's closer to linear,
then it's totally awesome and

89
00:03:50,950 --> 00:03:53,310
it blows the doors off value iteration.

90
00:03:53,310 --> 00:03:57,940
If it's more like exponential,
then It's probably still better than

91
00:03:57,940 --> 00:04:01,420
value iteration, but
it's definitely more of a wash.
