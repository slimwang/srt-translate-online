1
00:00:00,436 --> 00:00:01,905
Okay. Let's talk about a related problem.

2
00:00:01,905 --> 00:00:07,981
What happens when you have lots of threads both reading and writing the same memory locations?

3
00:00:07,981 --> 00:00:10,881
If lots of threads are all trying to read and write the same memory locations,

4
00:00:10,881 --> 00:00:12,284
then you're going to start to get conflicts.

5
00:00:12,284 --> 00:00:13,884
Let's look at an example.

6
00:00:13,884 --> 00:00:18,089
Suppose you had 10,000 threads that were all trying to increment 10 array elements.

7
00:00:18,089 --> 00:00:19,325
What's going to happen?

8
00:00:19,325 --> 00:00:20,726
Let's look at some code.

9
00:00:20,726 --> 00:00:23,928
This goes a little more complicated than we've seen in these examples before,

10
00:00:23,928 --> 00:00:25,131
so let me walk through it.

11
00:00:25,131 --> 00:00:27,869
What I'm going to do is I'm going to try writing with many threads

12
00:00:27,869 --> 00:00:30,700
to a small number of array elements.

13
00:00:30,700 --> 00:00:36,806
In this case, I'm going to write with 1,000,000 threads into 10 array elements,

14
00:00:36,806 --> 00:00:39,579
and I'm going to do so in blocks of a thousand.

15
00:00:39,579 --> 00:00:41,880
That's the #defines.

16
00:00:41,880 --> 00:00:44,608
It got a little helper function that just prints out an array.

17
00:00:44,608 --> 00:00:45,975
We'll use that for debugging.

18
00:00:45,975 --> 00:00:48,611
And then here's the kernel we're going to use.

19
00:00:48,611 --> 00:00:51,051
It's a kernel because it's labeled global.

20
00:00:51,051 --> 00:00:58,016
It is called increment naive, and it takes a pointer to global memory an array of integers.

21
00:00:58,016 --> 00:01:03,766
Each thread simply figures out what thread it is by looking at the block index,

22
00:01:03,766 --> 00:01:10,803
which block it is times how many threads there are in a block plus which thread it is inside that block.

23
00:01:10,803 --> 00:01:16,943
Now, what we're going to do is we're going to have every thread increment

24
00:01:16,943 --> 00:01:20,711
one of the elements in the array.

25
00:01:20,711 --> 00:01:27,933
To do that, we're just going to mod the thread index by the array size.

26
00:01:27,933 --> 00:01:34,023
Every thread is going, every consecutive threads are going to increment consecutive elements in the array,

27
00:01:34,023 --> 00:01:36,125
wrapping around at the array size.

28
00:01:36,125 --> 00:01:41,096
The fact that we have a million threads writing into only ten elements means

29
00:01:41,096 --> 00:01:46,736
that after each thread has added one to its corresponding element in the array,

30
00:01:46,736 --> 00:01:51,774
we're going to end up with 10 elements each containing the number 100,000.

31
00:01:51,774 --> 00:01:54,477
And then the code itself is simple.

32
00:01:54,477 --> 00:01:56,314
We have a timer class.

33
00:01:56,314 --> 00:01:59,849
Again, I've sort of hidden that away so you don't have to deal with it right now.

34
00:01:59,849 --> 00:02:03,253
We're going to declare some host memory.

35
00:02:03,253 --> 00:02:07,259
We're going to declare some GPU memory.

36
00:02:07,259 --> 00:02:10,425
And we're going to zero out that memory.

37
00:02:10,425 --> 00:02:12,796
You haven't seen cudaMemset before, but it's exactly what you'd think.

38
00:02:12,796 --> 00:02:19,101
We're going to set all of the bytes of this device array to zero.

39
00:02:19,101 --> 00:02:21,411
Now, we're going to launch the kernel.

40
00:02:21,411 --> 00:02:23,005
And I've put a timer around this,

41
00:02:23,005 --> 00:02:26,492
because one of the things I want to show you is that atomic operations can slow things down.

42
00:02:26,492 --> 00:02:29,743
So here's the kernel that we called increment_naive.

43
00:02:29,743 --> 00:02:34,091
We're going to launch it with a number of blocks equal

44
00:02:34,091 --> 00:02:36,591
to the total number of threads divided by the block width

45
00:02:36,591 --> 00:02:39,941
and the number of threads per block equal to the block width.

46
00:02:39,941 --> 00:02:44,940
Remember, these numbers initially are 1,000,000 and 1,000, okay?

47
00:02:44,940 --> 00:02:47,559
We're going to end up launching a thousand blocks,

48
00:02:47,559 --> 00:02:50,210
and we're going to pass in the device array,

49
00:02:50,210 --> 00:02:53,342
and then each thread is going to do its incrementing.

50
00:02:53,342 --> 00:02:59,605
And when it's all done we're going to stop the timer and copy back the array using cudaMemcpy.

51
00:02:59,605 --> 00:03:02,509
Now, we'll take that array that we just incremented

52
00:03:02,509 --> 00:03:07,592
and copy it back to the host and then I hid away a little print array helper function.

53
00:03:07,592 --> 00:03:09,114
It just prints out the contents of the array.

54
00:03:09,114 --> 00:03:12,426
Then I'm going to print out the amount of time taken milliseconds

55
00:03:12,426 --> 00:03:15,758
by this kernel that I measured with a timer.

56
00:03:15,758 --> 00:03:19,758
Okay. That's the whole Cuda program. Let's compile and run it.
