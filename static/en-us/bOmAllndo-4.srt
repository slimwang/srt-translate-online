1
00:00:00,012 --> 00:00:03,132
So now we're going to start actually doing the matrix multiplication,

2
00:00:03,132 --> 00:00:05,499
and so we're going to show the first 3 steps of this.

3
00:00:05,499 --> 00:00:09,583
The first thing we're going to do is take the value vector and the row pointer vector

4
00:00:09,583 --> 00:00:14,229
and together we're going to create a segmented representation of the value vector.

5
00:00:14,229 --> 00:00:19,903
Note that each segment in this resulting vector represents 1 row of the sparse matrix

6
00:00:19,903 --> 00:00:24,443
and the row pointer shows where each segment head begins.

7
00:00:24,443 --> 00:00:29,920
The next thing that we're going to do is gather these vector values using these column indices

8
00:00:29,920 --> 00:00:36,839
to create a corresponding list from which we can multiply each of these individual matrix entries.

9
00:00:36,839 --> 00:00:40,964
So we see that value a is located in column 0,

10
00:00:40,964 --> 00:00:46,055
which means it needs to be multiplied by the vector element in row 0.

11
00:00:46,055 --> 00:00:51,427
So what we're going to do is use these column indices to gather from this vector,

12
00:00:51,427 --> 00:00:53,798
and that's going to give us the following array.

13
00:00:53,798 --> 00:00:57,169
Okay, now we actually need to do the pairwise multiplications.

14
00:00:57,169 --> 00:01:00,653
So we simply use a map operation to multiply this vector--

15
00:01:00,653 --> 00:01:04,043
not caring about the segmentation--by this vector here,

16
00:01:04,043 --> 00:01:07,166
and it's going to give us yet another vector of the same size.

17
00:01:07,166 --> 00:01:12,718
And the final step is that we need to add up the partial products within each segment

18
00:01:12,718 --> 00:01:18,771
to be able to get the final output vector, and that's where we can use segmented scan.

19
00:01:18,771 --> 00:01:23,262
But now we have to add up the partial products on each row to get the output vector.

20
00:01:23,262 --> 00:01:26,668
Specifically, we need to add the partial products within each segment,

21
00:01:26,668 --> 00:01:29,228
and that's where our segmented scan comes in.

22
00:01:29,228 --> 00:01:35,666
We need to do an exclusive segmented sum scan to sum up each segment of partial products.

23
00:01:35,666 --> 00:01:40,504
It's actually a little bit more convenient to do a backwards exclusive segmented sum scan

24
00:01:40,504 --> 00:01:43,494
so all the sums instead end up at the head of each segment

25
00:01:43,494 --> 00:01:49,020
since then the row pointer array can be used to gather those per row sums into a dense vector.

26
00:01:49,020 --> 00:01:54,056
Now here we're actually using a segmented scan to perform what's really a segmented reduce.

27
00:01:54,056 --> 00:01:57,674
And if you want a good challenge, think about how you might want to build a segmented reduce

28
00:01:57,674 --> 00:02:00,371
that's a little bit more efficient than a segmented scan

29
00:02:00,371 --> 00:02:04,505
just to go back to our original matrix to make sure we're actually going to get the same answers.

30
00:02:04,505 --> 00:02:08,106
So here's our original matrix here and our original vector.

31
00:02:08,106 --> 00:02:14,498
And we note that we dot product this vector with this vector to get this answer,

32
00:02:14,498 --> 00:02:18,372
this vector dotted with this vector to get this answer,

33
00:02:18,372 --> 00:02:21,952
and then this vector dotted with this vector to get this answer.

34
00:02:21,952 --> 00:02:26,097
And you'll see if you look at each one of these rows, they're going to correspond to the sums

35
00:02:26,097 --> 00:02:28,656
within each one of these segments.

36
00:02:28,656 --> 00:02:32,319
But the nice part about multiplying using the sparse matrix representation

37
00:02:32,319 --> 00:02:35,187
is that we're never multiplying by any zeroes at all.

38
00:02:35,187 --> 00:02:37,404
So here we've actually had to multiply by zeroes,

39
00:02:37,404 --> 00:02:40,174
and then we just throw away the answers because they're going to be 0.

40
00:02:40,174 --> 00:02:45,900
On the other hand, when we're actually implementing this using a true sparse matrix representation,

41
00:02:45,900 --> 00:02:48,123
we're not going to have any zeroes present at all,

42
00:02:48,123 --> 00:02:50,238
so this is going to be substantially more efficient

43
00:02:50,238 --> 00:02:53,299
for any real matrix that has a significant amount of zeroes.
