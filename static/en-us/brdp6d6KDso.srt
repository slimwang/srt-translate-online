1
00:00:00,410 --> 00:00:02,670
To understand threads more deeply, it's important to have a

2
00:00:02,670 --> 00:00:06,250
clear picture of a traditional process and its address space.

3
00:00:06,250 --> 00:00:08,880
Typically, at the lower addresses, a process will have the

4
00:00:08,880 --> 00:00:13,290
instructions followed by literals, that is values in the code, then

5
00:00:13,290 --> 00:00:17,910
statically allocated memory, really, global variables, and then the heap.

6
00:00:17,910 --> 00:00:19,850
Which will grow upwards. And then at the top of

7
00:00:19,850 --> 00:00:22,510
the address space and working its way down, we have

8
00:00:22,510 --> 00:00:25,750
the current stack of a process. Remember that the stack includes

9
00:00:25,750 --> 00:00:29,430
all the information local to a procedure. Parameters, local

10
00:00:29,430 --> 00:00:32,710
variables, and the address of the instruction to return to

11
00:00:32,710 --> 00:00:35,140
when the current procedure is over. In a multithreaded

12
00:00:35,140 --> 00:00:38,405
environment, each thread has it's own stack, but it shares

13
00:00:38,405 --> 00:00:40,540
everything else with the original main thread of the

14
00:00:40,540 --> 00:00:44,230
process. So, from a certain perspective, a thread acts like

15
00:00:44,230 --> 00:00:47,860
an entire process unto itself, and hence threaded programming

16
00:00:47,860 --> 00:00:51,000
isn't much different from a traditional programming. The difference comes

17
00:00:51,000 --> 00:00:53,690
from the fact that part of the memory is shared. On the

18
00:00:53,690 --> 00:00:56,880
one hand, this makes it very easy for threads to communicate. But

19
00:00:56,880 --> 00:00:59,190
it also means that extra care is needed to make sure that

20
00:00:59,190 --> 00:01:02,300
the threads coordinate as they use this memory. I should point out

21
00:01:02,300 --> 00:01:06,100
as well, that one can do parallel programming without having multiple threads.

22
00:01:06,100 --> 00:01:09,020
Each parallelizable task could have it's own process with it's own address

23
00:01:09,020 --> 00:01:11,160
space. And it could communicate with

24
00:01:11,160 --> 00:01:13,690
the other processes through message passing.

25
00:01:13,690 --> 00:01:16,040
In fact if we wanted to take advantage of a distributed system,

26
00:01:16,040 --> 00:01:19,020
where each processor has it's own separate physical memory, we would be

27
00:01:19,020 --> 00:01:21,160
forced to use this approach. As

28
00:01:21,160 --> 00:01:23,160
we think about multi-threaded programming, however, the

29
00:01:23,160 --> 00:01:26,400
paradigm explored in this lesson, is more appropriate to think of a shared

30
00:01:26,400 --> 00:01:29,790
memory system, where we have multiple cores sharing a common piece of memory.
