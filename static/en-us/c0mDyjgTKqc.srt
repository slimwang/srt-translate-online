1
00:00:00,076 --> 00:00:04,408
But the interesting thing that we can do in something like NVVP

2
00:00:04,408 --> 00:00:12,413
is we can go in and we can analyze the program. So this is going to run the program many times.

3
00:00:12,413 --> 00:00:16,317
It's collecting a bunch of statistics about the program, it's averaging them together,

4
00:00:16,317 --> 00:00:23,259
and now that we've done this analysis we have a lot more information about these kernels.

5
00:00:23,259 --> 00:00:26,763
So if I click on this you see I have more statistics over here,

6
00:00:26,763 --> 00:00:31,773
and the one that I want to highlight for you is the global load efficiency,

7
00:00:31,773 --> 00:00:35,107
which tells us how efficient our global loads were.

8
00:00:35,107 --> 00:00:40,885
In other words, of all of the bytes that we fetched with each memory transaction, how many of them are actually useful?

9
00:00:40,885 --> 00:00:46,019
100%, looks pretty good. That's what we would expect from fully coalesced accesses.

10
00:00:46,019 --> 00:00:49,951
The global store efficiency, our stores to global memory,

11
00:00:49,951 --> 00:00:52,129
which in our case is writing the output matrix,

12
00:00:52,129 --> 00:00:56,165
achieved 12.5%, and that's pretty wretched, right,

13
00:00:56,165 --> 00:00:58,729
and that's again what we would expect from having inspected the code.

14
00:00:58,729 --> 00:01:07,337
Our total DRAM utilization is down at 7.6%, again remember this is not going to exactly match what we calculated outside the profiler,

15
00:01:07,337 --> 00:01:09,505
but we can tell that there's a problem.

16
00:01:09,505 --> 00:01:12,383
That was the parallel per-row kernel but only had a single thread block.

17
00:01:12,383 --> 00:01:18,584
We can see that the parallel per-element kernel doesn't do much better.

18
00:01:18,584 --> 00:01:23,669
It has slightly higher global store efficiency. Very slightly higher DRAM utilization.

19
00:01:23,669 --> 00:01:31,227
So, we've still got this problem. Our problem is clearly our ability to write to the output matrix is hampered.

20
00:01:31,227 --> 00:01:33,897
We're not achieving the bandwidth that we ought to.

21
00:01:33,897 --> 00:01:41,253
As I said before, I'm not intending to go through all the many, many things that you can analyze in NVVP.

22
00:01:41,253 --> 00:01:43,946
I'll pull it up once or twice again in the course of this lecture

23
00:01:43,946 --> 00:01:48,256
to just illustrate that there are tools to help you figure out what's going on.

24
00:01:48,256 --> 00:01:55,452
You can see, in fact, that down here it's actually analyzing the program for you and giving you some suggestions.

25
00:01:55,452 --> 00:01:59,504
It's saying look the multiprocessors in your program are mostly idle,

26
00:01:59,504 --> 00:02:03,257
you're not getting a lot of work done for the total amount of time this program runs,

27
00:02:03,257 --> 00:02:07,961
and your total compute to mem copy efficiency is low; in other words,

28
00:02:07,961 --> 00:02:12,268
you're not doing a lot of computation given the amount of time that you spend doing mem copies.

29
00:02:12,268 --> 00:02:17,041
Here's the mem copies in our timeline. You can see that we spent, you know, 2 point,

30
00:02:17,041 --> 00:02:22,753
let's see, 2.6 milliseconds copying information in and then

31
00:02:22,753 --> 00:02:29,003
8 milliseconds processing it and then another 2.6 milliseconds copying it out.

32
00:02:29,003 --> 00:02:32,823
So it warns us, hey, the total compute to mem copy efficiency is low.

33
00:02:32,823 --> 00:02:36,722
So these are really useful tools, you should use them. You should know about them,

34
00:02:36,722 --> 00:02:40,569
but we're not going to rely on them too much in this class because our point is to teach

35
00:02:40,569 --> 00:02:44,040
you how to reason about things from from first principles.

36
00:02:44,040 --> 00:02:49,103
So, our problem is bad coalescing on the write to the output matrix. What can we do about that?
