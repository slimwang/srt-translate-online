1
00:00:00,000 --> 00:00:04,000
Now I want to give you an idea of how well the segmentation program performs.

2
00:00:04,000 --> 00:00:07,000
Here I've trained it on a corpus of 4 billion words--

3
00:00:07,000 --> 00:00:10,000
not just the Shakespeare corpus but a larger corpus,

4
00:00:10,000 --> 00:00:14,000
and then I give it some test cases to try to find the best segmentation.

5
00:00:14,000 --> 00:00:19,000
So I gave it the test case here. The program came up with "base rate sought to,"

6
00:00:19,000 --> 00:00:22,000
but the correct answer was "base rates ought to."

7
00:00:22,000 --> 00:00:28,000
In this case, it just seems somewhat like bad luck that that was the right answer,

8
00:00:28,000 --> 00:00:32,000
but both segmentations seem like good segmentations.

9
00:00:32,000 --> 00:00:34,000
Next was this trial.

10
00:00:34,000 --> 00:00:38,000
My program came up with "small and in significant,"

11
00:00:38,000 --> 00:00:41,000
but the correct answer was "small and insignificant."

12
00:00:41,000 --> 00:00:45,000
Here it seems like it really has erred that "small and insignificant"

13
00:00:45,000 --> 00:00:49,000
seems like a much better segmentation than the one my program came up with.

14
00:00:49,000 --> 00:00:55,000
What I want you to tell me is what do you think could help us do a better job of getting the right answer.

15
00:00:55,000 --> 00:00:59,000
Would it be helpful to gather more data?

16
00:00:59,000 --> 00:01:02,000
Check that box if you think that would be helpful.

17
00:01:02,000 --> 00:01:08,000
Would it be helpful to make a Markov assumption rather than the naive Bayes assumption?

18
00:01:08,000 --> 00:01:10,000
Check here.

19
00:01:10,000 --> 00:01:16,000
Or would it be helpful to do a better job with our smoothing algorithm? Check here.

20
00:01:16,000 --> 99:59:59,999
And you can check more than one.
