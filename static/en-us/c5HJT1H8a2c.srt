1
00:00:00,180 --> 00:00:03,860
It is one of the method we can use, to process the kind of the data that we

2
00:00:03,860 --> 00:00:08,623
just saw. It is sometimes called decision-free learning. Recall that we were

3
00:00:08,623 --> 00:00:14,000
discussing case-based learning, we talked about discrimination tree learning.

4
00:00:14,000 --> 00:00:16,850
There, we learned the discrimination tree incrementally.

5
00:00:16,850 --> 00:00:20,473
A case would come one at a time, and we would ask the question,

6
00:00:20,473 --> 00:00:25,231
what feature would discriminate between the existing cases, and the new case?

7
00:00:25,231 --> 00:00:29,100
And we would pick a feature. Discrimination pre-learning provides no

8
00:00:29,100 --> 00:00:34,890
guarantee of the optimality of this tree. That is to say, at retrieval time,

9
00:00:34,890 --> 00:00:38,820
when a new problem comes along, traversing this tree might take a long

10
00:00:38,820 --> 00:00:44,470
time because this tree is not the most optimal tree was during these cases.

11
00:00:44,470 --> 00:00:48,390
We'll discuss an alternative method called decision tree learning,

12
00:00:48,390 --> 00:00:52,860
which will give us more optimal trees, however, at a cost. The cost will

13
00:00:52,860 --> 00:00:57,110
be that all the examples will need to be given right at the beginning.

14
00:00:57,110 --> 00:01:01,050
Let us return to our restaurant example. We want to learn a decision tree

15
00:01:01,050 --> 00:01:05,970
that will classify these five examples so that as a new problem comes along,

16
00:01:05,970 --> 00:01:10,930
we can quickly find which is the closest example to the new problem.

17
00:01:10,930 --> 00:01:15,350
To do this, we need to pick one of four features, restaurant, meal, day or

18
00:01:15,350 --> 00:01:21,120
cost that will separate these allergic reactions, so that one category

19
00:01:21,120 --> 00:01:27,300
contains either only false instances, or only true instances.

20
00:01:27,300 --> 00:01:31,630
As an example, supposing we think of restaurant as being the decisive feature.

21
00:01:31,630 --> 00:01:35,780
So we have picked restaurant as a decisive feature. Now, there are three kinds

22
00:01:35,780 --> 00:01:39,440
of restaurants. Kim's, Bob's, and Sam's. Whenever it's Kim's restaurant, or

23
00:01:39,440 --> 00:01:43,900
Bob's restaurant, there is no allergic reaction. Whenever it's Sam's restaurant,

24
00:01:43,900 --> 00:01:48,060
there can be allergic action shown in green here, or no allergic reaction,

25
00:01:48,060 --> 00:01:51,090
shown in red. So the good thing about this particular feature,

26
00:01:51,090 --> 00:01:56,470
restaurant, is that, it has separated all the five examples into two classes.

27
00:01:56,470 --> 00:02:01,850
Into the class Sam's, and into the class not Sam's. Not Sam class consists of

28
00:02:01,850 --> 00:02:05,900
only negative reactions, which is good, because we know that we

29
00:02:05,900 --> 00:02:10,413
have now been able to classify all of these five examples into two sets,

30
00:02:10,413 --> 00:02:15,260
one of which contains only negative examples. Now for these three examples,

31
00:02:15,260 --> 00:02:19,320
you must pick another feature that will separate them into positive and

32
00:02:19,320 --> 00:02:24,210
negative instances. In this case, we might consider cost to be that feature.

33
00:02:24,210 --> 00:02:28,280
When the cost is cheap, then we get positive examples. When the cost is

34
00:02:28,280 --> 00:02:32,990
expensive, then we get negative examples. This is a classification tree.

35
00:02:32,990 --> 00:02:35,570
And in fact, this is a very efficient classification tree.

36
00:02:35,570 --> 00:02:41,590
When a new problem comes around, for example visit6. Sam's, lunch, Friday,

37
00:02:41,590 --> 00:02:46,200
cost is expensive, and you want to decide what the allergic reaction might be,

38
00:02:46,200 --> 00:02:50,660
we simply have to travel through this tree, to find out, the closest neighbor,

39
00:02:50,660 --> 00:02:54,670
of that particular new example. This is called a decision tree.

40
00:02:54,670 --> 00:02:58,640
And this technique that we just discussed is called decision tree learning.

41
00:02:58,640 --> 00:03:03,150
This method of inductive decision tree learning worked much more efficiency and

42
00:03:03,150 --> 00:03:06,940
apparently more easily than earlier method that we have discussed. But

43
00:03:06,940 --> 00:03:10,524
the trade off is that we needed to know all the five examples right in

44
00:03:10,524 --> 00:03:13,620
the beginning. Of course, this technique simply appears to be efficient and

45
00:03:13,620 --> 00:03:17,283
easy. And that is because we had only five examples, and

46
00:03:17,283 --> 00:03:20,465
only four features that were describing all five examples.

47
00:03:20,465 --> 00:03:25,370
If the number of examples was very large, or the number of features that were

48
00:03:25,370 --> 00:03:29,540
describing the examples were very large. Then it's very hard to decide what

49
00:03:29,540 --> 00:03:32,760
exactly should be the feature that we should use to discriminate on.
