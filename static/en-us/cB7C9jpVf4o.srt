1
00:00:00,075 --> 00:00:03,642
It's important to remember that CUDA isn't just a specific language

2
00:00:03,642 --> 00:00:06,788
or a set of extensions to the CC++ language.

3
00:00:06,788 --> 00:00:09,726
CUDA is the whole GPU computing architecture.

4
00:00:09,726 --> 00:00:12,693
So let's talk about some other platforms that support CUDA.

5
00:00:12,693 --> 00:00:18,073
First we'll talk about other languages that support CUDA, then we'll talk about cross-platform solutions.

6
00:00:18,073 --> 00:00:23,834
So the lightweight version of targeting CUDA is simply to wrap CUDA C++ into the other language,

7
00:00:23,834 --> 00:00:28,888
so that you can transfer data to the GPU and call a CUDA C++ kernel from within the other language.

8
00:00:28,888 --> 00:00:35,099
And a great example of this is PyCUDA, which allows Python programs to call CUDA C++ kernels,

9
00:00:35,099 --> 00:00:37,793
and even to construct them programmatically from within Python.

10
00:00:37,793 --> 00:00:42,719
At a deeper level, an increasing number of languages are directly targeting the CUDA architecture.

11
00:00:42,719 --> 00:00:46,210
So fans of Python will also want to check out Copperhead.

12
00:00:46,210 --> 00:00:51,648
Copperhead is a data parallel subset of Python, that uses a library of data parallel

13
00:00:51,648 --> 00:00:54,654
functions such as map, produce, filter, scan and sort.

14
00:00:54,654 --> 00:00:57,824
When a Copperhead function is called, the runtime generates thrust code

15
00:00:57,824 --> 00:01:00,540
and calls it from the Python interpreter.

16
00:01:00,540 --> 00:01:05,200
Memory is managed by the Python garbage collector and lazily transferred to and from the GPU.

17
00:01:05,200 --> 00:01:08,155
So, a cool thing about Copperhead programs is that they interoperate

18
00:01:08,155 --> 00:01:13,240
with Python programs, using packages like numpy or matplotlib.

19
00:01:13,240 --> 00:01:16,637
This makes it easier to write entire applicatons and not just kernels.

20
00:01:16,637 --> 00:01:21,817
Cuda Fortran is a product form the Portland group That does exactly what it sounds like.

21
00:01:21,817 --> 00:01:24,178
It integrates CUDA constructs such as thread blocks

22
00:01:24,178 --> 00:01:26,799
and shared memory directly into the FORTRAN language.

23
00:01:26,799 --> 00:01:32,263
More on the research front, Halide is a brand new language specifically designed for image processing.

24
00:01:32,263 --> 00:01:37,232
This makes it a DSL or Domain Specific Language. And it's a really cool example of this.

25
00:01:37,232 --> 00:01:43,130
Halide targets GPUs as well as multi-core CPUs, on both desktops and on mobile devices.

26
00:01:43,130 --> 00:01:46,180
And many of the parallel optimization patterns we've discussed,

27
00:01:46,180 --> 00:01:50,080
such as tiling, are particularly easy to express and optimize in Halide.

28
00:01:50,080 --> 00:01:53,365
And that makes it possible to get very high-performance image processing code.

29
00:01:53,365 --> 00:01:57,205
Finally, the widely used Matlab platform supports CUDA both

30
00:01:57,205 --> 00:02:02,306
for wrapping CUDA kernels as well as directly targeting CUDA with Matlab code in core functions.

31
00:02:02,306 --> 00:02:05,016
So let's talk about Matlab a little further.
