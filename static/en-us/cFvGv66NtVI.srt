1
00:00:00,370 --> 00:00:03,790
So it seems like a good idea and
I think it's worth asking whether or

2
00:00:03,790 --> 00:00:05,470
not it actually works.

3
00:00:05,470 --> 00:00:06,220
>> Does it work?

4
00:00:06,220 --> 00:00:07,490
>> Well, it's complicated.

5
00:00:07,490 --> 00:00:09,120
>> Tell me more.

6
00:00:09,120 --> 00:00:12,370
>> There have been some examples where
things worked out extremely well.

7
00:00:12,370 --> 00:00:16,079
So let me just quickly summarize
some of them, so you have a sense.

8
00:00:16,079 --> 00:00:20,403
You mentioned neural networks, so neural
networks, including with non-linear

9
00:00:20,403 --> 00:00:24,059
connections, have been used with
backpropagation in this context.

10
00:00:24,059 --> 00:00:27,897
Folks like Gerry Tesauro and Tom
Dietterich have gotten some very nice

11
00:00:27,897 --> 00:00:30,827
results creating learning
systems that can actually

12
00:00:30,827 --> 00:00:34,670
do some fairly complicated things and
some difficult tasks.

13
00:00:34,670 --> 00:00:35,258
>> Like what?

14
00:00:35,258 --> 00:00:38,210
>> Well Gerry Tesauro's
work was in Backgammon, and

15
00:00:38,210 --> 00:00:40,970
he actually used the same
kind of approach in

16
00:00:40,970 --> 00:00:45,380
Jeopardy in the Watson system, for
figuring out how to do decision-making.

17
00:00:45,380 --> 00:00:50,360
What's the right strategy for handling
different situations in jeopardy, and

18
00:00:50,360 --> 00:00:53,055
the value function was being
predicted using a backprop net.

19
00:00:53,055 --> 00:00:57,780
In Wang/Dietterich's work, they were
actually predicting the likelihood of

20
00:00:57,780 --> 00:01:03,109
success of various sort of
shuttle scheduling strategies.

21
00:01:03,109 --> 00:01:07,970
They were actually doing for
NASA shuttle scheduling strategies.

22
00:01:07,970 --> 00:01:09,260
>> Say that three times fast.

23
00:01:09,260 --> 00:01:13,480
>> She sells seashells to
the shuttling scheduling system.

24
00:01:13,480 --> 00:01:14,960
>> Sure.
>> So those worked out really well.

25
00:01:14,960 --> 00:01:15,720
That being said,

26
00:01:15,720 --> 00:01:18,600
there's not that many other people
who've gotten that to work effectively.

27
00:01:18,600 --> 00:01:21,620
Certainly, whenever, I'd say to my
students, look these things worked so

28
00:01:21,620 --> 00:01:23,050
well, why don't you try it?

29
00:01:23,050 --> 00:01:26,240
They don't seem to be nearly as
successful as these folks have been.

30
00:01:26,240 --> 00:01:27,546
>> Hey, I got it to work for
my master's thesis.

31
00:01:27,546 --> 00:01:28,410
>> Did you really?

32
00:01:28,410 --> 00:01:29,100
For what problem?

33
00:01:30,110 --> 00:01:33,132
>> For tic-tac-toe and then for
a vision problem that,

34
00:01:33,132 --> 00:01:38,046
after I went through all this trouble,
I realized was just [INAUDIBLE] around.

35
00:01:38,046 --> 00:01:40,498
>> [LAUGH],
that's impressive, in quotes.

36
00:01:40,498 --> 00:01:42,738
>> Not impressive enough to
give me a master's thesis,

37
00:01:42,738 --> 00:01:44,394
which I think was the important thing.

38
00:01:44,394 --> 00:01:47,665
So maybe the impressive thing, is that
I got a master's thesis out of this.

39
00:01:47,665 --> 00:01:49,590
[LAUGH]
>> At MIT, of all places.

40
00:01:49,590 --> 00:01:50,300
>> Of all places.

41
00:01:50,300 --> 00:01:52,030
>> So it does sometimes work.

42
00:01:52,030 --> 00:01:54,850
I find that it's kind of difficult
to get it to work consistently.

43
00:01:54,850 --> 00:01:58,920
What happens often, when my students try
this, is they get a system that actually

44
00:01:58,920 --> 00:02:03,240
tends to start to learn well and then it
goes into some kind of a death spiral.

45
00:02:03,240 --> 00:02:05,000
Because, as I mentioned before,

46
00:02:05,000 --> 00:02:08,479
the bootstrapping aspect of this
means that you're really dependent on

47
00:02:08,479 --> 00:02:11,400
making solid predictions based
on your previous predictions.

48
00:02:11,400 --> 00:02:13,030
And when they start to go south,

49
00:02:13,030 --> 00:02:15,050
the whole thing can fall apart
actually really quickly.

50
00:02:15,050 --> 00:02:15,770
That being said,

51
00:02:15,770 --> 00:02:18,300
there's some other function
approximators that actually have been

52
00:02:18,300 --> 00:02:22,010
more successful that are variations
of linear function approximators.

53
00:02:22,010 --> 00:02:23,900
So Rich Sutton, in particular,

54
00:02:23,900 --> 00:02:27,948
has gotten CMAC to work on a on
a whole bunch of tough problems.

55
00:02:27,948 --> 00:02:32,210
CMACs are c-m-a-c, it's a particular
kind of function representation that

56
00:02:32,210 --> 00:02:34,980
is like a neural net, except
the first layer isn't really learned.

57
00:02:34,980 --> 00:02:37,360
It's kind of decided in advance
how you're going to break up

58
00:02:37,360 --> 00:02:38,230
the input space.

59
00:02:38,230 --> 00:02:41,630
And so it really is just
a generalization of straight up linear.

60
00:02:41,630 --> 00:02:45,930
These can be really effective, but they
don't generalize very aggressively and

61
00:02:45,930 --> 00:02:48,420
that's part of what makes them
a bit safer to use in this setting.
