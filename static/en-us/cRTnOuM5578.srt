1
00:00:00,510 --> 00:00:03,379
Hey, welcome back, so previously,

2
00:00:03,379 --> 00:00:08,490
I talked to you about the concepts
behind RNNs and LSTMs.

3
00:00:08,490 --> 00:00:12,039
But now I'm actually going to walk
through an implementation of these

4
00:00:12,039 --> 00:00:13,899
things in TensorFlow.

5
00:00:13,900 --> 00:00:17,019
So here I'm building
a character-wise RNN.

6
00:00:17,019 --> 00:00:20,503
So what that means is that it looks
at the characters in text and

7
00:00:20,504 --> 00:00:22,380
it can generate new characters.

8
00:00:22,379 --> 00:00:25,038
So it generates new texts
character by character.

9
00:00:25,038 --> 00:00:27,210
And I'm going to train
this on Anna Karenina,

10
00:00:27,210 --> 00:00:31,380
which is one of my favorite books
of all time, it's super good.

11
00:00:31,379 --> 00:00:33,879
So then the idea is that
train it on Anna Karenina,

12
00:00:33,880 --> 00:00:36,450
and then it can generate
new text from the book.

13
00:00:36,450 --> 00:00:40,859
And it's going to learn names in
the structure and all these cool things.

14
00:00:40,859 --> 00:00:46,320
So this network is based off
a blog post by Andrej Karpathy.

15
00:00:46,320 --> 00:00:48,420
You should check out this link here.

16
00:00:48,420 --> 00:00:51,548
I'll share this notebook with you so
you can work through it and

17
00:00:51,548 --> 00:00:52,973
also check out these links.

18
00:00:52,973 --> 00:00:55,493
So this was a really
good blog post of his,

19
00:00:55,493 --> 00:00:59,769
he has an implementation that
was in Torch and it's on GitHub.

20
00:00:59,770 --> 00:01:03,690
When I was building this,
I also checked out some other code,

21
00:01:03,689 --> 00:01:10,879
which you can find at this link,
r2rt, and this dude's GitHub repo.

22
00:01:10,879 --> 00:01:13,549
He made a character-wise RNN also.

23
00:01:13,549 --> 00:01:18,250
So below here is a diagram that
was made by Andrej Karpathy.

24
00:01:18,250 --> 00:01:22,340
And it basically shows the general
architecture of what I built here.

25
00:01:22,340 --> 00:01:27,700
So the idea is to encode each of your
characters with one-hot encoding.

26
00:01:27,700 --> 00:01:31,780
So this is your input layer, so you just
have your one-hot encoded characters.

27
00:01:31,780 --> 00:01:35,599
This goes to the hidden layer
which is then passed to the next

28
00:01:35,599 --> 00:01:37,129
character in the sequence, right.

29
00:01:38,590 --> 00:01:41,340
The output of this hidden layer also
goes to the output layer where you have

30
00:01:41,340 --> 00:01:42,450
your logits.

31
00:01:42,450 --> 00:01:47,500
And you use these logits with Softmax to
predict what the next character will be.

32
00:01:47,500 --> 00:01:50,930
And then your target characters are just
the next character in the sequence,

33
00:01:50,930 --> 00:01:51,650
right.

34
00:01:51,650 --> 00:01:55,550
So this is the general
architecture of this network,

35
00:01:55,549 --> 00:01:58,810
and you'll see how I implement
this in TensorFlow later.

36
00:02:00,390 --> 00:02:04,549
Great, so
here I'm going to import modules.

37
00:02:04,549 --> 00:02:06,637
This is where I'm loading the text.

38
00:02:06,638 --> 00:02:09,069
So Anna Karenina is in
the public domain, so

39
00:02:09,069 --> 00:02:11,280
you just find the text online.

40
00:02:11,280 --> 00:02:16,939
Here I'm making the vocab, so this is
basically a set of all the characters.

41
00:02:17,949 --> 00:02:19,189
And I take those characters and

42
00:02:19,189 --> 00:02:22,810
make a dictionary where I can
convert the characters to integers.

43
00:02:22,810 --> 00:02:27,000
And I have a dictionary that goes
backwards from integers to characters.

44
00:02:28,120 --> 00:02:32,770
And this is just a giant array of
all the characters that we have.

45
00:02:32,770 --> 00:02:35,450
So here I'm doing vocab_to_int.

46
00:02:35,449 --> 00:02:38,189
So I am making this character array,

47
00:02:38,189 --> 00:02:42,219
so all the characters
are converted to integers.

48
00:02:42,219 --> 00:02:44,009
So this is the first
100 characters you see.

49
00:02:44,009 --> 00:02:46,189
This is the first line of the book.

50
00:02:47,229 --> 00:02:51,129
And this is what it looks like when
it's converted to integers, cool.

51
00:02:52,169 --> 00:02:54,849
So now that we have our
data in a form we can use,

52
00:02:54,849 --> 00:03:00,704
we need to form batches, right, that
we're going to pass into our network.

53
00:03:00,705 --> 00:03:05,540
We also wannna split it
into a training set and

54
00:03:05,539 --> 00:03:11,009
validation set and also into the input
features and the targets that we want.

55
00:03:11,009 --> 00:03:14,639
Basically I wrote this
function to do just that.

56
00:03:14,639 --> 00:03:18,069
So it takes this character vector,

57
00:03:18,069 --> 00:03:22,389
this array that I have here, in
the batch size and the number of steps.

58
00:03:22,389 --> 00:03:27,209
So the number of steps is the sequence
length of characters that we're

59
00:03:27,210 --> 00:03:29,262
going to pass into our network.

60
00:03:29,262 --> 00:03:33,806
And so the longer the sequence is,
then the further

61
00:03:33,806 --> 00:03:38,790
back it can look for
correlations between characters.

62
00:03:38,789 --> 00:03:40,810
So it's just kind of
the more steps you have,

63
00:03:40,810 --> 00:03:44,110
the better performance
your network will have.

64
00:03:44,110 --> 00:03:47,771
But it also makes it longer to train,
of course.

65
00:03:47,770 --> 00:03:54,277
Okay, so the cool thing about
this is that our target's y.

66
00:03:54,277 --> 00:03:58,364
It's exactly the same as the input
characters, just shifted one over,

67
00:03:58,365 --> 00:03:58,840
right.

68
00:03:58,840 --> 00:04:05,569
So like in hello, our first character
is f and the first target is e.

69
00:04:05,569 --> 00:04:10,680
So you can just have your x as
just grab a bunch characters and

70
00:04:10,680 --> 00:04:13,110
y is just everything
just shifted over by 1.

71
00:04:13,110 --> 00:04:18,860
Here what I'm doing is dropping the last
few characters to make sure that

72
00:04:18,860 --> 00:04:22,250
when I'm going through and I'm making my
batches, that all the batches are full.

73
00:04:22,250 --> 00:04:27,639
So I'm just kind of leaving off
the tail of this character array.

74
00:04:27,639 --> 00:04:33,850
Great, so now that I have my x and my y,
I need to split these into batches.

75
00:04:33,850 --> 00:04:37,330
So what I'm going to do is I
take split(x, batch_size).

76
00:04:37,329 --> 00:04:42,113
What this does is it makes a number of
different arrays equal to batch size.

77
00:04:42,113 --> 00:04:46,086
And then when I stack them,
what this does is it makes rows,

78
00:04:46,086 --> 00:04:48,000
it stacks them vertically.

79
00:04:48,000 --> 00:04:50,980
And so
you have rows equal to batch size, and

80
00:04:50,980 --> 00:04:54,520
then columns equal to all
the rest of the data.

81
00:04:54,519 --> 00:04:58,146
So basically what we can do now with
this is that we can put a little window

82
00:04:58,146 --> 00:04:58,938
on top of this.

83
00:04:58,939 --> 00:05:01,562
And the height of this window is
going to be the batch size and

84
00:05:01,562 --> 00:05:05,020
then the width of the window is going to
be the number of steps in the sequence.

85
00:05:05,019 --> 00:05:08,289
So you can just move this window
along this big 2D array and

86
00:05:08,290 --> 00:05:11,750
just kind of slide it over and
get your batches.

87
00:05:11,750 --> 00:05:16,100
So here's just splitting out
the training and validation sets.

88
00:05:16,100 --> 00:05:19,100
So pretty standard, you look at this.

89
00:05:19,100 --> 00:05:23,590
If we do a batch size of 10 and our step
size are 200, then we see the train x,

90
00:05:23,589 --> 00:05:28,469
the shape is 10, so it's 10 rows for
the batches, pretty normal.

91
00:05:28,470 --> 00:05:33,110
And we can look at all the batches for
the first 10 steps,

92
00:05:33,110 --> 00:05:37,199
so this would be
a window into your data.

93
00:05:37,199 --> 00:05:41,879
So if your sequence steps
was doing 10 steps,

94
00:05:41,879 --> 00:05:43,620
right, so this would be the first batch.

95
00:05:43,620 --> 00:05:46,600
And for the second batch,
you would just shift it over 10 and

96
00:05:46,600 --> 00:05:47,390
you get the next batch.

97
00:05:48,649 --> 00:05:51,009
And this is basically
what that get_batch is,

98
00:05:51,009 --> 00:05:52,529
exactly what I just described.

99
00:05:52,529 --> 00:05:58,039
Where you are just sliding this window
along your data that you see here.

100
00:05:58,040 --> 00:05:59,319
[BLANK_AUDIO]

101
00:05:59,319 --> 00:06:03,719
So now this is where I
actually build the network.

102
00:06:03,720 --> 00:06:08,800
So this is all done in TensorFlow,
now I'm going to go through it now.

103
00:06:08,800 --> 00:06:12,629
So here just kind of typical,
your placeholders for the inputs and

104
00:06:12,629 --> 00:06:13,567
the targets.

105
00:06:13,567 --> 00:06:18,449
And this keep probability is a
placeholder that I'll use for drop out.

106
00:06:18,449 --> 00:06:19,310
So this is how

107
00:06:20,829 --> 00:06:25,060
many of the different connections I'll
be keeping in the drop out layers.

108
00:06:25,060 --> 00:06:28,259
In this network,
I am passing in characters, so

109
00:06:28,259 --> 00:06:33,209
actually integers, because I converted
them to integers in inputs and targets.

110
00:06:33,209 --> 00:06:36,009
And then here,
I'm encoding them to one_hot vectors.

111
00:06:37,209 --> 00:06:43,769
And this is the RNN layer, it is
actually pretty simple to do this part.

112
00:06:43,769 --> 00:06:48,118
So you just say I'm going to
make a BasicLSTMCell, and

113
00:06:48,119 --> 00:06:51,620
the (lstm_size) is the number
of hidden units in these cells.

114
00:06:51,620 --> 00:06:57,540
And then you just wrap it in Dropout, so
it just does Dropout for you, basically.

115
00:06:57,540 --> 00:07:00,700
And then it's super
easy to stack these up.

116
00:07:00,699 --> 00:07:02,478
You can do MultiRNNCells.

117
00:07:02,478 --> 00:07:07,220
So this automatically
routes the output from

118
00:07:07,220 --> 00:07:10,430
one layer of an LCM to
the next layer of the LCMs.

119
00:07:10,430 --> 00:07:15,675
So you can just stack however many tall,
how many deep LCM cells you want.

120
00:07:15,675 --> 00:07:18,228
So drop is from the DropoutWrapper and

121
00:07:18,228 --> 00:07:22,949
then you just multiply it by the number
of layers, and so you get that.

122
00:07:22,949 --> 00:07:27,587
Then here we are setting the cell state,
so that we're just setting it to zero,

123
00:07:27,588 --> 00:07:29,263
so it's the initial state.

124
00:07:29,262 --> 00:07:30,252
So it just starts all zeros and

125
00:07:30,252 --> 00:07:32,860
this is what we're going to
be training as we go through.

126
00:07:32,860 --> 00:07:38,213
All right, now this part runs
the data through the RNN.

127
00:07:38,213 --> 00:07:40,370
So what we're doing here
is we're splitting.

128
00:07:40,370 --> 00:07:46,247
So first we're splitting tf.split, our
x, our input data into number of steps.

129
00:07:46,247 --> 00:07:51,819
So what this is doing, it's making
just a column of sequences, right?

130
00:07:51,819 --> 00:07:57,317
So it's t = 1, i = 1, so
this is your first time step.

131
00:07:57,317 --> 00:07:58,899
You just get a vector,

132
00:07:58,899 --> 00:08:02,817
just a column vector that is
the length of your batches.

133
00:08:02,817 --> 00:08:07,240
So you're just kind of the stepping
through one time point at a time, and

134
00:08:07,240 --> 00:08:08,920
you're making this list.

135
00:08:08,920 --> 00:08:13,530
And so every element of this list
is one step in your sequence.

136
00:08:13,529 --> 00:08:16,876
And then you're passing each of this
steps in the sequence through your

137
00:08:16,877 --> 00:08:19,680
recurrent neural networks,
which you have defined here.

138
00:08:20,839 --> 00:08:25,777
And all that squeeze does is
it just removes any dimensions

139
00:08:25,778 --> 00:08:27,903
that are only of size 1.

140
00:08:27,903 --> 00:08:32,889
So basically, what we have here
is just a list of column vectors,

141
00:08:32,889 --> 00:08:36,980
and the length of this column
is the size of the batch.

142
00:08:38,418 --> 00:08:44,172
So in this line is actually where
we run through our network.

143
00:08:44,172 --> 00:08:48,230
So we're passing the inputs
into our network,

144
00:08:48,230 --> 00:08:51,570
I'm just taking cell, and
setting the initial state.

145
00:08:51,570 --> 00:08:53,890
And what this does automatically for

146
00:08:53,889 --> 00:08:58,689
us is that it passes the state from
each sequence step to the next one.

147
00:08:58,690 --> 00:09:01,760
So just does all that for
us, it's really convenient.

148
00:09:01,759 --> 00:09:05,509
And at the end, we get out the final
state and we get out the final outputs.

149
00:09:05,509 --> 00:09:08,807
And the outputs are the outputs
of the hidden layer for

150
00:09:08,807 --> 00:09:13,720
each step in the sequence,
and this is just a big list.

151
00:09:13,720 --> 00:09:17,040
So here, we'll keep the final state for
later, okay.

152
00:09:17,039 --> 00:09:18,429
So like I was saying,

153
00:09:18,429 --> 00:09:23,219
the outputs is a big list of the output
of each hidden layer at each step.

154
00:09:23,220 --> 00:09:27,190
So to make this easier,
we're just going to concatenate it into

155
00:09:28,740 --> 00:09:33,509
one big list, just get all these outputs
and mash them together into one array.

156
00:09:33,509 --> 00:09:38,756
Then what I do here is I reshape it so
that each row is one output,

157
00:09:38,756 --> 00:09:45,679
so lstm_size, so this the size of the
number of hidden layers in our cells.

158
00:09:45,679 --> 00:09:49,909
So I can reshape it into
the size of the lstms,

159
00:09:49,909 --> 00:09:54,969
and each row is a different output,
so every step has an output.

160
00:09:54,970 --> 00:09:59,430
And the number of columns,
the width of this

161
00:10:01,200 --> 00:10:06,360
output array is the number of
hidden units in the lstm cells.

162
00:10:06,360 --> 00:10:12,730
So now that I have all that done, I can
send all of the hidden outputs through

163
00:10:13,860 --> 00:10:18,649
some weights and biases and
do softmax, basically.

164
00:10:18,649 --> 00:10:20,506
So here I'm just calculating my logits.

165
00:10:20,506 --> 00:10:24,819
So you just do your matrix
multiplication of the output and

166
00:10:24,820 --> 00:10:28,439
the softmax weights and
add the softmax biases.

167
00:10:28,438 --> 00:10:31,596
And it pass through softmax
to the predictions.

168
00:10:31,596 --> 00:10:35,038
So here I'm calculating the loss now.

169
00:10:35,038 --> 00:10:39,168
So, again,
I look to reshaping my targets so

170
00:10:39,168 --> 00:10:43,750
that they match the same
kind of form as our logits.

171
00:10:43,750 --> 00:10:50,990
because remember logits, each row of
this array is one output for one step.

172
00:10:50,990 --> 00:10:55,509
So we're going to compare each output
of a step to the target at that step.

173
00:10:55,509 --> 00:10:56,939
And that's exactly
what we're doing here.

174
00:10:56,940 --> 00:11:00,260
So we're doing
softmax_cross_entropy_with_logits.

175
00:11:00,259 --> 00:11:05,470
So our logits and
our labels here calculate the cost.

176
00:11:05,470 --> 00:11:08,700
So we just reduce the means,
so just calculate the mean for

177
00:11:08,700 --> 00:11:13,310
the losses for
each of the steps in the sequence.

178
00:11:14,970 --> 00:11:18,790
So here you might remember
me talking about how

179
00:11:18,789 --> 00:11:22,699
gradients can explode in
a recurrent neural network.

180
00:11:22,700 --> 00:11:27,150
The easiest way to fix this is
that you just clip the gradients.

181
00:11:27,149 --> 00:11:31,312
So if the gradient is above 5,
you just set it to 5,

182
00:11:31,312 --> 00:11:35,205
and that's kind of what
this block of code does.

183
00:11:35,205 --> 00:11:38,018
It's a pretty simple thing to do,
that's nice.

184
00:11:38,018 --> 00:11:41,800
And here I'm just exporting the nodes
that I want, because the whole thing is

185
00:11:41,799 --> 00:11:44,469
just a function right, so
I just run the function.

186
00:11:45,559 --> 00:11:48,279
Then I export the nodes that I want, so

187
00:11:48,279 --> 00:11:51,346
that I can use them
later during training.

188
00:11:51,346 --> 00:11:56,110
Right, and here
are the hyperparameters I'm setting.

189
00:11:56,110 --> 00:11:59,932
So batch_size and number of steps in
the sequence that I'm looking at,

190
00:11:59,932 --> 00:12:00,879
the lstm_size.

191
00:12:00,879 --> 00:12:04,165
So this is the number of hidden
units in each lstm layer, and

192
00:12:04,166 --> 00:12:06,483
this is the numbers of layers I'm using.

193
00:12:06,482 --> 00:12:11,929
So then the more data you have,
the more parameters you need.

194
00:12:11,929 --> 00:12:16,060
And so you cam up your parameters
by including more layers and

195
00:12:16,061 --> 00:12:19,220
by increasing the size
of your hidden layers.

196
00:12:20,559 --> 00:12:23,109
Okay, and now we're training.

197
00:12:23,110 --> 00:12:27,110
So the idea here is that we
have all of our epochs, so

198
00:12:27,110 --> 00:12:30,350
I'm just going to use 20,
and this is just for saving.

199
00:12:31,820 --> 00:12:33,830
Don't worry about it too much.

200
00:12:33,830 --> 00:12:38,318
So here this is pretty typical,

201
00:12:38,317 --> 00:12:43,059
we just get our batches of data,
create your feed dictionary.

202
00:12:43,059 --> 00:12:46,059
So here, my inputs are x,
my targets are y.

203
00:12:46,059 --> 00:12:48,719
And for
training the dropout keep probability,

204
00:12:48,720 --> 00:12:50,705
I'm going to keep it at 0.5.

205
00:12:50,705 --> 00:12:54,509
new_state is being initialized, so
I'm taking the initial_state and

206
00:12:54,509 --> 00:12:58,220
I'm passing it as the initial_state for
the rnn.

207
00:12:58,220 --> 00:13:03,920
So what's going to happen is that for
every batch, I'm going to get the state

208
00:13:03,919 --> 00:13:08,089
after a batch, and I'm just going to
route it back into the next batch.

209
00:13:08,090 --> 00:13:10,860
So in this way, you can keep your state

210
00:13:10,860 --> 00:13:15,425
going through each batch as you're
going through the training.

211
00:13:15,424 --> 00:13:19,999
Session run, so getting the cost,
getting the final_state, which is I'm

212
00:13:19,999 --> 00:13:22,204
going to use the new_state for
the initial_state for the next batch.

213
00:13:22,205 --> 00:13:28,314
And then the optimizer, and so this is
what is actually reducing the loss for

214
00:13:28,313 --> 00:13:31,759
the network and changing the parameters.

215
00:13:33,700 --> 00:13:38,780
And then down here is just
the validation part, pretty simple.

216
00:13:38,779 --> 00:13:42,850
So again, I'm going to provide this
notebook for you so you can look through

217
00:13:42,850 --> 00:13:46,740
it and see how it all works, and
just kind of work it out for yourself.

218
00:13:46,740 --> 00:13:51,537
So this is some of the output for
how it trains.

219
00:13:51,537 --> 00:13:56,937
And I'm running this on a NVIDIA 1070,
and

220
00:13:56,937 --> 00:14:03,610
I get 0.13 seconds per
batch at these settings.

221
00:14:03,610 --> 00:14:06,370
Okay, so I trained it and

222
00:14:06,370 --> 00:14:10,600
now we can actually see what the results
are, and they're pretty cool.

223
00:14:10,600 --> 00:14:15,480
So first off, I wrote this
function here, so it's pick_top_n.

224
00:14:15,480 --> 00:14:19,340
So basically we have something
like 65 different characters.

225
00:14:19,340 --> 00:14:23,530
And so every time we try to predict
what the next character's going to be,

226
00:14:23,529 --> 00:14:25,740
it's going to be one of these 65.

227
00:14:25,740 --> 00:14:28,930
So to make it more reliable,
what I did is I said, okay,

228
00:14:28,929 --> 00:14:33,389
well let's just look at the 5
most likely, or you can set this.

229
00:14:34,549 --> 00:14:39,909
Just look at the most likely characters
and then pick from those, right?

230
00:14:39,909 --> 00:14:42,136
So I'm saying, basically, for 5,

231
00:14:42,136 --> 00:14:47,500
what are the 5 most likely characters to
come next, and then choose from those.

232
00:14:47,500 --> 00:14:52,590
So with sampling, the whole idea
is that you pass in a character,

233
00:14:52,590 --> 00:14:56,490
and then your network will predict for
you what the next character will be.

234
00:14:56,490 --> 00:14:59,743
And then you take that character, pass
it back in, and you get the next one.

235
00:14:59,743 --> 00:15:02,114
And you take that, you pass it back in,
and you get the next one.

236
00:15:02,114 --> 00:15:03,988
And so
you can just build up text this way.

237
00:15:03,988 --> 00:15:07,720
You're just continuously getting new
characters, passing it back into your

238
00:15:07,720 --> 00:15:11,759
network and then it all just works
out and just makes a cool thing.

239
00:15:11,759 --> 00:15:16,450
In here I'm also priming it,
so when I'm doing this,

240
00:15:16,450 --> 00:15:20,565
I'm passing in characters,
so in this case, Far.

241
00:15:20,565 --> 00:15:23,765
And it actually starts
generating the state for you, so

242
00:15:23,764 --> 00:15:27,615
that the rest of the characters
have something to be based on.

243
00:15:27,615 --> 00:15:30,815
You're starting to sequence
going through your network.

244
00:15:30,815 --> 00:15:34,025
So again, read up on this,

245
00:15:34,024 --> 00:15:36,544
go through this notebook, and
see how all of this all works.

246
00:15:37,649 --> 00:15:41,689
And this is what we get out of it.

247
00:15:41,690 --> 00:15:46,440
So this is after 3,560 iterations,
so not epochs but iterations, so

248
00:15:46,440 --> 00:15:49,090
going through all the batches and
the epochs.

249
00:15:50,340 --> 00:15:53,590
This network learned English and

250
00:15:53,590 --> 00:15:58,420
learned sentence structure and question
marks and to put quotes around stuff.

251
00:15:58,419 --> 00:16:02,799
And put commas before a quote and
a space, and it's really impressive.

252
00:16:02,799 --> 00:16:07,209
I just went from nothing and
then it learned English, really cool.

253
00:16:07,210 --> 00:16:11,050
However, mostly it doesn't
really make a lot of sense.

254
00:16:11,049 --> 00:16:14,589
It can get sentence structure and
you can figure out words, but

255
00:16:14,590 --> 00:16:19,110
actually making sense as a sentence,
it doesn't do that very well.

256
00:16:19,110 --> 00:16:23,050
He could not trouble to his wife, and
there was anything in them of the side

257
00:16:23,049 --> 00:16:27,019
of his weaky in the creature
at his forteren to him.

258
00:16:28,292 --> 00:16:30,889
I mean,
most of the words make sense, but

259
00:16:30,889 --> 00:16:35,799
as you're going through it, the whole
sentence doesn't really cohere.

260
00:16:35,799 --> 00:16:40,603
So it's pretty cool that it can learn
English words and sentence structure and

261
00:16:40,604 --> 00:16:41,783
spaces and stuff.

262
00:16:41,783 --> 00:16:45,934
There's apostrophes s,
like it knows this stuff, but

263
00:16:45,934 --> 00:16:50,189
it's not really understanding
sentences and meaning.

264
00:16:51,830 --> 00:16:56,840
So now I'm showing you this is
after the 200th iteration, showing

265
00:16:56,840 --> 00:17:01,629
you what the network knows as it's
training, which is pretty interesting.

266
00:17:01,629 --> 00:17:05,944
So after 200 iterations, it actually
knows where to put spaces, right?

267
00:17:05,944 --> 00:17:10,463
So it's, okay, I got a little word,
space, little word, space, word, space.

268
00:17:10,463 --> 00:17:13,806
Yeah and it's pretty impressive,
this is not all that much training and

269
00:17:13,806 --> 00:17:16,200
it's understanding that kind of stuff.

270
00:17:16,200 --> 00:17:18,750
It doesn't really know any words yet.

271
00:17:18,750 --> 00:17:24,150
It knows a few short words,
and, out, as, the, etc.

272
00:17:25,700 --> 00:17:29,640
Then at 600 iterations,
it's looking a little better.

273
00:17:29,640 --> 00:17:33,690
We're getting paragraph structure,
it's learning longer words, or

274
00:17:33,690 --> 00:17:35,890
it's kind of making up longer words.

275
00:17:35,890 --> 00:17:42,230
And it knows more short words, so we've
got some here, it's looking pretty good.

276
00:17:42,230 --> 00:17:44,660
So finally at 1,000 iterations,

277
00:17:44,660 --> 00:17:50,300
we see that it has learned how
to use quotation marks, right?

278
00:17:50,299 --> 00:17:53,701
So quotation, so it's somebody speaking
in the book or the dialogue and

279
00:17:53,701 --> 00:17:56,369
then a comma and another quotation.

280
00:17:56,369 --> 00:18:01,239
I'm going to provide this notebook for
you, and this is not the final form.

281
00:18:01,240 --> 00:18:05,460
This is still just
kind of me working on it.

282
00:18:05,460 --> 00:18:09,509
I'm going to keep it on our
GitHub Repo and make updates to it and

283
00:18:09,509 --> 00:18:11,029
make it nice and easy to use.

284
00:18:12,140 --> 00:18:16,090
And feel free to take it and
build off of it and

285
00:18:16,089 --> 00:18:17,789
make your own recurrent neural network

