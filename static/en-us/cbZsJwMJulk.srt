1
00:00:00,200 --> 00:00:02,680
So we have seen that performing these atomic read

2
00:00:02,680 --> 00:00:06,330
write operations on the lock repeatedly even while waiting

3
00:00:06,330 --> 00:00:09,150
for the lock is inefficient in terms of energy

4
00:00:09,150 --> 00:00:11,540
and it actually slows down the thread that is in

5
00:00:11,540 --> 00:00:14,490
the critical section. We want to make this more

6
00:00:14,490 --> 00:00:16,940
efficient and one way of doing that is to do

7
00:00:16,940 --> 00:00:19,670
a so-called test and atomic op approach in the

8
00:00:19,670 --> 00:00:25,510
locking function. So the original approach to using for example,

9
00:00:25,510 --> 00:00:31,610
an atomic exchange is to set R1 to 1, and as long as R1 is 1, try to swap

10
00:00:31,610 --> 00:00:38,280
between lock var, eventually we see a 0 and we exit. But as we now know, this

11
00:00:38,280 --> 00:00:41,711
keeps invalidating everybody else who is waiting on the

12
00:00:41,711 --> 00:00:43,974
lock, so we are getting a lot of cache

13
00:00:43,974 --> 00:00:47,382
misses while waiting. And active waiting like this is

14
00:00:47,382 --> 00:00:50,550
not only spending energy on the bus transfers all the

15
00:00:50,550 --> 00:00:52,998
time, but also keeps the bus busy so that

16
00:00:52,998 --> 00:00:55,590
useful work in other trays that are not busy

17
00:00:55,590 --> 00:00:58,630
waiting, you slow down. So this is kind of

18
00:00:58,630 --> 00:01:01,708
like a repeated atomic op approach. To make it

19
00:01:01,708 --> 00:01:04,459
more efficient, what we do is we do the

20
00:01:04,459 --> 00:01:08,330
same type of looping here, but we don't immediately

21
00:01:08,330 --> 00:01:11,790
try to swap. What we do first is use

22
00:01:11,790 --> 00:01:15,940
normal loads to wait for lockvar to become zero.

23
00:01:15,940 --> 00:01:19,440
So the idea is that were using normal loads

24
00:01:19,440 --> 00:01:23,760
while lockvar is busy. If we observe that its free,

25
00:01:23,760 --> 00:01:26,000
we try to grab it by doing an atomic

26
00:01:26,000 --> 00:01:29,351
exchange. We cannot try to grab it using the normal

27
00:01:29,351 --> 00:01:32,400
store, that's why we have to do the exchange.

28
00:01:32,400 --> 00:01:34,170
Bu the idea is that there is no point in

29
00:01:34,170 --> 00:01:37,610
trying to do the exchange unless the lock is actually

30
00:01:37,610 --> 00:01:41,200
free and the purpose of the exchange now is solely

31
00:01:41,200 --> 00:01:45,600
to ensure that only one of us grabs the lock once it becomes free. But the busy

32
00:01:45,600 --> 00:01:48,525
waiting while the lock is continuously free is

33
00:01:48,525 --> 00:01:51,846
done using normal reads. So now if we have

34
00:01:51,846 --> 00:01:54,634
core zero, one, and two, and core zero

35
00:01:54,634 --> 00:01:58,242
succeeds in doing an exchange, and gets the lockvar

36
00:01:58,242 --> 00:02:02,390
in the modified state. Core one will now reach

37
00:02:02,390 --> 00:02:06,480
this point and read lockvar. See that it's one.

38
00:02:06,480 --> 00:02:09,699
It's done using a normal load, so here we're

39
00:02:09,699 --> 00:02:12,260
moved to the own state, here we are moved

40
00:02:12,260 --> 00:02:14,966
to the shared state. And there is a cache

41
00:02:14,966 --> 00:02:18,068
to cache transfer here. Now we will keep doing

42
00:02:18,068 --> 00:02:21,032
this, but we'll keep getting cache hits so we

43
00:02:21,032 --> 00:02:25,420
no longer access the bus while waiting. Similarly core

44
00:02:25,420 --> 00:02:28,869
two will load the lockvar. It will also get

45
00:02:28,869 --> 00:02:33,180
the value and have it in the shared state.

46
00:02:33,180 --> 00:02:35,150
And now it also gets cached hits while

47
00:02:35,150 --> 00:02:38,610
waiting. So as long as core zero keeps the

48
00:02:38,610 --> 00:02:41,410
lock busy here, these two cores are no

49
00:02:41,410 --> 00:02:45,210
longer generating bus traffic. So core zero basically gets

50
00:02:45,210 --> 00:02:47,320
the entire coherence bus to itself so it

51
00:02:47,320 --> 00:02:50,950
can very quickly service cache misses. And cache hits

52
00:02:50,950 --> 00:02:54,900
here are much more efficient than bus accesses.

53
00:02:54,900 --> 00:02:58,260
And this is going to continue until core zero

54
00:02:58,260 --> 00:03:00,970
releases the lock, at which point, there is an

55
00:03:00,970 --> 00:03:04,640
invalidation, and these two will read again, see a

56
00:03:04,640 --> 00:03:07,150
value of one, try to compete for it using

57
00:03:07,150 --> 00:03:09,744
the exchange and so on. But those bursts of

58
00:03:09,744 --> 00:03:13,332
activity really happen only when they're actually trying to

59
00:03:13,332 --> 00:03:15,910
acquire the lock. As long as the lock is

60
00:03:15,910 --> 00:03:18,850
really busy, they just keep using normal loads in

61
00:03:18,850 --> 00:03:22,210
this loop. So this is much more energy efficient.
