1
00:00:00,000 --> 00:00:02,000
So let's do a little demonstration of this idea.

2
00:00:02,000 --> 00:00:05,000
We've got a little flight graph from one of the previous examples,

3
00:00:05,000 --> 00:00:07,000
and we can compute the clustering coefficient for any node.

4
00:00:07,000 --> 00:00:11,000
Let's say node 2. We're going to print the clustering coefficient for node 2.

5
00:00:11,000 --> 00:00:16,000
And then what we're going to do is actually randomly pick two of these neighbors.

6
00:00:16,000 --> 00:00:20,000
Now, I ended up having to go a little bit crazy here because you need to pick the neighbors

7
00:00:20,000 --> 00:00:24,000
without replacement meaning that we pick one neighbor and then the second neighbor

8
00:00:24,000 --> 00:00:26,000
we pick has to be different from the first one.

9
00:00:26,000 --> 00:00:29,000
So I went out of my way a little bit and said okay here's what we're going to do.

10
00:00:29,000 --> 00:00:32,000
We're going to run through all of the neighbors of v—these two.

11
00:00:32,000 --> 00:00:37,000
And for each of them, we're going to stick them into an array called the index.

12
00:00:37,000 --> 00:00:40,000
And so instead of randomly choosing from this set, we're going to randomly choose

13
00:00:40,000 --> 00:00:43,000
indices from this array and that will make it easier to make sure we don't repeat.

14
00:00:43,000 --> 00:00:46,000
So d is going to be the degree of node V.

15
00:00:46,000 --> 00:00:51,000
And as long as that degree is greater than 1 we're going to pick a random neighbor

16
00:00:51,000 --> 00:00:56,000
so that now the neighbor here is being chosen by its ordering from 0 to d-1.

17
00:00:56,000 --> 00:01:01,000
So v1 is the neighbor—the actual node name associated with that pick and then we do

18
00:01:01,000 --> 00:01:06,000
a second pick and this time we're going to do—we're going to again choose from 1 to d-1.

19
00:01:06,000 --> 00:01:11,000
We're going to add that to the pick that we already got with modular d to make it wrap around.

20
00:01:11,000 --> 00:01:14,000
So this is going to make sure that we're going to pick something that is different

21
00:01:14,000 --> 00:01:17,000
from what we just picked and we look up the corresponding ID.

22
00:01:17,000 --> 00:01:19,000
So we've got v1 and v2 are the two neighbors.

23
00:01:19,000 --> 00:01:22,000
We check whether they're connected and we add 1 to the total if they are.

24
00:01:22,000 --> 00:01:26,000
We repeat this whole loop a thousand times and at each point in time

25
00:01:26,000 --> 00:01:29,000
we take the total—the total number of times things were connected

26
00:01:29,000 --> 00:01:31,000
divided by the total number that we tried.

27
00:01:31,000 --> 00:01:36,000
And I claim that it ought to be the case that this number converges to

28
00:01:36,000 --> 00:01:38,000
the actual clustering coefficient.

29
00:01:38,000 --> 00:01:43,000
So let's try it. So in this particular case the clustering coefficient for node v is 0.3.

30
00:01:43,000 --> 00:01:46,000
And now let's watch what happens to the estimate.

31
00:01:46,000 --> 00:01:50,000
Well, certainly when there's only one sample that we've done so far

32
00:01:50,000 --> 00:01:53,000
it's either going to be 1 or 0; it's not going to be very close to 0.3.

33
00:01:53,000 --> 00:01:56,000
But as we repeat this over and over again you can see it's kind of settling in.

34
00:01:56,000 --> 00:01:59,000
It actually hit 0.3 briefly on the 48th try.

35
00:01:59,000 --> 00:02:03,000
But it's kind of bouncing around a little bit above 3, a little bit below 3,

36
00:02:03,000 --> 00:02:07,000
and the longer we run this the kind of less it seems to be changing—

37
00:02:07,000 --> 00:02:14,000
0.26, 0.27, 0.26, 0.27—Oh, it wandered up to 0.3 again and ended up at 0.31.

38
00:02:14,000 --> 00:02:19,000
So if we do this long enough and repeat this in enough times what we'll find is that

39
00:02:19,000 --> 00:02:23,000
the number really is 0.3 but in any given run it's going to be a little bit more, a little bit less.

40
00:02:23,000 --> 00:02:26,000
The longer that it runs, the more digits are going to be correct.

41
00:02:26,000 --> 00:02:31,000
And so in this particular case, this is a terrible approximation because it's actually not very

42
00:02:31,000 --> 00:02:35,000
close to the real answer and it took us a long time to get this extremely bad answer.

43
00:02:35,000 --> 00:02:38,000
There's some nice mathematical tricks like the Chernoff bound that tells us

44
00:02:38,000 --> 00:02:42,000
how many times we have to do a random sample as a function of the variance

45
00:02:42,000 --> 00:02:46,000
of the distribution before we can get an estimate of a particular level of accuracy

46
00:02:46,000 --> 00:02:48,000
with a particular level of certainty.

47
00:02:48,000 --> 00:02:51,000
So if you want to be really careful about this, you can actually look up

48
00:02:51,000 --> 00:02:55,000
the Chernoff bound and apply it to figure out how many samples are needed

49
00:02:55,000 --> 00:02:59,000
but for current purposes run it as long as you can run it and hope for the best.

50
00:02:59,000 --> 00:03:02,000
All right. So that's really all I wanted to go over in Unit 5

51
00:03:02,000 --> 00:03:06,000
and then we pick it up again in Unit 6 talking about computational complexity

52
00:03:06,000 --> 00:03:09,000
and its relation to social network analysis. See you then.
