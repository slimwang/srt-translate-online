1
00:00:00,230 --> 00:00:06,130
So a distributed loss level cache
is logically a single cache in that

2
00:00:06,130 --> 00:00:11,500
a data block will not be replicated
like it would be in private caches,

3
00:00:11,500 --> 00:00:14,310
where if two or three or four cores, for

4
00:00:14,310 --> 00:00:18,040
example, read a block, each of
their caches would have that block.

5
00:00:18,040 --> 00:00:19,100
And thus,

6
00:00:19,100 --> 00:00:23,910
really the total capacity of the caches
is not how much memory we are covering,

7
00:00:23,910 --> 00:00:28,410
because some memory appears many times,
possibly in these caches.

8
00:00:28,410 --> 00:00:32,710
A last level cache that is distributed
is logically still one cache,

9
00:00:32,710 --> 00:00:35,610
meaning if it's capacity is
let's say four megabytes,

10
00:00:35,610 --> 00:00:40,350
then we really cover four megabytes
of memory without any replication.

11
00:00:40,350 --> 00:00:45,250
But this distributed cache is sliced
up so that each tile, which contains

12
00:00:45,250 --> 00:00:50,580
a core and possibly local caches,
now also contains a part of this cache.

13
00:00:50,580 --> 00:00:57,670
So now what we have is in our mash,
we have a tile that contains a core,

14
00:00:57,670 --> 00:01:03,570
a level one cache, a level two cache,
and a slice of the L3 cache.

15
00:01:03,570 --> 00:01:08,123
So if this slice here that we
use per core is let's say 1MB,

16
00:01:08,123 --> 00:01:12,771
is really N x 1MB where N
is the number of cores, so

17
00:01:12,771 --> 00:01:16,360
now as you can see,
if we use four cores,

18
00:01:16,360 --> 00:01:21,087
we get 4MB, with 16 cores we have 16MB,
so we get what we want.

19
00:01:21,087 --> 00:01:26,120
The big cache that is there
to prevent memory accesses

20
00:01:26,120 --> 00:01:31,290
is growing with the number of cores, but
it doesn't have a single entry point.

21
00:01:31,290 --> 00:01:34,920
In fact,
each slice has its own entry point.

22
00:01:34,920 --> 00:01:37,840
So if we want the data
that is in this slice,

23
00:01:37,840 --> 00:01:40,630
we have to route our
message to that slice.

24
00:01:40,630 --> 00:01:43,110
If we want the data that
resides in another slice,

25
00:01:43,110 --> 00:01:45,210
we send the message there.

26
00:01:45,210 --> 00:01:50,420
If the data is spread among
the slices so that all

27
00:01:50,420 --> 00:01:54,850
of them get equal demands for the data,
then the natural traffic will be nicely

28
00:01:54,850 --> 00:01:59,690
distributed across our chip, so that
none of the links get over utilized.

29
00:01:59,690 --> 00:02:03,980
But if we have a level two
miss in a particular core,

30
00:02:03,980 --> 00:02:09,460
how do we know which slice of the level
three cache to ask for the data?

31
00:02:09,460 --> 00:02:12,390
One simple solution is
to spread the cache,

32
00:02:12,390 --> 00:02:16,440
a round robin among slices,
by cache index.

33
00:02:16,440 --> 00:02:19,677
So if we have our level
three cache with let's say,

34
00:02:19,677 --> 00:02:24,420
1,024 sets slice zero gets set zero.

35
00:02:24,420 --> 00:02:31,170
Slice one gets set one, etc., and
then let's say that we have eight cores.

36
00:02:31,170 --> 00:02:33,790
Then slice seven gets set seven.

37
00:02:33,790 --> 00:02:36,390
Slice zero gets set eight.

38
00:02:36,390 --> 00:02:39,090
Set nine is in slice one again and
so on.

39
00:02:39,090 --> 00:02:43,020
So now we can relatively easily,
after we break down the address to

40
00:02:43,020 --> 00:02:47,490
access the cache,
we know the set number, and

41
00:02:47,490 --> 00:02:52,970
the least significant tree bits in
this case tell us which slice to ask.

42
00:02:52,970 --> 00:02:58,010
If we have even more cores, then the
number of sets in the cache will grow,

43
00:02:58,010 --> 00:03:02,300
and as a result the number of bits
that we need to tell us which slice we

44
00:03:02,300 --> 00:03:06,850
have is growing and that means that the
number of bits in the index are growing,

45
00:03:06,850 --> 00:03:10,690
and we will need additional bits
to tell us in which set to look.

46
00:03:10,690 --> 00:03:14,420
Know that if we sequentially access
data, then we would be really be looking

47
00:03:14,420 --> 00:03:18,290
at one set and then the next and
then the next set, and

48
00:03:18,290 --> 00:03:22,120
this nicely spreads the load among
the slices of the level three cache.

49
00:03:22,120 --> 00:03:25,110
Because sequential axises
are basically going to be spread

50
00:03:25,110 --> 00:03:26,520
across the entire shape.

51
00:03:26,520 --> 00:03:31,710
However, the spreading of blocks
round robin across the entire chip

52
00:03:31,710 --> 00:03:36,689
may not be good for locality,
because a core is just as likely to

53
00:03:36,689 --> 00:03:40,740
access something that is in the opposite
corner of the chip, as it is to access

54
00:03:40,740 --> 00:03:45,320
its own slice in which case there is
no on chip network traffic whatsoever.

55
00:03:45,320 --> 00:03:49,949
So another way that can help with that
is that we spread the data around, but

56
00:03:49,949 --> 00:03:52,313
this time we spread it by page number.

57
00:03:52,313 --> 00:03:57,605
So all of the blocks that belong to the
same page would end up in the same slice

58
00:03:57,605 --> 00:04:03,233
of the level three, that helps because
now the operating system can map pages,

59
00:04:03,233 --> 00:04:06,840
so that it makes the accesses
to L3 more local.

60
00:04:06,840 --> 00:04:11,290
For example here, this core is
running a thread that has a stack.

61
00:04:11,290 --> 00:04:15,100
The pages that belong to that stack
should be those that end up in

62
00:04:15,100 --> 00:04:15,610
this slice.

63
00:04:15,610 --> 00:04:17,390
So that if you have a level two miss,

64
00:04:17,390 --> 00:04:22,540
we are very likely to access our
own local slice of the L3 cache.

65
00:04:22,540 --> 00:04:26,260
Of course, some of the shared
data will be elsewhere, but

66
00:04:26,260 --> 00:04:31,440
this dramatically improves the locality
in terms of how far do we need to go

67
00:04:31,440 --> 00:04:33,210
to get to the correct slice of L3.

68
00:04:34,320 --> 00:04:37,360
Any data that tends to
be accessed by one core

69
00:04:37,360 --> 00:04:39,860
can be now mapped by
the operating system so

70
00:04:39,860 --> 00:04:44,670
that it's usually found in the local
slice of the last level cache.
