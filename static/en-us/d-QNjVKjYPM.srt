1
00:00:00,340 --> 00:00:04,180
Here's F Michael, so
just like I wrote R is an expectation

2
00:00:04,180 --> 00:00:07,190
over the set of rewards that
you're going to see after k steps.

3
00:00:07,190 --> 00:00:08,960
I'm going to write F in the same way.

4
00:00:08,960 --> 00:00:11,870
And I've sort of got some
really funky notation here but

5
00:00:11,870 --> 00:00:13,170
let me try to explain it.

6
00:00:13,170 --> 00:00:18,095
In English,
F is simply the [LAUGH] expectation of

7
00:00:18,095 --> 00:00:22,140
me in ending up in s
prime after k steps.

8
00:00:22,140 --> 00:00:25,090
So this Delta is because I can't
draw lowercase deltas well,

9
00:00:25,090 --> 00:00:26,808
this is a delta function.

10
00:00:26,808 --> 00:00:31,220
It is one in the case where
the kth state that I see

11
00:00:31,220 --> 00:00:34,200
is actually equal to s prime,
and zero otherwise.

12
00:00:34,200 --> 00:00:39,930
And Delta to the k is just, well the
discount factor raised to the kth power.

13
00:00:39,930 --> 00:00:44,110
So, F is an expectation over after
I've executed the option that

14
00:00:44,110 --> 00:00:46,550
I will end up in state s prime.

15
00:00:46,550 --> 00:00:50,170
Having started in state s and
having taken k steps.

16
00:00:50,170 --> 00:00:52,430
Now notice, why do I even
have to go through all this?

17
00:00:52,430 --> 00:00:56,160
Why couldn't I just put T of s,o,
s prime here?

18
00:00:56,160 --> 00:00:59,140
Well first off, T is defined in terms
of A is not defined in terms of o,

19
00:00:59,140 --> 00:01:00,480
but that's okay.

20
00:01:00,480 --> 00:01:02,570
The reason I have to do this
little funky thing, and

21
00:01:02,570 --> 00:01:04,629
why we call it F instead of T,
is because.

22
00:01:04,629 --> 00:01:07,910
As you pointed out in the beginning,
there needs to be a discount factor

23
00:01:07,910 --> 00:01:13,270
attached to the value of the next state,
which, in this case, is s prime.

24
00:01:13,270 --> 00:01:17,130
So what we've done is, we've rolled
up that discount factor inside of F.

25
00:01:17,130 --> 00:01:21,360
So F is the expectation
of me starting in s,

26
00:01:21,360 --> 00:01:25,600
taking option zero,
ending up in s prime after k steps.

27
00:01:25,600 --> 00:01:29,409
And the Delta sub k gets sort of
brought along with that expectation so

28
00:01:29,409 --> 00:01:33,427
that I can appropriately discount the s
prime state that I will end up in.

29
00:01:33,427 --> 00:01:34,334
>> Hm, okay.

30
00:01:34,334 --> 00:01:37,988
>> Right, so in the end, both R and
F are just expectations,

31
00:01:37,988 --> 00:01:42,241
they're exactly doing what they
were doing in the original Bellman

32
00:01:42,241 --> 00:01:44,573
equation that we were using before.

33
00:01:44,573 --> 00:01:47,869
In the original Bellman equation
the reward was just the immediate reward

34
00:01:47,869 --> 00:01:49,580
that we got for taking an action here.

35
00:01:49,580 --> 00:01:54,515
The reward is just an expectation over
all the rewards that we're going to see,

36
00:01:54,515 --> 00:01:56,765
given that we've taken
this particular option.

37
00:01:56,765 --> 00:01:59,625
If the option is in
fact a single action,

38
00:01:59,625 --> 00:02:02,135
then it would be exactly the same
thing as a reward we would get for

39
00:02:02,135 --> 00:02:03,510
being in that state and
taking that action.

40
00:02:03,510 --> 00:02:07,756
Similarly, F is just the transition
function, except now it's sort of

41
00:02:07,756 --> 00:02:11,238
the accumulation of all
the transitions of starting in s and

42
00:02:11,238 --> 00:02:12,530
ending up in s prime.

43
00:02:12,530 --> 00:02:15,010
After k steps given that I took

44
00:02:15,010 --> 00:02:18,570
the particular actions that are inside
of the option, and dividing by pi.

45
00:02:18,570 --> 00:02:20,780
Now there's a couple of
neat facts about this and

46
00:02:20,780 --> 00:02:22,450
I will write them down
on the next slide.

47
00:02:22,450 --> 00:02:25,360
But I just want to point out what's
particularly cool about this,

48
00:02:25,360 --> 00:02:27,010
is that it really is a Bellman equation.

49
00:02:28,080 --> 00:02:32,130
It really does allow us to do all of
the things that the Bellman equation,

50
00:02:32,130 --> 00:02:33,920
Bellman operator allowed
us to do in MDPs.

51
00:02:35,470 --> 00:02:40,090
But it allows us to analyze or
to reason about or

52
00:02:40,090 --> 00:02:46,510
to plan over this multiple
variable time MDP.

53
00:02:46,510 --> 00:02:48,710
So, in fact,
this kind of MDP has a name,

54
00:02:48,710 --> 00:02:50,320
which I probably should
have written down earlier.

55
00:02:50,320 --> 00:02:55,860
And it's called SMDP, where the S
stands for, what do you think, Michael?

56
00:02:55,860 --> 00:02:56,910
Something?

57
00:02:56,910 --> 00:02:59,050
Yes, and that something is Semi, so

58
00:02:59,050 --> 00:03:01,810
this is called
a Semi-Markov Decision Process.

59
00:03:01,810 --> 00:03:03,920
And the difference between
the Markov Decision process and

60
00:03:03,920 --> 00:03:08,060
the Semi-Markov Decision Process is,
you get to make these jumps in time.

61
00:03:09,250 --> 00:03:12,580
And because we have this Bellman
operator that takes into account

62
00:03:12,580 --> 00:03:16,090
all the possible jumps in
time that you might take.

63
00:03:16,090 --> 00:03:20,080
In the end,
it grounds out to an underlying MDP and

64
00:03:20,080 --> 00:03:23,030
allows us to inherit all the things
that Bellman allowed us to in here.
