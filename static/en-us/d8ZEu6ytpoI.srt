1
00:00:00,440 --> 00:00:03,320
So I want to look at
HMMs in the context of

2
00:00:03,320 --> 00:00:05,500
what's referred to as
Gesture Recognition.

3
00:00:05,500 --> 00:00:09,250
It's sort of an old approach, an old
problem, especially using HMMs but it's,

4
00:00:09,250 --> 00:00:10,410
it's, it's a good paradigm.

5
00:00:11,560 --> 00:00:15,850
There's a conference called Face and
Gesture, in, Computer Vision.

6
00:00:15,850 --> 00:00:19,080
So, obviously gesture must be important
because there's a whole conference

7
00:00:19,080 --> 00:00:20,610
called Face and Gesture.

8
00:00:20,610 --> 00:00:23,400
And it's a long story going back to
college of mine about why face and

9
00:00:23,400 --> 00:00:24,940
gesture are put together.

10
00:00:24,940 --> 00:00:29,757
I'll tell it to you some other
time over drinks of some flavor.

11
00:00:29,757 --> 00:00:36,672
[COUGH] the typical scenario was you
do some examples of each gesture.

12
00:00:36,672 --> 00:00:40,135
The system learns or is trained to
have some sort of model for each.

13
00:00:40,135 --> 00:00:42,005
And like said lots of things used HMMs.

14
00:00:42,005 --> 00:00:46,885
And at run time one would compare the
output of each of the different model,

15
00:00:46,885 --> 00:00:50,570
you would evaluate what you saw in
terms of the likelihood sequence.

16
00:00:50,570 --> 00:00:53,210
And if that probability
exceeded some threshold,

17
00:00:53,210 --> 00:00:55,500
you'd say, uh-huh,
a particular gesture happened.

18
00:00:55,500 --> 00:00:57,380
So this was done, as I said awhile ago,

19
00:00:57,380 --> 00:00:59,730
sort of the mid-90s,
then it started to taper off.

20
00:00:59,730 --> 00:01:02,860
Because partially this whole
idea of having sort of

21
00:01:02,860 --> 00:01:07,630
individual trained gestures that people
would do to, to make something happen,

22
00:01:07,630 --> 00:01:11,430
that didn't feel like just the
recognition of individual vocabularies.

23
00:01:11,430 --> 00:01:13,320
So it's sort of tapered off.

24
00:01:13,320 --> 00:01:17,250
But then all of a sudden, new life was
injected into the field of gesture

25
00:01:17,250 --> 00:01:20,455
recognition and it was because
of human robotics interaction.

26
00:01:20,455 --> 00:01:21,640
Okay?

27
00:01:21,640 --> 00:01:26,910
People really love the idea of
being able to gesture to a robot.

28
00:01:26,910 --> 00:01:30,230
Which if you've ever been to my lab we
sometimes gesture to the robot in a not

29
00:01:30,230 --> 00:01:31,350
so friendly way.

30
00:01:31,350 --> 00:01:33,300
But this idea that you
could do things and

31
00:01:33,300 --> 00:01:37,400
control a robot, first of all,
speech is very difficult

32
00:01:37,400 --> 00:01:39,850
in a noisy environment unless
you're wearing a microphone.

33
00:01:40,892 --> 00:01:43,180
Gesture, you might be able to
do something more remotely.

34
00:01:43,180 --> 00:01:46,450
And it was just this general
idea that it was kind of cool,

35
00:01:46,450 --> 00:01:51,110
to be able to make some gestures and
have a robot respond appropriately.

36
00:01:51,110 --> 00:01:53,930
So and here I just pulled out a paper,
Reed 2014.

37
00:01:53,930 --> 00:01:57,420
This was at some conference
that just happened, and

38
00:01:57,420 --> 00:02:03,500
since I'm recording this in 2014, it's
a, you know, I'm not making this up.

39
00:02:03,500 --> 00:02:05,940
This is, you know,
what's currently going on.

40
00:02:05,940 --> 00:02:09,160
I'm not advocating one way or
the other the importance or

41
00:02:09,160 --> 00:02:11,000
whatever of this
particular piece of work.
