1
00:00:00,240 --> 00:00:06,330
So one way of speeding up hit times
is to overlap one hit with another,

2
00:00:06,330 --> 00:00:09,930
and we can achieve this, for example,
through pipelining the cache.

3
00:00:09,930 --> 00:00:13,860
If the cache takes
multiple cycles to access,

4
00:00:13,860 --> 00:00:18,870
we can have a situation where an access
come in cycle N, and it's a hit.

5
00:00:18,870 --> 00:00:23,778
And now if another access comes in
cycle N+1, and it would be a hit, too,

6
00:00:23,778 --> 00:00:28,940
in a non-pipeline cache, this second
access has to wait until the first

7
00:00:28,940 --> 00:00:32,750
access is done using the cache, and
it takes multiple cycles to do that.

8
00:00:32,750 --> 00:00:37,290
So in this situation, the hit time,
as seen by each access,

9
00:00:37,290 --> 00:00:42,310
is the actual hit time of a cache
plus the wait time that the access

10
00:00:42,310 --> 00:00:46,940
suffers because it cannot access the
cache until the previous one is done.

11
00:00:46,940 --> 00:00:50,650
And in that situation,
pipelining the cache so that we can send

12
00:00:50,650 --> 00:00:55,590
in accesses one after the other
will improve the overall hit time.

13
00:00:55,590 --> 00:00:59,011
Now it may sound straightforward
how to pipeline the cache.

14
00:00:59,011 --> 00:01:01,320
You just divide it in,
let's say, three stages.

15
00:01:01,320 --> 00:01:04,989
But how do you split
what amounts to basically

16
00:01:04,989 --> 00:01:08,990
a read from a large array because
that's really what the cache is?

17
00:01:08,990 --> 00:01:13,780
Remember that the cache access consists
of using the index part of the address

18
00:01:13,780 --> 00:01:14,870
to find the set.

19
00:01:15,890 --> 00:01:21,250
Reading out the tags and valid bits that
correspond to the blocks in that set.

20
00:01:21,250 --> 00:01:23,830
Comparing the tags, and
checking the valid bits for

21
00:01:23,830 --> 00:01:26,580
each of these to see
whether it has a hit.

22
00:01:26,580 --> 00:01:31,180
Combining these so that we know
whether we have an overall hit and

23
00:01:31,180 --> 00:01:32,850
where in our set.

24
00:01:32,850 --> 00:01:36,200
Once we know where,
we read out the data and

25
00:01:36,200 --> 00:01:41,460
use the offset to choose the right
part of the large cache block.

26
00:01:41,460 --> 00:01:44,300
At which point, we have the data
that the processor wants, and

27
00:01:44,300 --> 00:01:46,020
our cache access is done.

28
00:01:46,020 --> 00:01:51,020
So one example of a cache pipeline
would be to have this part,

29
00:01:51,020 --> 00:01:55,600
reading out the tags and so
on from the cache array, be stage one.

30
00:01:55,600 --> 00:01:57,740
Determining the hits and

31
00:01:57,740 --> 00:02:02,080
beginning the data read would be
stage two, and finishing the actual

32
00:02:02,080 --> 00:02:05,660
data read all the way to getting
the data would be stage three.

33
00:02:05,660 --> 00:02:10,740
So as you can see, we can pipeline
the cache access, even if we

34
00:02:10,740 --> 00:02:16,510
don't know how to actually break down
the reading from the cache array,

35
00:02:16,510 --> 00:02:21,540
especially if the tags and the valid
bits are read before we determine

36
00:02:21,540 --> 00:02:26,520
the hit, and we only read the data part
of the cache after we determine the hit.

37
00:02:26,520 --> 00:02:29,810
In that case,
just those two can be separate stages.

38
00:02:29,810 --> 00:02:34,340
Usually, the actual cache hit time for
level one caches will be one, two or

39
00:02:34,340 --> 00:02:35,875
three cycles.

40
00:02:35,875 --> 00:02:39,390
One-cycle caches don't need pipelining,
and two- and three-cycle

41
00:02:39,390 --> 00:02:43,330
caches can be relatively easily
pipelined into two or three stages.

42
00:02:43,330 --> 00:02:46,780
So usually,
level one caches will be pipelined.
