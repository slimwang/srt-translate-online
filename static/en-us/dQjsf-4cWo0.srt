1
00:00:00,000 --> 00:00:04,000
So let's remember the code we had at the end of unit 2 for crawling the web.

2
00:00:04,000 --> 00:00:09,000
So we used 2 variables. We initialized "tocrawl" to the seed, a list containing just the seed,

3
00:00:09,000 --> 00:00:12,000
and we're going to use "tocrawl" to keep track of the pages to crawl.

4
00:00:12,000 --> 00:00:19,000
We initialized "crawled" to the empty list, and we're keeping track of the pages we found using "crawled."

5
00:00:19,000 --> 00:00:23,000
Then we had a loop that would continue as long as there were pages left to crawl.

6
00:00:23,000 --> 00:00:25,000
We'd pop the last page off the "tocrawl" list.

7
00:00:25,000 --> 00:00:32,000
If it's not already crawled, then we'll union into "tocrawl" all the links that we can find on that page,

8
00:00:32,000 --> 00:00:36,000
and then we'll add that page to the list of pages we've already crawled.

9
00:00:36,000 --> 00:00:41,000
So now we want to figure out how to change this so instead of just finding all the URLs,

10
00:00:41,000 --> 00:00:42,000
we're building up our index.

11
00:00:42,000 --> 00:00:46,000
We're looking at the actual content of the pages, and we're adding it to our index.

12
00:00:46,000 --> 00:00:52,000
So the first change to make, we're updating the index, and we're going to change the return result.

13
00:00:52,000 --> 00:00:56,000
So instead of returning "crawled" what we want to return at the end is the index.

14
00:00:56,000 --> 00:01:02,000
`If we wanted to keep track of all the URLs crawled, we could still return "crawled" and return both "crawled" and "index,"

15
00:01:02,000 --> 00:01:05,000
but let's keep things simple and just return "index."

16
00:01:05,000 --> 00:01:09,000
That's what we really want for being able to respond to search queries.

17
00:01:09,000 --> 00:01:12,000
So now we have one other change to make, and this is the important one.

18
00:01:12,000 --> 00:01:19,000
We need to find a way to update the index to reflect all the words that are found on the page that we've just crawled.

19
00:01:19,000 --> 00:01:22,000
I'm going to make one change before we do that.

20
00:01:22,000 --> 00:01:28,000
Since both "get<u>all</u>links" and what we need to do to add the words to the index depend on the page,

21
00:01:28,000 --> 00:01:33,000
let's introduce a new variable and store the content of the page in that variable.

22
00:01:33,000 --> 00:01:37,000
This will save us from having to call "get_page" twice. "Get_page" is fairly expensive.

23
00:01:37,000 --> 00:01:40,000
It requires a web request to get the content of the page.

24
00:01:40,000 --> 00:01:45,000
It makes a lot more sense to store that in a new variable, and that will simplify this code.

25
00:01:45,000 --> 00:01:48,000
So now we just need to pass in content.

26
00:01:48,000 --> 00:01:51,000
So we have one missing statement,

27
00:01:51,000 --> 00:01:56,000
and I'll leave it to you to see if you can figure out statement we need there to finish the web crawler.

28
00:01:56,000 --> 00:01:59,000
When it's done, the result of "crawl-web," what we return as "index"

29
00:01:59,000 --> 00:02:02,000
should be an index of all the content we find starting from the seed.
