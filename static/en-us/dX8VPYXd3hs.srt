1
00:00:00,000 --> 00:00:03,000
Now here's where reinforcement learning comes into play:

2
00:00:03,000 --> 00:00:06,000
What if you don't know R--the Reward function?

3
00:00:06,000 --> 00:00:09,000
What if you don't even know P--the transition model of the world?

4
00:00:09,000 --> 00:00:12,000
Then you can't solve the Markov Decision Process

5
00:00:12,000 --> 00:00:14,000
because you don't have what you need to solve it.

6
00:00:14,000 --> 00:00:16,000
However, with reinforcement learning,

7
00:00:16,000 --> 00:00:19,000
you can learn R and P by interacting with the world

8
00:00:19,000 --> 00:00:22,000
or you can learn substitutes that will tell you

9
00:00:22,000 --> 00:00:26,000
as much as you know, so that you never actually have to compute with R and P.

10
00:00:26,000 --> 00:00:30,000
What you learn, exactly, depends on what you already know and what you want to do.

11
00:00:30,000 --> 00:00:32,000
So we have several choices.

12
00:00:32,000 --> 00:00:36,000
One choice is we can build a utility-based agent.

13
00:00:36,000 --> 00:00:41,000
So we're going to list agent types, based on what we know,

14
00:00:41,000 --> 00:00:43,000
what we want to learn,

15
00:00:43,000 --> 00:00:45,000
and what we then use once we've learned.

16
00:00:45,000 --> 00:00:47,000
So for a utility-based agent,

17
00:00:47,000 --> 00:00:51,000
if we already know T, the transition model,

18
00:00:51,000 --> 00:00:54,000
but we don't know R, the Reward model,

19
00:00:54,000 --> 00:00:57,000
then we can learn R--and use that,

20
00:00:57,000 --> 00:01:01,000
along with P, to learn our utility function;

21
00:01:01,000 --> 00:01:04,000
and then go ahead and use the utility function

22
00:01:04,000 --> 00:01:07,000
just as we did in normal Markov Decision Processes.

23
00:01:07,000 --> 00:01:09,000
So that's one agent design.

24
00:01:09,000 --> 00:01:11,000
Another design that we'll see in this Unit

25
00:01:11,000 --> 00:01:14,000
is called a Q-learning agent.

26
00:01:14,000 --> 00:01:17,000
In this one, we don't have to know P or R;

27
00:01:17,000 --> 00:01:22,000
and we learn a value function, which is usually denoted by Q.

28
00:01:22,000 --> 00:01:26,000
And that's a type of utility

29
00:01:26,000 --> 00:01:28,000
but, rather than being a utility over states,

30
00:01:28,000 --> 00:01:32,000
it's a utility of state action pairs--and that tells us:

31
00:01:32,000 --> 00:01:36,000
For any given state and any given action,

32
00:01:36,000 --> 00:01:38,000
what's the utility of that result--

33
00:01:38,000 --> 00:01:42,000
without knowing the utilities and rewards, individually?

34
00:01:42,000 --> 00:01:45,000
And then we can just use that Q directly.

35
00:01:45,000 --> 00:01:49,000
So we don't actually have to ever learn the transition model, P,

36
00:01:49,000 --> 00:01:51,000
with a Q-learning agent.

37
00:01:51,000 --> 00:01:54,000
And finally, we can have a reflex agent

38
00:01:54,000 --> 00:01:57,000
where, again, we don't need to know P and R to begin with;

39
00:01:57,000 --> 00:02:02,000
and we learn directly, the policy, pi of S;

40
00:02:02,000 --> 00:02:05,000
and then we just go ahead and apply pi.

41
00:02:05,000 --> 00:02:09,000
So it's called a reflex agent because it's pure stimulus response:

42
00:02:09,000 --> 00:02:11,000
I'm in a certain state, I take a certain action.

43
00:02:11,000 --> 00:02:15,000
I don't have to think about modeling the world, in terms of:

44
00:02:15,000 --> 00:02:17,000
What are the transitions--where am I going to go next?

45
00:02:17,000 --> 99:59:59,999
I just go ahead and take that action.
