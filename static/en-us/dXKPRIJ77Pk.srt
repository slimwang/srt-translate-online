1
00:00:00,340 --> 00:00:00,920
Okay, Michael.
So

2
00:00:00,920 --> 00:00:03,610
with that, I think we're kind
of done with this lesson.

3
00:00:03,610 --> 00:00:07,550
Those are the three things
that I wanted to mention and

4
00:00:07,550 --> 00:00:10,670
I think we can now go back
to where you left off and

5
00:00:10,670 --> 00:00:13,200
talk about what we have learned,
except I have the pen.

6
00:00:13,200 --> 00:00:15,500
And so
you have to tell me what we've learned.

7
00:00:15,500 --> 00:00:16,210
I don't have to tell you.

8
00:00:16,210 --> 00:00:16,710
>> This is true.

9
00:00:16,710 --> 00:00:18,560
So now when you see the three things,

10
00:00:18,560 --> 00:00:20,500
what were the three things
that you talked about?

11
00:00:20,500 --> 00:00:23,456
>> Your trying to trick me into
telling me what we learned.

12
00:00:23,456 --> 00:00:25,411
>> [LAUGH] Yeah a little bit.

13
00:00:25,411 --> 00:00:26,261
All right, so.

14
00:00:26,261 --> 00:00:26,941
All right.

15
00:00:26,941 --> 00:00:29,420
So when I had the pen, we talked about.

16
00:00:29,420 --> 00:00:32,380
Well two main things,
well the overall topic is the CCC.

17
00:00:32,380 --> 00:00:32,960
>> Right.

18
00:00:32,960 --> 00:00:35,595
>> Which is Comedy Central Committees.

19
00:00:35,595 --> 00:00:36,738
[SOUND]
>> No!

20
00:00:36,738 --> 00:00:39,150
>> Closed caption clowns.

21
00:00:39,150 --> 00:00:39,680
>> No.

22
00:00:39,680 --> 00:00:42,450
>> So, coordinating,
communicating, and coaching.

23
00:00:42,450 --> 00:00:43,050
>> Right.

24
00:00:43,050 --> 00:00:44,300
>> Ha!
>> I think what we learned is that

25
00:00:44,300 --> 00:00:45,440
alliteration can be fun.

26
00:00:45,440 --> 00:00:50,000
>> Well, sure, and in particular,
I talked about two particular models.

27
00:00:50,000 --> 00:00:52,394
One was the DEC POMD VP model.

28
00:00:52,394 --> 00:00:54,590
Which least at least let us define or

29
00:00:54,590 --> 00:00:58,640
ask questions about what it means to
coordinate and communicate optimally

30
00:00:58,640 --> 00:01:03,500
when we've got agents that don't share
a brain but they do share the reward

31
00:01:03,500 --> 00:01:07,280
function so what's good for each one
of them is good for the community.

32
00:01:07,280 --> 00:01:07,900
>> Right.

33
00:01:07,900 --> 00:01:10,430
Shared our distinct brains.

34
00:01:10,430 --> 00:01:12,240
>> I like that.

35
00:01:12,240 --> 00:01:15,020
>> Then we talked about
the inverse reinforcement learning

36
00:01:15,020 --> 00:01:19,090
idea which was that we could actually
use examples of behavior demonstrations

37
00:01:19,090 --> 00:01:23,030
of behavior to infer reward functions
and then once we have those reward

38
00:01:23,030 --> 00:01:25,465
functions we could try to
optimize them in other settings.

39
00:01:25,465 --> 00:01:28,180
>> Yep we go from behaviors to
rewards instead of going from

40
00:01:28,180 --> 00:01:29,890
rewards to behavior.

41
00:01:29,890 --> 00:01:32,070
And that is what makes it inverse.

42
00:01:32,070 --> 00:01:34,060
>> And having those rewards
means that it generalizes to

43
00:01:34,060 --> 00:01:34,970
other settings as well.

44
00:01:34,970 --> 00:01:38,860
>> Right, and I think that a point that
I made on that, when I started taking

45
00:01:38,860 --> 00:01:43,520
over the pen, was that this actually
is a kind of reward shaping,

46
00:01:43,520 --> 00:01:47,830
by getting human beings to give us
hints about what we ought to be doing.

47
00:01:47,830 --> 00:01:50,240
>> Okay, but
in the form of behavior demonstrations.

48
00:01:50,240 --> 00:01:52,080
But in the form of
behavior demonstrations.

49
00:01:52,080 --> 00:01:55,610
And that is a nice little
point worth making,

50
00:01:55,610 --> 00:01:59,270
which is demonstrations
are one way to do coaching.

51
00:01:59,270 --> 00:02:00,980
What were the other ways
we did some coaching?

52
00:02:00,980 --> 00:02:03,950
>> So,
yeah demonstrations is what we did and

53
00:02:03,950 --> 00:02:07,130
then there was this kind
of policy feedback policy.

54
00:02:07,130 --> 00:02:08,789
I guess you called it policy shaping.

55
00:02:08,789 --> 00:02:10,190
>> Right.
>> But it was a sort of idea that you

56
00:02:10,190 --> 00:02:12,680
can get feedback on the actions
being taken by the agent and

57
00:02:12,680 --> 00:02:13,900
that can be incorporated or

58
00:02:13,900 --> 00:02:18,130
combined with other suggestions as well
as what the learner is picking up on.

59
00:02:18,130 --> 00:02:22,270
>> And so between reward shaping,
demonstrations and policy shaping,

60
00:02:22,270 --> 00:02:28,070
we got a nice little set of ways that
human beings can communicate to agents.

61
00:02:28,070 --> 00:02:30,050
So let's see.
I know I did more than this.

62
00:02:30,050 --> 00:02:31,260
There were some examples that I used.

63
00:02:31,260 --> 00:02:32,560
What kind of examples did I use?

64
00:02:32,560 --> 00:02:35,215
>> Well, in the policy shaping setting,
we talked about Pac-Man.

65
00:02:35,215 --> 00:02:36,240
Pac-Man
>> Yeah.

66
00:02:36,240 --> 00:02:39,390
So we learned that Pac-Man is like
the one example you should use

67
00:02:39,390 --> 00:02:40,140
for everything.

68
00:02:40,140 --> 00:02:45,300
>> Sure except for what we did at
the end, end which was drama management.

69
00:02:45,300 --> 00:02:47,550
>> Right,
right we talked about drama management.

70
00:02:47,550 --> 00:02:49,260
And in particular, [LAUGH] yeah.

71
00:02:49,260 --> 00:02:51,820
In particular we introduced TTD-MDPs.

72
00:02:51,820 --> 00:02:56,470
Where TTD MDP has something to
do with trajectories and drama.

73
00:02:56,470 --> 00:02:57,900
>> Targets and stuff.

74
00:02:57,900 --> 00:02:58,720
That's exactly right.

75
00:02:58,720 --> 00:03:01,360
And I think we had a nice
little conversation here,

76
00:03:01,360 --> 00:03:04,630
tying some of these things
back to things we did before.

77
00:03:04,630 --> 00:03:08,940
We actually returned and
brought up again constraints and

78
00:03:08,940 --> 00:03:11,720
options as a way of doing coaching and
communication.

79
00:03:11,720 --> 00:03:16,100
And try to really drive home the point
that there are some human beings and

80
00:03:16,100 --> 00:03:19,060
they think a particular way and
then there are machines and

81
00:03:19,060 --> 00:03:20,870
they think a particular way.

82
00:03:20,870 --> 00:03:24,695
And we should try to move
the machines closer to the humans.

83
00:03:24,695 --> 00:03:27,455
Rather than forcing the humans to come
all the way over to the machines.

84
00:03:27,455 --> 00:03:28,565
>> Yeah I like that as a philosophy.

85
00:03:28,565 --> 00:03:30,225
>> And
that's really what I think is important.

86
00:03:30,225 --> 00:03:31,275
Yeah that's a good philosophy.

87
00:03:31,275 --> 00:03:33,515
Well I think that covers
everything Michael.

88
00:03:33,515 --> 00:03:34,695
>> Nice.
>> I feel pretty good about that.

89
00:03:34,695 --> 00:03:36,405
Nice, so does this mean we're done?

90
00:03:36,405 --> 00:03:38,855
I think this means we're done
with the class, kind of.

91
00:03:38,855 --> 00:03:40,405
>> Really?
So maybe we should do some kind of

92
00:03:40,405 --> 00:03:42,575
wrap up to see how
everything ties together.

93
00:03:42,575 --> 00:03:43,075
>> Yeah, let's do that.

94
00:03:43,075 --> 00:03:43,965
That way I don't have to say goodbye,

95
00:03:43,965 --> 00:03:47,980
why don't you come down to Atlanta and
maybe we can record some stuff together.

96
00:03:47,980 --> 00:03:49,120
>> Great I will have done that.

97
00:03:49,120 --> 00:03:51,382
>> Perfect.
Or is it perfect or

98
00:03:51,382 --> 00:03:53,834
is that the past perfect poo perfect.

99
00:03:53,834 --> 00:03:54,410
>> Poo perfect.

100
00:03:54,410 --> 00:03:56,335
>> Yes the perfect good perfect.

101
00:03:56,335 --> 00:03:58,938
Okay Michael,
well I will see you in Atlanta soon.

102
00:03:58,938 --> 00:03:59,851
>> Bye Charles.

103
00:03:59,851 --> 00:04:00,351
>> Bye.
