1
00:00:00,210 --> 00:00:01,970
Okay Michael so let me take you back to the pseudo code

2
00:00:01,970 --> 00:00:04,220
that we, we have for MIMIC and see if we can plugin some

3
00:00:04,220 --> 00:00:07,660
of the stuff that we we just talked about just as a way

4
00:00:07,660 --> 00:00:11,820
of refreshing. So as you recall, the goals to generate samples from some

5
00:00:11,820 --> 00:00:15,210
Pea soup theta sub T of X. So in this case were just

6
00:00:15,210 --> 00:00:19,500
going to estimate that by finding the best dependency tree we can, that is

7
00:00:19,500 --> 00:00:22,790
the best Pi function, okay? So we're going to be starting with some dependency

8
00:00:22,790 --> 00:00:25,220
tree And we're going to generate samples from that, and we know how to

9
00:00:25,220 --> 00:00:28,030
generate samples from that given a dependency tree

10
00:00:31,300 --> 00:00:33,040
like say this. We simply started a

11
00:00:33,040 --> 00:00:36,810
node, generate according to it's unconditional distribution and

12
00:00:36,810 --> 00:00:39,865
then generate samples from each according to it's

13
00:00:39,865 --> 00:00:43,090
conditional distribution given it's parents. So, first we

14
00:00:43,090 --> 00:00:44,960
generate a sample from here. Then we

15
00:00:44,960 --> 00:00:46,740
generate one from here, then from here, then

16
00:00:46,740 --> 00:00:48,970
from here, and then from here. Now where

17
00:00:48,970 --> 00:00:51,320
do we get these conditional probability tables, Michael?

18
00:00:51,320 --> 00:00:52,880
>> That's a good question. I think

19
00:00:52,880 --> 00:00:55,160
it's pretty direct from the mutual information.

20
00:00:55,160 --> 00:00:56,490
>> Exactly. In order to compute

21
00:00:56,490 --> 00:00:59,450
the mutual information, that is in order to compute the

22
00:00:59,450 --> 00:01:03,100
entropies, we have to know the probabilities. So, we generate

23
00:01:03,100 --> 00:01:05,470
all of these samples, and that tells us now, for

24
00:01:05,470 --> 00:01:09,410
every single feature, X of I, how often that was say,

25
00:01:09,410 --> 00:01:11,020
taking on a value of one, or taking on a

26
00:01:11,020 --> 00:01:14,270
value for zero. If we assume everything's binary. So that means

27
00:01:14,270 --> 00:01:17,520
we have unconditional probability distributions. Or at least estimates of

28
00:01:17,520 --> 00:01:19,350
unconditional probability distributions, for every

29
00:01:19,350 --> 00:01:21,270
single one of our features, yes?

30
00:01:21,270 --> 00:01:21,420
>> Yep.

31
00:01:21,420 --> 00:01:22,020
>> And

32
00:01:22,020 --> 00:01:24,540
at the same time, because we have these samples, we know

33
00:01:24,540 --> 00:01:28,690
for every pair of features. The probability that one took on a

34
00:01:28,690 --> 00:01:31,520
value, given a value for the other. So, already have the

35
00:01:31,520 --> 00:01:34,570
conditional probability table for each of those as well. So, just the

36
00:01:34,570 --> 00:01:38,760
act of generating these samples and building that mutual information graph,

37
00:01:38,760 --> 00:01:42,160
gives us all the conditional and unconditional probability tables that we need.

38
00:01:42,160 --> 00:01:44,400
So given that we can generate samples and time linear and

39
00:01:44,400 --> 00:01:46,660
the number of features, and we're done. So this part is easy.

40
00:01:47,730 --> 00:01:49,410
By the way you asked me a question before. I

41
00:01:49,410 --> 00:01:51,760
just want to make it clear, that we come back with an

42
00:01:51,760 --> 00:01:56,470
undirected graph. And, you can pick any single node as the

43
00:01:56,470 --> 00:02:00,700
root, and that will induce a specific directed tree. And it

44
00:02:00,700 --> 00:02:03,140
actually just follows from the chain rule. I'll leave that

45
00:02:03,140 --> 00:02:05,600
as an, an exercise to the, the viewer. But there you

46
00:02:05,600 --> 00:02:08,030
go. So now we know how to generate samples. From some

47
00:02:08,030 --> 00:02:11,090
tree that we have. We generate a bunch of these samples.

48
00:02:13,180 --> 00:02:17,080
We find the best ones. Retaining only those samples, and now we know

49
00:02:17,080 --> 00:02:21,820
how to estimate the next one, by simply doing the maximum spanning tree over

50
00:02:21,820 --> 00:02:25,620
all the mutual information. And then we just repeat. And we keep doing it.

51
00:02:25,620 --> 00:02:30,550
And there you go. A particular version of mimic. With dependency trees. Got it?

52
00:02:30,550 --> 00:02:31,890
>> Cool.

53
00:02:31,890 --> 00:02:35,290
>> Right. So, again, just to really drive this point home,

54
00:02:35,290 --> 00:02:38,610
you don't have to use dependency trees. You can use unconditional

55
00:02:38,610 --> 00:02:41,410
probability distributions, which are also very easy to sample from and

56
00:02:41,410 --> 00:02:44,580
very easy to estimate. You could come up with more complicated things,

57
00:02:44,580 --> 00:02:47,760
if you wanted to. Try to find the best Bayesian network. You

58
00:02:47,760 --> 00:02:50,010
can do all of these other kinds of things and that'll work

59
00:02:50,010 --> 00:02:53,260
just fine. Dependency Trees though are very powerful, because they do

60
00:02:53,260 --> 00:02:56,840
allow you to capture these relationships, that is to say they give

61
00:02:56,840 --> 00:03:00,100
you a probability distribution that has structure that we were looking for,

62
00:03:00,100 --> 00:03:03,140
while not having to pay an exponential cost for doing the estimation.
