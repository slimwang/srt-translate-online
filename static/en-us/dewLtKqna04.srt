1
00:00:00,750 --> 00:00:02,910
And that's the correct answer. All right. So let's

2
00:00:02,910 --> 00:00:06,080
actually go through that exercise and derive how we

3
00:00:06,080 --> 00:00:07,530
do that. because it's not so bad in two

4
00:00:07,530 --> 00:00:09,900
dimensions and it generalizes to higher dimensions as well.

5
00:00:09,900 --> 00:00:10,730
>> Okay.

6
00:00:10,730 --> 00:00:13,000
>> So it turns out that we can use calculus to do this, I

7
00:00:13,000 --> 00:00:17,790
am not going to step through the two-variable example for reasons that I am

8
00:00:17,790 --> 00:00:22,290
embarrassed to say. But I am going to show you a different example. So

9
00:00:22,290 --> 00:00:26,420
imagine that what we're trying to do is that we've got a bunch of data

10
00:00:26,420 --> 00:00:30,770
points, and we're trying to find the best constant function, right? So the best

11
00:00:30,770 --> 00:00:36,420
function that has the form, the value of the function for

12
00:00:36,420 --> 00:00:41,550
any given X is always the same constant, C. So if our data looks like this, we

13
00:00:41,550 --> 00:00:43,960
got a bunch of X's and a bunch of Y's,

14
00:00:43,960 --> 00:00:45,530
then what we're going to do, we're going to say for

15
00:00:45,530 --> 00:00:49,530
any given value of C, any given constant, we can have

16
00:00:49,530 --> 00:00:51,480
an error. What's the error going to be? The error is

17
00:00:51,480 --> 00:00:54,410
going to be the sum over all of the data points. Speaker 1: The square

18
00:00:54,410 --> 00:00:58,460
difference between that constant we chose and what

19
00:00:58,460 --> 00:01:00,660
the actual yi value is. So that's why.

20
00:01:00,660 --> 00:01:01,290
>> Michael.

21
00:01:01,290 --> 00:01:03,300
>> These differences here. Yes, Charles.

22
00:01:04,379 --> 00:01:05,665
>> Can I ask you a question?

23
00:01:05,665 --> 00:01:07,200
>> Sure.

24
00:01:07,200 --> 00:01:08,739
>> Why are we doing sum of squares?

25
00:01:11,010 --> 00:01:15,130
>> There is many different error functions and sometimes called

26
00:01:15,130 --> 00:01:17,430
a, a relative concept called the loss function. There is

27
00:01:17,430 --> 00:01:19,250
lots of difference once that could work, you can do

28
00:01:19,250 --> 00:01:22,410
the absolute error, you can do the squared error, you can

29
00:01:22,410 --> 00:01:26,060
do various kinds of squashed errors where you know. The

30
00:01:26,060 --> 00:01:28,880
errors count different depending on how, how much deviation there is.

31
00:01:30,320 --> 00:01:32,790
It turns out that this one is particularly well behaved

32
00:01:32,790 --> 00:01:36,550
because of this reason that I'm explaining now that that because

33
00:01:36,550 --> 00:01:40,240
this error function is smooth as a function of the constant

34
00:01:40,240 --> 00:01:44,450
C, we can use calculus to actually find the minimum error

35
00:01:44,450 --> 00:01:47,360
value. But there's lots of other things that could work and

36
00:01:47,360 --> 00:01:51,540
they actually do find utility in various different machine learning settings.

37
00:01:51,540 --> 00:01:52,400
>> Okay.

38
00:01:52,400 --> 00:01:53,970
>> So just now using the chain rule, if

39
00:01:53,970 --> 00:01:56,560
you want to find how do this error function

40
00:01:56,560 --> 00:01:59,320
output change as a function of input c. We

41
00:01:59,320 --> 00:02:02,300
can take the derivative of this sum you know,

42
00:02:02,300 --> 00:02:06,950
bring the two over. Times this, times the derivative of the inside, which

43
00:02:06,950 --> 00:02:09,460
is negative one in this case. And now this gives us a nice, smooth

44
00:02:09,460 --> 00:02:13,480
function saying what the error is as a function of c. And if we

45
00:02:13,480 --> 00:02:15,880
want to find the minimum, what do we want to do to this quantity?

46
00:02:15,880 --> 00:02:18,859
>> Set it equal to zero, because that's what I remember from Calculus.

47
00:02:20,510 --> 00:02:23,220
>> That's right. So in particular if the error you know, the error

48
00:02:23,220 --> 00:02:27,420
function is a nice smooth thing the derivative is negative and then zero and

49
00:02:27,420 --> 00:02:29,450
then positive. When it hits zero that's when the thing

50
00:02:29,450 --> 00:02:32,170
has bottomed-out. Alright. So now we just need to solve

51
00:02:32,170 --> 00:02:35,690
this, this equation for c. So we have one equation

52
00:02:35,690 --> 00:02:40,000
and one unknown. Alright, so that gets us this. But,

53
00:02:40,000 --> 00:02:43,150
this quantity, it's just the constant added to itself n

54
00:02:43,150 --> 00:02:46,590
times. So it's n times c. We move that to

55
00:02:46,590 --> 00:02:49,700
the other side. We get n times c. N is

56
00:02:49,700 --> 00:02:52,460
the number of data points as you recall. Is the

57
00:02:52,460 --> 00:02:57,660
sum of the yi's. We divide

58
00:02:57,660 --> 00:03:02,580
two by n and what do we see? So what is it Charles?

59
00:03:02,580 --> 00:03:05,080
>> The best constant is the average of all your y's.

60
00:03:05,080 --> 00:03:09,750
>> Great, it's the mean. The mean comes back. Right, so

61
00:03:09,750 --> 00:03:12,170
in the case of finding the best constant here, we just have

62
00:03:12,170 --> 00:03:15,070
to average the y, the y's together and that catches thing that

63
00:03:15,070 --> 00:03:17,560
minimizes the squared error. So squared error is this really nice thing

64
00:03:17,560 --> 00:03:20,820
because it tends to bring things like mean back into the picture. It's really

65
00:03:20,820 --> 00:03:23,310
very convenient. And, it generalizes to higher,

66
00:03:23,310 --> 00:03:26,950
higher order of function tier, not higher

67
00:03:26,950 --> 00:03:29,790
functions, but more variables like, like lines.

68
00:03:29,790 --> 00:03:31,740
Sorry. Lines that have some, some non

69
00:03:31,740 --> 00:03:35,770
constant slope. By doing the same kind

70
00:03:35,770 --> 00:03:37,840
of process and things actually work really nicely.
