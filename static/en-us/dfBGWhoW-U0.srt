1
00:00:00,380 --> 00:00:03,330
There is one more important point to be noted here.

2
00:00:03,330 --> 00:00:07,220
This again is the illustration from incremental concept learning. When we

3
00:00:07,220 --> 00:00:10,480
were talking about incremental concept learning, we talked about technique for

4
00:00:10,480 --> 00:00:14,337
learning. We did not talk about how this concept were going to be used.

5
00:00:14,337 --> 00:00:17,620
How about in correcting mistakes, we're talking about

6
00:00:17,620 --> 00:00:22,638
how the agent actually uses the knowledge it learns. This point too,

7
00:00:22,638 --> 00:00:28,370
is centered [INAUDIBLE] reason. The first reason is that knowledge based AI,

8
00:00:28,370 --> 00:00:33,500
looks at reasoning. Looks at action, besides how knowledge is going to be used.

9
00:00:34,620 --> 00:00:39,230
And then, determined what knowledge is to be learned. Assess the target for

10
00:00:39,230 --> 00:00:42,480
learning, secondly you may recall this particular figure for

11
00:00:42,480 --> 00:00:46,640
the target of architecture that we'd drawn earlier. You may see that reasoning,

12
00:00:46,640 --> 00:00:49,580
learning and memory are closely connected, and all of that is occurring on

13
00:00:49,580 --> 00:00:55,120
the surface of action selection. This figure suggests that we not only learn,

14
00:00:55,120 --> 00:01:00,080
so that we can do action selection. But additionally, as we do action selection,

15
00:01:00,080 --> 00:01:03,420
and we get feedback from the world, it informs the learning.

16
00:01:03,420 --> 00:01:08,160
As this figure suggests, intelligent agents, cognitive systems, not only learn,

17
00:01:08,160 --> 00:01:11,630
so that they can take actions on the world. But further, that the world gives

18
00:01:11,630 --> 00:01:17,255
some feedback, and that feedback informs the learning. Once again, failure

19
00:01:17,255 --> 00:01:22,310
a great opportunities for learning. One additional point to be made here,

20
00:01:22,310 --> 00:01:26,830
learning by correcting mistakes, use learning as a problem-solving activity.

21
00:01:26,830 --> 00:01:30,430
An agent meets failure, it needs to learn from the failure.

22
00:01:30,430 --> 00:01:34,324
It converts this learning task into a problem-solving task. Let us first,

23
00:01:34,324 --> 00:01:37,910
identify what knowledge are related to failure. Then, let us build and

24
00:01:37,910 --> 00:01:42,720
explanation for this. Then we'll repair it. This learning is closely intertwined

25
00:01:42,720 --> 00:01:48,130
with memory, reasoning, action, and feedback from the world. Notice also,

26
00:01:48,130 --> 00:01:52,670
that there's reasoning, learning, and memory here. In the deliberation module

27
00:01:52,670 --> 00:01:56,660
closely connected with the metacognition module. Here, the reasoning, memory,

28
00:01:56,660 --> 00:02:00,040
and learning may be about action selection in the world.

29
00:02:00,040 --> 00:02:03,440
But in a metacognition module may have its own reasoning, learning, and

30
00:02:03,440 --> 00:02:08,750
memory capacities. And some of the learning in the metacognition is about fixing

31
00:02:08,750 --> 00:02:12,880
the errors in the deliberative reasoning. So, metacognition is thinking about

32
00:02:12,880 --> 00:02:17,970
thinking. The agent uses the knowledge, to think about the action selection, and

33
00:02:17,970 --> 00:02:23,550
it conducted those actions in the world. Metacognition is thinking about

34
00:02:23,550 --> 00:02:27,880
what went wrong in its original thinking. What was the knowledge error?

35
00:02:27,880 --> 00:02:31,010
We'll return to metacognition in the lesson on metareasoning.
