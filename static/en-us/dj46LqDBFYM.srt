1
00:00:00,080 --> 00:00:02,510
Okay, let's take a look at some formal measures of

2
00:00:02,510 --> 00:00:06,520
data quality. There are essentially five measures of data quality that

3
00:00:06,520 --> 00:00:08,600
I'd like to talk about. This is the model we'll

4
00:00:08,600 --> 00:00:11,973
use in this course. And regardless of who's talking about data

5
00:00:11,973 --> 00:00:15,170
quality, and what labels they apply to measures data quality.

6
00:00:15,170 --> 00:00:17,915
They're going to fall roughly into these five categories that we'll talk

7
00:00:17,915 --> 00:00:21,510
about. So first let's talk about validity. With validity, we're

8
00:00:21,510 --> 00:00:25,100
measuring the degree to which entries in our data set conform

9
00:00:25,100 --> 00:00:28,320
to a defined schema, or to other constraints we

10
00:00:28,320 --> 00:00:31,630
might have. As we move through this lesson, we'll discuss

11
00:00:31,630 --> 00:00:34,130
each of these in more detail. Here, I'm just

12
00:00:34,130 --> 00:00:37,230
going to provide an overview. We can also look at

13
00:00:37,230 --> 00:00:40,330
accuracy. This is the degree to which entries conform

14
00:00:40,330 --> 00:00:42,750
to gold standard data. What I mean by that, is

15
00:00:42,750 --> 00:00:46,440
probably best expressed with an example. So, do all street

16
00:00:46,440 --> 00:00:50,230
addresses in a data set that we're cleaning, actually exist?

17
00:00:50,230 --> 00:00:53,050
In order to test that type of accuracy question,

18
00:00:53,050 --> 00:00:55,540
we would need some gold standard. That is, some set

19
00:00:55,540 --> 00:01:00,350
of data that we actually trust. Completeness is pretty straightforward,

20
00:01:00,350 --> 00:01:02,540
do we have all the records we should have? While

21
00:01:02,540 --> 00:01:06,480
explaining it, is pretty straightforward, actually measuring completeness is a

22
00:01:06,480 --> 00:01:09,410
very difficult thing to do. We also want to look

23
00:01:09,410 --> 00:01:12,610
for consistency within our data. In many systems, we will

24
00:01:12,610 --> 00:01:15,270
have multiple records, that have some overlap in the data

25
00:01:15,270 --> 00:01:18,250
they contain. We want to ensure that there is consistency among

26
00:01:18,250 --> 00:01:23,400
the fields, that represent the same data across systems. And finally

27
00:01:23,400 --> 00:01:27,545
uniformity, this one's easy. Do all our values for distance, for

28
00:01:27,545 --> 00:01:30,990
example, use the same units? Is it miles, or is it kilometers?
