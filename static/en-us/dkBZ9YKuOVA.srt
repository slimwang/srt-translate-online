1
00:00:00,280 --> 00:00:02,480
Speaking of solutions, this is the last little bit of thing

2
00:00:02,480 --> 00:00:05,630
that you need to know. And that is. This defines a

3
00:00:05,630 --> 00:00:08,480
problem. But, what we also want to have, whenever we have

4
00:00:08,480 --> 00:00:12,900
a problem. Is a solution. So, the solution to the Markov

5
00:00:12,900 --> 00:00:16,650
Decision Process, is something called a policy. And, what a policy

6
00:00:16,650 --> 00:00:20,140
does. Is, it's a function, that takes in a state. And

7
00:00:20,140 --> 00:00:23,290
returns an action, in other words, for any given state that

8
00:00:23,290 --> 00:00:25,470
you're in, it tells you the action that you should take.

9
00:00:25,470 --> 00:00:26,720
>> Like as a hint?

10
00:00:26,720 --> 00:00:28,840
>> No, it just tells you, this is the at.

11
00:00:28,840 --> 00:00:30,890
Well, I mean, I suppose you don't have to do

12
00:00:30,890 --> 00:00:33,330
it, but the way we think about Markov Decision Processes,

13
00:00:33,330 --> 00:00:35,140
is that this is the action that will be taken.

14
00:00:35,140 --> 00:00:37,450
>> I see, so it's more of an order.

15
00:00:37,450 --> 00:00:39,440
>> Yes, it's a command. Okay.

16
00:00:39,440 --> 00:00:41,770
>> So that's all a policy is. A policy is

17
00:00:41,770 --> 00:00:44,270
solution to a Markov Decision Process. And there is a

18
00:00:44,270 --> 00:00:47,380
special policy, which I'm writing here as policy star, or

19
00:00:47,380 --> 00:00:51,080
the optimal policy, and that is the policy that maximizes

20
00:00:51,080 --> 00:00:55,530
your long-term expected reward. So if all the policies you

21
00:00:55,530 --> 00:00:57,870
could take, of all the decisions you might take, this

22
00:00:57,870 --> 00:01:00,850
is the policy that optimizes the amount of reward that

23
00:01:00,850 --> 00:01:04,480
you're going to receive or expect to receive over your lifetime.

24
00:01:04,480 --> 00:01:06,690
>> So, like, at the end?

25
00:01:06,690 --> 00:01:08,240
>> Well, at yeah, at the end, or at

26
00:01:08,240 --> 00:01:10,840
any given point in time, how much reward you're receiving.

27
00:01:10,840 --> 00:01:12,640
From the Markov Decision Process point of view, there doesn't

28
00:01:12,640 --> 00:01:16,530
have to be an end. Okay. Though in this example,

29
00:01:16,530 --> 00:01:18,750
you don't get anything, and then at the end, you get paid off.

30
00:01:18,750 --> 00:01:21,120
>> Right, or unpaid off.

31
00:01:21,120 --> 00:01:21,780
>> Right.

32
00:01:21,780 --> 00:01:23,630
>> If you fall into the red square. So

33
00:01:23,630 --> 00:01:26,910
actually, your question points out something very important here. I

34
00:01:26,910 --> 00:01:29,210
mentioned earlier when I talked about the three kinds of

35
00:01:29,210 --> 00:01:31,720
learning that there, supervised learning and reinforced learning were sort

36
00:01:31,720 --> 00:01:34,520
of. Similar, except that instead of getting Ys and

37
00:01:34,520 --> 00:01:37,590
Xs we were given Ys and, Xs and Zs. And

38
00:01:37,590 --> 00:01:40,250
this is exactly what's happening here. Here what we would

39
00:01:40,250 --> 00:01:42,290
like to have if we wanted to learn a policy

40
00:01:42,290 --> 00:01:45,940
is a bunch of sa pairs as training examples. Well

41
00:01:45,940 --> 00:01:48,210
here's the state and the action you should've took, taken, here's

42
00:01:48,210 --> 00:01:50,110
another state and the action you should've taken, so on and

43
00:01:50,110 --> 00:01:53,170
so forth. And then we would learn a function, the policy,

44
00:01:53,170 --> 00:01:57,150
that maps states to actions. But what we actually see

45
00:01:57,150 --> 00:02:00,250
in the reinforcement learning world, in the Markov Decision Process world,

46
00:02:00,250 --> 00:02:04,100
is we see states, actions, and then the rewards that we

47
00:02:04,100 --> 00:02:07,390
received. And so in fact, this problem of seeing a sequence

48
00:02:07,390 --> 00:02:11,250
of states, actions, and rewards. It's very different

49
00:02:11,250 --> 00:02:13,340
from the problem of being told. This is the

50
00:02:13,340 --> 00:02:15,700
correct action to take to maximize a function.

51
00:02:15,700 --> 00:02:17,165
Or find a function that maps from state to

52
00:02:17,165 --> 00:02:19,710
action. Instead, we say well, if you're in

53
00:02:19,710 --> 00:02:21,860
this state, and you take this action, this is

54
00:02:21,860 --> 00:02:23,540
the reward that you would see. And then

55
00:02:23,540 --> 00:02:26,060
from that, we need to find the optimal action.

56
00:02:26,060 --> 00:02:29,500
>> So Pi star is being the F from that previous slide?

57
00:02:29,500 --> 00:02:29,920
>> Right.

58
00:02:29,920 --> 00:02:32,610
>> And R is being Z?

59
00:02:32,610 --> 00:02:35,680
>> Yes. And y is being a.

60
00:02:35,680 --> 00:02:39,040
>> And s is being x or x is being s

61
00:02:39,040 --> 00:02:40,470
>> Got you.

62
00:02:40,470 --> 00:02:40,930
>> Right.

63
00:02:40,930 --> 00:02:42,900
>> So but, I'm, okay I'm a little confused

64
00:02:42,900 --> 00:02:45,880
about this notion of a policy. So we have

65
00:02:45,880 --> 00:02:47,720
the, the, the thing we tried to do to

66
00:02:47,720 --> 00:02:50,820
get the goals was up, up, right, right, right. Yes.

67
00:02:50,820 --> 00:02:54,140
>> I don't see how to capture that as a policy.

68
00:02:54,140 --> 00:02:57,720
>> It's actually fairly straightforward. What a policy would say is:

69
00:02:57,720 --> 00:02:59,180
What state are you in? Tell me what action you

70
00:02:59,180 --> 00:03:02,420
should take. So, the policy, basically is this: When you're in

71
00:03:02,420 --> 00:03:06,430
the state, start, the start state, the action you should take

72
00:03:06,430 --> 00:03:10,540
is up. And it would have a mapping. For every state

73
00:03:10,540 --> 00:03:13,200
that you might see, whether it's this state, this state,

74
00:03:13,200 --> 00:03:17,030
this state, this state, this state, this state, this state, or

75
00:03:17,030 --> 00:03:19,890
even these two states, and it will tell you what action

76
00:03:19,890 --> 00:03:22,910
you should take. And that's what a policy is. A policy,

77
00:03:22,910 --> 00:03:26,220
very simply, is nothing more than a function that tells you what

78
00:03:26,220 --> 00:03:29,550
action to take at every, in any state you happen to come across.

79
00:03:29,550 --> 00:03:31,750
>> Okay, but the, but the. The question that

80
00:03:31,750 --> 00:03:34,510
you asked before was about up, up, right, right, right.

81
00:03:34,510 --> 00:03:34,990
>> Mm hm.

82
00:03:34,990 --> 00:03:38,470
>> And, it seems like, because of

83
00:03:38,470 --> 00:03:40,850
the stochastic transitions. You might not be in

84
00:03:40,850 --> 00:03:43,340
the same state. Like, you don't know what

85
00:03:43,340 --> 00:03:45,370
state you're in, when you take those actions.

86
00:03:45,370 --> 00:03:47,910
>> No, so, one of the things for what we're talking about

87
00:03:47,910 --> 00:03:49,620
here, for the Markov Decision Process.

88
00:03:49,620 --> 00:03:52,780
Is, there're states, there're actions, there're rewards.

89
00:03:52,780 --> 00:03:56,540
You always know what state you're in, and you know what reward you receive.

90
00:03:56,540 --> 00:04:00,420
>> So does that mean you can't do up, up right right right?

91
00:04:00,420 --> 00:04:02,630
>> Well, the way it would work in a Markov Decision Process,

92
00:04:02,630 --> 00:04:06,260
so what you're describing is is what's often called a plan. You

93
00:04:06,260 --> 00:04:09,780
know, it's, tell me what sequence of actions I should take from

94
00:04:09,780 --> 00:04:13,020
here. What Markov Decision Process does and what a pr, a policy

95
00:04:13,020 --> 00:04:16,410
does is it doesn't tell you what sequence of actions to take from a particular

96
00:04:16,410 --> 00:04:20,589
state. It tells you what action to take in a particular state. You will then

97
00:04:20,589 --> 00:04:23,560
end up in another state because of

98
00:04:23,560 --> 00:04:25,820
the transition model, the transition function. And then

99
00:04:25,820 --> 00:04:27,080
when you're in that state you ask the

100
00:04:27,080 --> 00:04:30,330
policy what actions should I take now? Okay.

101
00:04:30,330 --> 00:04:33,440
>> Right, so this is actually a key point.

102
00:04:34,890 --> 00:04:38,490
Although we talked about it in the language of planning,

103
00:04:38,490 --> 00:04:41,190
which is very common for the people who di, for example

104
00:04:41,190 --> 00:04:43,500
take any ag course, the thing about this in terms of planning,

105
00:04:43,500 --> 00:04:45,510
what are the things that I can do to accomplish my

106
00:04:45,510 --> 00:04:49,140
goals? The Markov Decision Process way of thinking about it, the reinforcement

107
00:04:49,140 --> 00:04:51,980
way of thinking about it, or the typical reinforcement learning way

108
00:04:51,980 --> 00:04:55,180
of thinking about it, really doesn't talk about plans directly. But instead,

109
00:04:55,180 --> 00:04:59,660
talks about policies. Which from which you can infer a plan, but

110
00:04:59,660 --> 00:05:03,540
this has the advantage that it tells you what to do everywhere.

111
00:05:03,540 --> 00:05:07,390
And it's robust to the underlying stochastic of the word. World.

112
00:05:07,390 --> 00:05:11,090
>> So, is it clear that's all you need to be able to behave well.

113
00:05:11,090 --> 00:05:15,060
>> Well, it's certainly the case, that if you have a policy and that policy

114
00:05:15,060 --> 00:05:18,370
is optimal, it does tell you what to do, no matter what situation you're in.

115
00:05:18,370 --> 00:05:19,215
>> 'Kay.

116
00:05:19,215 --> 00:05:21,630
>> And so, if you have that, then that's definitely

117
00:05:21,630 --> 00:05:23,780
all you need to behave well. But I mean could it

118
00:05:23,780 --> 00:05:25,540
be that you wanted to do something like up, up,

119
00:05:25,540 --> 00:05:28,280
right, right, right which you cant write down as a policy?

120
00:05:28,280 --> 00:05:28,650
>> And why cant

121
00:05:28,650 --> 00:05:29,910
you write that down as a policy?

122
00:05:29,910 --> 00:05:33,070
>> Because the policies are only telling you what action to do as a function

123
00:05:33,070 --> 00:05:37,580
of the state not sort of like how far along you are in the sequence.

124
00:05:37,580 --> 00:05:40,880
>> Right unless, of course, you fold that into your state some how. But thats

125
00:05:40,880 --> 00:05:45,350
exactly right, the way to think about this is. The idea of coming up with

126
00:05:45,350 --> 00:05:49,870
a concrete plan of what to do for the next 20 time steps is different

127
00:05:49,870 --> 00:05:53,800
from the problem of whatever step I happen to be in, whatever state I happen

128
00:05:53,800 --> 00:05:55,350
to be in, what's the next best thing

129
00:05:55,350 --> 00:05:58,590
I can do? And just always asking that question.

130
00:05:58,590 --> 00:05:59,340
>> Hm.

131
00:05:59,340 --> 00:06:01,110
>> If you always ask that question,

132
00:06:01,110 --> 00:06:03,230
that will induce a sequence, but that sequence

133
00:06:03,230 --> 00:06:05,580
is actually dependent upon the set of states

134
00:06:05,580 --> 00:06:07,960
that you see. Whereas in the other case

135
00:06:07,960 --> 00:06:10,270
where we wrote down a particular policy, you'll

136
00:06:10,270 --> 00:06:12,250
notice that was only dependent upon the state

137
00:06:12,250 --> 00:06:16,030
you started in and it had to ignore the states that you saw along the way.

138
00:06:17,033 --> 00:06:18,800
>> And the only way to fix that would be

139
00:06:18,800 --> 00:06:21,220
to say, well, after I've taken an action, let me look at the state

140
00:06:21,220 --> 00:06:24,430
I'm in and see if I should do something different with it. But if

141
00:06:24,430 --> 00:06:28,310
you're going to do that, then why are you trying to compute the complete set

142
00:06:28,310 --> 00:06:32,290
of states? Or I'm sorry, the complete set of actions that you might take.

143
00:06:32,290 --> 00:06:33,350
>> Okay.

144
00:06:33,350 --> 00:06:35,390
>> Okay, so there you go. Now, a lot

145
00:06:35,390 --> 00:06:38,380
of what we're going to be talking about next Michael, is,

146
00:06:38,380 --> 00:06:40,930
given that we have MDP, we have this Markov Decision

147
00:06:40,930 --> 00:06:44,080
Process defined like this. How do we go from this

148
00:06:44,080 --> 00:06:47,080
problem definition to finding a good policy, and

149
00:06:47,080 --> 00:06:50,090
in particular, finding the optimal policy? That makes sense.

150
00:06:50,090 --> 00:06:51,680
>> Good. And there you go.
