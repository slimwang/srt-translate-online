1
00:00:00,000 --> 00:00:03,000
I want to talk for a moment about how to scale databases.

2
00:00:03,000 --> 00:00:07,000
Now, like databases themselves, this is a very broad topic,

3
00:00:07,000 --> 00:00:09,000
but there's a couple concepts I'd like to introduce

4
00:00:09,000 --> 00:00:11,000
because they're going to affect us in this class.

5
00:00:11,000 --> 00:00:13,000
There's two reasons you might need to scale a database.

6
00:00:13,000 --> 00:00:16,000
One is too much load,

7
00:00:16,000 --> 00:00:19,000
as in a database that's just doing too much work.

8
00:00:19,000 --> 00:00:22,000
You've got some website that is getting

9
00:00:22,000 --> 00:00:25,000
millions of requests a day--thousands of requests a second--

10
00:00:25,000 --> 00:00:28,000
and you've got this one machine that's got your data on it

11
00:00:28,000 --> 00:00:30,000
and it just can't keep up with all the work.

12
00:00:30,000 --> 00:00:33,000
So what you might do in this case is take your database--

13
00:00:33,000 --> 00:00:36,000
which are often represented as little cynlinders--

14
00:00:36,000 --> 00:00:39,000
and replicate it to other databases.

15
00:00:39,000 --> 00:00:44,000
So every time we insert a piece of data to this database,

16
00:00:44,000 --> 00:00:47,000
we send it over to the other guys.

17
00:00:47,000 --> 00:00:50,000
So all of our database writes go into this one master database.

18
00:00:50,000 --> 00:00:55,000
In turn, all of that data gets replicated to these slave databases.

19
00:00:55,000 --> 00:00:58,000
So now we have 3 databases with all of our data on it,

20
00:00:58,000 --> 00:01:03,000
and so if you've got a site like Reddit that's getting thousands and thousands of requests a second,

21
00:01:03,000 --> 00:01:06,000
instead of sending all of the database reads--

22
00:01:06,000 --> 00:01:09,000
all the lookups--to this database,

23
00:01:09,000 --> 00:01:12,000
we can send it to these databases.

24
00:01:12,000 --> 00:01:15,000
And that alleviates a lot of this load off of this master database.

25
00:01:15,000 --> 00:01:18,000
Now, there are much more complicated schemes

26
00:01:18,000 --> 00:01:21,000
where you might have a multi-master system

27
00:01:21,000 --> 00:01:26,000
or a bunch of databases that are all working together,

28
00:01:26,000 --> 00:01:29,000
but this is a fairly common setup

29
00:01:29,000 --> 00:01:32,000
and a fairly common approach to spreading around the read.

30
00:01:32,000 --> 00:01:35,000
Because generally in most systems you have a lot more reads

31
00:01:35,000 --> 00:01:37,000
than you do writes.

32
00:01:37,000 --> 00:01:39,000
So if your master can keep up with all of the writes,

33
00:01:39,000 --> 00:01:41,000
your slaves can handle all of the reads.

34
00:01:41,000 --> 00:01:43,000
There are some downsides to this.

35
00:01:43,000 --> 00:01:46,000
One is that it doesn't increase the speed of writes.

36
00:01:46,000 --> 00:01:49,000
We're still bottlenecked by this master receiving all this data,

37
00:01:49,000 --> 00:01:52,000
who in turn has to send it to all these slaves.

38
00:01:52,000 --> 00:01:55,000
So if we can still fit all our writes on this one machine, we're in good shape

39
00:01:55,000 --> 00:01:58,000
and we can spread the reads over as many machines as we need.

40
00:01:58,000 --> 00:02:01,000
Some of these slaves can even replicate the other machines.

41
00:02:01,000 --> 00:02:05,000
Another downside is this notion of replication lag.

42
00:02:05,000 --> 00:02:09,000
And this occurs when you write a piece of data to the master

43
00:02:09,000 --> 00:02:13,000
but the read hits one of these slaves before the data has propagated,

44
00:02:13,000 --> 00:02:15,000
and that's called replication lag.

45
00:02:15,000 --> 00:02:17,000
You can sometimes get some funny behavior.

46
00:02:17,000 --> 00:02:20,000
And the reason I bring it up now is because in the database we'll be using in this class,

47
00:02:20,000 --> 00:02:24,000
you can occasionally see symptoms of this type of behavior.

48
00:02:24,000 --> 00:02:27,000
It's not necessarily owing to a master/slave setup,

49
00:02:27,000 --> 00:02:32,000
but this is the general concept.

50
00:02:32,000 --> 00:02:37,000
What happens if you have too much data for one machine?

51
00:02:37,000 --> 00:02:40,000
The replication handles the case

52
00:02:40,000 --> 00:02:42,000
where you have too much load.

53
00:02:42,000 --> 00:02:46,000
Basically, you've got to do too many reads, so you can replicate your database--scale this up.

54
00:02:46,000 --> 00:02:48,000
What if you just have too much data,

55
00:02:48,000 --> 00:02:51,000
your master database--or any one of these databases--can't even hold of your data,

56
00:02:51,000 --> 00:02:54,000
it doesn't fit in memory or even in disk,

57
00:02:54,000 --> 00:02:56,000
there's just too much?

58
00:02:56,000 --> 00:02:58,000
One of the things you can do is you can shard the database.

59
00:02:58,000 --> 00:03:01,000
This is a fairly simple approach, where instead of having one master

60
00:03:01,000 --> 00:03:04,000
you might have a couple that are all the same size, despite my drawing.

61
00:03:04,000 --> 00:03:06,000
So let's say we're storing our links database.

62
00:03:06,000 --> 00:03:10,000
One approach to shardding might be to store links 1-100 here--

63
00:03:10,000 --> 00:03:12,000
actually, more like 1-a billion here--

64
00:03:12,000 --> 00:03:15,000
and 101-200 here

65
00:03:15,000 --> 00:03:18,000
and 201-300 here,

66
00:03:18,000 --> 00:03:21,000
and basically you store some of your links here and some of your links here

67
00:03:21,000 --> 00:03:23,000
and some of your links here in this database.

68
00:03:23,000 --> 00:03:26,000
Obviously, 1-100 probably isn't the correct approach.

69
00:03:26,000 --> 00:03:29,000
You could probably use a hashing approach as if we had a hashtable

70
00:03:29,000 --> 00:03:32,000
and these were all cells in our hashtable

71
00:03:32,000 --> 00:03:34,000
and you hash on some particular key.

72
00:03:34,000 --> 00:03:39,000
In this case, I'm referring to link ids--the id of whatever you're storing--

73
00:03:39,000 --> 00:03:42,000
to figure out which database you want to store it in.

74
00:03:42,000 --> 00:03:45,000
This is cool because now

75
00:03:45,000 --> 00:03:48,000
if we triple our write load from this scenario over here,

76
00:03:48,000 --> 00:03:51,000
we have 3 database machines to handle it.

77
00:03:51,000 --> 00:03:54,000
Likewise, we already have 3 machines to handle the read load.

78
00:03:54,000 --> 00:03:57,000
And of course you can replicate these machines as well.

79
00:03:57,000 --> 00:04:00,000
A lot of systems both shard and replicate your machines

80
00:04:00,000 --> 00:04:03,000
if you really want to get fancy.

81
00:04:03,000 --> 00:04:06,000
But there are some downsides to shardding as well.

82
00:04:06,000 --> 00:04:09,000
One of the downsides is the queries get much more complex.

83
00:04:09,000 --> 00:04:12,000
What if I said that this helps the case

84
00:04:12,000 --> 00:04:15,000
of find me the link whose id is 150?

85
00:04:15,000 --> 00:04:18,000
We just say, okay, which does 150 go to?

86
00:04:18,000 --> 00:04:20,000
You know what hash is to this machine,

87
00:04:20,000 --> 00:04:22,000
and we do our read and that's that.

88
00:04:22,000 --> 00:04:26,000
What if I said get me all the links sorted by hotness, for example.

89
00:04:26,000 --> 00:04:29,000
Well, we don't know where that link may reside,

90
00:04:29,000 --> 00:04:31,000
especially if we're using a hashing algorithm

91
00:04:31,000 --> 00:04:34,000
that's more sophisticated than 1-100.

92
00:04:34,000 --> 00:04:36,000
That's called a range query,

93
00:04:36,000 --> 00:04:39,000
and a range query might have to hit all of your machines.

94
00:04:39,000 --> 00:04:44,000
Then we've lost one of our advantages to using this.

95
00:04:44,000 --> 00:04:47,000
Now, you could replicate these to spread that around,

96
00:04:47,000 --> 00:04:50,000
but you have to hit all these machines and then merge the results probably

97
00:04:50,000 --> 00:04:53,000
and do the sort again and memory.

98
00:04:53,000 --> 00:04:56,000
Some queries become very, very complex.

99
00:04:56,000 --> 00:05:00,000
Another downside is that joins become difficult or impossible.

100
00:05:00,000 --> 00:05:02,000
That makes sense.

101
00:05:02,000 --> 00:05:05,000
If we have this massive database that has multiple tables on it, we can do joints.

102
00:05:05,000 --> 00:05:08,000
But if we start shardding our database up--

103
00:05:08,000 --> 00:05:11,000
we have one database that doesn't even fit on one machine.

104
00:05:11,000 --> 00:05:13,000
So if we can't fit one database on our machine,

105
00:05:13,000 --> 00:05:15,000
how are we going to fit multiple databases?

106
00:05:15,000 --> 00:05:18,000
This notion that you have all of your tables in the same place to do a join--

107
00:05:18,000 --> 00:05:20,000
to do those fancy join SQL queries--

108
00:05:20,000 --> 00:05:22,000
becomes a lot more difficult.

109
00:05:22,000 --> 00:05:25,000
Now, there's a lot of research going on

110
00:05:25,000 --> 00:05:29,000
of building systems that can overcome these downsides,

111
00:05:29,000 --> 00:05:31,000
but if you take the naive approach to replicating and charting,

112
00:05:31,000 --> 00:05:34,000
these are some of the things you'll have to deal with.

113
00:05:34,000 --> 00:05:37,000
Generally the benefits outweigh the downsides,

114
00:05:37,000 --> 00:05:40,000
because if you design your data in such a way that you don't need to do joins

115
00:05:40,000 --> 00:05:43,000
or you don't need to do complex queries,

116
00:05:43,000 --> 00:05:46,000
you're in very good shape with both of these.

117
00:05:46,000 --> 00:05:49,000
Or maybe you have multiple setups--

118
00:05:49,000 --> 00:05:53,000
one setup for handling your general load and another setup for handling your complex queries.

119
00:05:53,000 --> 00:05:56,000
That's okay, and that's actually very common.

120
00:05:56,000 --> 00:05:58,000
Now, I wanted to introduce these concepts

121
00:05:58,000 --> 00:06:01,000
because the database we'll be using in this class--

122
00:06:01,000 --> 00:06:03,000
Google the App Engine Datastore--

123
00:06:03,000 --> 00:06:07,000
actually has some of these limitations.

124
00:06:07,000 --> 00:06:11,000
There are no joins allowed, even though it does provide a SQL interface.

125
00:06:11,000 --> 00:06:15,000
And a lot of more complex queries that you could do in a general SQL database,

126
00:06:15,000 --> 00:06:19,000
you can't do in Google Datastore, but that's okay.

127
00:06:19,000 --> 00:06:21,000
You get this really nice benefit of having a database

128
00:06:21,000 --> 00:06:26,000
that is shardded and replicated to wazoo and back.

129
00:06:26,000 --> 00:06:29,000
It's actually very fast and reliable.

130
00:06:29,000 --> 00:06:31,000
You're probably not going to have to worry about systems crashing.

131
00:06:31,000 --> 00:06:34,000
It's something that replicating a database can help with

132
00:06:34,000 --> 00:06:36,000
and shardding a database can help with.

133
00:06:36,000 --> 00:06:39,000
Okay, so I just wanted to introduce those concepts.

134
00:06:39,000 --> 00:06:42,000
Let's have a quick quiz on it before we move along.

135
00:06:42,000 --> 00:06:45,000
Which is an appropriate technique for increasing the read speed from a database?

136
00:06:45,000 --> 00:06:48,000
Get a faster machine. Replicate the database.

137
00:06:48,000 --> 00:06:52,000
Store less data. Or press the turbo button.
