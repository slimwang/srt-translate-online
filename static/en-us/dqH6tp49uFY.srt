1
00:00:00,000 --> 00:00:03,000
For example, in the 4 by 3 GridWorld,

2
00:00:03,000 --> 00:00:08,000
what if we don't know where the plus 1 and minus 1 rewards are when we start out?

3
00:00:08,000 --> 00:00:13,000
A reinforcement learning agent can learn to explore the territory,

4
00:00:13,000 --> 00:00:15,000
find where the rewards are,

5
00:00:15,000 --> 00:00:17,000
and then learn an optimal policy.

6
00:00:17,000 --> 00:00:19,000
Whereas, an MDP solver can only do that

7
00:00:19,000 --> 00:00:22,000
once it knows exactly where the rewards are.

8
00:00:22,000 --> 00:00:27,000
Now, this idea of wandering around and then finding a plus 1 or a minus 1

9
00:00:27,000 --> 00:00:32,000
is analogous to many forms of games, such as backgammon--

10
00:00:32,000 --> 00:00:35,000
and here's an example: backgammon is a stochastic game;

11
00:00:35,000 --> 00:00:38,000
and at the end, you either win or lose.

12
00:00:38,000 --> 00:00:40,000
And in the 1990s, Gary Tesauro at IBM

13
00:00:40,000 --> 00:00:43,000
wrote a program to play backgammon.

14
00:00:43,000 --> 00:00:49,000
His first attempt tried to learn the utility of a Game state, U of S,

15
00:00:49,000 --> 00:00:53,000
using examples that were labelled by human expert backgammon players.

16
00:00:53,000 --> 00:00:55,000
But this was tedious work for the experts,

17
00:00:55,000 --> 00:00:58,000
so only a small number of states were labelled.

18
00:00:58,000 --> 00:01:00,000
The program tried to generalize from that,

19
00:01:00,000 --> 00:01:02,000
using supervised learning,

20
00:01:02,000 --> 00:01:04,000
and was not able to perform very well.

21
00:01:04,000 --> 00:01:11,000
So Tesauro's second attempt used no human expertise and no supervision.

22
00:01:11,000 --> 00:01:14,000
Instead, he had 1 copy of his program play against another;

23
00:01:14,000 --> 00:01:18,000
and at the end of the game, the winner got a positive reward,

24
00:01:18,000 --> 00:01:20,000
and the loser, a negative.

25
00:01:20,000 --> 00:01:22,000
So he used reinforcement learning;

26
00:01:22,000 --> 00:01:25,000
he backed up that knowledge throughout the Game states,

27
00:01:25,000 --> 00:01:27,000
and he was able to arrive at a function

28
00:01:27,000 --> 00:01:30,000
that had no input from human expert players,

29
00:01:30,000 --> 00:01:32,000
but, still, was able to perform

30
00:01:32,000 --> 00:01:35,000
at the level of the very best players in the world.

31
00:01:35,000 --> 00:01:41,000
He was able to do this, after learning from examples of about 200,000 games.

32
00:01:41,000 --> 00:01:43,000
Now, that may seem like a lot--

33
00:01:43,000 --> 00:01:46,000
but it really only covers about 1 trillionth

34
00:01:46,000 --> 00:01:49,000
of the total state space of backgammon.

35
00:01:49,000 --> 00:01:51,000
Now, here's another example:

36
00:01:51,000 --> 00:01:54,000
This is a remote controlled helicopter

37
00:01:54,000 --> 00:01:56,000
that Professor Andrew Ng at Stanford trained,

38
00:01:56,000 --> 00:01:58,000
using reinforcement learning;

39
00:01:58,000 --> 00:02:00,000
and the helicopter--oh--oh, sorry--

40
00:02:00,000 --> 00:02:04,000
I made a mistake--I put this picture upside down

41
00:02:04,000 --> 00:02:08,000
because--really, Ng trained the helicopter

42
00:02:08,000 --> 00:02:11,000
to be able to fly fancy maneuvers--like flying upside down.

43
00:02:11,000 --> 00:02:15,000
And he did that by looking at only a few hours

44
00:02:15,000 --> 00:02:18,000
of training data from expert helicopter pilots

45
00:02:18,000 --> 00:02:20,000
who would take over the remote controls,

46
00:02:20,000 --> 00:02:23,000
pilot the helicopter--and those would all be recorded--

47
00:02:23,000 --> 00:02:27,000
and then, you would get rewards from when it did something good,

48
00:02:27,000 --> 00:02:29,000
or when it did something bad;

49
00:02:29,000 --> 00:02:32,000
and Ng was able to use reinforcement learrning

50
00:02:32,000 --> 00:02:34,000
to build an automated helicopter pilot,

51
00:02:34,000 --> 00:02:36,000
just from those training examples.

52
00:02:36,000 --> 00:02:39,000
And that automated pilot, too, can perform tricks

53
00:02:39,000 --> 00:02:43,000
that only a handful of humans are capable of performing.

54
00:02:43,000 --> 00:02:49,000
But enough of this still picture--let's watch a video of Ng's helicopters in action.

55
00:02:49,000 --> 00:02:52,000
[Stanford University Autonomous Helicopter]

56
00:02:52,000 --> 00:03:05,000
[sound of helicopter flying] [Chaos]

57
00:03:05,000 --> 99:59:59,999
[Stanford University Autonomous Helicopter]
