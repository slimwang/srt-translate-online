1
00:00:00,330 --> 00:00:01,930
So how are we going to do this?

2
00:00:01,930 --> 00:00:03,200
Well, it's not going to surprise you.

3
00:00:03,200 --> 00:00:06,970
We're going to use principle component
analysis because when you see a picture

4
00:00:06,970 --> 00:00:10,410
that looks like this,
where I want the extended dimension,

5
00:00:10,410 --> 00:00:13,090
I want to use that extended variance.

6
00:00:13,090 --> 00:00:16,830
So here's how we're going to do
principle component analysis for this.

7
00:00:16,830 --> 00:00:19,040
Assume we have M data points, okay?

8
00:00:19,040 --> 00:00:21,360
So x1 through xM.

9
00:00:21,360 --> 00:00:25,310
Alright, where they are in
some D dimensional space.

10
00:00:25,310 --> 00:00:26,030
R to the D.

11
00:00:26,030 --> 00:00:28,940
Where D is big, really big.

12
00:00:28,940 --> 00:00:30,488
At least 10,000.

13
00:00:30,488 --> 00:00:33,740
Maybe it's a million, okay,
so if I had a 1k by 1k image.

14
00:00:33,740 --> 00:00:37,720
That would be a million pixels, so
I have a million dimensional space.

15
00:00:38,920 --> 00:00:44,000
So, what we want is some
direction in that big space,

16
00:00:44,000 --> 00:00:46,680
that captures most of the variation.

17
00:00:46,680 --> 00:00:47,480
Okay?

18
00:00:47,480 --> 00:00:49,740
And furthermore, and
this is sort of important.

19
00:00:50,900 --> 00:00:53,710
Not sort of important,
this is really important.

20
00:00:53,710 --> 00:00:57,480
If I talk about a direction, and
let's suppose u as my direction.

21
00:00:58,480 --> 00:01:01,520
How would I find out
where it lies along that?

22
00:01:01,520 --> 00:01:04,519
Well, whats nice about this is because
we're doing everything linearly

23
00:01:04,519 --> 00:01:08,190
we're just going to take the dot
product of x minus its mean.

24
00:01:08,190 --> 00:01:09,630
Why are we subtracting the mean?

25
00:01:09,630 --> 00:01:11,960
because none of this stuff works
if you don't subtract the mean.

26
00:01:11,960 --> 00:01:14,840
Remember, we're doing
variation about the mean.

27
00:01:14,840 --> 00:01:17,910
So we subtract the mean and
we take the dot product, and

28
00:01:17,910 --> 00:01:23,660
then u X of,
u of X i that would be the coefficient.

29
00:01:23,660 --> 00:01:28,430
So we want to find the u that
captures most of the variance.

30
00:01:28,430 --> 00:01:31,590
All right, well you know already how to
do this because we've been talking about

31
00:01:31,590 --> 00:01:34,930
it right, so here's this same
expression that we had before, but

32
00:01:34,930 --> 00:01:38,000
I sort of used the notation
we just developed, right?

33
00:01:38,000 --> 00:01:40,680
We have, we're going to say,
the variance along u.

34
00:01:40,680 --> 00:01:43,910
Well we want it over all
the possible points.

35
00:01:43,910 --> 00:01:47,430
So what we say is okay, well we're
just going to take that average.

36
00:01:47,430 --> 00:01:49,680
Here is x minus mu.

37
00:01:49,680 --> 00:01:53,700
Dotted with u,
we multiply it also by its transpose.

38
00:01:53,700 --> 00:01:56,350
So that we get the square value.

39
00:01:56,350 --> 00:01:59,720
Okay, so that's just the same thing
that we had from the previous slide,

40
00:01:59,720 --> 00:02:02,090
I'm just going to rearrange
that a little bit.

41
00:02:02,090 --> 00:02:06,030
Right, so
I can pull the U's out here, and

42
00:02:06,030 --> 00:02:09,350
the, the another U out there
because it's transpose, transpose.

43
00:02:09,350 --> 00:02:12,660
And what I have left here in the middle,
well what is that?

44
00:02:12,660 --> 00:02:15,120
Right, that's just
the covariance matrix and

45
00:02:15,120 --> 00:02:16,950
we'll call it the covariance
matrix sigmund.

46
00:02:16,950 --> 00:02:20,920
We've talked about how for
finding this, the axis of,

47
00:02:20,920 --> 00:02:24,370
of, maximum variance,
we need that covariance matrix.

48
00:02:24,370 --> 00:02:26,540
But think about this
just a little bit more.

49
00:02:26,540 --> 00:02:30,220
So that variance of u is u
transpose sigma u, all right?
