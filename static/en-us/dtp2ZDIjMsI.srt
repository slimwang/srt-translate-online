1
00:00:00,000 --> 00:00:05,000
Now, in some sense, you've learned all you need to know about reinforcement learning.

2
00:00:05,000 --> 00:00:09,000
Yes, it's a huge field, and there's a lot of other details that we haven't covered

3
00:00:09,000 --> 00:00:11,000
but you've seen all the basics.

4
00:00:11,000 --> 00:00:13,000
The theory is there and it works.

5
00:00:13,000 --> 00:00:16,000
But in another sense, we haven't gone very far

6
00:00:16,000 --> 00:00:21,000
because what we've done works for these small 4 by 3 Grid Worlds,

7
00:00:21,000 --> 00:00:24,000
But it won't work very well for larger problems:

8
00:00:24,000 --> 00:00:27,000
dealing with flying helicopters or playing backgammon--

9
00:00:27,000 --> 00:00:29,000
because there's just too many states

10
00:00:29,000 --> 00:00:33,000
and we can't visit every one of the states

11
00:00:33,000 --> 00:00:36,000
and build up the correct utility values, or Q values,

12
00:00:36,000 --> 00:00:41,000
for all the billions or trillions or quadrillions of states we would need to represent.

13
00:00:41,000 --> 00:00:44,000
So let's go back to a simpler type of example.

14
00:00:44,000 --> 00:00:47,000
Here's a state in a Pacman game

15
00:00:47,000 --> 00:00:49,000
and we can see that this is a bad state,

16
00:00:49,000 --> 00:00:53,000
where Pacman is surrounded by 2 bad guys,

17
00:00:53,000 --> 00:00:56,000
and there's no place for him to escape.

18
00:00:56,000 --> 00:01:00,000
And so reinforcement learning could quickly learn that this is bad.

19
00:01:00,000 --> 00:01:02,000
But the problem is that that state has

20
00:01:02,000 --> 00:01:06,000
no relation whatsoever to this state.

21
00:01:06,000 --> 00:01:10,000
Where conceptually, it's the same problem--

22
00:01:10,000 --> 00:01:12,000
that the Pacman is stuck in a corner

23
00:01:12,000 --> 00:01:15,000
and there are bad guys own either sides of him.

24
00:01:15,000 --> 00:01:18,000
But in terms of a concrete state,

25
00:01:18,000 --> 00:01:20,000
the 2 are completely different.

26
00:01:20,000 --> 00:01:22,000
So what we want to be able to do is

27
00:01:22,000 --> 00:01:26,000
find some generalization, so that these 2 states look the same,

28
00:01:26,000 --> 00:01:29,000
and what I learn for this state--

29
00:01:29,000 --> 00:01:32,000
that learning can transfer over into this state.

30
00:01:32,000 --> 00:01:37,000
And so, just as we did in supervised machine learning, where we wanted to take

31
00:01:37,000 --> 00:01:42,000
similar points in the state and be able to reason about them, together,

32
00:01:42,000 --> 00:01:45,000
we want to be able to do the same thing for reinforcement training.

33
00:01:45,000 --> 99:59:59,999
And we can use the same type of approach.
