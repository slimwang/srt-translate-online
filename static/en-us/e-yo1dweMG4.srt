1
00:00:00,290 --> 00:00:01,100
Here. Let me put it to you this way

2
00:00:01,100 --> 00:00:05,420
Michael. What is MIMIC giving you for all of that

3
00:00:05,420 --> 00:00:08,550
work that it's doing in building dependency trees and, and

4
00:00:08,550 --> 00:00:11,072
you know, running Prim's algorithm and finding maximum spanning trees.

5
00:00:11,072 --> 00:00:13,150
>> I'm going to say structure because that's

6
00:00:13,150 --> 00:00:14,640
the word that you've been using repeatedly.

7
00:00:14,640 --> 00:00:16,149
>> You get structure. And, and another way of

8
00:00:16,149 --> 00:00:18,930
thinking about structure in this case is you're getting information.

9
00:00:18,930 --> 00:00:21,670
You get a lot more information because you get structure,

10
00:00:23,030 --> 00:00:25,970
you get a lot more information for iteration as well.

11
00:00:25,970 --> 00:00:27,220
So, that's the price you're paying,

12
00:00:27,220 --> 00:00:30,050
you're getting more information every single time

13
00:00:30,050 --> 00:00:34,640
you do an iteration, at the cost of building this maximum spanning trees or

14
00:00:34,640 --> 00:00:35,995
whatever it is you're doing in,

15
00:00:35,995 --> 00:00:39,090
in estimating your probability distribution. So, why

16
00:00:39,090 --> 00:00:44,020
would it be worth it to do that? What's the other source of expense?

17
00:00:44,020 --> 00:00:45,660
>> Well, so, all right. So there's

18
00:00:45,660 --> 00:00:47,550
all this computation within an iteration, but

19
00:00:47,550 --> 00:00:51,910
what it's, what it's doing, what it's trying to do is trying to find inputs

20
00:00:51,910 --> 00:00:55,070
that have high scores and so you do have to compute the

21
00:00:55,070 --> 00:00:58,430
scores for all those inputs. So the, the fitness calculation is very important.

22
00:00:58,430 --> 00:01:02,360
>> Right, so MIMIC tends to work very well when the cost of evaluating

23
00:01:02,360 --> 00:01:05,129
your fitness function is high. So, it's

24
00:01:05,129 --> 00:01:06,710
really important that I only have to take

25
00:01:06,710 --> 00:01:11,770
100 iterations if every single time I look at a fitness function and try

26
00:01:11,770 --> 00:01:16,700
to figure, compute it for some particular x, I pay some huge cost in time.

27
00:01:16,700 --> 00:01:17,410
>> Well,

28
00:01:17,410 --> 00:01:20,410
have you told us how many function evaluations there are in an

29
00:01:20,410 --> 00:01:23,820
iteration? because it seems like, it could be a lot in MIMIC.

30
00:01:23,820 --> 00:01:25,710
>> Well, you get one, basically for every sample you generate.

31
00:01:25,710 --> 00:01:28,600
>> Yeah and, and so, but so. I guess,

32
00:01:28,600 --> 00:01:32,105
you can compare iterations but you can also compare samples.

33
00:01:32,105 --> 00:01:34,070
>> Right, so they would depend upon how many

34
00:01:34,070 --> 00:01:35,990
samples you feel like you need to generate, and of

35
00:01:35,990 --> 00:01:38,920
course you can be very clever because. Remember, theta

36
00:01:38,920 --> 00:01:41,793
will generate a bunch of samples for theta plus 1.

37
00:01:41,793 --> 00:01:43,000
>> Mm-hm.

38
00:01:43,000 --> 00:01:45,150
>> So, if you keep track of the ones that

39
00:01:45,150 --> 00:01:47,140
you've values you've seen before, you don't have to recompute

40
00:01:47,140 --> 00:01:50,740
them. So it's actually pretty hard to know exactly what

41
00:01:50,740 --> 00:01:53,880
that's going to be. But let's imagine that at every iteration, you

42
00:01:53,880 --> 00:01:57,560
generate 100 samples. So, at most, you're going to have to

43
00:01:57,560 --> 00:02:01,640
evaluate f of x, 100 times. So, MIMIC is still

44
00:02:01,640 --> 00:02:05,100
a win over something else, if the number of iterations

45
00:02:05,100 --> 00:02:07,740
that it takes is 100 or more than 100 times fewer.

46
00:02:07,740 --> 00:02:08,473
>> Wow.

47
00:02:08,473 --> 00:02:09,692
>> Right?

48
00:02:09,692 --> 00:02:10,680
>> Yeah.

49
00:02:10,680 --> 00:02:13,990
>> So, can you think of functions, fitness functions,

50
00:02:13,990 --> 00:02:17,620
real fitness functions that might actually be expensive to compute?

51
00:02:17,620 --> 00:02:20,200
>> Well, yeah, sure. I mean a lot of this stuff that is,

52
00:02:20,200 --> 00:02:23,550
is important is like that. So, if you're trying to design a rocket

53
00:02:23,550 --> 00:02:28,670
ship or something like that, that. Fitness evaluation is, is doing

54
00:02:28,670 --> 00:02:33,788
a detailed simulation of how it performs. And that could be a very costly thing.

55
00:02:33,788 --> 00:02:36,620
>> Right, actually it's, that's a good example because MIMIC has

56
00:02:36,620 --> 00:02:39,250
been used for things like antenna design. It's been used for

57
00:02:39,250 --> 00:02:42,690
things like designing exactly where you would put a rocket in

58
00:02:42,690 --> 00:02:45,821
order to minimize fuel cost sending it to the moon. These

59
00:02:45,821 --> 00:02:49,490
sorts of things where the, the cost really isn't evaluating some

60
00:02:49,490 --> 00:02:52,760
particular configuration where you have to run a simulation or you

61
00:02:52,760 --> 00:02:56,890
have to compute a huge number of values of equations and

62
00:02:56,890 --> 00:02:59,220
so on and so forth, you know, to figure out the answer.

63
00:02:59,220 --> 00:03:00,550
Another case where it comes up a lot is

64
00:03:00,550 --> 00:03:05,340
where Your evaluation function, your fitness function, involves human beings.

65
00:03:05,340 --> 00:03:06,480
>> Oh, interesting, yeah.

66
00:03:06,480 --> 00:03:08,310
>> Where you generate something, and you ask

67
00:03:08,310 --> 00:03:10,000
a human, how does this look, or does

68
00:03:10,000 --> 00:03:12,650
this do what you want it to do because humans, as it turns out, are really slow.

69
00:03:12,650 --> 00:03:15,700
>> [LAUGH] Yeah, they're not measured in megaflops.

70
00:03:15,700 --> 00:03:17,370
>> That's right. So, you end up

71
00:03:17,370 --> 00:03:20,100
with cases where fitness functions are expensive, something

72
00:03:20,100 --> 00:03:21,960
like this becomes a big win. And that's

73
00:03:21,960 --> 00:03:24,470
a general point to make, though, that when.

74
00:03:24,470 --> 00:03:26,840
You are looking at all of the different algorithms, at different

75
00:03:26,840 --> 00:03:29,120
models or at everything that we have been doing, both for unsupervised

76
00:03:29,120 --> 00:03:32,682
learning and earlier for supervisor learning. A lot of times, your trade-off

77
00:03:32,682 --> 00:03:35,720
is just in terms of whether you are going to over-fit or not.

78
00:03:35,720 --> 00:03:39,060
It's whether it's worth the price you need to pay in

79
00:03:39,060 --> 00:03:42,580
terms of, either space or time complexity to go with one model

80
00:03:42,580 --> 00:03:45,150
versus the other. We've been talking about it in terms of sample

81
00:03:45,150 --> 00:03:47,610
complexity, but there's still time complexity

82
00:03:47,610 --> 00:03:49,120
and space complexity to worry about.

83
00:03:49,120 --> 00:03:49,650
>> Cool. I

84
00:03:49,650 --> 00:03:49,900
get it.

85
00:03:49,900 --> 00:03:52,876
>> Okay, cool. So normally what we would do next is we would

86
00:03:52,876 --> 00:03:56,168
say what, whatever we learn, but I actually kind of think we just did that.

87
00:03:56,168 --> 00:03:58,580
>> [LAUGH] indeed.

88
00:03:58,580 --> 00:04:01,650
>> So, in fact, let me do something about that.
