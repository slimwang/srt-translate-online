1
00:00:00,240 --> 00:00:02,580
Okay Michael, so here's our second ,quiz, is

2
00:00:02,580 --> 00:00:05,689
a row. In the last quiz, we talked about

3
00:00:05,689 --> 00:00:07,850
running time and space time, but now we're going to

4
00:00:07,850 --> 00:00:13,690
talk about ,how the k-nn algorithm, actually works. And

5
00:00:13,690 --> 00:00:17,990
in particular how different choices, between distance metrics, values

6
00:00:17,990 --> 00:00:21,470
of k, and how you're going to put them together,

7
00:00:21,470 --> 00:00:23,270
can give you different answers, okay? So, what I

8
00:00:23,270 --> 00:00:25,600
have over here on the left is training data.

9
00:00:25,600 --> 00:00:27,720
This is a regression problem and you're training

10
00:00:27,720 --> 00:00:29,970
data is made up of xy pairs. X is

11
00:00:29,970 --> 00:00:33,040
two dimensional. Okay? So this is a function

12
00:00:33,040 --> 00:00:38,810
from R squared to some value in R1. Okay?

13
00:00:38,810 --> 00:00:40,610
>> Mm-hm.

14
00:00:40,610 --> 00:00:43,550
>> So, the first dimension represents, something

15
00:00:43,550 --> 00:00:46,370
and the second dimension, represents something. And then

16
00:00:46,370 --> 00:00:50,740
there's some particular, output over here. And what I want you to do, is given a

17
00:00:50,740 --> 00:00:56,460
query point, 4, 2 produce what the proper y or output

18
00:00:56,460 --> 00:00:59,580
ought to be, given all of this training did. You're with me?

19
00:00:59,580 --> 00:01:00,680
>> Yeah.

20
00:01:00,680 --> 00:01:03,610
>> Okay, so I want you to do it in four different cases, I

21
00:01:03,610 --> 00:01:09,150
want you to do it in the case where, your distance matrix is euclidean, Okay.

22
00:01:09,150 --> 00:01:13,091
>> The distance metric, in R2?

23
00:01:13,091 --> 00:01:14,250
>> Yes.

24
00:01:14,250 --> 00:01:15,900
>> Oh I see because, we're going to measure the distance

25
00:01:15,900 --> 00:01:17,650
between the query and the different data points.

26
00:01:17,650 --> 00:01:17,860
>> Right.

27
00:01:17,860 --> 00:01:18,613
>> Yeah. Okay. Uh-huh.

28
00:01:18,613 --> 00:01:21,100
>> Mm-hm. So it's euclidean, for a case of one

29
00:01:21,100 --> 00:01:24,990
nearest neighbor and three nearest neighbor and I want you

30
00:01:24,990 --> 00:01:27,530
to take, for example, in the three nearest neighbor case.

31
00:01:27,530 --> 00:01:30,100
I want you take their output and average them. Okay?

32
00:01:30,100 --> 00:01:30,800
>> Okay.

33
00:01:30,800 --> 00:01:35,590
>> Now, in the I also want you to do the same thing. But in the case

34
00:01:35,590 --> 00:01:38,450
where instead of using Euclidean distance, we use Manhattan

35
00:01:38,450 --> 00:01:41,140
distance. But again, for both 1 nearest neighbor and

36
00:01:41,140 --> 00:01:43,080
3 nearest neighbor And in any case where

37
00:01:43,080 --> 00:01:44,850
we have ties, like in three nearest neighbor where

38
00:01:44,850 --> 00:01:46,060
we absolutely have to have at least three of

39
00:01:46,060 --> 00:01:48,810
these things show up, just let 'em average. Okay?

40
00:01:48,810 --> 00:01:49,030
>> Got you.

41
00:01:49,030 --> 00:01:50,480
>> Now we're doing averaging instead of

42
00:01:50,480 --> 00:01:53,130
straight voting, because, this is a regression problem.

43
00:01:53,130 --> 00:01:53,920
>> Got it.

44
00:01:53,920 --> 00:01:55,400
>> Okay. Any questions?

45
00:01:55,400 --> 00:02:01,230
>> Maybe. Let's see. Three nearest neighbor. And so if there's ties, we, we

46
00:02:01,230 --> 00:02:04,030
use the college ranking trick of including

47
00:02:04,030 --> 00:02:06,550
everybody who's at least as good as the

48
00:02:06,550 --> 00:02:08,860
k, largest or k closest.

49
00:02:08,860 --> 00:02:10,729
>> Yes, exactly.

50
00:02:10,729 --> 00:02:13,650
>> Okay, yeah, no I think, I think I can take a stab at this.

51
00:02:13,650 --> 00:02:15,090
>> Okay, cool then go.
