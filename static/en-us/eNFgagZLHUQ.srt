1
00:00:00,570 --> 00:00:03,300
The gating values for each gate get

2
00:00:03,300 --> 00:00:07,680
controlled by a tiny logistic
regression on the input parameters.

3
00:00:07,680 --> 00:00:11,060
Each of them has its own
set of shared parameters.

4
00:00:11,060 --> 00:00:14,180
And there's an additional hyperbolic
tangent sprinkled in here

5
00:00:14,180 --> 00:00:16,600
to keep the outputs between -1 and 1.

6
00:00:16,600 --> 00:00:20,220
It looks complicated but
once you write down the math

7
00:00:20,220 --> 00:00:23,018
It's literally five lines of code,
as you'll see in the assignment.

8
00:00:23,018 --> 00:00:27,930
And it's well-behaved, continues,
and differentiable all the way,

9
00:00:27,930 --> 00:00:31,360
which means we can optimize
those parameters very easily.

10
00:00:31,360 --> 00:00:33,610
So why do LSTMs work?

11
00:00:33,610 --> 00:00:37,710
Without going into too many details,
all these little gates help the model

12
00:00:37,710 --> 00:00:42,870
keep its memory longer when it needs to,
and ignore things when it should.

13
00:00:42,870 --> 00:00:46,010
As a result,
the optimization is much easier, and

14
00:00:46,010 --> 00:00:48,090
the gradient vanishing, vanishes.
