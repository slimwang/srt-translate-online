1
00:00:00,130 --> 00:00:02,719
So now let's try to apply the idea of pipelining

2
00:00:02,719 --> 00:00:06,610
to a processor. In a traditional processor, that is, nowaday

3
00:00:06,610 --> 00:00:08,656
is too simple so we only use it to teach

4
00:00:08,656 --> 00:00:12,180
students, but it still shows how things work. We have

5
00:00:12,180 --> 00:00:15,510
a program counter, we use the program counter, to act

6
00:00:15,510 --> 00:00:19,840
as the instruction memory. We get the instruction from there.

7
00:00:19,840 --> 00:00:22,280
We look at the instruction to decode it, to figure

8
00:00:22,280 --> 00:00:25,220
out which type of instruction it is. Meanwhile we could be

9
00:00:25,220 --> 00:00:29,200
reading our registers. Once we have read our registers, we can

10
00:00:29,200 --> 00:00:31,760
feed them to the ALU, where we are going to do the

11
00:00:31,760 --> 00:00:35,370
add or subtract or an XO or whatever the instruction wants

12
00:00:35,370 --> 00:00:38,010
us to do. The decoding logic is going to tell the ALU

13
00:00:38,010 --> 00:00:40,822
what to do. Once we get the result of the ALU,

14
00:00:40,822 --> 00:00:43,318
we could be done and just write the result to the

15
00:00:43,318 --> 00:00:46,800
register, or we could have a load or a store instruction.

16
00:00:46,800 --> 00:00:51,150
In which case, what the ALU computed is really the address

17
00:00:51,150 --> 00:00:53,830
that we use to access our data memory, and what

18
00:00:53,830 --> 00:00:56,360
comes out of data memory is what we end up

19
00:00:56,360 --> 00:01:00,072
writing to our registers. And then of course there is

20
00:01:00,072 --> 00:01:02,844
stuff to do with branches and so on, but basically

21
00:01:02,844 --> 00:01:05,550
we can do one instruction per cycle by starting at

22
00:01:05,550 --> 00:01:09,642
the BC, fetching the instruction, accessing the registers, doing the

23
00:01:09,642 --> 00:01:13,338
operation, accessing the data memory and then writing the result

24
00:01:13,338 --> 00:01:16,752
back to registers. So you can see that the instruction

25
00:01:16,752 --> 00:01:19,594
kind of goes through these five phases. We fetch

26
00:01:19,594 --> 00:01:22,762
the instruction, we read registers and decode, we

27
00:01:22,762 --> 00:01:24,994
use the ALU, we access the memory and

28
00:01:24,994 --> 00:01:28,400
finally, we write the register. The time to

29
00:01:28,400 --> 00:01:31,990
do this might be something like 20 nanoseconds.

30
00:01:31,990 --> 00:01:34,440
So now we can do one instruction every

31
00:01:34,440 --> 00:01:38,390
20 nanoseconds. So how do we apply pipelining

32
00:01:38,390 --> 00:01:42,180
to this? Well, the idea is that if here

33
00:01:42,180 --> 00:01:47,424
we have our fetch, the code, ALU memory and width part

34
00:01:47,424 --> 00:01:52,590
of this processor. And here we show what happens in cycle one.

35
00:01:52,590 --> 00:01:57,493
We will fetch some instruction I1. In cycle two instruction

36
00:01:57,493 --> 00:02:02,320
I1 moves to decode. In cycle C3 instruction I1

37
00:02:02,320 --> 00:02:07,326
will be doing the aerial operation. In cycle C4 I1 will

38
00:02:07,326 --> 00:02:13,970
move to do memory, and then in cycle C5 I1 will write the result. And the next

39
00:02:13,970 --> 00:02:16,480
cycle we can begin the instruction two. If

40
00:02:16,480 --> 00:02:18,920
we do that, then we really will finish one

41
00:02:18,920 --> 00:02:22,444
instruction every 20 nanoseconds. If we apply the

42
00:02:22,444 --> 00:02:25,057
idea of pipelining here, in cycle one, we are

43
00:02:25,057 --> 00:02:28,620
still fetching instruction I1. In cycle two, we will

44
00:02:28,620 --> 00:02:32,850
be decoding instruction I1, but we can begin fetching

45
00:02:32,850 --> 00:02:38,946
the instruction I2. When instruction I1 move to do the ALU operation, I2 can

46
00:02:38,946 --> 00:02:45,398
move to be decoded and I3 can be fetched. In the fourth cycle, I1 is the memory

47
00:02:45,398 --> 00:02:51,539
stage, I2 will be doing the ALU operation, I3 will be here being decoded and I4

48
00:02:51,539 --> 00:02:57,990
will be fetched. And then when I1 is in the last part of the pipeline,

49
00:02:57,990 --> 00:03:00,450
I2 will be right after it in the

50
00:03:00,450 --> 00:03:04,190
memory state. I3 will be doing the ALU operation.

51
00:03:04,190 --> 00:03:06,740
I4 will be decoded, and we will be

52
00:03:06,740 --> 00:03:10,010
fetching an instruction I5. So now the idea is

53
00:03:10,010 --> 00:03:13,070
that once the I1 leaves the pipeline, very

54
00:03:13,070 --> 00:03:15,966
soon after that I2 will leave the pipeline. If

55
00:03:15,966 --> 00:03:19,251
we divide this 20 nanosecond into five equal pieces

56
00:03:19,251 --> 00:03:22,960
for these stages, each stage will be four nanoseconds.

57
00:03:22,960 --> 00:03:26,210
And now, the latency to do I1 is till

58
00:03:26,210 --> 00:03:29,740
20 nanoseconds. So it takes 20 nanoseconds to do

59
00:03:29,740 --> 00:03:33,438
an instruction. But the throughput will be one instruction,

60
00:03:33,438 --> 00:03:36,870
will finish every four nanoseconds because once I1 leaves,

61
00:03:36,870 --> 00:03:39,246
that's how long it takes for I2 to leave,

62
00:03:39,246 --> 00:03:42,580
and then we just keep pouring instructions out. So,

63
00:03:42,580 --> 00:03:45,122
it's similar to the oil pipeline. It takes a

64
00:03:45,122 --> 00:03:47,675
while to fill the pipeline, but once we do,

65
00:03:47,675 --> 00:03:50,670
instructions keep pouring out at a very high rate.
