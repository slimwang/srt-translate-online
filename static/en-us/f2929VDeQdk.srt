1
00:00:00,270 --> 00:00:01,350
So welcome back to Computer Vision.

2
00:00:01,350 --> 00:00:03,050
We're talking about doing tracking.

3
00:00:03,050 --> 00:00:07,120
Now, last time we introduced this
idea of tracking as induction.

4
00:00:07,120 --> 00:00:10,540
That is, if I know what's going on at
time t, or I have some belief about

5
00:00:10,540 --> 00:00:15,700
the state of the universe of time t,
I make a prediction for time t plus 1.

6
00:00:15,700 --> 00:00:17,180
Then based upon that prediction,

7
00:00:17,180 --> 00:00:19,490
that might affect how I
go get my measurement.

8
00:00:19,490 --> 00:00:22,800
I get my measurement, and it's noisy,
and I correct for t plus 1.

9
00:00:22,800 --> 00:00:26,090
And that continual loop,
that's what we mean by tracking.

10
00:00:26,090 --> 00:00:28,940
And we did a little bit of, sort of,
onerous mathematics where we talked

11
00:00:28,940 --> 00:00:32,229
about, the prediction part
leveraged two things.

12
00:00:32,229 --> 00:00:35,892
It leveraged our belief of where we
were the last time that makes sense and

13
00:00:35,892 --> 00:00:40,096
all right, in terms of, we might believe
the position, we believe the velocity or

14
00:00:40,096 --> 00:00:40,777
something.

15
00:00:40,777 --> 00:00:43,190
And we had to have a dynamics model,
that is,

16
00:00:43,190 --> 00:00:46,690
something that tells us how
things will change, all right.

17
00:00:46,690 --> 00:00:50,412
So given some belief about how some
things were, and how some things change,

18
00:00:50,412 --> 00:00:53,290
that gives me a new belief
about where things might be.

19
00:00:53,290 --> 00:00:58,030
And then the correction, the,
the update, after we took a measurement,

20
00:00:58,030 --> 00:00:59,630
remember we made use of Bays Rule.

21
00:00:59,630 --> 00:01:03,899
And the idea is, we take the prediction,
we multiply that by a likelihood, and

22
00:01:03,899 --> 00:01:07,240
we'll talk more about that,
not so much today, but next time.

23
00:01:07,240 --> 00:01:12,000
And combining those and then normalizing
allows us to know the prediction,

24
00:01:12,000 --> 00:01:17,530
our best guess now, of the state at
time t after the measurement at time t.

25
00:01:17,530 --> 00:01:20,470
And that was referred to as
the posterior distribution.

26
00:01:21,760 --> 00:01:25,620
So today, we're going to do a specific
case of what's called linear dynamics,

27
00:01:25,620 --> 00:01:26,470
all right?

28
00:01:26,470 --> 00:01:32,424
Where the predictions are based upon
a linear function of our current belief.

29
00:01:32,424 --> 00:01:36,368
And we're going to assume that we've
got Gaussian noise both in terms of how

30
00:01:36,368 --> 00:01:40,185
things might change, so we have
a distribution, we believe how things

31
00:01:40,185 --> 00:01:43,640
might change plus there's some
noise added, Gaussian noise.

32
00:01:43,640 --> 00:01:45,510
And furthermore we're
taking measurements and

33
00:01:45,510 --> 00:01:48,200
our measurements are a function
of the state, plus noise, but

34
00:01:48,200 --> 00:01:49,610
what kind of noise?

35
00:01:49,610 --> 00:01:50,830
>> Gaussian noise!

36
00:01:50,830 --> 00:01:51,780
>> Very good.

37
00:01:51,780 --> 00:01:55,600
And we'll show a a simple
linear recursive filter called

38
00:01:55,600 --> 00:02:00,110
the Kalman filter that's designed
to handle this nice clean case.
