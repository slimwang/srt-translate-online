1
00:00:00,025 --> 00:00:02,980
So, or hm, that's a joke.

2
00:00:02,980 --> 00:00:03,969
Did you get that joke, Megan?

3
00:00:03,969 --> 00:00:06,310
You got the joke.

4
00:00:06,310 --> 00:00:09,300
She's quiet today,
which is an unusual thing for Meg.

5
00:00:09,300 --> 00:00:13,185
Here's how it works,
if we have the HMM, so

6
00:00:13,185 --> 00:00:16,135
if we know the HMM,
we know a and b and pi.

7
00:00:17,435 --> 00:00:20,455
We can use the forward algorithm and
the backward algorithm, and

8
00:00:20,455 --> 00:00:22,745
this is, part I'm not going to show you.

9
00:00:22,745 --> 00:00:25,315
To estimate, that's what's cap E.

10
00:00:25,315 --> 00:00:29,966
The distribution of what state I'm
likely to be in a time t, okay?

11
00:00:29,966 --> 00:00:31,686
because one has to do with
the sequence is forwards,

12
00:00:31,686 --> 00:00:33,295
one has to do with
the sequence is backward.

13
00:00:33,295 --> 00:00:35,950
I can evaluate the probability of all,
of the whole sequence.

14
00:00:35,950 --> 00:00:39,968
That allows me to estimate,
what's typically called gamma i of t,

15
00:00:39,968 --> 00:00:44,406
the probability that I'm in state one
at time t, state two, state three,

16
00:00:44,406 --> 00:00:45,126
all right?

17
00:00:45,126 --> 00:00:46,940
I estimate that distribution.

18
00:00:46,940 --> 00:00:52,360
Well, if you give me that distribution,
and I've actually seen

19
00:00:52,360 --> 00:00:57,640
at every time t what was observed, then
given the likelihood that I'm in each of

20
00:00:57,640 --> 00:01:00,950
the states, probability that I'm in each
of the states, and the observed data,

21
00:01:00,950 --> 00:01:05,410
I can figure out what
the most likely b sub j of k.

22
00:01:05,410 --> 00:01:08,000
That is, what do I think
the emission probabilities are?

23
00:01:08,000 --> 00:01:10,629
And I picked the ones that
would maximize the total

24
00:01:10,629 --> 00:01:12,480
observation probability.

25
00:01:12,480 --> 00:01:16,560
It's no different than saying maximum
likelihood, you know, if, if I, if I'm

26
00:01:16,560 --> 00:01:20,360
drawing something, and I pull, you know,
seven, I pull ten things out, and seven

27
00:01:20,360 --> 00:01:23,810
of them are black, and three are red, I
say there's a 70% chance of being black.

28
00:01:23,810 --> 00:01:27,270
That value is the one that
would've maximized the likelihood

29
00:01:27,270 --> 00:01:28,730
of pulling out seven of them.

30
00:01:28,730 --> 00:01:30,430
It's a maximization procedure.

31
00:01:30,430 --> 00:01:35,280
So I can choose the b sub k's that
maximize the total observation,

32
00:01:35,280 --> 00:01:39,940
given those probabil,
the idea which state I'm in, okay?

33
00:01:39,940 --> 00:01:44,820
Also, given that distribution,
I can also say well, if I think there's

34
00:01:44,820 --> 00:01:49,580
a 75% chance that I'm in
state one at this time, and

35
00:01:49,580 --> 00:01:54,650
a 90% chance that I'm in state two at
the next time, that tells me that well,

36
00:01:54,650 --> 00:01:59,280
you know, the likelihood of going from
one to two must be kind of high, right?

37
00:01:59,280 --> 00:02:02,693
And basically by looking over all of
time and looking at my guess about

38
00:02:02,693 --> 00:02:07,070
the states, I can come up with the best
possible transition probabilities.

39
00:02:07,070 --> 00:02:11,640
And when I say best, is when, again,
the ones that maximize the probability.

40
00:02:11,640 --> 00:02:15,910
That gives me my new a i j's,
and I've got new b k's.

41
00:02:15,910 --> 00:02:18,970
I can also guess new pi's,
but those never matter.

42
00:02:18,970 --> 00:02:21,190
That's my new machine, right?

43
00:02:21,190 --> 00:02:24,630
So after I do the end step, called the
maximization step, of getting the new

44
00:02:24,630 --> 00:02:29,180
A's and B's, I now have a new machine,
I can go back and do the E step again.

45
00:02:29,180 --> 00:02:32,810
I can estimate the probabilities of
being at a particular state of time,

46
00:02:32,810 --> 00:02:34,830
T, and I redo that process.

47
00:02:34,830 --> 00:02:38,083
And that iterative process,
it's called expectation maximization.

48
00:02:38,083 --> 00:02:42,580
The particular algorithm for HMMs
was called the Baum-Welch algorithm.

49
00:02:42,580 --> 00:02:46,830
Like I said, there's plenty of tutorials
out there for you to look at it.

50
00:02:46,830 --> 00:02:50,430
The fundamental idea is that by using
those recursive algorithms, and

51
00:02:50,430 --> 00:02:53,550
then estimating the probability of being
at a particular state at a particular

52
00:02:53,550 --> 00:02:55,990
time, I can re-estimate the machine.
