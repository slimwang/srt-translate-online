1
00:00:00,000 --> 00:00:02,000
So, we've learned quite a bit so far.

2
00:00:02,000 --> 00:00:06,000
We've learned about Markov Decision Processes.

3
00:00:06,000 --> 00:00:10,000
We have fully observable with a set of states

4
00:00:10,000 --> 00:00:14,000
and corresponding actions where they have stochastic action effects

5
00:00:14,000 --> 00:00:19,000
characterized by a conditional probability entity of P of S prime

6
00:00:19,000 --> 00:00:22,000
given that we apply action A in state S.

7
00:00:22,000 --> 00:00:25,000
We seek to maximize a reward function

8
00:00:25,000 --> 00:00:27,000
that we define over states.

9
00:00:27,000 --> 00:00:30,000
You can equally define over states in action pairs.

10
00:00:30,000 --> 00:00:33,000
The objective was to maximize the expected

11
00:00:33,000 --> 00:00:36,000
future accumulative and discounted rewards,

12
00:00:36,000 --> 00:00:38,000
as shown by this formula over here.

13
00:00:38,000 --> 00:00:42,000
The key to solving them was called value iteration

14
00:00:42,000 --> 00:00:45,000
where we assigned a value to each state.

15
00:00:45,000 --> 00:00:47,000
There's alternative techniques that have assigned values

16
00:00:47,000 --> 00:00:50,000
to state action pairs, often called Q(s, a),

17
00:00:50,000 --> 00:00:53,000
but we didn't really consider this so far.

18
00:00:53,000 --> 00:00:55,000
We defined a recursive update rule

19
00:00:55,000 --> 00:00:58,000
to update V(s) that was very logical

20
00:00:58,000 --> 00:01:00,000
after we understood that we have an action choice,

21
00:01:00,000 --> 00:01:03,000
but nature chooses for us the outcome of the action

22
00:01:03,000 --> 00:01:07,000
in a stochastic transition probability over here.

23
00:01:07,000 --> 00:01:10,000
And then we observe the value iteration converged

24
00:01:10,000 --> 00:01:12,000
and we're able to define a policy if we're assuming

25
00:01:12,000 --> 00:01:16,000
the argmax under the value iteration expression,

26
00:01:16,000 --> 00:01:18,000
which I don't spell out over here.

27
00:01:18,000 --> 00:01:20,000
This is a beautiful framework.

28
00:01:20,000 --> 00:01:22,000
It's really different from planning than before

29
00:01:22,000 --> 00:01:26,000
because of the stochasticity of the action effects.

30
00:01:26,000 --> 00:01:29,000
Rather than making a single sequence of states and actions,

31
00:01:29,000 --> 00:01:31,000
as would be the case in deterministic planning,

32
00:01:31,000 --> 00:01:35,000
now we make an entire field a so-called policy

33
00:01:35,000 --> 00:01:39,000
that assigns an action to every possible state.

34
00:01:39,000 --> 00:01:42,000
And we compute this using a technique called value iteration

35
00:01:42,000 --> 99:59:59,999
that spreads value in reverse order through the field of states.
