1
00:00:00,250 --> 00:00:00,970
Okay, Michael.

2
00:00:00,970 --> 00:00:04,700
So, let me sort of jump in on that
last thing that you asked me.

3
00:00:04,700 --> 00:00:07,730
We've just been talking about
how to compute probabilities

4
00:00:08,800 --> 00:00:11,940
of optimal actions based upon
what human beings are giving us.

5
00:00:11,940 --> 00:00:13,070
And that's fine.

6
00:00:13,070 --> 00:00:16,540
But it turns out we actually have
multiple sources of information, and

7
00:00:16,540 --> 00:00:17,900
we would like to be
able to combine them.

8
00:00:17,900 --> 00:00:21,150
In particular,
we have some policy that we've learned

9
00:00:21,150 --> 00:00:25,380
that gives us a probability distribution
over actions that we learn from a human,

10
00:00:25,380 --> 00:00:27,500
that's why it says pi sub H here.

11
00:00:27,500 --> 00:00:30,320
And you notice here I'm treating
a policy as a probability distribution

12
00:00:30,320 --> 00:00:33,990
over actions, and that's all neat.

13
00:00:33,990 --> 00:00:37,040
And these, in fact, are the numbers
that we came up with in our last quiz.

14
00:00:37,040 --> 00:00:39,679
But while the agent is
running around in the world.

15
00:00:40,680 --> 00:00:44,130
The agent has information from
running around in the world, right?

16
00:00:44,130 --> 00:00:45,990
There were rewards.

17
00:00:45,990 --> 00:00:48,160
I've actually gone through and
played Pac-Man.

18
00:00:48,160 --> 00:00:50,700
And I've been eaten and I've won and
I've gone through rooms and

19
00:00:50,700 --> 00:00:52,160
I've done all these kinds of things.

20
00:00:52,160 --> 00:00:56,160
And it seems kind of silly to ignore
what I actually know about what's going

21
00:00:56,160 --> 00:00:59,360
on in the world, particularly in the
case where human being might, in fact,

22
00:00:59,360 --> 00:01:00,720
be incorrect sometimes,

23
00:01:00,720 --> 00:01:05,450
or may really believe that
the optimal policy is one thing,

24
00:01:05,450 --> 00:01:09,910
but in fact, for the underlying NVP,
the optimal policy is something else.

25
00:01:09,910 --> 00:01:13,280
So, I just want to ask you a couple
of really simple questions.

26
00:01:13,280 --> 00:01:16,880
Let's imagine that I've got some
algorithm that runs around in the world,

27
00:01:16,880 --> 00:01:22,440
on its own, and learns distributions
over actions in every given state, okay?

28
00:01:22,440 --> 00:01:27,510
And it says after a little while that
the probability that action x is optimal

29
00:01:27,510 --> 00:01:33,250
as 1/3, action y is optimal as 1/3, and
action z is being optimal is also 1/3.

30
00:01:33,250 --> 00:01:35,660
And so now I've got two
sources of information here,

31
00:01:35,660 --> 00:01:40,620
what the humans told me, and I know
that's 2/3, 1/6, 1/6 because math, and

32
00:01:40,620 --> 00:01:44,590
I've learned from experience so
far that all actions are equally likely.

33
00:01:44,590 --> 00:01:45,800
>> Which is pretty non-committal.

34
00:01:45,800 --> 00:01:46,840
>> It is pretty non-committal.

35
00:01:46,840 --> 00:01:48,970
In fact it might be what
happens when you start off.

36
00:01:48,970 --> 00:01:53,920
If you had to choose an action in this
case, which action would you choose?

37
00:01:53,920 --> 00:01:54,420
>> X.

38
00:01:54,420 --> 00:01:54,950
>> Why?

39
00:01:54,950 --> 00:01:56,100
>> Is that the quiz?

40
00:01:56,100 --> 00:01:56,950
>> No, that's a question.

41
00:01:56,950 --> 00:01:57,590
>> It's Y?

42
00:01:57,590 --> 00:02:00,270
Wow, I had no idea why it would be Y.

43
00:02:00,270 --> 00:02:01,963
>> Explain to me why you chose X.

44
00:02:01,963 --> 00:02:03,650
Sorry.

45
00:02:04,790 --> 00:02:07,580
>> I see, why I chose x?

46
00:02:07,580 --> 00:02:11,930
Because well according to the algorithm,
it doesn't really care, doesn't have any

47
00:02:11,930 --> 00:02:15,240
reason to believe one over the other,
but the human has reason to believe it.

48
00:02:15,240 --> 00:02:17,320
I guess there's one
missing parameter maybe,

49
00:02:17,320 --> 00:02:19,490
which is how much we
should believe the human.

50
00:02:19,490 --> 00:02:21,480
And so,
if we believe the human not at all,

51
00:02:21,480 --> 00:02:23,940
it's still not being bad to go for X.

52
00:02:23,940 --> 00:02:24,830
>> But here's the thing.

53
00:02:24,830 --> 00:02:26,090
We actually have that parameter.

54
00:02:26,090 --> 00:02:27,390
>> That was C!

55
00:02:27,390 --> 00:02:30,650
>> That was C, and in fact,
it doesn't matter what C is,

56
00:02:30,650 --> 00:02:33,530
because that's all inside
this distribution.

57
00:02:33,530 --> 00:02:36,960
This distribution, two-thirds,
one-sixth, one-sixth, actually already

58
00:02:36,960 --> 00:02:40,300
incorporates all the information
about the uncertainty of the human.

59
00:02:40,300 --> 00:02:41,680
I don't think it does.

60
00:02:41,680 --> 00:02:42,540
>> Yeah, absolutely does.

61
00:02:42,540 --> 00:02:46,020
The probability that X
is optimal is in fact

62
00:02:46,020 --> 00:02:49,780
a function of the data that
the human has given us.

63
00:02:49,780 --> 00:02:53,328
And how likely they are to be correct
in the data that they've given us.

64
00:02:53,328 --> 00:02:57,429
>> But it seems like it could change
over time as we've come to believe

65
00:02:57,429 --> 00:02:59,320
the algorithm more.

66
00:02:59,320 --> 00:03:01,144
Then we should believe the human less.

67
00:03:01,144 --> 00:03:04,293
>> It's really interesting you say that
because all the other various techniques

68
00:03:04,293 --> 00:03:06,800
that have been out there in
the world actually do that.

69
00:03:06,800 --> 00:03:12,370
They actually try to capture what
the human has said and what you've

70
00:03:12,370 --> 00:03:16,650
learned in the world and then over time
believe the human less and less often

71
00:03:16,650 --> 00:03:20,250
because whatever the real world is
telling you is got to be correct.

72
00:03:20,250 --> 00:03:24,260
What's actually beautiful about
this particular mechanism that we

73
00:03:24,260 --> 00:03:27,450
described in the last couple of
slides is that that parameter c,

74
00:03:27,450 --> 00:03:30,730
that confidence that we have that
probability the human is correct

75
00:03:30,730 --> 00:03:33,230
actually sort of captures
everything we need to know,

76
00:03:33,230 --> 00:03:36,690
if we are completely certain about
the human then these numbers will

77
00:03:36,690 --> 00:03:40,390
reflect that by peaking over
whatever action should be optimal.

78
00:03:40,390 --> 00:03:41,740
And if we're uncertain about the human,

79
00:03:41,740 --> 00:03:44,490
then these things will tend
to stay towards the uniform.
