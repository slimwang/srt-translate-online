1
00:00:00,034 --> 00:00:03,423
All right, it's time to wrap up. Here's what I hope you've taken away from this unit.

2
00:00:03,423 --> 00:00:09,640
Remember APOD--Analyze, Parallelize, Optimize, and Deploy.

3
00:00:09,640 --> 00:00:15,901
The important points here are do profile-guided optimization at every step and deploy early and often

4
00:00:15,901 --> 00:00:19,391
rather than optimizing forever in a vacuum. I can't emphasize this enough.

5
00:00:19,391 --> 00:00:22,838
Optimization takes effort and often complicates code,

6
00:00:22,838 --> 00:00:27,580
so optimize only where and when you need it and go around this cycle multiple times.

7
00:00:27,580 --> 00:00:30,399
Now, most codes are limited by memory bandwidth.

8
00:00:30,399 --> 00:00:33,765
So compare your performance to the theoretical peak bandwidth

9
00:00:33,765 --> 00:00:36,340
and if it's lacking, see what you can do about it.

10
00:00:36,340 --> 00:00:39,928
Things that will help improve in order from most to least important

11
00:00:39,928 --> 00:00:45,088
are assure sufficient occupancy, make sure you have enough threads to keep the machine busy.

12
00:00:45,088 --> 00:00:48,628
This doesn't mean having as many threads as you can possibly fit on the machine,

13
00:00:48,628 --> 00:00:51,623
but you do need enough that the machine is basically busy.

14
00:00:51,623 --> 00:00:54,465
Coalesce global memory accesses.

15
00:00:54,465 --> 00:00:59,004
Really strive to see if you can find a way to cast your algorithm so that you achieve perfect coalescing.

16
00:00:59,004 --> 00:01:02,181
And if you can't, consider whether you can do a transpose operation

17
00:01:02,181 --> 00:01:05,711
or something that will get poor coalescing once

18
00:01:05,711 --> 00:01:11,458
but then put the data into a memory form where all your subsequent accesses will get good coalescing.

19
00:01:11,458 --> 00:01:13,493
Remember Little's law.

20
00:01:13,493 --> 00:01:15,360
In order to achieve the maximum bandwidth,

21
00:01:15,360 --> 00:01:18,513
you may need to reduce the latency between your memory accesses.

22
00:01:18,513 --> 00:01:23,292
And so for example, we saw that in 1 case we spent too much time waiting at barriers.

23
00:01:23,292 --> 00:01:25,085
By reducing the number of threads in a block,

24
00:01:25,085 --> 00:01:28,270
we are able to reduce the average time spent waiting at a barrier

25
00:01:28,270 --> 00:01:31,985
and help saturate that global memory bandwidth.

26
00:01:31,985 --> 00:01:36,212
We've talked about minimizing the branch divergence experience by threads.

27
00:01:36,212 --> 00:01:39,910
Remember that this really applies to threads that diverge within a warp.

28
00:01:39,910 --> 00:01:44,590
If the warps themselves diverge--in other words, if all the threads within a warp take the same branch,

29
00:01:44,590 --> 00:01:48,287
go in the same code path--then that comes for free.

30
00:01:48,287 --> 00:01:51,345
There's no additional penalty for threads in different warps diverging.

31
00:01:51,345 --> 00:01:56,480
It's only when threads within a warp diverge that you have to execute both sides of the branch.

32
00:01:56,480 --> 00:02:00,178
As a rule, you should generally try to avoid branchy code--

33
00:02:00,178 --> 00:02:03,874
code with of lots of if statements, switch statements, and so on.

34
00:02:03,874 --> 00:02:08,342
And you should generally be thinking about avoiding thread workload imbalance.

35
00:02:08,342 --> 00:02:10,685
In other words, if you have loops in your kernels

36
00:02:10,685 --> 00:02:14,468
that might execute a very different number of times between threads,

37
00:02:14,468 --> 00:02:18,018
then that 1 thread that's taking much longer than average thread

38
00:02:18,018 --> 00:02:20,729
can end holding the rest of the threads hostage.

39
00:02:20,729 --> 00:02:23,829
All that said, don't let a little bit of thread divergence freak you out.

40
00:02:23,829 --> 00:02:27,594
Remember we analyzed a real-world example of dealing with boundary conditions

41
00:02:27,594 --> 00:02:31,333
at the edge of an image and figured out that in fact the if statements

42
00:02:31,333 --> 00:02:35,699
to guard the edge of the images weren't really costing us very much.

43
00:02:35,699 --> 00:02:39,276
Only a few warps ended up being divergent.

44
00:02:39,276 --> 00:02:42,604
If you're limited by the actual computational performance of your kernel

45
00:02:42,604 --> 00:02:45,574
rather than the time it takes to get the data to and from your kernel,

46
00:02:45,574 --> 00:02:48,427
then consider using fast math operations.

47
00:02:48,427 --> 00:02:53,416
This includes things like the intrinsics for sine and cosine and so forth.

48
00:02:53,416 --> 00:02:56,157
They go quite a bit faster than their math.h counterparts,

49
00:02:56,157 --> 00:02:58,931
at the cost of a few bits of precision.

50
00:02:58,931 --> 00:03:02,282
And remember that when you use double precision it should be on purpose.

51
00:03:02,282 --> 00:03:08,164
So just typing the literal 3.14, well, that's a 64-bit double precision number,

52
00:03:08,164 --> 00:03:10,571
and the compiler will treat it as such,

53
00:03:10,571 --> 00:03:15,012
whereas typing 3.14f tells the compiler, hey, this is a single precision operation,

54
00:03:15,012 --> 00:03:19,157
you don't have to promote everything I multiply this by or add this to

55
00:03:19,157 --> 00:03:21,497
to be a double precision number.

56
00:03:21,497 --> 00:03:24,552
Finally, if you're limited by host device memory transfer time,

57
00:03:24,552 --> 00:03:29,863
consider using streams and asynchronous memcpys to overlap computation and memory transfers.

58
00:03:30,647 --> 00:03:33,672
And that's it. Now go forth and optimize your codes.
