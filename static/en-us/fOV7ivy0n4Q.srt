1
00:00:00,160 --> 00:00:02,160
All right, Michael. So, I think that brings us

2
00:00:02,160 --> 00:00:03,740
to the end of what I wanted to talk about

3
00:00:03,740 --> 00:00:06,340
anyway. So, can you help me remember what it is

4
00:00:06,340 --> 00:00:08,910
that we've learned today? In particular, what you've learned today?

5
00:00:08,910 --> 00:00:12,230
>> Sure. So, I guess the first think I learned is

6
00:00:12,230 --> 00:00:15,350
that game theory can make you depressed. And, in fact, in

7
00:00:15,350 --> 00:00:19,770
particular that my friend Charles, given the opportunity. Would totally drop

8
00:00:19,770 --> 00:00:23,860
a dime on me just to save a month of incarceration.

9
00:00:23,860 --> 00:00:24,350
>> Yeah.

10
00:00:24,350 --> 00:00:24,960
>> Wait, no,

11
00:00:24,960 --> 00:00:26,470
no, no, I don't think you summarized that

12
00:00:26,470 --> 00:00:29,080
correctly. Game theory is not depressed. It's depressing.

13
00:00:29,080 --> 00:00:30,930
>> Oh, yeah. That's a good point, that's a good point.

14
00:00:30,930 --> 00:00:34,880
>> And Michael is not cruel. He is the victim of cruelty.

15
00:00:34,880 --> 00:00:37,530
>> I don't think so. Because you want to know what the secret here is,

16
00:00:37,530 --> 00:00:39,760
Michael? You've got a little matrix of

17
00:00:39,760 --> 00:00:42,230
numbers. Those number capture what's going on.

18
00:00:42,230 --> 00:00:46,410
The truth, Michael, is that, if we were in Prisoner's Dilemma. I would cooperate

19
00:00:46,410 --> 00:00:50,010
with you because my utility is not simply the number of months that I

20
00:00:50,010 --> 00:00:54,980
would spend in jail. But it's the number of months you also would spend in jail.

21
00:00:54,980 --> 00:00:55,900
>> Oo. Interesting.

22
00:00:55,900 --> 00:00:57,180
>> So if I, the best way to

23
00:00:57,180 --> 00:00:59,395
beat Prisoner's Dilemma, is to change the numbers.

24
00:00:59,395 --> 00:01:04,370
>> [LAUGH] I see. It's like the Kobayashi Maru of game theory.

25
00:01:04,370 --> 00:01:06,700
>> Exactly. So there's an interesting question for you

26
00:01:06,700 --> 00:01:10,320
right there, Michael. If I had prisoner's dilemma here, here,

27
00:01:10,320 --> 00:01:11,600
let's write it out so you can remember. If I

28
00:01:11,600 --> 00:01:15,010
had prisoner's dilemma here, we already know that we're going to

29
00:01:15,010 --> 00:01:17,200
end up here, because that's what the numbers tell

30
00:01:17,200 --> 00:01:19,580
us to do. But what we'd have to do

31
00:01:19,580 --> 00:01:21,890
is change the game. So how would you change

32
00:01:21,890 --> 00:01:24,450
the game in prisoner's dilemma? I see. So, if, if

33
00:01:24,450 --> 00:01:27,470
we're thinking about it in particular in terms of

34
00:01:27,470 --> 00:01:29,550
I care about how long you spend in jail.

35
00:01:29,550 --> 00:01:30,780
Maybe not as much as I care about how

36
00:01:30,780 --> 00:01:32,780
much I spend in jail. Like maybe half as much.

37
00:01:32,780 --> 00:01:33,441
>> Mm-hm.

38
00:01:33,441 --> 00:01:35,660
>> Then, the payments shift, right?

39
00:01:35,660 --> 00:01:35,950
>> Right.

40
00:01:35,950 --> 00:01:38,210
>> So, now we have like minus1 and a

41
00:01:38,210 --> 00:01:40,180
half, minus 1 and a half in the upper left

42
00:01:40,180 --> 00:01:40,880
hand corner.

43
00:01:40,880 --> 00:01:49,540
>> Mm-hm. Minus 9 comma minus 4.5, minus 4.5, minus 9 and minus 9, minus 9.

44
00:01:49,540 --> 00:01:49,820
>> So, yeah.

45
00:01:49,820 --> 00:01:52,390
>> So now that, that bottom right becomes a lot

46
00:01:52,390 --> 00:01:55,120
less attractive if we actually care about the other person.

47
00:01:55,120 --> 00:01:55,840
>> Right.

48
00:01:55,840 --> 00:01:58,360
>> Well that's, that's, that's, okay, I'm less depressed now.

49
00:01:58,360 --> 00:02:02,480
>> Except of course, that requires that you feel that way internally and that

50
00:02:02,480 --> 00:02:04,290
I feel that way internally. There's another

51
00:02:04,290 --> 00:02:06,130
way that you could change the game here.

52
00:02:06,130 --> 00:02:08,490
Which is, what happens to sniches in jail?

53
00:02:08,490 --> 00:02:10,280
>> They are rewarded.

54
00:02:10,280 --> 00:02:12,240
>> No. No they're not.

55
00:02:13,590 --> 00:02:14,530
>> They're punished.

56
00:02:14,530 --> 00:02:15,360
>> Yes.

57
00:02:15,360 --> 00:02:16,050
>> Oh.

58
00:02:16,050 --> 00:02:19,460
>> So, if your a part of the criminal fraternity, and you don't like

59
00:02:19,460 --> 00:02:23,450
prisoners dilemma, then what you have to do is to create a system Where the

60
00:02:23,450 --> 00:02:26,860
people who snitch get punished. So it's not just the months that they spend

61
00:02:26,860 --> 00:02:28,680
in jail, it's everything else that's going to

62
00:02:28,680 --> 00:02:30,840
happen to them if they drop a dime.

63
00:02:30,840 --> 00:02:31,450
>> So

64
00:02:31,450 --> 00:02:36,450
you're saying that minus 6, minus 6, ends up being worse?

65
00:02:36,450 --> 00:02:37,230
>> No, what I'm.

66
00:02:37,230 --> 00:02:40,820
>> No, the minus, wait, no, wait, what? Yeah. Oh, the

67
00:02:40,820 --> 00:02:44,060
zero ends up getting, oh I see the zero ends up being

68
00:02:44,060 --> 00:02:46,470
worse. Because even though your not in jail your going to get

69
00:02:47,830 --> 00:02:53,180
I don't know somehow thwarted or, or punished for your past behaviors.

70
00:02:53,180 --> 00:02:54,130
>> Accosted.

71
00:02:54,130 --> 00:02:55,090
>> Interesting.

72
00:02:55,090 --> 00:02:56,540
>> That's right. So that's

73
00:02:56,540 --> 00:02:58,800
what you have to do and that works not just with criminals but with

74
00:02:58,800 --> 00:03:00,110
the real world. Whenever your in this

75
00:03:00,110 --> 00:03:02,330
sort of situation like a prisoners dilemma,

76
00:03:02,330 --> 00:03:04,720
you can change the game by changing

77
00:03:04,720 --> 00:03:07,460
everyone's utilities. Like for example hiring police

78
00:03:07,460 --> 00:03:11,310
officer, police officers or hiring members of

79
00:03:11,310 --> 00:03:13,160
the mob to take care of everything.

80
00:03:13,160 --> 00:03:14,500
>> I see. So it almost seems like what

81
00:03:14,500 --> 00:03:16,290
you're talking about is a kind of inverse game

82
00:03:16,290 --> 00:03:18,600
theory, where if there's a particular behavior that I

83
00:03:18,600 --> 00:03:22,180
want to see. How do I set up the payments and

84
00:03:22,180 --> 00:03:25,760
rewards so that that behavior is encouraged.

85
00:03:25,760 --> 00:03:27,080
>> Right, and by the way that has

86
00:03:27,080 --> 00:03:31,680
a name, and it's called mechanism design. Mechanism design?

87
00:03:31,680 --> 00:03:32,230
>> Yes.

88
00:03:32,230 --> 00:03:35,748
>> I'm not sure I understand either of those words. [LAUGH]

89
00:03:35,748 --> 00:03:38,340
>> Well, that's where you're trying to set up the set of incentives,

90
00:03:38,340 --> 00:03:41,450
the mechanisms that you're using to pay people. You're trying to design them in

91
00:03:41,450 --> 00:03:44,520
such a way to get particular behavior. This is what a lot of

92
00:03:44,520 --> 00:03:47,040
economics is all about. This is what a lot of government is all about.

93
00:03:47,040 --> 00:03:51,410
Tax breaks for example, for mortgage interest, encourages you

94
00:03:51,410 --> 00:03:53,380
to buy a home, rather than rent a home.

95
00:03:53,380 --> 00:03:55,590
>> I see, by changing the payoff structure.

96
00:03:55,590 --> 00:03:55,990
>> Right.

97
00:03:55,990 --> 00:03:56,830
>> Oh that's neat.

98
00:03:56,830 --> 00:04:01,302
>> And so that's what we learned today. At least right now. [LAUGH]

99
00:04:01,302 --> 00:04:03,780
>> Okay. Alright. So, let's see. So, just to try

100
00:04:03,780 --> 00:04:06,070
to rattle off some of the other things. The whole

101
00:04:06,070 --> 00:04:09,390
notion of Game Theory. We talked about, especially the idea

102
00:04:09,390 --> 00:04:12,360
that. You can think about a game as a tree or

103
00:04:12,360 --> 00:04:15,070
you could represent it as a matrix.

104
00:04:15,070 --> 00:04:18,019
And, I believe you said, repeatedly, the matrix

105
00:04:18,019 --> 00:04:21,529
has everything. Is that how you said it? Or the matrix is all you need.

106
00:04:21,529 --> 00:04:22,029
>> Yip!

107
00:04:24,110 --> 00:04:27,870
Let's see. We talked about minimax and maximin.

108
00:04:27,870 --> 00:04:28,735
>> Mm-hm.

109
00:04:28,735 --> 00:04:32,830
>> We, we relaxed a bunch of constraint on games.

110
00:04:32,830 --> 00:04:33,365
>> Mm-hm.

111
00:04:33,365 --> 00:04:37,160
>> So we, we looked at both perfect and hidden information.

112
00:04:37,160 --> 00:04:38,745
>> Mm-hm.

113
00:04:38,745 --> 00:04:41,870
>> We looked at both zero sum and non zero sum.

114
00:04:41,870 --> 00:04:45,581
>> We learned a lot today. We looked at deterministic

115
00:04:45,581 --> 00:04:49,650
and [UNKNOWN] I would want to say, but you called it non-deterministic.

116
00:04:49,650 --> 00:04:51,410
And assuming that we can get rid of the first two

117
00:04:51,410 --> 00:04:56,110
bullet items that look like maybe they were jokes. I would suggest

118
00:04:56,110 --> 00:04:59,130
saying things like, we talked about what strategies are and that they

119
00:04:59,130 --> 00:05:04,610
come in different flavors. We talked about the evil prisoners dilemma game.

120
00:05:04,610 --> 00:05:09,697
>> Mm-hm. What else? You gotta give me more, otherwise it's [INAUDIBLE].

121
00:05:09,697 --> 00:05:14,520
>> Oh, more, good point. Andrew Moore gave lots of really good examples that

122
00:05:14,520 --> 00:05:15,153
>> yes.

123
00:05:15,153 --> 00:05:18,240
>> Michael may be cruel, but Andrew Moore is awesome.

124
00:05:18,240 --> 00:05:21,290
>> He's more cool than me. Andrew Moor is very

125
00:05:21,290 --> 00:05:24,470
cool. All of the examples that we've used today, or almost

126
00:05:24,470 --> 00:05:26,580
all of the examples we used today actually come from

127
00:05:26,580 --> 00:05:30,500
Andrew Moore's slides. Andrew Moore is a professor at Carnegie Mellon

128
00:05:30,500 --> 00:05:32,388
or at least he was before he went off to

129
00:05:32,388 --> 00:05:35,960
Google. And is a really smart guy who cares very much

130
00:05:35,960 --> 00:05:38,500
about machine learning. And game theory and produced a bunch

131
00:05:38,500 --> 00:05:40,420
of slides that it turns out lots and lots of people

132
00:05:40,420 --> 00:05:44,720
use in their own courses. And his examples were so good for game theory. That I

133
00:05:44,720 --> 00:05:46,980
decided to coop them with his permission of

134
00:05:46,980 --> 00:05:49,550
course. He tells everyone that they may use them.

135
00:05:49,550 --> 00:05:50,820
And, in fact, we have pointers to the

136
00:05:50,820 --> 00:05:54,780
slides in the resources links and folders for all

137
00:05:54,780 --> 00:05:56,080
of you to look at. And I recommend

138
00:05:56,080 --> 00:05:58,490
that you do. Did we learn anything else, Michael?

139
00:05:58,490 --> 00:05:59,810
>> The only other thing that I would want to

140
00:05:59,810 --> 00:06:04,290
mention is NASH which is a concept that is nashtastic.

141
00:06:04,290 --> 00:06:06,280
>> It is nashtastic.

142
00:06:06,280 --> 00:06:09,240
There are by the way, I should mention briefly. Other kinds

143
00:06:09,240 --> 00:06:12,890
of equilibria concepts they're beyond the scope of this class. But

144
00:06:12,890 --> 00:06:15,030
there's a whole lot more to game theory as you might

145
00:06:15,030 --> 00:06:18,830
imagine. And sometimes when they ask you to in these situations where

146
00:06:18,830 --> 00:06:20,720
you can't do what you want to do, you end up in

147
00:06:20,720 --> 00:06:24,340
these prisoner's lemonade situations. Other kinds of equilibria can get you

148
00:06:24,340 --> 00:06:28,200
out of it. And I'm going to argue without explaining why

149
00:06:28,200 --> 00:06:31,590
that the way that they get around this is by introducing Other

150
00:06:31,590 --> 00:06:35,520
ways of doing various kinds of communication. and, in fact, I claim they're

151
00:06:35,520 --> 00:06:39,150
a particular part of mechanism design. But that's a topic for another day.

152
00:06:39,150 --> 00:06:40,350
>> Okay. Fair enough.

153
00:06:40,350 --> 00:06:41,840
>> Okay. Did we learn anything else, Michael?

154
00:06:41,840 --> 00:06:44,420
>> I don't know. That's what I was thinking about. I mean, that seems

155
00:06:44,420 --> 00:06:48,480
like a lot to absorb. And, the other thing is that repeated games, even

156
00:06:48,480 --> 00:06:51,950
the prisoner's dilemma kind of unravel if you know when the end. And I

157
00:06:51,950 --> 00:06:54,960
was going to look into what happens if you don't know when they're going to end.

158
00:06:54,960 --> 00:06:56,700
>> Okay. So, I guess,

159
00:06:56,700 --> 00:06:58,760
that will be something that we will learn. Next time.

160
00:06:58,760 --> 00:07:01,110
>> Right, what we, what will we have will learned?

161
00:07:01,110 --> 00:07:04,220
>> Yes. Future past tense.

162
00:07:04,220 --> 00:07:05,811
>> [LAUGH]

163
00:07:05,811 --> 00:07:07,650
>> Alright, Michael, well, I think that's about it.

164
00:07:07,650 --> 00:07:10,800
At least my brain is full. So I will talk

165
00:07:10,800 --> 00:07:12,590
to you next time, and you get to lead what

166
00:07:12,590 --> 00:07:15,770
I believe is the last full lesson of the course.

167
00:07:15,770 --> 00:07:18,970
>> Oh, exciting. It is excuiting. Alright, well bye

168
00:07:18,970 --> 00:07:20,530
Michael. You have fun. I'll see you next time.

169
00:07:20,530 --> 00:07:22,050
>> Alright. Bye Charles.

170
00:07:22,050 --> 00:07:22,840
>> Bye.
