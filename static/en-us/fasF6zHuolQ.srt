1
00:00:00,000 --> 00:00:04,100
In addition to constant-factor approximation algorithms, which we will look at

2
00:00:04,100 --> 00:00:07,810
in more detail soon, we are also going to be talking about advanced algorithms

3
00:00:07,810 --> 00:00:13,120
that are known as polynomial-time approximation schemes. Scary term.

4
00:00:13,120 --> 00:00:18,140
What the idea of a polynomial-time approximation scheme is, it's a sort of more

5
00:00:18,140 --> 00:00:19,790
advanced constant-factor approximation,

6
00:00:19,790 --> 00:00:22,850
so you can think of it as a sort of scale.

7
00:00:22,850 --> 00:00:26,980
The idea behind a polynomial-time approximation scheme is that the running time

8
00:00:26,980 --> 00:00:29,810
actually depends on the quality that you want to get.

9
00:00:29,810 --> 00:00:33,160
With a constant-factor approximation you always get the same guarantee,

10
00:00:33,160 --> 00:00:37,040
say a factor to approximation, and in a polynomial-time approximation scheme,

11
00:00:37,040 --> 00:00:40,210
you can more or less decide what the quality is that you want,

12
00:00:40,210 --> 00:00:44,330
but the better a guarantee you're demanding, with respect to quality,

13
00:00:44,330 --> 00:00:46,950
the more running time you'll have to put into that algorithm.

14
00:00:46,950 --> 00:00:48,020
That's the trade-off here.

15
00:00:48,020 --> 00:00:51,020
But first of all let's look at constant-factor approximation.

16
00:00:51,020 --> 00:00:55,920
We are now going to meet Alice again. Alice, as you know, is working on vertex cover.

17
00:00:55,920 --> 00:01:01,000
So far she has been the most lucky in this course, because she seems to actually

18
00:01:01,000 --> 00:01:04,129
be working on a very well-behaved NP-complete problem.

19
00:01:04,129 --> 00:01:07,550
We also learned that there's good search trees, there's good preprocessing,

20
00:01:07,550 --> 00:01:10,170
and vertex cover was even fixed parameter tractable.

21
00:01:10,170 --> 00:01:14,340
Alice can be quite optomistic about approximating vertex cover,

22
00:01:14,340 --> 00:01:16,980
and actually she has come up with 2 different ideas

23
00:01:16,980 --> 00:01:19,370
of how you could approximate vertex covers.

24
00:01:19,370 --> 00:01:21,830
Here are the 2 possible ideas that she has come up with.

25
00:01:21,830 --> 00:01:27,330
The first algorithm takes this input graph and then looks if some edges are still not covered

26
00:01:27,330 --> 00:01:32,300
in that graph, and if that is the case it takes the uncovered edge and puts both of the endpoints

27
00:01:32,300 --> 00:01:34,300
of that edge into the vertex cover.

28
00:01:34,300 --> 00:01:39,360
Then the algorithm looks again if there is still some uncovered edges, if there are any

29
00:01:39,360 --> 00:01:43,310
of those, then it takes again one and puts both endpoints into the vertex cover

30
00:01:43,310 --> 00:01:45,310
and so on until all edges are covered.

31
00:01:45,310 --> 00:01:48,020
And she calls that algorithm "Take 2," because it always takes 2 endpoints

32
00:01:48,020 --> 00:01:51,710
into the vertex cover each time you go through that loop until you're done.

33
00:01:51,710 --> 00:01:56,750
Here is her second idea, and she calls that idea "Greedy," because what that algorithm does--

34
00:01:56,750 --> 00:02:00,440
It's still the same in the first line here. It looks if some edges are uncovered.

35
00:02:00,440 --> 00:02:06,870
Then what it does is it goes through all of the vertices to determine which vertex could cover

36
00:02:06,870 --> 00:02:09,870
the most edges that are still uncovered.

37
00:02:09,870 --> 00:02:13,370
So which vertex is next to the most edges that have not yet an endpoint in the vertex cover,

38
00:02:13,370 --> 00:02:16,190
and then it puts that vertex in the vertex cover.

39
00:02:16,190 --> 00:02:20,850
It kind of tries to maximize the coverage that it can achieve in each round

40
00:02:20,850 --> 00:02:23,240
that this loop here is run.

41
00:02:23,240 --> 00:02:27,420
So looking at this strategy, so these 2 algorithms, what I would like you to think about

42
00:02:27,420 --> 00:02:32,420
for a moment here is, which of these 2 algorithms should we expect to perform better

43
00:02:32,420 --> 00:02:34,440
in terms of approximation quality?

44
00:02:34,440 --> 99:59:59,999
The one that always takes 2 endpoints, or the one that tries to maximize coverage?
