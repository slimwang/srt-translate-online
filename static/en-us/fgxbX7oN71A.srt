1
00:00:00,200 --> 00:00:01,839
Okay Charles, what have we learned? And I mean

2
00:00:01,839 --> 00:00:05,150
specifically in the context of this game theory two lesson.

3
00:00:05,150 --> 00:00:08,670
>> That's a that's a good question. We learned

4
00:00:08,670 --> 00:00:12,580
about Iterated Prisoners Dilemma. Which turns out to be cool,

5
00:00:12,580 --> 00:00:14,770
and it solves the problem, and we learned about

6
00:00:14,770 --> 00:00:21,200
how we can connect iterated prison's dilemma to reinforcement learning.

7
00:00:21,200 --> 00:00:22,590
>> What do you mean?

8
00:00:22,590 --> 00:00:23,220
>> Through the discount.

9
00:00:23,220 --> 00:00:25,230
>> Yeah, so I think of that as being the

10
00:00:25,230 --> 00:00:26,660
idea of repeated games.

11
00:00:26,660 --> 00:00:29,990
>> Right. Let's see, what else have we learned? So we learned about

12
00:00:29,990 --> 00:00:33,410
iterated prison's dilemma, which allowed us to get past this really scary thing

13
00:00:33,410 --> 00:00:35,370
with repeated games, connected it with

14
00:00:35,370 --> 00:00:38,290
reinforcement learning. The discounting. And then we

15
00:00:38,290 --> 00:00:40,880
learned other things like for example, I

16
00:00:40,880 --> 00:00:43,080
don't remember. What, what did we learn?

17
00:00:43,080 --> 00:00:45,920
>> Well so the, the connection between iterated prisoner's dilemma and

18
00:00:45,920 --> 00:00:51,320
repeated games was the idea that we can actually encourage cooperation.

19
00:00:51,320 --> 00:00:54,506
And in fact, there's a whole bunch of new Nash equilibria that appear

20
00:00:54,506 --> 00:00:57,640
when you work in repeated games. That was the concept of the Folk Theorem.

21
00:00:57,640 --> 00:01:00,180
>> Right. The Folk Theorem. So, the Folk Theorem

22
00:01:00,180 --> 00:01:02,740
is really cool. And this whole notion of repeated

23
00:01:02,740 --> 00:01:05,650
games really seems like a clever way of getting

24
00:01:05,650 --> 00:01:09,460
out of what appear to be limitations in game theory.

25
00:01:09,460 --> 00:01:12,740
>> Right. Yeah. And in particular by using things like threats.

26
00:01:12,740 --> 00:01:15,820
>> Right. But only plausible threats.

27
00:01:15,820 --> 00:01:16,330
>> Right, so that

28
00:01:16,330 --> 00:01:18,270
was the next thing we talked about. The idea

29
00:01:18,270 --> 00:01:20,980
that an equilibrium could be subgame perfect or not,

30
00:01:20,980 --> 00:01:23,145
and if it wasn't then the threats could be

31
00:01:23,145 --> 00:01:26,970
implausible. But in the subgame perfect setting, they're more plausible.

32
00:01:26,970 --> 00:01:30,360
>> Right let's see and then we learned about Min-max Q.

33
00:01:30,360 --> 00:01:31,510
>> Well there was one last thing we did

34
00:01:31,510 --> 00:01:34,950
on the repeated games, which was the Computational Folk theorem.

35
00:01:34,950 --> 00:01:37,130
>> Yes you're right. So basically what we learned

36
00:01:37,130 --> 00:01:40,610
is that Michael Littman does cool stuff in game theory.

37
00:01:40,610 --> 00:01:41,400
>> Or at least he does

38
00:01:41,400 --> 00:01:43,230
stuff that he's willing to talk about in a MOOC.

39
00:01:43,230 --> 00:01:46,100
>> Yes, so that's, that's there's actually a

40
00:01:46,100 --> 00:01:49,340
technical term for that right? MOOC acceptable research?

41
00:01:49,340 --> 00:01:50,260
>> Oh, I didn't know that.

42
00:01:50,260 --> 00:01:50,565
>> Mm-hm.

43
00:01:50,565 --> 00:01:54,220
>> So all these things are by virtue of the

44
00:01:54,220 --> 00:01:56,690
fact that they showed up in this class look acceptable.

45
00:01:56,690 --> 00:01:57,260
>> Exactly.

46
00:01:57,260 --> 00:02:00,330
>> Alright, you're right, but then we switch to stochastic games.

47
00:02:00,330 --> 00:02:01,083
>> Mm-hm.

48
00:02:01,083 --> 00:02:04,480
>> And they generalize MDP's and repeated games.

49
00:02:05,990 --> 00:02:06,198
>> Mm-hm.

50
00:02:06,198 --> 00:02:07,130
>> Anything else?

51
00:02:07,130 --> 00:02:10,400
>> Well, that particularly got us to min-max

52
00:02:10,400 --> 00:02:12,940
Q and then eventually to Nash Q. But

53
00:02:12,940 --> 00:02:16,260
despite the fact that Nash Q doesn't work, we ended up in a place of hope.

54
00:02:16,260 --> 00:02:19,550
>> [LAUGH] We end with some hopefulness.

55
00:02:19,550 --> 00:02:20,830
>> Yeah, and you know, I think that

56
00:02:20,830 --> 00:02:23,205
that's actually a lesson for the entire course.

57
00:02:23,205 --> 00:02:25,110
That at the end of the day, sometimes

58
00:02:25,110 --> 00:02:27,005
it doesn't always work, but there is always hope.

59
00:02:27,005 --> 00:02:32,620
>> [LAUGH] We don't give up and that's, that's, that's how research works.

60
00:02:32,620 --> 00:02:35,908
Even when we have impossibility results for things like clustering, or

61
00:02:35,908 --> 00:02:38,350
multi-agent multi-agent learning and decision

62
00:02:38,350 --> 00:02:41,990
making, we still keep struggling forward.

63
00:02:41,990 --> 00:02:45,610
>> And keep learning, and isn't that what's really important. I think so.

64
00:02:45,610 --> 00:02:47,900
>> Its important for us, and its important for machines.

65
00:02:47,900 --> 00:02:50,990
>> Yes, that is beautiful. I feel like we've made

66
00:02:50,990 --> 00:02:52,835
it to a good place, Michael. Perhaps we should stop.

67
00:02:52,835 --> 00:02:56,056
>> [LAUGH] Well it has been, it has been

68
00:02:56,056 --> 00:02:57,750
delightful getting to talk to everyone, and it has been

69
00:02:57,750 --> 00:02:59,870
very fun getting to talk with you, Charles.

70
00:02:59,870 --> 00:03:03,080
And thanks to everybody for making this happen.

71
00:03:03,080 --> 00:03:06,980
>> I agree. And we have one more chance to talk with one another as we

72
00:03:06,980 --> 00:03:10,380
wrap up the class. And I look forward to that. So, I will see you then, Michael.

73
00:03:10,380 --> 00:03:12,380
>> Awesome. Do we get to see each other in person for that?

74
00:03:12,380 --> 00:03:14,890
>> We get to see each other in person for that. That will be fun.

75
00:03:14,890 --> 00:03:15,230
>> Yay!

76
00:03:15,230 --> 00:03:17,630
>> Okay, well, bye, Michael. I'll see you next time.

77
00:03:17,630 --> 00:03:19,290
>> Bye. See yeah.

78
00:03:19,290 --> 00:03:19,620
>> Bye, bye.
