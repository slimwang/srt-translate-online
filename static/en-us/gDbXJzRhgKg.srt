1
00:00:00,220 --> 00:00:03,630
From the high level description of what happens on page faults and how it is

2
00:00:03,630 --> 00:00:08,140
handled, you can see that the behavior of

3
00:00:08,140 --> 00:00:12,090
this GMS global memory management, is as follows.

4
00:00:12,090 --> 00:00:18,570
So overtime, you can see that if there is an idle node in the land, then

5
00:00:18,570 --> 00:00:22,460
that idle node, its working set is continuously

6
00:00:22,460 --> 00:00:25,810
going to decrease as it accommodates more and

7
00:00:25,810 --> 00:00:32,290
more of its peers pages that are swapped out to fit in its global cache. And

8
00:00:32,290 --> 00:00:36,430
eventually a completely idle node, becomes a memory

9
00:00:36,430 --> 00:00:39,600
server, for peers on the network. So that's the

10
00:00:39,600 --> 00:00:42,890
expected behavior of this logarithm. And the key

11
00:00:42,890 --> 00:00:45,700
attribute of the logarithm is the fact, that

12
00:00:45,700 --> 00:00:47,800
the split between Local and Global is not

13
00:00:47,800 --> 00:00:50,980
static, but is dynamic, depending on what is happening

14
00:00:50,980 --> 00:00:53,350
at a particular node. For instance, even the

15
00:00:53,350 --> 00:00:57,400
same node, if it was serving as a memory

16
00:00:57,400 --> 00:00:59,340
server for my peers, because I had gone

17
00:00:59,340 --> 00:01:02,330
out to lunch, I come back, and start actively

18
00:01:02,330 --> 00:01:05,760
using my workstation. In that case, I'm going to

19
00:01:05,760 --> 00:01:09,270
go back, and the community service part of my

20
00:01:09,270 --> 00:01:12,145
machine is going to start decreasing. So the Local

21
00:01:12,145 --> 00:01:15,990
Global split is not static, but it shifts depending

22
00:01:15,990 --> 00:01:19,640
on what local memory pressure exist at that node.
