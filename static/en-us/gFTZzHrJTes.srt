1
00:00:00,210 --> 00:00:03,250
The difference between using
reinforcement learning for control and

2
00:00:03,250 --> 00:00:06,850
using reinforcement learning without
control is basically whether or

3
00:00:06,850 --> 00:00:10,260
not there's actions that
are being chosen by the learner.

4
00:00:10,260 --> 00:00:12,570
And so just to make that make sense,

5
00:00:12,570 --> 00:00:15,330
I'm going to swap back in
the notion of Bellman equation.

6
00:00:15,330 --> 00:00:17,010
So this is the Bellman
equations without actions,

7
00:00:17,010 --> 00:00:20,070
this is what we've been talking
about for the last few videos.

8
00:00:20,070 --> 00:00:21,340
Do you recognize the Bellman Equation?

9
00:00:21,340 --> 00:00:24,210
The value of a state is the reward

10
00:00:24,210 --> 00:00:27,210
plus the discounted expected
value of the next state.

11
00:00:27,210 --> 00:00:30,270
>> Yeah, I mean usually you
write that with an a in there.

12
00:00:30,270 --> 00:00:32,530
>> Right, right, but I said no actions.

13
00:00:32,530 --> 00:00:33,290
>> Okay.
>> We're going to

14
00:00:33,290 --> 00:00:34,760
put an a back in in just a moment.

15
00:00:34,760 --> 00:00:35,780
>> Okay, that'll make me feel better.

16
00:00:35,780 --> 00:00:38,830
>> Certainly when we were talking
about TD we didn't really have

17
00:00:38,830 --> 00:00:39,820
much of an ocean of actions.

18
00:00:39,820 --> 00:00:43,350
We were really talking about sequence of
states that were being experienced and

19
00:00:43,350 --> 00:00:44,570
the rewards associated with them.

20
00:00:44,570 --> 00:00:47,280
And in fact the update
rule that we talked about,

21
00:00:47,280 --> 00:00:50,840
this is one of the update
rules that we talked about.

22
00:00:50,840 --> 00:00:54,130
Says that if we go from some state and
we get some reward and

23
00:00:54,130 --> 00:00:55,110
we end up in some new state,

24
00:00:55,110 --> 00:00:59,010
then what we're going to do is our new
value function estimate is going to be.

25
00:01:00,160 --> 00:01:02,540
The, well, it's going to be
the same as the old one, except for

26
00:01:02,540 --> 00:01:04,120
in the state that we just left.

27
00:01:04,120 --> 00:01:08,950
Where we're going to move a little bit
towards the reward we got plus our

28
00:01:08,950 --> 00:01:12,345
prediction, our discounted prediction
of the state that we just landed in.

29
00:01:12,345 --> 00:01:15,433
So what update rule is this of the
different update rules we talked about?

30
00:01:15,433 --> 00:01:16,472
>> TD(0).

31
00:01:16,472 --> 00:01:18,705
>> Yeah, TD(0), factorial.

32
00:01:18,705 --> 00:01:22,335
So what we need to do now is get control
back into the picture here, and so

33
00:01:22,335 --> 00:01:24,055
we're going to do that now.
