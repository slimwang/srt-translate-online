1
00:00:00,920 --> 00:00:04,360
We would like of course to do better in terms of prediction accuracy.

2
00:00:05,720 --> 00:00:08,330
Unfortunately there's quite a lot of variance in this data.

3
00:00:09,830 --> 00:00:15,590
So, we've already been looking at intertweet time.

4
00:00:15,590 --> 00:00:16,810
Meaning if there's a tweet,

5
00:00:16,810 --> 00:00:22,360
t1 and t2, intertweet time is the number of seconds between them.

6
00:00:22,360 --> 00:00:25,980
But what we're trying to predict is more granular than this right?

7
00:00:25,980 --> 00:00:31,090
Specifically, if it's been delta t seconds since the tweet occurred,

8
00:00:31,090 --> 00:00:33,800
we want to try to predict the amount of time,

9
00:00:33,800 --> 00:00:37,820
labeled p here, until the person tweets again.

10
00:00:37,820 --> 00:00:43,300
So in order to do this, we have collect a bunch of training examples.

11
00:00:43,300 --> 00:00:46,450
So our training examples, are going to look like this.

12
00:00:46,450 --> 00:00:48,340
We want lots of pairs.

13
00:00:48,340 --> 00:00:51,300
Of elapsed time since the last tweet.

14
00:00:51,300 --> 00:00:54,030
And how far we have to go, until the next tweet.

15
00:00:54,030 --> 00:00:57,230
To get an understanding of the variance in our data.

16
00:00:57,230 --> 00:01:01,270
Let's create this training data from the given raw data.

17
00:01:01,270 --> 00:01:03,090
And then make some graphs.

18
00:01:03,090 --> 00:01:07,040
You can see here, we're just producing these very same points.

19
00:01:07,040 --> 00:01:10,740
From the given raw data and timeUntilNext, which is

20
00:01:10,740 --> 00:01:16,800
just the time between tweets, we produce a series of more granular points.

21
00:01:16,800 --> 00:01:20,190
How much time has elapsed, and how much to go until the next tweet.

22
00:01:20,190 --> 00:01:25,390
And they collect them together in the list data_points.

23
00:01:25,390 --> 00:01:30,020
So, to get an idea of variance let's take a look.

24
00:01:30,020 --> 00:01:32,020
At a few fixed points.

25
00:01:32,020 --> 00:01:35,930
Specifically, let's look at, what the data looks like when ten

26
00:01:35,930 --> 00:01:42,020
seconds has elapsed, 150 seconds has elapsed, and 100 seconds has elapsed.

27
00:01:42,020 --> 00:01:46,970
To get an initial idea about what the variance looks like in our data,

28
00:01:46,970 --> 00:01:49,210
let's fix it at a couple of points.

29
00:01:49,210 --> 00:01:50,720
Specifically.

30
00:01:50,720 --> 00:01:54,420
Let's look at what the data looks like after 10 seconds has elapsed,

31
00:01:54,420 --> 00:01:57,070
and 150 seconds has elapsed.

32
00:01:57,070 --> 00:02:00,370
Let's zero in on the 10 second graph first.

33
00:02:00,370 --> 00:02:02,500
So let's look at what this graph is saying.

34
00:02:02,500 --> 00:02:07,830
This is just a histogram, saying that if 10 seconds has elapsed since

35
00:02:07,830 --> 00:02:12,190
my last tweet, how long did I have to wait until the next tweet occurred.

36
00:02:12,190 --> 00:02:16,080
So here, zero seconds had to elapse until the next tweet occurred.

37
00:02:16,080 --> 00:02:18,365
And at the other end of the graph,

38
00:02:18,365 --> 00:02:21,710
90,000 seconds have elapsed until the next tweet occurs.

39
00:02:21,710 --> 00:02:25,490
And as you can see, it's skewed strongly towards zero seconds.

40
00:02:25,490 --> 00:02:28,220
Again, it looks like an exponential distribution.

41
00:02:28,220 --> 00:02:31,100
Now, let's look at what this looks like for 150 seconds.

42
00:02:32,320 --> 00:02:34,080
Again, this is a graph of.

43
00:02:34,080 --> 00:02:38,460
After 150 seconds has elapsed since the last tweet,

44
00:02:38,460 --> 00:02:41,560
how long did we have to wait until the next tweet.

45
00:02:41,560 --> 00:02:44,070
At the left end, we had to wait zero seconds.

46
00:02:44,070 --> 00:02:46,500
At the right end, 90 thousand seconds.

47
00:02:46,500 --> 00:02:50,200
As you can see again, it looks just like an exponential distribution.

48
00:02:50,200 --> 00:02:53,830
In fact, let's look at a graph of the standard deviation.

49
00:02:53,830 --> 00:02:56,870
Again, the x axis here is the number of seconds that

50
00:02:56,870 --> 00:02:59,150
has elapsed since the last tweet.

51
00:02:59,150 --> 00:03:02,860
So zero seconds, 10 seconds, 20 and so on.

52
00:03:02,860 --> 00:03:08,540
The y axis describes the number of seconds until the next tweet occurs.

53
00:03:08,540 --> 00:03:12,340
And this graph describes the standard deviation in that quantity.

54
00:03:13,400 --> 00:03:15,200
So as you can see.

55
00:03:15,200 --> 00:03:18,640
The spread of the times that we have to wait until the next tweet

56
00:03:18,640 --> 00:03:23,570
occurs increases as the time elapses since the last tweet.

57
00:03:23,570 --> 00:03:24,903
Let's visualize this in another way.

58
00:03:24,903 --> 00:03:30,308
This graph shows a 95% confidence interval of the time we have to wait,

59
00:03:30,308 --> 00:03:33,980
until the next tweet as a function of lapsed time.

60
00:03:35,040 --> 00:03:37,270
So after 20 seconds has elapsed.

61
00:03:37,270 --> 00:03:39,360
Here's the spread of how long we have to wait.

62
00:03:39,360 --> 00:03:44,560
As you can see it spreads all the way from zero seconds to 30,000 seconds.

63
00:03:44,560 --> 00:03:47,750
After 60 seconds has elapsed the spread goes all the way

64
00:03:47,750 --> 00:03:50,540
from zero seconds to 35 thousand seconds.

65
00:03:50,540 --> 00:03:55,050
So as you can see, this is just another way

66
00:03:55,050 --> 00:03:59,970
to describe the graph we just saw of the increasing standard deviation.

67
00:03:59,970 --> 00:04:03,040
The confidence band just increases as time goes on.

68
00:04:03,040 --> 00:04:07,860
This is just another way of saying that as time goes on, we're more and

69
00:04:07,860 --> 00:04:12,690
more uncertain of how long it's going to be until they tweet next.

70
00:04:12,690 --> 00:04:16,589
Any estimator that we create, which is a function of delta t.

71
00:04:16,589 --> 00:04:20,050
Will do so by essentially averaging over this spread.

72
00:04:20,050 --> 00:04:22,140
At any given point along the graph.

73
00:04:22,140 --> 00:04:26,480
Again as you can see the spread gets wider as the elapsed time goes on.

74
00:04:26,480 --> 00:04:29,660
Reflecting greater and greater uncertainty in the data.

75
00:04:29,660 --> 00:04:32,240
As the time since the last tweet grows.

76
00:04:32,240 --> 00:04:35,120
Elapsed time becomes less and less good.

77
00:04:35,120 --> 00:04:37,880
At determining what the inter-tweet time is.

78
00:04:37,880 --> 00:04:42,370
And any predictor we make based on delta t will suffer from this.

79
00:04:42,370 --> 00:04:46,280
This problem is called inherent variance.

80
00:04:46,280 --> 00:04:49,790
Note, this is not the same thing as model variance,

81
00:04:49,790 --> 00:04:54,320
which refers to how the output of your model is sensitive to training data.

82
00:04:54,320 --> 00:04:57,870
Inherent variance refers to the following concepts.

83
00:04:57,870 --> 00:05:00,500
Let's suppose, that our real model,

84
00:05:00,500 --> 00:05:05,730
meaning the actual real life phenomenon was in fact a linear one,

85
00:05:05,730 --> 00:05:11,000
meaning you had some function f and it was a function of the single variable x.

86
00:05:11,000 --> 00:05:16,140
As data scientists we collect records of the form x, f.

87
00:05:16,140 --> 00:05:19,560
From these records, we will try to build an estimator and

88
00:05:19,560 --> 00:05:22,270
we'll probably have a pretty easy time of it, right?

89
00:05:22,270 --> 00:05:24,180
For a given fixed value, x1,

90
00:05:24,180 --> 00:05:28,217
we will always see the very same value of f beside it.

91
00:05:28,217 --> 00:05:31,050
A times x1.

92
00:05:31,050 --> 00:05:34,090
Over and over as we collect records.

93
00:05:34,090 --> 00:05:39,540
Every time we see x1, we will always see a times x1 beside it.

94
00:05:39,540 --> 00:05:44,620
Now, let's suppose we introduce some noise into the situation.

95
00:05:44,620 --> 00:05:50,910
The way to think about this is, x occurs but before we can measure it as f.

96
00:05:50,910 --> 00:05:56,400
Some noise process occurs to x, and the result is the thing that we measure.

97
00:05:56,400 --> 00:06:02,130
The consequence is, now we no longer see the same f for a given x.

98
00:06:02,130 --> 00:06:05,730
The first time we see x1, we might see value 1 beside it.

99
00:06:06,850 --> 00:06:10,890
The next time value 2 and so on.

100
00:06:10,890 --> 00:06:12,810
This spread of values, v1,

101
00:06:12,810 --> 00:06:18,660
v2 and so on, produces the confidence band effect that we saw earlier.

102
00:06:18,660 --> 00:06:22,150
That of course makes finding the right value to guess for

103
00:06:22,150 --> 00:06:24,900
a given x1 much more difficult.

104
00:06:24,900 --> 00:06:26,250
In the next video.

105
00:06:26,250 --> 00:06:29,350
We'll talk about sources of this variance, and what we can do about it.
