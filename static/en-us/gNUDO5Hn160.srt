1
00:00:00,530 --> 00:00:07,862
The next myth is regarding memory effects. And that myth concerns the assertion

2
00:00:07,862 --> 00:00:13,210
that the lost of locality in a micro-kernel base design, is much more than in a

3
00:00:13,210 --> 00:00:16,530
monolithic structure. Or the structure that is

4
00:00:16,530 --> 00:00:19,640
advocated in spin and exo-kernel. But before we

5
00:00:19,640 --> 00:00:21,520
talk about the memory effects. There's a

6
00:00:21,520 --> 00:00:25,740
primer on the memory hierarchy. You know that,

7
00:00:25,740 --> 00:00:27,714
in the process of architecture, you have the

8
00:00:27,714 --> 00:00:30,600
CPU, you have the TLB. And you have several

9
00:00:30,600 --> 00:00:34,360
levels of caches, main memory. And the virtual memory

10
00:00:34,360 --> 00:00:37,810
that resides on the disk. And these caches are

11
00:00:37,810 --> 00:00:41,230
typically physically tagged. Now, what do we mean by

12
00:00:41,230 --> 00:00:44,450
memory effects? What we mean by that is, if

13
00:00:44,450 --> 00:00:47,150
you have this hardware address space, and this of

14
00:00:47,150 --> 00:00:50,940
course is much bigger than the amount of space

15
00:00:50,940 --> 00:00:54,540
that's available in these caches. And in

16
00:00:54,540 --> 00:00:58,610
fact, we know that the entire hardware address

17
00:00:58,610 --> 00:01:00,600
space may not even be in physical

18
00:01:00,600 --> 00:01:03,850
memory. Because In a demand-based system, when a

19
00:01:03,850 --> 00:01:07,460
process is executing, the pages that it

20
00:01:07,460 --> 00:01:11,460
needs will be demand paged from the virtual

21
00:01:11,460 --> 00:01:13,430
memory that's on the disk, and brought

22
00:01:13,430 --> 00:01:16,060
into physical memory. And when the processor is

23
00:01:16,060 --> 00:01:19,460
executing instructions, then the instructions and data

24
00:01:19,460 --> 00:01:22,000
contained in physical memory move into the memory

25
00:01:22,000 --> 00:01:28,990
hierarchy close to. The CPU, so that the CPU can access the working set of

26
00:01:28,990 --> 00:01:35,440
the currently running thread by getting it from the cache that is closest to it.

27
00:01:35,440 --> 00:01:38,360
That's the hope in this memory hierarchy. What

28
00:01:38,360 --> 00:01:41,188
we mean by memory effects is when we

29
00:01:41,188 --> 00:01:45,430
context switch between protection domains, how warm

30
00:01:45,430 --> 00:01:48,350
are the caches? Now recall that if

31
00:01:48,350 --> 00:01:54,480
we have small protection domains. P1, P2, P3, P4. Let's say they are all small

32
00:01:54,480 --> 00:01:57,800
protection domains. In that case, led case

33
00:01:57,800 --> 00:02:01,010
suggestion is, don't put each of these in

34
00:02:01,010 --> 00:02:03,560
its own hardware address space. Pack them

35
00:02:03,560 --> 00:02:06,320
together in the same hardware address space and

36
00:02:06,320 --> 00:02:10,199
then force protection, for these processes from

37
00:02:10,199 --> 00:02:14,410
one another, through segment registers. So when we

38
00:02:14,410 --> 00:02:18,650
have small protection domains, then, the caches

39
00:02:18,650 --> 00:02:20,640
are going to be pretty warm. Even when you

40
00:02:20,640 --> 00:02:23,030
do a context switch from one process

41
00:02:23,030 --> 00:02:25,670
to another process. There's a good chance, that

42
00:02:25,670 --> 00:02:28,860
the cache hierarchy is going to contain the working

43
00:02:28,860 --> 00:02:32,230
set of the newly scheduled small protection domain.

44
00:02:32,230 --> 00:02:38,710
So in other words, the memory effects can be mitigated by carefully structuring

45
00:02:38,710 --> 00:02:41,160
the protection domains, in the hardware

46
00:02:41,160 --> 00:02:44,810
address space. So debunking the second myth

47
00:02:44,810 --> 00:02:50,980
with respect to address space switching, also helps in reducing the ill effects

48
00:02:50,980 --> 00:02:54,840
of implicit costs, associated with address

49
00:02:54,840 --> 00:02:58,400
space switching, because these small protection domains.

50
00:02:58,400 --> 00:03:01,490
Occupy only a small memory footprint, and

51
00:03:01,490 --> 00:03:04,560
therefore occupy only a small memory footprint in

52
00:03:04,560 --> 00:03:07,050
the caches as well. And therefore, when we

53
00:03:07,050 --> 00:03:10,270
go across small protection domains, there's a good

54
00:03:10,270 --> 00:03:14,280
chance that, the locality for the newly

55
00:03:14,280 --> 00:03:17,980
scheduled small protection domain is going to be contained

56
00:03:17,980 --> 00:03:23,490
in the cache hierarchy. So the caches are probably going to be warm, when we do

57
00:03:23,490 --> 00:03:28,460
context switches, so long as the protection domains are small. We already

58
00:03:28,460 --> 00:03:31,730
mentioned that if the protection domains

59
00:03:31,730 --> 00:03:35,680
are large, you cannot avoid that. Whether

60
00:03:35,680 --> 00:03:42,100
it is a monolithic kernel or. The exokernel, or the spin type

61
00:03:42,100 --> 00:03:48,730
of extensibility. If the memory footprint of the system service is so big,

62
00:03:48,730 --> 00:03:50,850
it is going to pollute the cache when

63
00:03:50,850 --> 00:03:53,820
we visit that particular system service. So

64
00:03:53,820 --> 00:03:56,440
even if we have a monolithic kernel,

65
00:03:56,440 --> 00:04:01,260
and the monolithic kernel has subsystems, that occupy

66
00:04:01,260 --> 00:04:03,960
a significant portion of the hardware address

67
00:04:03,960 --> 00:04:06,810
space. Even though we are not doing any

68
00:04:06,810 --> 00:04:10,380
context for it, the ill effects of

69
00:04:10,380 --> 00:04:14,101
implicit costs in the memory hierarchy is going to

70
00:04:14,101 --> 00:04:16,100
be felt. Because the cache is after

71
00:04:16,100 --> 00:04:19,829
all a physically tagged. And therefore, when we

72
00:04:19,829 --> 00:04:24,190
go from large protection domains, or large subsystems

73
00:04:24,190 --> 00:04:26,830
in the context of a monolithic kernel. You

74
00:04:26,830 --> 00:04:30,820
have to incur the cache pollution. So that

75
00:04:30,820 --> 00:04:33,690
ill effect is unavoidable for large protection domains.

76
00:04:33,690 --> 00:04:40,110
So the only place where a monolithic kernel can win, or an exokernel can win, or

77
00:04:40,110 --> 00:04:46,160
a spin can win, is in small protection domains. And a microkernel can also

78
00:04:46,160 --> 00:04:48,870
win for a small protection domain, by

79
00:04:48,870 --> 00:04:52,140
packing multiple small protection domains in the

80
00:04:52,140 --> 00:04:58,085
same hardware address space. So this begs the question. Why was it so bad in

81
00:04:58,085 --> 00:05:05,400
Mac? Recall, I mentioned that, in Mac, on the same hardware, border crossing

82
00:05:05,400 --> 00:05:12,840
cost 800 more cycles, compared to the border crossing in the L3 microkernel.

83
00:05:12,840 --> 00:05:17,777
So we can ask the question, why was it so bad in the Mac microkernel?
