1
00:00:00,290 --> 00:00:04,464
Okay, Michael, so I wanted to,
before diving into a couple of examples,

2
00:00:04,464 --> 00:00:08,020
I wanted to remind you of
a conversation that we had earlier.

3
00:00:08,020 --> 00:00:10,700
Does this little picture
look familiar to you?

4
00:00:10,700 --> 00:00:13,240
>> It looks like the Monte Carlo
tree search stuff we talked about in

5
00:00:13,240 --> 00:00:14,520
the options lesson.

6
00:00:14,520 --> 00:00:15,160
>> It actually is.

7
00:00:15,160 --> 00:00:17,400
So, here's the point that I wanted,
I made the point at the time, but

8
00:00:17,400 --> 00:00:19,630
I wanted to really kind
of drive it home here.

9
00:00:20,940 --> 00:00:25,440
If you recall, I asked you to come up
with some options for solving Pac-Man.

10
00:00:25,440 --> 00:00:27,740
And you came up with four of them.

11
00:00:27,740 --> 00:00:29,190
Do you remember what they were?

12
00:00:29,190 --> 00:00:31,120
>> Yeah, I think they're on a slide,
fortunately.

13
00:00:31,120 --> 00:00:32,430
I just have to remember what they mean.

14
00:00:32,430 --> 00:00:33,780
It's like eat with three
different bullets,

15
00:00:33,780 --> 00:00:35,030
and they're getting successively larger.

16
00:00:35,030 --> 00:00:39,260
But I think it's actually food pellet,
power pellet, monster or

17
00:00:39,260 --> 00:00:42,030
ghost, and then avoid ghost.

18
00:00:42,030 --> 00:00:42,720
>> And avoid ghost.

19
00:00:42,720 --> 00:00:45,290
Right.
And if you recall, at the time

20
00:00:45,290 --> 00:00:48,780
we briefly discussed that these
things really do look like options.

21
00:00:48,780 --> 00:00:51,430
And this last one doesn't really look
like an option in the sense that it

22
00:00:51,430 --> 00:00:53,755
doesn't have a goal and
there's something you can succeed.

23
00:00:53,755 --> 00:00:55,230
In some sense you can only fail.

24
00:00:55,230 --> 00:00:57,690
And this turned out to be
interesting for a couple of reasons.

25
00:00:57,690 --> 00:01:01,370
One, just because you came up with them,
and in fact we've done this study

26
00:01:01,370 --> 00:01:04,780
multiple, multiple humans at
various levels of expertise.

27
00:01:04,780 --> 00:01:07,390
And, basically,
they all come up with the same four.

28
00:01:07,390 --> 00:01:10,650
Sometimes they'll come up with a fifth
that kind of distinguishes between

29
00:01:10,650 --> 00:01:14,220
eating a real ghost that's edible and
a ghost that isn't edible,

30
00:01:14,220 --> 00:01:14,860
and that kind of thing.

31
00:01:14,860 --> 00:01:16,170
But basically,
they come with the same four.

32
00:01:16,170 --> 00:01:19,280
And the point here, and why I want to
bring it up is that if you just ask

33
00:01:19,280 --> 00:01:23,500
people to give you what they know,
you ask them to communicate,

34
00:01:23,500 --> 00:01:25,740
you ask them to coach an agent.

35
00:01:25,740 --> 00:01:28,400
They will often come up with
things that are different from

36
00:01:28,400 --> 00:01:30,225
what your machine expects.

37
00:01:30,225 --> 00:01:33,970
And the sort of claim I want to make is
that often in research what we do is we

38
00:01:33,970 --> 00:01:37,200
come up with wonderful machine learning
algorithms, decide what information they

39
00:01:37,200 --> 00:01:39,730
need, and then try to force humans
to give us that information.

40
00:01:39,730 --> 00:01:42,290
But actually, human beings will
naturally sometimes give us that

41
00:01:42,290 --> 00:01:45,480
information, but often give us
different kinds of information.

42
00:01:45,480 --> 00:01:47,960
And if you look at it the wrong way,
you would say, well, look,

43
00:01:47,960 --> 00:01:49,220
these three are useful.

44
00:01:49,220 --> 00:01:51,750
This last one can't be implemented,
so I'm going to ignore it.

45
00:01:51,750 --> 00:01:54,360
But in fact, even in the Monte Carlo
tree search example,

46
00:01:54,360 --> 00:01:58,170
this last one, this constraint,
actually helped us

47
00:01:58,170 --> 00:02:01,830
develop a better algorithm because it
gave us a better rollout strategy.

48
00:02:01,830 --> 00:02:05,470
And, in fact,
as the students listening will know,

49
00:02:05,470 --> 00:02:08,830
because they've read the paper
on this that we made available,

50
00:02:08,830 --> 00:02:11,940
that it actually improves
performance an amazing amount.

51
00:02:11,940 --> 00:02:16,250
So, the point of this is that when
we ask humans to communicate,

52
00:02:16,250 --> 00:02:19,040
to be a part of the sort
of team of how we teach and

53
00:02:19,040 --> 00:02:22,870
demonstrate to our agents, we need to
meet them where they are as opposed to

54
00:02:22,870 --> 00:02:25,470
try to force them to meet us
where our algorithms are.

55
00:02:25,470 --> 00:02:27,548
>> And I want to give you just
a couple examples of that.

56
00:02:27,548 --> 00:02:28,090
That sound fair?

57
00:02:28,090 --> 00:02:28,916
>> Yeah, that's really cool.

58
00:02:28,916 --> 00:02:29,600
>> Cool.
All right.

59
00:02:29,600 --> 00:02:32,850
So, the first one's going to be
something called policy shaping.

60
00:02:32,850 --> 00:02:36,440
And as a preview, just want you
to know the whole point of this

61
00:02:36,440 --> 00:02:40,960
is to say that all the stuff you talked
about with the word shaping was wrong.

62
00:02:40,960 --> 00:02:42,230
>> Yeah.
Let's take a look.
