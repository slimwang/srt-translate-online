1
00:00:00,540 --> 00:00:03,500
Talk about HMMs just in general,
and then we'll move onto vision.

2
00:00:03,500 --> 00:00:08,600
In general, HMMs are generative
models of time series,

3
00:00:08,600 --> 00:00:11,290
with some notion of a hidden state, that
is they can talk about the probability

4
00:00:11,290 --> 00:00:13,730
of generating a particular output.

5
00:00:13,730 --> 00:00:17,950
The forward-backward algorithms, those
recursive ones allow you for computing

6
00:00:17,950 --> 00:00:24,000
over the states and you can also use
that computing to train up your models.

7
00:00:24,000 --> 00:00:28,870
And I will say that HMMs were their
big win was in originally in speech

8
00:00:28,870 --> 00:00:33,690
recognition dealing with low level
auditory features and using that to

9
00:00:33,690 --> 00:00:38,550
recover phonemes and then phonemes into
parts of words and things like that.

10
00:00:38,550 --> 00:00:42,180
And then in Computer Vision, although I
will say there's a whole new area these

11
00:00:42,180 --> 00:00:46,590
days called conditional random fields,
which is really better as I mentioned.

12
00:00:46,590 --> 00:00:50,550
If you've got a long sequence and
you want to do segmentation,

13
00:00:50,550 --> 00:00:53,290
conditional random fields are a slightly
more complicated version.

14
00:00:53,290 --> 00:00:55,140
Actually, they're
significantly more complicated

15
00:00:56,450 --> 00:00:59,400
graphical models of
representing time series.

16
00:00:59,400 --> 00:01:02,980
And it tends to do better than HMMs for
the actual segmentation.

17
00:01:02,980 --> 00:01:06,150
But from the notion of a generative
model of saying, was it an A, or a B, or

18
00:01:06,150 --> 00:01:08,830
a C HMMs are still used extensively.
