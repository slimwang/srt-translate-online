1
00:00:00,411 --> 00:00:03,873
So one of the really exciting new things that you guys have recently put into

2
00:00:03,873 --> 00:00:09,479
CUDA is this idea of dynamic parallelism, and so we've had some material on that in the course.

3
00:00:09,479 --> 00:00:14,089
So before we talk about what it actually is, can you talk about what problem you're trying to solve?

4
00:00:14,089 --> 00:00:18,396
>> The thing we really want to do, at least, the way that I think about it,

5
00:00:18,396 --> 00:00:21,824
the thing that I want to be able to do with the GPU is to make it easier to program

6
00:00:21,824 --> 00:00:24,994
and to broaden the number of problems you can do on the GPU.

7
00:00:24,994 --> 00:00:31,135
so the GPU starts out being really, really good at very broad bulk parallelism.

8
00:00:31,135 --> 00:00:38,273
You can throw a lot of threads at a problem. It handles them very efficiently, but for less regular problems,

9
00:00:38,273 --> 00:00:41,752
for more sort of diverse or fine grain parallelism,

10
00:00:41,752 --> 00:00:45,648
it's hard to express that in CUDA today.

11
00:00:45,648 --> 00:00:52,727
So the dynamic parallelism effort was to try and aim at an easier way

12
00:00:52,727 --> 00:00:55,197
to extract more parallelism from your problem.

13
00:00:55,197 --> 00:01:02,032
So we wanted to solve the kinds of problems, things like recursion, things like task parallelism,

14
00:01:02,032 --> 00:01:06,636
those types of problems that are difficult to express in a single big grid of threads,

15
00:01:06,636 --> 00:01:13,007
sort of paradigm, we might want to enable new types of programming problems to be solved.
