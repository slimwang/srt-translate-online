1
00:00:00,310 --> 00:00:02,530
To help us see the need for caches, we'll examine

2
00:00:02,530 --> 00:00:06,400
this little snippet of code. From an application programmer's perspective,

3
00:00:06,400 --> 00:00:09,640
a statement like this one, the sum plus equals a

4
00:00:09,640 --> 00:00:12,950
of i, would seem to happen in one step. Even

5
00:00:12,950 --> 00:00:15,980
in assembly, it would only take a few instructions. In

6
00:00:15,980 --> 00:00:18,010
terms of time on the clock, however, not all of

7
00:00:18,010 --> 00:00:21,990
these instructions are the same. Sum, being a local variable,

8
00:00:21,990 --> 00:00:25,860
is most likely stored in a register in the CPU,

9
00:00:25,860 --> 00:00:29,070
therefore there is almost no cost to retrieving its value. a

10
00:00:29,070 --> 00:00:32,070
of i, however, is in main memory, and retrieving its value

11
00:00:32,070 --> 00:00:36,100
can take 100 CPU cycles. And even then, the bus bandwidth

12
00:00:36,100 --> 00:00:38,890
might only be able to pass back two bytes a cycle

13
00:00:38,890 --> 00:00:41,830
or thereabouts, to get the data back. Trips to main memory

14
00:00:41,830 --> 00:00:44,450
are expensive, and this trend has only become worse over the

15
00:00:44,450 --> 00:00:48,300
past few decades with hardware advances. The solution to this problem

16
00:00:48,300 --> 00:00:51,800
has been to place a smaller, faster set of memory made

17
00:00:51,800 --> 00:00:54,840
with faster technology, closer to the CPU. We call this

18
00:00:54,840 --> 00:00:58,950
memory a cache. And here, we're looking at a latency of

19
00:00:58,950 --> 00:01:02,930
about ten cycles. So, when the CPU wants the contents

20
00:01:02,930 --> 00:01:06,130
of some memory address, it looks in the cache first. If

21
00:01:06,130 --> 00:01:08,870
it finds the data it needs, great, we've just achieved

22
00:01:08,870 --> 00:01:11,990
a factor of ten speedups. We call this a cache hit.

23
00:01:11,990 --> 00:01:14,610
If the data it wants is not in the cache then

24
00:01:14,610 --> 00:01:16,800
we have to go to main memory. We call this a

25
00:01:16,800 --> 00:01:20,030
cache miss. The key is keeping these cache meshes

26
00:01:20,030 --> 00:01:23,670
infrequent, so that on average, time spent accessing is more

27
00:01:23,670 --> 00:01:26,350
like the time it takes to access the cache,

28
00:01:26,350 --> 00:01:29,380
than it is like the time to access main memory.
