1
00:00:00,200 --> 00:00:00,920
The particular case,

2
00:00:00,920 --> 00:00:05,960
I'd like us to talk about in more detail
is linear value function approximation.

3
00:00:05,960 --> 00:00:11,040
So that is in particular the case
where our Q function is represented

4
00:00:11,040 --> 00:00:15,950
as a weighted linear combination of
the features that represent the state.

5
00:00:15,950 --> 00:00:17,240
And maybe the state in action,

6
00:00:17,240 --> 00:00:19,170
but we're going to keep
the action separate for now.

7
00:00:19,170 --> 00:00:21,650
>> Okay.
>> So this is how the Q function

8
00:00:21,650 --> 00:00:24,630
can be represented,
if we're doing things linearly.

9
00:00:24,630 --> 00:00:28,340
So we have this set of weights, wai.

10
00:00:28,340 --> 00:00:29,770
And we have a set of features or

11
00:00:29,770 --> 00:00:33,395
at least this function f that maps
states to particular feature values.

12
00:00:33,395 --> 00:00:35,230
And let's say we've got
n different features.

13
00:00:35,230 --> 00:00:37,100
So if we're representing
things linearly,

14
00:00:37,100 --> 00:00:40,070
what we're doing is we're saying
let's sum over all the features for

15
00:00:40,070 --> 00:00:44,100
the current state, the value of that
feature times the weight that is being

16
00:00:44,100 --> 00:00:46,750
used to represent the cube function for
this action.

17
00:00:46,750 --> 00:00:49,739
And just yes, some of it, essentially
take the dot product between this

18
00:00:49,739 --> 00:00:51,358
feature vector and the weight vector.

19
00:00:51,358 --> 00:00:55,807
And it's these parameters, these wa's,
that are actually providing us

20
00:00:55,807 --> 00:01:00,326
the generalization, the ability to make
a prediction to new states because

21
00:01:00,326 --> 00:01:04,650
there's parameters that are shared
between all the different states.

22
00:01:04,650 --> 00:01:06,960
>> Sure, that makes sense.

23
00:01:06,960 --> 00:01:11,440
So it's like in their own network
with nothing nonlinear going on.

24
00:01:11,440 --> 00:01:16,635
>> Yeah, in this particular case it
is exactly a [LAUGH] linear function.

25
00:01:16,635 --> 00:01:19,600
>> [LAUGH]
>> [LAUGH] Which you can think of as

26
00:01:19,600 --> 00:01:22,220
being under all that without any
nonlinear and with a single layer.

27
00:01:22,220 --> 00:01:24,040
>> Yeah, and so
in fact you have a whole set of them.

28
00:01:24,040 --> 00:01:27,600
You have a whole set of neural networks
all kind of happening in parallel.

29
00:01:27,600 --> 00:01:31,562
I suppose you could claim that
they're one neural network with

30
00:01:31,562 --> 00:01:34,380
a the non-linearity, which is kind of
some kind of max function or something

31
00:01:34,380 --> 00:01:38,560
that figures out which action you should
take based upon each of the Q(S,a)'s.

32
00:01:38,560 --> 00:01:41,280
And so you really are kind of
learning a neural network.

33
00:01:41,280 --> 00:01:42,700
>> Yeah, I can agree with that.

34
00:01:42,700 --> 00:01:45,987
I mean, we're going to talk a little bit
more about using more general neural

35
00:01:45,987 --> 00:01:49,500
network structures in this setting,
instead of just linear weights.

36
00:01:49,500 --> 00:01:52,050
But the linear weight case is kind
of easier for us to think about.

37
00:01:52,050 --> 00:01:54,370
And there's actually been a tremendous
amount of research on this particular

38
00:01:54,370 --> 00:01:56,900
case because the hope has been that

39
00:01:56,900 --> 00:02:01,030
it's more well-behaved than what we get
with non-linear function approximation.

40
00:02:01,030 --> 00:02:01,590
>> Usually is.

41
00:02:01,590 --> 00:02:04,180
>> Let's think a little bit about what
these weights are actually representing.

42
00:02:04,180 --> 00:02:04,900
>> Okay.

43
00:02:04,900 --> 00:02:06,170
>> One way you can think
about the weight for

44
00:02:06,170 --> 00:02:09,050
a feature is that it gives a kind
of sense of importance for

45
00:02:09,050 --> 00:02:12,570
each of the features in how much they're
contributing to the action's value.

46
00:02:12,570 --> 00:02:14,285
>> Sure, so, if the weight, ai,

47
00:02:14,285 --> 00:02:17,100
is 0, then it doesn't
matter what the feature is.

48
00:02:17,100 --> 00:02:18,220
It doesn't contribute at all.

49
00:02:18,220 --> 00:02:21,070
>> That's right, and so, then,
it completely ignores the feature for

50
00:02:21,070 --> 00:02:21,830
that prediction.

51
00:02:21,830 --> 00:02:26,120
>> And if the weight is a billion,
then it means a lot, probably.

52
00:02:26,120 --> 00:02:28,440
And if it's negative a billion
it also means a lot,

53
00:02:28,440 --> 00:02:30,050
just in the different direction.

54
00:02:30,050 --> 00:02:31,650
>> Right sort of anti-importance.

55
00:02:31,650 --> 00:02:35,234
>> Well, not necessarily, I mean,
the feature depends upon how you

56
00:02:35,234 --> 00:02:38,510
want to interpret the xi
produces a negative number.

57
00:02:38,510 --> 00:02:40,949
And you're actually,
the weight is also negative.

58
00:02:40,949 --> 00:02:44,270
If you're saying that that
feature is really important and

59
00:02:44,270 --> 00:02:47,718
the negativeness just it's kind of
a function of the fact that you want you

60
00:02:47,718 --> 00:02:50,380
want Q values to be higher
if you're going to use them.

61
00:02:50,380 --> 00:02:52,422
>> Okay, all right, I think that's
a good way to think about it.

62
00:02:52,422 --> 00:02:54,930
All right, so
this just kind of makes sense or

63
00:02:54,930 --> 00:02:59,520
you feel like you appreciate the notion
of a linear value function approximator?

64
00:02:59,520 --> 00:03:01,390
>> Yeah, I think so, that makes sense.

65
00:03:01,390 --> 00:03:04,060
You just add up all your features and
weight them appropriately.

66
00:03:04,060 --> 00:03:06,800
>> Cool, all right, so let's kind of
make sure that we understand this.

67
00:03:06,800 --> 00:03:07,300
>> Okay.
