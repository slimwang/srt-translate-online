1
00:00:00,272 --> 00:00:02,825
All right, so let's do a brief aside about the

2
00:00:02,825 --> 00:00:07,046
[LAUGH], this is like a bad history of reinforcement learning so

3
00:00:07,046 --> 00:00:08,840
it's not that it's a bad history, it's just badly

4
00:00:08,840 --> 00:00:11,730
told. So but the basic idea is this, once upon a

5
00:00:11,730 --> 00:00:15,459
time and we're going to call it, say the 1930s people

6
00:00:15,459 --> 00:00:18,390
observed. That if you put an animal, this is supposed to

7
00:00:18,390 --> 00:00:22,170
be a rat, in, in a, you know, box, say. And

8
00:00:22,170 --> 00:00:25,930
give it choices of looking in place B and place A

9
00:00:25,930 --> 00:00:27,550
and, say, one of them has cheese in it and the other one

10
00:00:27,550 --> 00:00:30,910
doesn't, but it can't see which because, you know, the doors are closed.

11
00:00:30,910 --> 00:00:34,340
And and let's say that we, we consistently do something like this. We

12
00:00:34,340 --> 00:00:38,270
turn on a red light whenever the cheese is in the A place. And

13
00:00:38,270 --> 00:00:41,370
we turn on a blue light whenever the cheese is in the, B

14
00:00:41,370 --> 00:00:44,840
place. And what you observe is if you do this consistently, then what you

15
00:00:44,840 --> 00:00:47,620
find is that if the animal sees the stimulus like the red light

16
00:00:47,620 --> 00:00:51,190
going on and it takes an action like peeking inside the, the or sticking

17
00:00:51,190 --> 00:00:53,830
it's head inside the box A and it gets

18
00:00:53,830 --> 00:00:55,750
a reward which in this case would be getting

19
00:00:55,750 --> 00:00:59,218
to eat some cheese. That that set up strengthens

20
00:00:59,218 --> 00:01:02,060
its response to the stimulus. In other words, in the

21
00:01:02,060 --> 00:01:04,129
future when it sees the red light it's going to

22
00:01:04,129 --> 00:01:06,580
be more likely to go in and look in box

23
00:01:06,580 --> 00:01:09,880
A. And this notion of strengthening, you can think

24
00:01:09,880 --> 00:01:13,290
of it as reinforcing the connection. Reinforcing just means strengthening.

25
00:01:13,290 --> 00:01:14,635
>> Hm.

26
00:01:14,635 --> 00:01:16,370
>> So, this was studied for a long time and there's,

27
00:01:16,370 --> 00:01:18,320
there's tremendous amount of interesting research

28
00:01:18,320 --> 00:01:19,730
about this, but if you start to,

29
00:01:19,730 --> 00:01:23,190
you know, think about it as a computer scientist. A natural way of, of

30
00:01:23,190 --> 00:01:25,920
kind of thinking about what this problem is, is that you know, a

31
00:01:25,920 --> 00:01:29,290
stimulus is kind of like a state, and an action is kind of like

32
00:01:29,290 --> 00:01:32,042
an action, and a reward, well, I kind of stole all these words

33
00:01:32,042 --> 00:01:34,990
but it's kind of like a reward. And it leads you to this idea

34
00:01:34,990 --> 00:01:38,160
that what you really want to do. Is figure out how to choose

35
00:01:38,160 --> 00:01:41,622
actions to maximize reward as a function of the state. And so we started

36
00:01:41,622 --> 00:01:45,067
to take this concept and we called it reinforcement learning because that's what

37
00:01:45,067 --> 00:01:47,270
the psychologists were calling it. But,

38
00:01:47,270 --> 00:01:49,530
for us it just means reward maximization.

39
00:01:49,530 --> 00:01:52,910
This notion of strengthening is really not part of the story for us

40
00:01:52,910 --> 00:01:56,300
at all. So, we're really taking this word and using it, you know, wrong.

41
00:01:56,300 --> 00:01:58,910
>> Hm. Well, that all makes sense except

42
00:01:58,910 --> 00:02:01,800
for one thing, which is the idea that this

43
00:02:01,800 --> 00:02:03,940
happened in the 1930s. Because we all know

44
00:02:03,940 --> 00:02:06,300
that rat dinosaurs did not exist in the 1930s.

45
00:02:07,640 --> 00:02:09,380
>> Yeah, they, well they went extinct in the 40s.

46
00:02:09,380 --> 00:02:10,080
>> Hm, hm.

47
00:02:11,270 --> 00:02:15,200
>> There's this one little epilogue to the story which I find really kind of

48
00:02:15,200 --> 00:02:17,900
amusing, and that is the computer scientists did

49
00:02:17,900 --> 00:02:20,510
pretty well at figuring out algorithms for solving

50
00:02:20,510 --> 00:02:23,480
problems like this. And the psychologists really do

51
00:02:23,480 --> 00:02:27,810
care about how stimuli and, and, motor actions

52
00:02:27,810 --> 00:02:30,660
and reward all Interact with each other and

53
00:02:30,660 --> 00:02:33,010
they've turned to the computer scientists to say,

54
00:02:33,010 --> 00:02:35,854
how might the brain be doing this? What, what is the problem that

55
00:02:35,854 --> 00:02:38,140
the brain actually might be solving? And

56
00:02:38,140 --> 00:02:40,160
so, guess what word [LAUGH] they borrowed

57
00:02:40,160 --> 00:02:44,520
back when they started to think about these things. Reinforcement. So, now they

58
00:02:44,520 --> 00:02:46,330
talk about reinforcement learning and they don't

59
00:02:46,330 --> 00:02:49,490
mean reinforcement, either. They mean reward maximization.

60
00:02:49,490 --> 00:02:50,105
>> So we won.

61
00:02:50,105 --> 00:02:53,437
>> [LAUGH] Yeah, I do not think that was really our plan but

62
00:02:53,437 --> 00:02:57,340
but it's, it's nice to know that we had some you know, impact.

63
00:02:57,340 --> 00:02:58,071
>> Well, that's very

64
00:02:58,071 --> 00:02:59,662
good you brought planning back into it and so

65
00:02:59,662 --> 00:03:02,105
they learned because of our plan, that's pretty good Michael.

66
00:03:02,105 --> 00:03:04,758
>> [LAUGH] All right, so that's the end of this aside.

67
00:03:04,758 --> 00:03:05,630
>> Okay.

68
00:03:05,630 --> 00:03:07,330
>> Let's go, let's go, let's go back to learning things.
