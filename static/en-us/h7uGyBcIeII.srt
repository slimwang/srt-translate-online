1
00:00:00,250 --> 00:00:01,090
We're in a grid world.

2
00:00:01,090 --> 00:00:02,050
>> Okay.
>> And

3
00:00:02,050 --> 00:00:04,430
you have to learn a reward function.

4
00:00:04,430 --> 00:00:08,080
You have to learn what's good or
not good to do in this grid, and

5
00:00:08,080 --> 00:00:09,510
you're going to get to
learn it by watching me.

6
00:00:09,510 --> 00:00:11,740
So I'm going to control the agent.

7
00:00:11,740 --> 00:00:13,000
The agent's name is Frank.

8
00:00:13,000 --> 00:00:17,635
>> Okay.
>> So he's going to go like this [SOUND]

9
00:00:17,635 --> 00:00:21,108
and then he stays there.

10
00:00:21,108 --> 00:00:21,780
Okay?

11
00:00:21,780 --> 00:00:22,360
>> Okay.
>> So

12
00:00:22,360 --> 00:00:26,860
I claim that you can actually make some
very good educated guesses about how

13
00:00:26,860 --> 00:00:30,650
rewards work in this environment, just
based on that one little demonstration.

14
00:00:30,650 --> 00:00:33,140
>> I think that's probably true.

15
00:00:33,140 --> 00:00:36,640
>> So is there anything that
the agent finds positively rewarding?

16
00:00:36,640 --> 00:00:37,470
>> Green.
>> Green.

17
00:00:37,470 --> 00:00:38,410
Yes, good.

18
00:00:38,410 --> 00:00:42,340
Is there anything that
the agent finds aversive?

19
00:00:42,340 --> 00:00:47,210
>> So, I could guess that blue is
aversive because you avoided the blue,

20
00:00:47,210 --> 00:00:51,050
but that also happened to be one of
the shortest path to get to the green.

21
00:00:51,050 --> 00:00:52,600
Yeah, that's true, that's true.

22
00:00:52,600 --> 00:00:55,120
So it might have just been
a coincidence, but I think you have some

23
00:00:55,120 --> 00:00:58,060
evidence that maybe blue was
something I was trying to avoid.

24
00:00:58,060 --> 00:00:59,100
>> Right.
When I-

25
00:00:59,100 --> 00:01:00,080
>> What about red?

26
00:01:00,080 --> 00:01:03,330
>> So red is not something you're
trying to avoid because you

27
00:01:03,330 --> 00:01:04,440
could have avoided it.

28
00:01:04,440 --> 00:01:05,440
>> That's right.
>> I don't know about

29
00:01:05,440 --> 00:01:07,490
orange because you couldn't
avoid orange to get the green.

30
00:01:07,490 --> 00:01:09,360
>> Excellent, and that was
exactly the inferences that I was

31
00:01:09,360 --> 00:01:10,050
hoping you could make.

32
00:01:10,050 --> 00:01:13,490
So, again, you can learn a tremendous
amount about what's valuable and

33
00:01:13,490 --> 00:01:16,790
what's not valuable by, not just the
path that was taken, but, in a sense,

34
00:01:16,790 --> 00:01:18,450
by the paths that were not taken.

35
00:01:18,450 --> 00:01:20,610
And what the other options are or

36
00:01:20,610 --> 00:01:22,950
were and
what the other options weren't, right?

37
00:01:22,950 --> 00:01:24,250
So you couldn't learn
anything about orange.

38
00:01:24,250 --> 00:01:27,470
It could be that he hates orange,
not as much he likes green.

39
00:01:27,470 --> 00:01:30,510
So Frank was willing to go through
orange and it didn't matter where.

40
00:01:30,510 --> 00:01:33,560
But red, yeah, totally could've
gone around red and didn't bother.

41
00:01:33,560 --> 00:01:35,090
Right?
So that you can avoid blue and

42
00:01:35,090 --> 00:01:38,130
red just by going like that,
but he didn't do that, so

43
00:01:38,130 --> 00:01:40,370
he probably is indifferent to red or
something.

44
00:01:40,370 --> 00:01:44,190
So, yes, it's actually a really cool
idea and we can actually learn things

45
00:01:44,190 --> 00:01:46,930
about what people want to do and
what they don't want to do.

46
00:01:46,930 --> 00:01:52,030
How tolerant are they of like sudden
stops in a car or a bumpy road?

47
00:01:52,030 --> 00:01:53,010
That sort of thing.

48
00:01:53,010 --> 00:01:55,900
How do you trade that off against
how fast you're trying to

49
00:01:55,900 --> 00:01:56,790
get to your destination?

50
00:01:57,800 --> 00:01:59,630
It's hard to get people to
tell you numbers for that,

51
00:01:59,630 --> 00:02:02,490
but it is actually not so
hard to have them demonstrate,

52
00:02:02,490 --> 00:02:05,720
and then infer these kinds of
values from their behavior.

53
00:02:05,720 --> 00:02:07,640
So, obviously, as we're sitting here,

54
00:02:07,640 --> 00:02:10,520
we can't really get into
the details of the algorithm.

55
00:02:10,520 --> 00:02:13,630
But just to give you a kind of
a hint of the form of this,

56
00:02:13,630 --> 00:02:16,460
there's an algorithm
that I worked on called

57
00:02:16,460 --> 00:02:18,530
Maximum Likelihood Inverse
Reinforcement Learning, but

58
00:02:18,530 --> 00:02:20,250
there's a whole bunch of
different algorithms.

59
00:02:20,250 --> 00:02:25,980
This one I happen to understand
the best because I helped make it up.

60
00:02:25,980 --> 00:02:29,090
And the basic game is here,
guess a reward function.

61
00:02:29,090 --> 00:02:33,200
In this case, we were guessing reward
functions that mapped colors to values.

62
00:02:33,200 --> 00:02:37,560
We then compute an optimal policy
with that reward function in mind.

63
00:02:37,560 --> 00:02:41,040
Measure the probability of the data of
the behavioral trajectories that we saw

64
00:02:41,040 --> 00:02:45,830
given that policy and then compute
a gradient on how can the value of

65
00:02:45,830 --> 00:02:49,950
the reward change in a way that
would make the data more likely?

66
00:02:49,950 --> 00:02:51,690
And then we just go back and

67
00:02:51,690 --> 00:02:56,194
do that again until we find
something that is a local maximum.

68
00:02:56,194 --> 00:02:58,360
>> That makes sense,
how does it work in practice?

69
00:02:58,360 --> 00:02:59,210
>> It seems to work pretty well.

70
00:02:59,210 --> 00:03:01,620
It can be slow and
it can get stuck in local minima, but

71
00:03:01,620 --> 00:03:03,690
we haven't really had
significant problems with that.

72
00:03:03,690 --> 00:03:06,210
It actually pretty consistently comes
up with a reward function that's

73
00:03:06,210 --> 00:03:07,060
pretty solid.

74
00:03:07,060 --> 00:03:09,800
There's tricky things like
keeping in mind the fact that,

75
00:03:09,800 --> 00:03:14,110
when I showed you the trajectory, I went
through green and then I stayed there.

76
00:03:14,110 --> 00:03:15,980
So how do we represent that?

77
00:03:15,980 --> 00:03:17,240
Is this an absorbing state?

78
00:03:17,240 --> 00:03:20,790
And if so, do I just include
it once in my trajectory?

79
00:03:20,790 --> 00:03:23,300
In which case, it just saw one little
green, in which case it's not so

80
00:03:23,300 --> 00:03:24,540
excited about green.

81
00:03:24,540 --> 00:03:26,510
Or is it really important and
it kind of sat there forever?

82
00:03:26,510 --> 00:03:28,810
In which case it really didn't
see much of anything else.

83
00:03:28,810 --> 00:03:31,830
So getting getting those terminal
states right can be a bit of a trick.

84
00:03:31,830 --> 00:03:33,730
>> That makes sense, okay, I like that.
