1
00:00:00,200 --> 00:00:04,780
So now you should have a good intuition for what PCA is, and how to use it.

2
00:00:04,780 --> 00:00:07,600
The last example I'm going to give you, starting very soon,

3
00:00:07,600 --> 00:00:11,830
is one of the coolest examples of where PCA is actually used in the wild.

4
00:00:11,830 --> 00:00:14,260
But before we get into that, let me just pause and, and

5
00:00:14,260 --> 00:00:15,990
talk about when you want to use PCA.

6
00:00:16,990 --> 00:00:19,330
When is it an appropriate approach?

7
00:00:19,330 --> 00:00:23,460
The first one is if you want access to latent features that you think might be

8
00:00:23,460 --> 00:00:25,540
showing up in the patterns in your data.

9
00:00:25,540 --> 00:00:28,370
Maybe the entire point of what you're trying to do is figure out if

10
00:00:28,370 --> 00:00:30,010
there's a latent feature.

11
00:00:30,010 --> 00:00:33,480
In other words you just want to know the size of the first principal component.

12
00:00:33,480 --> 00:00:35,270
An example of this might be something like,

13
00:00:35,270 --> 00:00:38,240
can you measure who the big shots are at Enron.

14
00:00:38,240 --> 00:00:41,240
The second one of course is dimensionality reduction.

15
00:00:41,240 --> 00:00:44,570
There's a number of things that PCA can do to help you out on this front.

16
00:00:44,570 --> 00:00:48,300
The first is that it can help you visualize high-dimensional data.

17
00:00:48,300 --> 00:00:51,914
So, of course when you're drawing a scatterplot you only have two dimensions

18
00:00:51,914 --> 00:00:55,598
that are available to you, but many times you'll have more than two features.

19
00:00:55,598 --> 00:00:58,997
So there's kind a struggle of how to represent three or four or many

20
00:00:58,997 --> 00:01:03,200
numbers about a data point if you only have two dimensions in which to draw it.

21
00:01:04,290 --> 00:01:07,157
And so what you can do, is you can project it down to the first two

22
00:01:07,157 --> 00:01:11,070
principal components and just plot that, and just draw that scatter point.

23
00:01:11,070 --> 00:01:14,270
And then things like, k-means clustering might be a lot easier for

24
00:01:14,270 --> 00:01:15,620
you to visualize.

25
00:01:15,620 --> 00:01:18,410
You're still capturing most of the information in the data but

26
00:01:18,410 --> 00:01:21,280
now you can draw it with those, with those two dimensions.

27
00:01:21,280 --> 00:01:22,870
Another thing the PCA can help with is if you

28
00:01:22,870 --> 00:01:24,940
suspect that there's noise in your data.

29
00:01:24,940 --> 00:01:27,380
And, in almost all data there will be noise.

30
00:01:27,380 --> 00:01:30,900
The hope is that the first or the second, your strongest principal components,

31
00:01:30,900 --> 00:01:32,830
are capturing the actual patterns in the data.

32
00:01:32,830 --> 00:01:36,010
And the smaller principle components are just representing

33
00:01:36,010 --> 00:01:38,380
noisy variations about those patterns.

34
00:01:38,380 --> 00:01:41,460
So by throwing away the less important principle components,

35
00:01:41,460 --> 00:01:42,399
you're getting rid of that noise.

36
00:01:43,440 --> 00:01:47,020
The last one, and what we'll use as our example for the rest of this lesson,

37
00:01:47,020 --> 00:01:50,920
is using PCA as pre-processing before you use another algorithm, so

38
00:01:50,920 --> 00:01:53,160
a regression or a classification task.

39
00:01:53,160 --> 00:01:55,940
As you know if you have very high dimensionality, and

40
00:01:55,940 --> 00:01:59,710
if you have a complex, say, classification algorithm.

41
00:01:59,710 --> 00:02:01,760
The algorithm can be very high variance,

42
00:02:01,760 --> 00:02:04,360
it can end up fitting to noise in the data.

43
00:02:04,360 --> 00:02:05,800
It can end up running really slow.

44
00:02:05,800 --> 00:02:08,740
There are lots of things that can happen when you have very high input

45
00:02:08,740 --> 00:02:10,639
dimensionality with some of these algorithms.

46
00:02:10,639 --> 00:02:13,660
But, of course, the algorithm might work really well for the problem at hand.

47
00:02:13,660 --> 00:02:17,470
So one of the things you can do is use PCA to reduce the dimensionality of

48
00:02:17,470 --> 00:02:18,840
your input features.

49
00:02:18,840 --> 00:02:21,830
So that then your, say, classification algorithm works better.

50
00:02:22,960 --> 00:02:25,030
And this is the example that we'll do next.

51
00:02:25,030 --> 00:02:27,320
It's something called eigenfaces, and

52
00:02:27,320 --> 00:02:30,790
it's a method of applying PCA to pictures of people.

53
00:02:30,790 --> 00:02:34,930
So this is very high dimensionality space, you have many,

54
00:02:34,930 --> 00:02:36,610
many pixels in the picture.

55
00:02:36,610 --> 00:02:39,890
But say, you want to identify who is pictured in the image.

56
00:02:39,890 --> 00:02:43,700
You're running some kind of facial identification, or what have you.

57
00:02:43,700 --> 00:02:47,550
So with PCA you can reduce the very high input dimensionality,

58
00:02:47,550 --> 00:02:50,190
into something that's maybe a factor of ten lower.

59
00:02:50,190 --> 00:02:53,930
And feed this into an SVM, which can then do the actual classification of

60
00:02:53,930 --> 00:02:55,670
trying to figure out who's pictured.

61
00:02:55,670 --> 00:02:57,870
So now the inputs, instead of being the original pixels or

62
00:02:57,870 --> 00:03:00,840
the images, are the principal components.

63
00:03:00,840 --> 00:03:02,850
So let me show you this example and you'll see what I mean.
