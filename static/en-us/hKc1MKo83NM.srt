1
00:00:00,300 --> 00:00:02,560
Hi, my name is Bushger, and today we are going

2
00:00:02,560 --> 00:00:06,140
to talk about information theory. Now, information theory is not

3
00:00:06,140 --> 00:00:09,580
really a machine learning algorithm, so we'll, kind of, first

4
00:00:09,580 --> 00:00:12,954
understand why we need to learn information theory. Usually, you

5
00:00:12,954 --> 00:00:16,149
could teach a whole course on information theory, but for

6
00:00:16,149 --> 00:00:19,300
now, it is sufficient to know the basics. So first

7
00:00:19,300 --> 00:00:22,090
we'll try to understand where information theory is used in

8
00:00:22,090 --> 00:00:25,800
motion learning. So consider this to be any machine learning algorithm.

9
00:00:25,800 --> 00:00:28,940
For example, let this learner be, or a decision learner,

10
00:00:28,940 --> 00:00:32,189
now we have several inputs, x1, x2, x3, and one

11
00:00:32,189 --> 00:00:35,460
output. For simplification, let's assume that this is a regression

12
00:00:35,460 --> 00:00:38,380
problem. That's why we have one output. We want to

13
00:00:38,380 --> 00:00:41,670
ask interesting questions like how is x1 related to y,

14
00:00:41,670 --> 00:00:44,760
x2 related to y, x3 related to y. Why do

15
00:00:44,760 --> 00:00:47,526
you want to ask such questions? If you remember, from

16
00:00:47,526 --> 00:00:50,856
our IDT algorithm, the first step is to find out which

17
00:00:50,856 --> 00:00:56,830
input best splits our output. So we need to find out which of these, x1, x2,

18
00:00:56,830 --> 00:00:59,600
or x3 gives you the most information about

19
00:00:59,600 --> 00:01:02,310
why. So we have to first understand what the

20
00:01:02,310 --> 00:01:07,410
word information in information theory means. In general

21
00:01:07,410 --> 00:01:10,100
every input vector and output vector, in the

22
00:01:10,100 --> 00:01:12,848
machine learning context, can be considered as a

23
00:01:12,848 --> 00:01:16,420
probability density function. So, information theory is a mathematical

24
00:01:16,420 --> 00:01:20,150
framework which allows us to compare these density

25
00:01:20,150 --> 00:01:22,390
functions, so that we can ask interesting questions

26
00:01:22,390 --> 00:01:25,600
like are these input vectors similar? If they're

27
00:01:25,600 --> 00:01:28,000
not similar, then how different they are? And so

28
00:01:28,000 --> 00:01:32,740
on. We call this measure as mutual information.

29
00:01:32,740 --> 00:01:35,170
Or we could ask if this feature has any

30
00:01:35,170 --> 00:01:38,920
information at all. So we'll call this measure

31
00:01:38,920 --> 00:01:41,652
entropy. So we are going to find out what

32
00:01:41,652 --> 00:01:44,370
these terms mean, how they're related to information learning in

33
00:01:44,370 --> 00:01:47,880
general, and we'll briefly look at the history of this field.
