1
00:00:00,450 --> 00:00:03,460
The problem with scaling
gradient descent is simple.

2
00:00:03,460 --> 00:00:05,500
You need to compute this gradient.

3
00:00:05,500 --> 00:00:07,480
Here is another rule of thumb.

4
00:00:07,480 --> 00:00:11,230
If computing your loss takes
n floating point operations,

5
00:00:11,230 --> 00:00:14,990
computing its gradient takes
about three times that compute.

6
00:00:14,990 --> 00:00:18,150
As we saw earlier,
this lost function is huge.

7
00:00:18,150 --> 00:00:21,880
It depends on every single
element in your training set.

8
00:00:21,880 --> 00:00:24,350
That can be a lot of compute
if your data set is big.

9
00:00:24,350 --> 00:00:28,400
And we want to be able to train
lots of data because in practice

10
00:00:28,400 --> 00:00:33,100
on real problems you will always get
more gains the more data you use.

11
00:00:33,100 --> 00:00:34,900
And because gradient
descent is intuitive,

12
00:00:34,900 --> 00:00:37,270
you have to do that for many steps.

13
00:00:37,270 --> 00:00:41,000
That means going through your
data tens or hundreds of times.

14
00:00:41,000 --> 00:00:43,650
That's not good, so instead,
we're going to cheat.

15
00:00:43,650 --> 00:00:47,190
Instead of computing the loss, we're
going to compute an estimate of it,

16
00:00:47,190 --> 00:00:50,640
a very bad estimate,
a terrible estimate in fact.

17
00:00:50,640 --> 00:00:54,280
That estimate is going to be so bad,
you might wonder why it works at all.

18
00:00:54,280 --> 00:00:55,480
And you would be right,

19
00:00:55,480 --> 00:00:59,630
because we're going to also have to
spend some time making it less terrible.

20
00:00:59,630 --> 00:01:03,490
The estimate we're going to use is
simply computing the average loss for

21
00:01:03,490 --> 00:01:06,360
a very small random fraction
of the training data.

22
00:01:06,360 --> 00:01:09,790
Think between 1 and
1000 training samples each time.

23
00:01:10,790 --> 00:01:13,240
I say random because
it's very important.

24
00:01:13,240 --> 00:01:16,150
If the way you pick your
samples isn't random enough,

25
00:01:16,150 --> 00:01:18,070
It no longer works at all.

26
00:01:18,070 --> 00:01:22,060
So, we're going to take a very small
sliver of the training data, compute

27
00:01:22,060 --> 00:01:26,890
the loss for that sample, compute the
derivative for that sample and pretend

28
00:01:26,890 --> 00:01:30,530
that that derivative is the right
direction to use to do gradient descent.

29
00:01:31,630 --> 00:01:33,800
It is not at all the right direction,
and

30
00:01:33,800 --> 00:01:37,910
in fact at times it might increase
the real loss, not reduce it.

31
00:01:37,910 --> 00:01:41,090
But we're going to compensate
by doing this many, many times,

32
00:01:41,090 --> 00:01:44,740
taking very, very small steps each time.

33
00:01:44,740 --> 00:01:47,308
So each step is a lot
cheaper to compute.

34
00:01:47,308 --> 00:01:48,900
But we pay a price.

35
00:01:48,900 --> 00:01:52,910
We have to take many more smaller steps,
instead of one large step.

36
00:01:52,910 --> 00:01:55,710
On balance, though, we win by a lot.

37
00:01:55,710 --> 00:01:57,580
In fact,
as you'll see in the assignments,

38
00:01:57,580 --> 00:02:01,470
doing this is vastly more efficient
than doing gradient descent.

39
00:02:01,470 --> 00:02:04,170
This technique is called
stochastic gradient descent and

40
00:02:04,170 --> 00:02:06,100
is at the core of deep learning.

41
00:02:06,100 --> 00:02:09,440
That's because stochastic gradient
descent scales well with both data and

42
00:02:09,440 --> 00:02:13,570
model size, and
we want both big data and big models.

43
00:02:13,570 --> 00:02:17,910
Stochastic gradient descent, SGD for
short, is nice and scalable.

44
00:02:17,910 --> 00:02:21,420
But because it is fundamentally a pretty
bad optimizer, that happens to be

45
00:02:21,420 --> 00:02:25,420
the only one that's fast enough, it
comes with a lot of issues in practice.
