1
00:00:00,430 --> 00:00:01,180
Hi, Michael.

2
00:00:01,180 --> 00:00:03,300
>> Hey Charles, well that was fun.

3
00:00:03,300 --> 00:00:05,700
>> That was fun, we should do it again.

4
00:00:05,700 --> 00:00:06,910
>> I don't think we need to do it again.

5
00:00:06,910 --> 00:00:09,640
I think we've now covered
everything we wanted to say about

6
00:00:09,640 --> 00:00:10,790
reinforcement learning.

7
00:00:10,790 --> 00:00:12,330
>> And
I'm completely sure that we haven't.

8
00:00:12,330 --> 00:00:13,640
>> Well, what haven't we covered?

9
00:00:13,640 --> 00:00:15,480
>> Well,
what haven't we haven't we covered?

10
00:00:15,480 --> 00:00:18,560
>> Well, we haven't,
we haven't we covered TD-Lambda.

11
00:00:18,560 --> 00:00:20,190
>> Sure.
So, we talked about TD lambda.

12
00:00:20,190 --> 00:00:21,580
We talked about a few algorithms here or

13
00:00:21,580 --> 00:00:23,740
there, but we haven't really
touched on the big questions.

14
00:00:23,740 --> 00:00:25,380
We haven't covered
everything that we should.

15
00:00:25,380 --> 00:00:26,500
>> Like?

16
00:00:26,500 --> 00:00:27,080
>> BURLAP!

17
00:00:27,080 --> 00:00:28,540
>> We have definitely covered BURLAP.

18
00:00:28,540 --> 00:00:32,680
>> Okay, sure, there was a tutorial, and
there were assignments about it, but

19
00:00:32,680 --> 00:00:35,920
you haven't answered the big
question about BURLAP.

20
00:00:35,920 --> 00:00:36,790
Which is what?

21
00:00:36,790 --> 00:00:37,590
>> Why BURLAP?

22
00:00:37,590 --> 00:00:39,630
Why do we need anything like BURLAP?

23
00:00:39,630 --> 00:00:44,000
Why don't you just use the UCI
database and Weka we're done.

24
00:00:44,000 --> 00:00:48,450
>> Right so the UCI data set their
depository and Weka are really great for

25
00:00:48,450 --> 00:00:52,100
supervised learning but
they're not actually well situated for

26
00:00:52,100 --> 00:00:53,080
reinforcement learning.

27
00:00:53,080 --> 00:00:56,070
Weka doesn't include reinforcement
learning algorithms it

28
00:00:56,070 --> 00:01:00,130
the UCI repository doesn't include
reinforcement learning problems in it.

29
00:01:00,130 --> 00:01:01,960
You really need a different set up for

30
00:01:01,960 --> 00:01:03,370
dealing with reinforcement
learning problems.

31
00:01:03,370 --> 00:01:05,010
>> You need a different, okay,
I don't understand that.

32
00:01:05,010 --> 00:01:08,680
So we have algorithms, we've talked
about many of them in class.

33
00:01:08,680 --> 00:01:09,210
>> That's right.

34
00:01:09,210 --> 00:01:12,770
>> So algorithms, are algorithms,
are algorithms, are algorithms, and

35
00:01:12,770 --> 00:01:15,360
this is all machine learning and
so all we need is data.

36
00:01:15,360 --> 00:01:18,040
And that's what repositories are for,
data.

37
00:01:18,040 --> 00:01:21,180
So, well yes, BURLAP is trying
to answer both of those issues.

38
00:01:21,180 --> 00:01:22,670
One of the things that
BURLAP is trying to be is

39
00:01:22,670 --> 00:01:24,730
a collection of those algorithms or
algorithms or

40
00:01:24,730 --> 00:01:29,710
algorithms or algorithms and it's also
providing a replacement for data.

41
00:01:29,710 --> 00:01:33,260
So data in the reinforcement learning
setting is really hard to work with.

42
00:01:33,260 --> 00:01:36,430
Instead, what BURLAP has
is a set of simulators,

43
00:01:36,430 --> 00:01:39,830
the ability to Act
like the environment so

44
00:01:39,830 --> 00:01:43,525
that the learning algorithm can
interact with it and learn from them.

45
00:01:43,525 --> 00:01:47,369
[CROSSTALK] I see so the keyword there
is in Iraq, so the notion of supervised

46
00:01:47,369 --> 00:01:50,739
learning as you just got data you got
him put it up with pairs input and

47
00:01:50,739 --> 00:01:54,527
output pairs, but as we pointed out
multiple times reinforcement learning

48
00:01:54,527 --> 00:01:57,601
is not just about input output you and
back Get to see states and

49
00:01:57,601 --> 00:01:59,001
rewards you take actions.

50
00:01:59,001 --> 00:02:02,280
Those are the kinds of thing that you
do and that requires interaction.

51
00:02:02,280 --> 00:02:05,760
>> Right and the data that you actually
see depends on how you're exploring

52
00:02:05,760 --> 00:02:09,800
the environment so just capturing a big
static set of data and presenting that

53
00:02:09,800 --> 00:02:12,082
to the learner isn't really
the reinforcement learning process.

54
00:02:12,082 --> 00:02:12,686
>> Okay, so I like that.

55
00:02:12,686 --> 00:02:14,010
So I heard two things there.

56
00:02:14,010 --> 00:02:15,660
One thing I like, and
one thing I don't like.

57
00:02:15,660 --> 00:02:18,180
The thing that I like is that you
distinguished reinforcement learning

58
00:02:18,180 --> 00:02:19,530
from supervised learning.

59
00:02:19,530 --> 00:02:21,530
You talked about the need for
interaction and

60
00:02:21,530 --> 00:02:25,950
how that's sort of crucial, and you said
your solution was having simulators.

61
00:02:25,950 --> 00:02:28,540
>> Right, so,
I agree that that's not a great thing.

62
00:02:28,540 --> 00:02:33,610
So simulators are good in the sense that
you can actually interact with them,

63
00:02:33,610 --> 00:02:36,500
and so it's a way of transferring and
an environment or

64
00:02:36,500 --> 00:02:39,366
a test problem from one research
group to another research Group.

65
00:02:39,366 --> 00:02:40,038
[CROSSTALK] Sounds good.

66
00:02:40,038 --> 00:02:42,395
But on the bad side
they're not really data,

67
00:02:42,395 --> 00:02:44,830
it's not actual data
coming from a problem.

68
00:02:44,830 --> 00:02:45,775
It's just fit.

69
00:02:45,775 --> 00:02:49,167
[CROSSTALK] So then you're suggesting
this is the second thing that I didn't

70
00:02:49,167 --> 00:02:52,771
thing I like you're suggesting that we
don't have any way to make reinforcement

71
00:02:52,771 --> 00:02:54,260
learning work in the real world.

72
00:02:54,260 --> 00:02:57,672
It is more challenging, but it is not
the case that reinforcement learning

73
00:02:57,672 --> 00:03:00,330
hasn't been used to solve
some real problems.

74
00:03:00,330 --> 00:03:01,260
>> Like what?

75
00:03:01,260 --> 00:03:03,200
>> So, it's worth probably
mentioning a few of them.

76
00:03:03,200 --> 00:03:05,410
We talked about backgammon
as part of the class,

77
00:03:05,410 --> 00:03:09,140
the TD-Gammon algorithm that actually
learn to play backgammon at a level

78
00:03:09,140 --> 00:03:11,700
comparable to,
if not better than, human beings.

79
00:03:11,700 --> 00:03:14,690
There's other sorts of problems that are
probably worth talking about as well.

80
00:03:14,690 --> 00:03:17,870
Elevator control is an interesting
problem to think about.

81
00:03:17,870 --> 00:03:20,013
From a reinforcement
learning perspective.

82
00:03:20,013 --> 00:03:21,630
>> How so?
How is an elevator controller

83
00:03:21,630 --> 00:03:22,620
reinforcement learning about?

84
00:03:22,620 --> 00:03:25,030
>> Elevator control if you think about
the elevators in a building they're

85
00:03:25,030 --> 00:03:28,950
trying to work together to move
people to where they want to go.

86
00:03:28,950 --> 00:03:32,350
And so we have actions,
we have states and we have rewards.

87
00:03:32,350 --> 00:03:35,840
The actions are where the elevators
are trying to go, whether or

88
00:03:35,840 --> 00:03:38,320
not they're going to open
their doors at a given floor.

89
00:03:38,320 --> 00:03:41,380
The states are what
is the arrangement of

90
00:03:41,380 --> 00:03:42,820
all the different
elevators at the moment.

91
00:03:42,820 --> 00:03:45,040
Which floors have elevators on them,
which don't.

92
00:03:45,040 --> 00:03:46,010
>> Which buttons have been pressed.

93
00:03:46,010 --> 00:03:47,270
>> Which buttons have been pressed.

94
00:03:47,270 --> 00:03:49,540
So who's waiting on which floors.

95
00:03:49,540 --> 00:03:51,380
And from the reward perspective

96
00:03:52,560 --> 00:03:56,520
you can try to get the system to try to
minimize wait time for people, right?

97
00:03:56,520 --> 00:03:57,720
So that's something
that you can measure,

98
00:03:57,720 --> 00:04:00,600
how long is somebody waiting on the
third floor for the elevator to come,

99
00:04:00,600 --> 00:04:03,360
and then eventually getting
to the destination floor and

100
00:04:03,360 --> 00:04:04,720
that's something you can measure and

101
00:04:04,720 --> 00:04:08,355
something that you can try to
find ways of behaving to improve.

102
00:04:08,355 --> 00:04:10,360
Okay cool, so
that's a reinforcement learning problem.

103
00:04:10,360 --> 00:04:11,530
I like that, anything else?

104
00:04:11,530 --> 00:04:13,874
>> Yeah, so that one actually
was done in simulation.

105
00:04:13,874 --> 00:04:14,380
>> Okay.

106
00:04:14,380 --> 00:04:20,329
>> But there has been some good work
in reinforcement learning in robotics.

107
00:04:20,329 --> 00:04:24,020
So one that's maybe worth mentioning
is the the helicopter work.
