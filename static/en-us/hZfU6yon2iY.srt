1
00:00:00,180 --> 00:00:05,410
To briefly summarize the Viola-Jones
face detector, couple of key ideas.

2
00:00:05,410 --> 00:00:09,520
First of all, they're going to use these
rectangular features and really the big

3
00:00:09,520 --> 00:00:13,850
idea was the integral image that makes
it easy to compute those rapidly.

4
00:00:15,370 --> 00:00:18,260
Second, they used AdaBoost for
doing both feature selection and

5
00:00:18,260 --> 00:00:20,130
for training the classifier in general.

6
00:00:20,130 --> 00:00:22,340
We didn't talk a lot about that here.

7
00:00:22,340 --> 00:00:26,110
Sometimes I talk more about it, but,
these days you just go look up AdaBoost,

8
00:00:26,110 --> 00:00:27,970
it's easy enough to implement.

9
00:00:27,970 --> 00:00:30,090
The really useful idea
was the cascade filter,

10
00:00:30,090 --> 00:00:31,490
an, and
other people had done cascades and

11
00:00:31,490 --> 00:00:35,990
various kinds of things, but, the way it
was applied to face detection was here.

12
00:00:35,990 --> 00:00:39,448
Training is very s,
not very s, training is slow.

13
00:00:39,448 --> 00:00:42,880
Remember those 180,000 different
possible features, and you try them and

14
00:00:42,880 --> 00:00:44,180
you're looking for
all these weak learners.

15
00:00:44,180 --> 00:00:47,780
So there's a lot of potential
weak learners to choose from.

16
00:00:47,780 --> 00:00:52,070
But, detection run time is very fast.

17
00:00:52,070 --> 00:00:57,220
So when I pick up my little camera or
I point my I my smartphone,

18
00:00:57,220 --> 00:00:59,860
it can find the faces very quickly.

19
00:00:59,860 --> 00:01:02,420
And it is really, really effective.

20
00:01:02,420 --> 00:01:07,990
It is rare to see a paper turn into
a both commercial technology and

21
00:01:07,990 --> 00:01:11,210
impact the field so
dramatically, so quickly.

22
00:01:11,210 --> 00:01:14,400
And, you know, that's some measure
of the effectiveness of the paper.

23
00:01:14,400 --> 00:01:18,009
And neither Paul nor
Mike gave me any funds to endorse them.

24
00:01:19,050 --> 00:01:19,770
All right.

25
00:01:19,770 --> 00:01:22,400
So let's summarize
boosting real quickly.

26
00:01:22,400 --> 00:01:25,880
It combines feature selection
with classification.

27
00:01:25,880 --> 00:01:27,940
Remember that selecting
of the weak learners?

28
00:01:27,940 --> 00:01:30,400
So you can think of that as
sort of selecting, not to,

29
00:01:30,400 --> 00:01:32,730
you can think of that as,
as finding features.

30
00:01:32,730 --> 00:01:36,100
And pretty much any scheme you come up
with for defining weak learners and

31
00:01:36,100 --> 00:01:37,680
trying them will work.

32
00:01:37,680 --> 00:01:43,070
The complexity is linear in
the number of training examples.

33
00:01:43,070 --> 00:01:45,330
You're basically trying
different weak learners.

34
00:01:45,330 --> 00:01:46,560
You pick however many you want.

35
00:01:46,560 --> 00:01:49,330
And you apply them against your
various training examples.

36
00:01:49,330 --> 00:01:51,910
We mentioned that here because the
method that we're going to talk about,

37
00:01:51,910 --> 00:01:54,940
some of the bets we talk about later
become sort of somewhat quadratic

38
00:01:54,940 --> 00:01:58,680
because you have to compare each of
the points to each of the other points.

39
00:01:58,680 --> 00:02:00,810
You don't have to do that here.

40
00:02:00,810 --> 00:02:05,190
Testing is very, very fast,
it's ve, very easy to run.

41
00:02:05,190 --> 00:02:07,987
You can write boosting code,
in fact, most, many people have.

42
00:02:07,987 --> 00:02:11,446
You might ask,
why wouldn't you write your own code?

43
00:02:11,446 --> 00:02:13,083
>> Why wouldn't you write your own code?

44
00:02:13,083 --> 00:02:13,685
>> because the me,

45
00:02:13,685 --> 00:02:16,879
some of the methods that we're
going to talk about are actually,

46
00:02:16,879 --> 00:02:20,450
the bookkeeping is very difficult,
and people use other libraries.

47
00:02:20,450 --> 00:02:24,220
And then when the stuff doesn't work,
it's much harder to track down,

48
00:02:24,220 --> 00:02:27,940
was it the classification method,
was it the parameters within, etc.?

49
00:02:27,940 --> 00:02:31,370
Because it's kind of a black box to you,
boosting you can write your own.

50
00:02:32,390 --> 00:02:34,060
There are some disadvantages.

51
00:02:34,060 --> 00:02:36,840
It does need lots and
lots of training examples.

52
00:02:36,840 --> 00:02:40,740
But remember, that was an assumption
that we made, not this lecture but

53
00:02:40,740 --> 00:02:43,110
the one before, that when we're
doing discriminative training,

54
00:02:43,110 --> 00:02:45,270
we're assuming we've
got lots of examples.

55
00:02:46,500 --> 00:02:51,860
Now it's sometimes found to not
work as well as some other methods.

56
00:02:51,860 --> 00:02:54,580
For example, support vector machines
which is something we're going to talk

57
00:02:54,580 --> 00:02:55,750
about in a little bit.

58
00:02:55,750 --> 00:02:58,565
And lately, there's been a lot
of work on random forests.

59
00:02:58,565 --> 00:03:00,215
Which I won't do in this class, but

60
00:03:00,215 --> 00:03:04,195
is sort of the, I a even newer
approach to, to doing that.

61
00:03:04,195 --> 00:03:09,405
And it's been particularly taken it a
bit on the chin for many-class problems,

62
00:03:09,405 --> 00:03:11,465
so not a binary classifier, etc.

63
00:03:11,465 --> 00:03:15,065
But it is still a, a pretty
effective way of doing these things.
