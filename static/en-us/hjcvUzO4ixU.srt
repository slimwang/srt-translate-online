1
00:00:00,140 --> 00:00:03,390
Linear networks are like
we mentioned before.

2
00:00:03,390 --> 00:00:05,180
The learning rule is really simple for
those.

3
00:00:05,180 --> 00:00:06,186
So that's great.

4
00:00:06,186 --> 00:00:09,280
People [INAUDIBLE] Parr have
gotten cool things to happen for

5
00:00:09,280 --> 00:00:15,320
things like cell phone, channel
allocation, and so that's pretty cool.

6
00:00:15,320 --> 00:00:18,170
You have to be really, really
careful about what features you use.

7
00:00:18,170 --> 00:00:18,833
Right?
because I probably

8
00:00:18,833 --> 00:00:22,108
should have mentioned this before
the important thing about the features.

9
00:00:22,108 --> 00:00:25,453
I'm going to say that it's important
that we find features that are both

10
00:00:25,453 --> 00:00:28,080
computationally efficient and
value informative.

11
00:00:28,080 --> 00:00:30,350
And what I mean by that is
computational efficient.

12
00:00:30,350 --> 00:00:33,430
What would be an example of a feature
that is really really great but

13
00:00:33,430 --> 00:00:35,150
is not particularly
computationally efficient?

14
00:00:35,150 --> 00:00:37,839
>> The one that tells us what
the proper action is I don't know.

15
00:00:39,050 --> 00:00:40,990
>> Or
what the value is actually in this case.

16
00:00:40,990 --> 00:00:41,890
Exactly so.

17
00:00:41,890 --> 00:00:44,836
So if there was a feature that
actually encoded the value function

18
00:00:44,836 --> 00:00:45,360
that would be great.

19
00:00:45,360 --> 00:00:47,670
That would be really good for learning.

20
00:00:47,670 --> 00:00:50,020
The thing is we don't know how
to get that feature efficiently.

21
00:00:50,020 --> 00:00:52,040
If we did we wouldn't
have to learn at all.

22
00:00:52,040 --> 00:00:56,550
So we have to somehow get features
that we can extract efficiently from

23
00:00:56,550 --> 00:00:57,465
the state,

24
00:00:57,465 --> 00:01:02,190
but again that's really easy to do we
can just return the state as a feature.

25
00:01:02,190 --> 00:01:05,300
It's really important that it
actually also be value informative.

26
00:01:05,300 --> 00:01:07,825
So that one feature actually
gives you lots of information And

27
00:01:07,825 --> 00:01:11,135
about what the predictive
value ought to be so

28
00:01:11,135 --> 00:01:15,015
somehow balancing these two things
is often very tricky to get to work.

29
00:01:15,015 --> 00:01:18,674
>> Right so going back to our taxi
example if it turns out that knowing how

30
00:01:18,674 --> 00:01:20,628
far away the taxi is from some place or

31
00:01:20,628 --> 00:01:22,983
how far away a passenger
is from some place.

32
00:01:22,983 --> 00:01:26,403
And you can compute that pretty easily
then you should do it because it's

33
00:01:26,403 --> 00:01:29,463
helpful and in your learning
are written as to figure it out so

34
00:01:29,463 --> 00:01:31,480
that is really good thing, right?

35
00:01:31,480 --> 00:01:32,703
>> Yes a distance to passenger could be

36
00:01:32,703 --> 00:01:34,590
something that's both
computationally efficient.

37
00:01:34,590 --> 00:01:37,538
because you can just kind of pull it
right out of the state representation.

38
00:01:37,538 --> 00:01:41,223
And probably value informative because
it's actually very relevant to know,

39
00:01:41,223 --> 00:01:43,588
are we close to being able
to pick up the passenger or

40
00:01:43,588 --> 00:01:46,610
is the passenger quite
distant difficult to reach.

41
00:01:46,610 --> 00:01:49,940
So yeah I think that's probably
the kind of things that we would want.

42
00:01:49,940 --> 00:01:53,998
We probably should try that before
we tell people that we think that's

43
00:01:53,998 --> 00:01:54,838
a great idea.

44
00:01:54,838 --> 00:01:57,831
But in general these are the kind
of properties that you're trying

45
00:01:57,831 --> 00:01:58,427
to balance.

46
00:01:58,427 --> 00:02:00,961
>> Okay.
>> And I want to mention because this is

47
00:02:00,961 --> 00:02:05,039
very hot right now another example
of using neural networks to learn

48
00:02:05,039 --> 00:02:09,187
value functions has been carried
out recently by Google DeepMind and

49
00:02:09,187 --> 00:02:11,570
they were using deep neural networks.

50
00:02:11,570 --> 00:02:13,370
Do you know what deep
neural networks are?

51
00:02:13,370 --> 00:02:15,920
>> Yeah, they're are networks
that are very profound.

52
00:02:15,920 --> 00:02:17,795
>> Yes they're poetic neural networks.

53
00:02:17,795 --> 00:02:18,670
>> Mm-hm.

54
00:02:18,670 --> 00:02:23,320
>> No, they are neural networks that are
neural networks except some of them have

55
00:02:23,320 --> 00:02:28,620
were instead of say three layers they
could have ten, 15 they can actually be

56
00:02:28,620 --> 00:02:31,260
quite substantially more
sophisticated and complicated.

57
00:02:31,260 --> 00:02:36,252
And they often involve using things
like convolutions to work well on

58
00:02:36,252 --> 00:02:38,328
the visual kinds of inputs.

59
00:02:38,328 --> 00:02:43,272
So what Minnetal all did at
Google DeepMind is they actually learned

60
00:02:43,272 --> 00:02:47,091
effective strategies for
Atari video games mapping

61
00:02:47,091 --> 00:02:51,966
the pixels on the screen to joystick
actions which is really neat.

62
00:02:51,966 --> 00:02:52,980
>> It is
>> Right, so

63
00:02:52,980 --> 00:02:56,812
it's actually learning to play the game
given very similar kind of input to what

64
00:02:56,812 --> 00:02:57,448
people get.

65
00:02:57,448 --> 00:03:01,075
And in fact they were able to get it
to learn very effectively to the point

66
00:03:01,075 --> 00:03:04,620
where you could argue that it was
comparable to people on average.

67
00:03:04,620 --> 00:03:08,500
>> At least on some games But did it
learn how to play thermonuclear war?

68
00:03:08,500 --> 00:03:11,590
>> No, it learned that
the winning move is not to play.

69
00:03:11,590 --> 00:03:12,210
>> Nice.

70
00:03:12,210 --> 00:03:12,870
>> No, that's not true.

71
00:03:12,870 --> 00:03:16,000
That was a WarGames reference, and
in fact, it did not learn not to play.

72
00:03:16,000 --> 00:03:17,770
It was not given the option
to learn not to play.

73
00:03:17,770 --> 00:03:19,390
It just had to play
whatever it was given.

74
00:03:19,390 --> 00:03:20,508
>> Hm.
Grad school.

75
00:03:20,508 --> 00:03:23,360
>> [LAUGH] And we're back.

76
00:03:23,360 --> 00:03:25,900
No, it is true that, on some of
the games, it was superhuman, and

77
00:03:25,900 --> 00:03:28,160
in other of the games,
it was way, way subhuman.

78
00:03:28,160 --> 00:03:30,420
And that was why I said,
you could argue, on average,

79
00:03:30,420 --> 00:03:31,900
that was about human level.

80
00:03:31,900 --> 00:03:35,860
>> Mm, okay, but that's like arguing
that, on average, if I have my

81
00:03:35,860 --> 00:03:39,190
head in an oven and my feet in ice
water, on average, I'm comfortable.

82
00:03:39,190 --> 00:03:39,690
>> Yes.
