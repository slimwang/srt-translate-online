1
00:00:00,250 --> 00:00:04,342
Now let me focus on a specific aspect of writing high performance kernel code.

2
00:00:04,342 --> 00:00:07,140
At a high level, GPU programming looks like this.

3
00:00:07,140 --> 00:00:10,013
There's a bunch of data sitting in global memory,

4
00:00:10,013 --> 00:00:12,984
and you have an algorithm that you want to run on that data.

5
00:00:12,984 --> 00:00:16,050
That algorithm will get executed by threads running on SMs,

6
00:00:16,050 --> 00:00:20,312
and for various reasons, you might want to stage the data through shared memory.

7
00:00:20,312 --> 00:00:24,401
And you've seen for yourself that loading or storing global memory into shared memory

8
00:00:24,401 --> 00:00:26,731
or into local variables of the threads

9
00:00:26,731 --> 00:00:29,474
can be complicated if you are striving for really high performance.

10
00:00:29,474 --> 00:00:31,302
Think about problem set 2

11
00:00:31,302 --> 00:00:33,400
where you loaded an image tile into shared memory

12
00:00:33,400 --> 00:00:37,068
and also had to load a halo of extra cells around the image tile

13
00:00:37,068 --> 00:00:39,608
in order to account for the width of the blur,

14
00:00:39,608 --> 00:00:45,502
and this can be tricky because the number of threads that want to perform a computation

15
00:00:45,502 --> 00:00:48,050
on the pixels in that tile

16
00:00:48,050 --> 00:00:51,995
is naturally a different number than the number of threads

17
00:00:51,995 --> 00:00:56,600
you would launch if your goal was simply to load the pixels in the tile

18
00:00:56,600 --> 00:01:00,222
as well as the pixels outside of the tile.

19
00:01:00,222 --> 00:01:03,900
Or think about the tile transpose example in Unit 5

20
00:01:03,900 --> 00:01:10,223
where we staged through shared memory in order to pay careful attention to coalescent global memory.

21
00:01:10,223 --> 00:01:12,976
Think about our discussion of Little's Law

22
00:01:12,976 --> 00:01:15,545
and the trade offs that we went over between latency,

23
00:01:15,545 --> 00:01:18,447
bandwidth, occupancy, the number of threads,

24
00:01:18,447 --> 00:01:20,531
and the transaction size per thread.

25
00:01:20,531 --> 00:01:24,140
Remember that we saw a graph that looked sort of like this

26
00:01:24,140 --> 00:01:28,459
where the bandwidth that we achieved as we increased the number of threads

27
00:01:28,459 --> 00:01:33,037
was higher if we were able to access 4 floating point values in a single load

28
00:01:33,037 --> 00:01:35,431
versus 2 floating point values in a single load

29
00:01:35,431 --> 00:01:39,941
versus a single floating point value in a load.

30
00:01:39,941 --> 00:01:43,707
Finally, there are ninja level optimizations we haven't even talked about in this class,

31
00:01:43,707 --> 00:01:46,351
like using the Kepler LDG intrinsic.

32
00:01:46,351 --> 00:01:49,885
In short, CUDA's explicit use of user managed shared memory

33
00:01:49,885 --> 00:01:52,550
enables predictable high performance.

34
00:01:52,550 --> 00:01:55,981
In contrast, the hardware manage caches that you find on CPUs

35
00:01:55,981 --> 00:01:58,880
can result in unpredictable performance,

36
00:01:58,880 --> 00:02:02,565
especially when you have many multiple threads sharing the same cache, and they're interfering with each other.

37
00:02:02,565 --> 00:02:04,835
So that's an advantage of explicit shared memory,

38
00:02:04,835 --> 00:02:08,422
but that advantage comes at the cost of an additional burden on the programmer

39
00:02:08,422 --> 00:02:12,235
who has to explicitly manage the movement of data in and out from global memory.

40
00:02:12,235 --> 00:02:14,306
And this is a big part of where CUB comes in.

41
00:02:14,306 --> 00:02:18,255
CUB puts an abstraction around the algorithm and its memory access pattern

42
00:02:18,255 --> 00:02:22,846
and deals opaquely with the movement of data from global memory,

43
00:02:22,846 --> 00:02:27,650
possibly through shared memory, into the actual local variables of the threads.

44
00:02:27,650 --> 00:02:31,389
I want to mention another programming power tool called Cuda DMA

45
00:02:31,389 --> 00:02:35,693
that focuses specifically around the movement of data from global into shared memory.
