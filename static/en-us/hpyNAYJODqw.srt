1
00:00:00,140 --> 00:00:01,850
So at this point, I should stop and talk a

2
00:00:01,850 --> 00:00:04,000
little bit about the type of data that we'll be working

3
00:00:04,000 --> 00:00:06,210
with, in a number of places in the rest of

4
00:00:06,210 --> 00:00:09,790
this course. So I'm sure you're aware that Wikipedia, in addition

5
00:00:09,790 --> 00:00:12,450
to the article text for many types of data, also

6
00:00:12,450 --> 00:00:16,660
includes info box data. Now, info box data is essentially structure

7
00:00:16,660 --> 00:00:20,800
data that complements the subject of the article. In this

8
00:00:20,800 --> 00:00:25,030
case, this is an article for the city of Hyderabad and,

9
00:00:25,030 --> 00:00:26,860
in addition to a description of the city

10
00:00:26,860 --> 00:00:30,020
here and other interesting information about the city,

11
00:00:30,020 --> 00:00:33,010
we have data in the form of, population,

12
00:00:34,010 --> 00:00:37,170
land area in square kilometers, as well as a

13
00:00:37,170 --> 00:00:39,100
number of other useful pieces of data about

14
00:00:39,100 --> 00:00:42,820
this city. Now, Wikipedia has info boxes for a

15
00:00:42,820 --> 00:00:46,090
lot of different types of data. People, animals,

16
00:00:46,090 --> 00:00:50,160
automobiles and so on. An organization called DBpedia has

17
00:00:50,160 --> 00:00:52,980
taken the info box data from Wikipedia and produced it as

18
00:00:52,980 --> 00:00:55,990
a collection of data sets that we can download. Here's a

19
00:00:55,990 --> 00:00:58,960
description of some of the data sets themselves and some of

20
00:00:58,960 --> 00:01:02,010
the details about them. Now, in our case, we're going to

21
00:01:02,010 --> 00:01:06,080
be working with the city's data set. This data is distributed

22
00:01:06,080 --> 00:01:09,200
as a CSV file that we can download. It contains all

23
00:01:09,200 --> 00:01:11,960
of the info box data for a particular snapshot in time

24
00:01:11,960 --> 00:01:15,614
for all cities described on Wikipedia, for which there is info

25
00:01:15,614 --> 00:01:18,940
box data. Now, in order to prepare this data as

26
00:01:18,940 --> 00:01:22,120
CSV, the DBPedia folks had to do a little bit

27
00:01:22,120 --> 00:01:24,670
of work in encoding it, to get certain types of

28
00:01:24,670 --> 00:01:27,910
data values into individual cells. Let's take a look at

29
00:01:27,910 --> 00:01:30,760
the city's data set. We're going to look here, in

30
00:01:30,760 --> 00:01:34,030
Excel, because this data is so dense, we really do

31
00:01:34,030 --> 00:01:37,160
need the added structure that the spreadsheet app provides. So

32
00:01:37,160 --> 00:01:40,730
here's a row for Hyderabad. And, there are about 20,000

33
00:01:40,730 --> 00:01:43,930
rows in this CSV file; so, lots of cities

34
00:01:43,930 --> 00:01:46,260
are represented here. And, we can see that there are

35
00:01:46,260 --> 00:01:49,670
lots of null cell values. This is coding in this

36
00:01:49,670 --> 00:01:52,710
data set which indicates that there is no value supplied

37
00:01:52,710 --> 00:01:55,732
for that particular piece of data. Now, something that's

38
00:01:55,732 --> 00:01:58,212
also important to bear in mind is that while this

39
00:01:58,212 --> 00:02:01,374
data is actually in pretty good shape, it's entirely human

40
00:02:01,374 --> 00:02:06,329
generated. You can edit it by editing the Wikipedia page,

41
00:02:06,329 --> 00:02:09,823
as usual. There is some of that data here. So

42
00:02:09,823 --> 00:02:12,274
what this means of course, is that there's a lot of

43
00:02:12,274 --> 00:02:15,130
dirty data here. Data that's going to need to be cleaned

44
00:02:15,130 --> 00:02:18,230
up. So this data set provides a nice example of wrangling

45
00:02:18,230 --> 00:02:20,930
data, because we're going to have to decode some of the

46
00:02:20,930 --> 00:02:23,120
fields in order to get them into the form that we

47
00:02:23,120 --> 00:02:26,080
want. We're going to be looking at transforming these lines of

48
00:02:26,080 --> 00:02:31,170
data into JSON documents, and then, eventually, storing them into MongoDB.

49
00:02:31,170 --> 00:02:32,720
Let's take a look at a couple of examples

50
00:02:32,720 --> 00:02:36,510
of the task that we have ahead of us. Okay,

51
00:02:36,510 --> 00:02:39,500
this is the postal code field, and we can see

52
00:02:39,500 --> 00:02:43,200
that there are lots of different formats represented here. Something

53
00:02:43,200 --> 00:02:45,560
else that's interesting about this data, is the way

54
00:02:45,560 --> 00:02:50,250
they've encoded arrays. Some fields have multiple values stored within

55
00:02:50,250 --> 00:02:54,150
them. Here's the utcOffset field. And in this case, we

56
00:02:54,150 --> 00:02:56,590
can see that there are two values for the utcOffset

57
00:02:56,590 --> 00:02:59,790
for this city and a couple of others here. There's two

58
00:02:59,790 --> 00:03:03,310
different values that represent the utcOffset for when the city is on

59
00:03:03,310 --> 00:03:07,350
Daylight Savings Time versus when it's not. And you can see that

60
00:03:07,350 --> 00:03:11,470
we have some HTML entities represented here, and some Unicode characters, and

61
00:03:11,470 --> 00:03:13,302
some other characters that we're going to need to a little

62
00:03:13,302 --> 00:03:15,670
work with. So we've got our work cut out for us with

63
00:03:15,670 --> 00:03:18,700
this data in terms of pulling it out of the CSV form,

64
00:03:18,700 --> 00:03:21,920
parsing it into individuals fields, and cleaning up some of the problematic

65
00:03:21,920 --> 00:03:25,080
data that's here. After that, then we'll take a look

66
00:03:25,080 --> 00:03:28,182
at getting into MangoDB. So as you can see this

67
00:03:28,182 --> 00:03:30,500
data set provides a nice opportunity for us to explore

68
00:03:30,500 --> 00:03:33,492
things like auditing a data set, to see where the

69
00:03:33,492 --> 00:03:36,850
problems lie. And then thinking about exactly what we should

70
00:03:36,850 --> 00:03:39,530
do with fields like this and other structures that we're

71
00:03:39,530 --> 00:03:42,140
going to find in this data set. And then finally,

72
00:03:42,140 --> 00:03:44,170
we'll take a look at putting this data into MangoDB.
