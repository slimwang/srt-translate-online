1
00:00:00,150 --> 00:00:01,400
>> All right Charles, so what would you, what would

2
00:00:01,400 --> 00:00:04,910
you call let's, let's let's think about this bottom one first.

3
00:00:04,910 --> 00:00:05,060
>> Okay.

4
00:00:05,060 --> 00:00:08,029
>> So what would you call an approach to reinforcement learning that

5
00:00:08,029 --> 00:00:10,990
what it does is it builds a model and then plans with it?

6
00:00:10,990 --> 00:00:12,960
>> A planner?

7
00:00:12,960 --> 00:00:16,149
>> Well, it's not a planner. I mean, the planner's inside of it.

8
00:00:16,149 --> 00:00:16,640
>> Sure.

9
00:00:16,640 --> 00:00:19,960
>> The overall system, this sort of blue box. Turns

10
00:00:19,960 --> 00:00:22,690
transitions into policies so it's kind of a reinforcement learner.

11
00:00:22,690 --> 00:00:23,180
>> Yeah.

12
00:00:23,180 --> 00:00:25,810
>> But it's one that builds a model inside.

13
00:00:27,410 --> 00:00:30,300
>> I would call that. You know what I would

14
00:00:30,300 --> 00:00:33,850
call that, I would call that model-based learning or model-based planning.

15
00:00:33,850 --> 00:00:36,402
>> Actually it's called model-based reinforcement learning.

16
00:00:36,402 --> 00:00:37,170
>> Mm-hm.

17
00:00:37,170 --> 00:00:39,030
>> So this is, this is, in fact, my,

18
00:00:39,030 --> 00:00:41,480
you said, reinforcement learning is your favorite kind of learning?

19
00:00:41,480 --> 00:00:41,714
>> Mm-hm.

20
00:00:41,714 --> 00:00:43,010
>> Model-based reinforcement learning is

21
00:00:43,010 --> 00:00:44,640
my favorite kind of of reinforcement learning.

22
00:00:44,640 --> 00:00:44,690
>> Mm.

23
00:00:44,690 --> 00:00:46,790
>> But we're not going to get to talk about it

24
00:00:46,790 --> 00:00:48,200
very much, so I wanted to at least have this

25
00:00:48,200 --> 00:00:50,510
slide to give people a chance to, at least, you

26
00:00:50,510 --> 00:00:52,350
know, these are all pieces that you can imagine doing. Right?

27
00:00:52,350 --> 00:00:54,810
This, this piece you can think of as being what

28
00:00:54,810 --> 00:00:57,050
we did in, when we're talking about supervised learning. And this

29
00:00:57,050 --> 00:00:59,310
piece is what you talked about last time. So you

30
00:00:59,310 --> 00:01:02,610
know, you could build a model based reinforcement learner that way.

31
00:01:02,610 --> 00:01:03,240
>> Yeah, that makes sense.

32
00:01:03,240 --> 00:01:05,880
>> Alright, how about this other idea, where, where you start off

33
00:01:05,880 --> 00:01:08,320
with a model and then you pretend you don't have a model, you

34
00:01:08,320 --> 00:01:12,430
pretend you're just in a learning situation by turning that model into,

35
00:01:12,430 --> 00:01:15,760
into transitions just by simulating them.

36
00:01:15,760 --> 00:01:17,940
Then the learner interacts with the simulator.

37
00:01:17,940 --> 00:01:19,080
And then spits out a policy.

38
00:01:19,080 --> 00:01:22,300
>> Well, I could come up with one of two answers.

39
00:01:22,300 --> 00:01:22,440
>> Okay.

40
00:01:22,440 --> 00:01:26,940
>> So I could try to do pattern matching on the answer to the second one. And

41
00:01:26,940 --> 00:01:28,600
since that one's model based, then this one's

42
00:01:28,600 --> 00:01:33,460
transition based, which is kind of cute. Or I

43
00:01:33,460 --> 00:01:35,500
could say, well one difference between at least

44
00:01:35,500 --> 00:01:38,420
inside the kind of blue box is it's sort

45
00:01:38,420 --> 00:01:40,540
of a model free reinforcement module. because the

46
00:01:40,540 --> 00:01:43,210
learner does not ever get to see the model.

47
00:01:43,210 --> 00:01:45,430
>> This is true, the learning piece is model free

48
00:01:45,430 --> 00:01:49,640
but we're using model free learner in service of planning.

49
00:01:49,640 --> 00:01:50,330
>> Okay.

50
00:01:50,330 --> 00:01:53,564
>> So, I don't know, I don't have a good

51
00:01:53,564 --> 00:01:59,400
this like model free planner, or an RL-based planner, maybe.

52
00:01:59,400 --> 00:02:01,010
Like I said, I wasn't going to grade it, I just

53
00:02:01,010 --> 00:02:03,290
was kind of interested to see what, what you'd say.

54
00:02:03,290 --> 00:02:07,690
>> Hm. Well I, I was pretty, I felt pretty good about the model-based RL one.

55
00:02:07,690 --> 00:02:08,250
>> Yeah, well this

56
00:02:08,250 --> 00:02:11,050
is the actual term that's used in the field. I'm, I'm sure there

57
00:02:11,050 --> 00:02:14,030
are some terms that are used here but nothing, nothing really very consistently.

58
00:02:14,030 --> 00:02:16,900
>> Hm. What if I called it the blue box planner?

59
00:02:16,900 --> 00:02:18,730
>> You could call it that, and, but had I, had I

60
00:02:18,730 --> 00:02:20,850
drawn it with a black box, it would, it had a better name.

61
00:02:20,850 --> 00:02:22,300
>> mm, oo, that would have been really

62
00:02:22,300 --> 00:02:24,880
cool, black box planner. I like that. Okay.

63
00:02:24,880 --> 00:02:27,000
>> But but I want to point out that one of the most

64
00:02:27,000 --> 00:02:29,390
successful applications of reinforcement learning are

65
00:02:29,390 --> 00:02:33,905
least most celebrated was Jerry Tassara's

66
00:02:33,905 --> 00:02:37,950
backgammon playing program, which used exactly this approach. Backgammon

67
00:02:37,950 --> 00:02:40,300
is a board game. So we have a complete model

68
00:02:40,300 --> 00:02:42,110
of it, but we don't really know how to plan

69
00:02:42,110 --> 00:02:44,598
it, it's a very complex, very large state space. So

70
00:02:44,598 --> 00:02:46,781
what he did is, he actually used a simulator

71
00:02:46,781 --> 00:02:49,023
backgammon, to generate transitions, and

72
00:02:49,023 --> 00:02:51,206
then used reinforcement learning approach

73
00:02:51,206 --> 00:02:54,100
TD, to, to create a policy for that. So it,

74
00:02:54,100 --> 00:02:58,080
his TD Gammon system followed exactly this, this overall pattern.

75
00:02:58,080 --> 00:02:59,276
>> Yeah, it was very influential

76
00:02:59,276 --> 00:03:01,820
work and generated many mildly embarrassing Master's theses.

77
00:03:01,820 --> 00:03:05,470
>> [LAUGH]

78
00:03:05,470 --> 00:03:07,140
I can only think of one. Oh, actually I

79
00:03:07,140 --> 00:03:08,920
can think, that's not true, I can think of two.

80
00:03:08,920 --> 00:03:09,540
>> Oh really?

81
00:03:09,540 --> 00:03:11,610
>> Well, which one are you, you're thinking of yours.

82
00:03:11,610 --> 00:03:14,620
>> Yes, I'm thinking of mine. I try not to think about it.

83
00:03:14,620 --> 00:03:17,830
>> The, the other one is, Justin Boyin, who's a good

84
00:03:17,830 --> 00:03:20,680
friend. And, a highly influential reinforcement

85
00:03:20,680 --> 00:03:24,440
learning contributor, and now Googler. Who,

86
00:03:24,440 --> 00:03:26,680
his Masters thesis was also, it's basically

87
00:03:26,680 --> 00:03:28,950
another TD. He did backgammon with RL.

88
00:03:28,950 --> 00:03:31,830
>> Oh, I didn't realize that. I, I actually like him. I think

89
00:03:31,830 --> 00:03:36,130
he's very cool. [LAUGH]. So, I'm sure that, his was not mildly embarrassing.

90
00:03:37,830 --> 00:03:39,395
>> I mean, you know, everybody's master's

91
00:03:39,395 --> 00:03:41,910
thesis are mildly embarrassing because you're, you're struggling

92
00:03:41,910 --> 00:03:44,310
to learn about how to talk about research,

93
00:03:44,310 --> 00:03:45,830
and so you're not going to get it perfect.

94
00:03:45,830 --> 00:03:48,550
>> Yeah, and in my case, it was it definitely model free.

95
00:03:48,550 --> 00:03:49,870
>> Right, the only

96
00:03:49,870 --> 00:03:53,170
non-embarrassing master's thesis that I'm aware of.

97
00:03:53,170 --> 00:03:54,660
>> Shanon's.

98
00:03:54,660 --> 00:03:58,850
>> Oh sure, alright, well Shannon. What was his master's thesis?

99
00:03:58,850 --> 00:04:00,240
>> It was information theory.

100
00:04:00,240 --> 00:04:01,850
>> Oh, yeah. It's pretty impressive.

101
00:04:01,850 --> 00:04:02,290
>> Yeah.

102
00:04:02,290 --> 00:04:04,760
>> Rob Schapire's master's thesis is pretty cool.

103
00:04:04,760 --> 00:04:05,970
>> I have no doubt.

104
00:04:05,970 --> 00:04:09,720
>> The boosting guy. He did, he did pom de pe learning.

105
00:04:09,720 --> 00:04:11,720
>> For his master's thesis?

106
00:04:11,720 --> 00:04:13,720
>> Yeah. With the diversity representation.

107
00:04:13,720 --> 00:04:14,830
>> I am a terrible human being.

108
00:04:14,830 --> 00:04:19,526
[LAUGH]. I mean wow. I mean it is true though, you look at someone like Shannon

109
00:04:19,526 --> 00:04:21,124
and you just think to yourself, oh, for

110
00:04:21,124 --> 00:04:23,823
his Master's thesis he did, he invented information theory.

111
00:04:23,823 --> 00:04:26,840
[LAUGH] You know, like wow, I wonder what he did for his PhD thesis? You know

112
00:04:26,840 --> 00:04:28,770
what he did for his PhD thesis? Nobody

113
00:04:28,770 --> 00:04:30,800
knows, nobody cares, because he based it on. [LAUGH]

114
00:04:30,800 --> 00:04:33,100
>> He, he had already done the information theories. They're like

115
00:04:33,100 --> 00:04:35,210
here, fill out a piece of paper, you can pick up

116
00:04:35,210 --> 00:04:37,330
a PHD. He did something on AI, I can't remember what

117
00:04:37,330 --> 00:04:39,970
it was, but it was, it was totally unimportant and nobody cares

118
00:04:39,970 --> 00:04:43,018
about it. But information theory? Yeah. [LAUGH].

119
00:04:43,018 --> 00:04:43,450
>> Thanks.

120
00:04:43,450 --> 00:04:46,000
>> All right, well, yeah, okay. [LAUGH].

121
00:04:46,000 --> 00:04:48,770
>> II think, I think Schapire's PHD was boosting.

122
00:04:50,460 --> 00:04:53,970
>> Thanks Rob. Make us all look bad, why don't you.
