1
00:00:01,347 --> 00:00:02,716
So, if we set things up this way, we actually

2
00:00:02,716 --> 00:00:04,969
get some wonderful properties coming out. So here are some

3
00:00:04,969 --> 00:00:08,185
things we know about this set up for zero-sum stochastic

4
00:00:08,185 --> 00:00:12,037
games. Value iteration works, so we can actually solve this

5
00:00:12,037 --> 00:00:16,012
system of equations by using the value iteration trick, which

6
00:00:16,012 --> 00:00:19,237
is to say, we initialize these Q values to whatever

7
00:00:19,237 --> 00:00:22,762
and then we just iterate this as an assignment, right,

8
00:00:22,762 --> 00:00:26,910
we just say, you know, equals. So value iteration works.

9
00:00:26,910 --> 00:00:31,130
This minimax Q algorithm converges under the same kinds

10
00:00:31,130 --> 00:00:33,520
of conditions that Q learning converges, so we get this

11
00:00:33,520 --> 00:00:36,590
nice, you know, Q learning analogue in this multi-agent

12
00:00:36,590 --> 00:00:40,340
setting. The Q star that's defined by these equations is

13
00:00:40,340 --> 00:00:43,490
unique, so we iterate it and we find it

14
00:00:43,490 --> 00:00:46,400
and it's just, there's just that one answer. The policies

15
00:00:46,400 --> 00:00:49,310
for the two players can be computed independently, that

16
00:00:49,310 --> 00:00:52,050
is to say, if two different players are running minimax

17
00:00:52,050 --> 00:00:55,760
Q on their own and not really coordinating with each other except for by

18
00:00:55,760 --> 00:00:57,790
playing the game, that the policies that

19
00:00:57,790 --> 00:01:00,400
they get out will actually converge to minimax

20
00:01:00,400 --> 00:01:05,040
optimal policies. So it really does solve the zero sum game, which is maybe not

21
00:01:05,040 --> 00:01:06,870
so surprising because, you know, they are

22
00:01:06,870 --> 00:01:08,533
trying to kill each other after all. [LAUGH]

23
00:01:08,533 --> 00:01:08,690
>> Yeah.

24
00:01:08,690 --> 00:01:10,180
>> So, the idea that they'd have to

25
00:01:10,180 --> 00:01:12,415
collaborate to do that efficiently would be weird.

26
00:01:12,415 --> 00:01:14,200
>> [LAUGH] I never thought about it

27
00:01:14,200 --> 00:01:15,480
like that, but, yeah, that would be weird.

28
00:01:15,480 --> 00:01:17,090
>> The, this update that

29
00:01:17,090 --> 00:01:20,430
I've written here can be computed efficiently, which is to say

30
00:01:20,430 --> 00:01:22,710
in polynomial time. Because this minimax

31
00:01:22,710 --> 00:01:26,170
can be computed using linear programming.

32
00:01:26,170 --> 00:01:26,810
>> Yes of course.

33
00:01:28,140 --> 00:01:31,720
>> And, finally, if we actually iterate this Q

34
00:01:31,720 --> 00:01:35,730
equation and, and it's converging to Q star, knowing Q

35
00:01:35,730 --> 00:01:39,590
star is enough to figure out how to behave optimally.

36
00:01:39,590 --> 00:01:42,130
So we can convert these Q values into an actual

37
00:01:42,130 --> 00:01:46,170
behavior, again, by using the, the solution in the linear program.

38
00:01:46,170 --> 00:01:48,994
>> So it's just like MDPs of value iteration with Q-learning?

39
00:01:48,994 --> 00:01:50,630
>> Exactly. It's like we've gone to, to

40
00:01:50,630 --> 00:01:52,840
a second agent and it really hasn't impacted things

41
00:01:52,840 --> 00:01:54,980
negatively at all. This is, this is all

42
00:01:54,980 --> 00:01:57,810
the, pretty much all the things that we want,

43
00:01:57,810 --> 00:01:59,300
come out. There, there are some things that

44
00:01:59,300 --> 00:02:01,466
don't come out. For example, in the case of

45
00:02:01,466 --> 00:02:03,914
an MDP, we can solve these, this system

46
00:02:03,914 --> 00:02:07,320
of linear equations in polynomial time. Not just by

47
00:02:07,320 --> 00:02:10,150
value iteration, but we can actually set up it as a single linear

48
00:02:10,150 --> 00:02:13,250
program and solve it and be done in linear time or, sorry, not linear

49
00:02:13,250 --> 00:02:16,280
time, polynomial time. This is not known to be true in the zero-sum

50
00:02:16,280 --> 00:02:18,440
stochastic game case, it's not known whether

51
00:02:18,440 --> 00:02:19,780
it can be solved in polynomial time.

52
00:02:19,780 --> 00:02:20,500
>> Hm.

53
00:02:20,500 --> 00:02:23,630
>> So there, it is a little harder as a problem, but it's, you know, not

54
00:02:23,630 --> 00:02:25,340
harder, not deeply harder and not harder in

55
00:02:25,340 --> 00:02:27,010
a way that matters in a machine learning setting.

56
00:02:27,010 --> 00:02:27,720
>> Cool.

57
00:02:27,720 --> 00:02:30,170
>> So this is really great. So let's,

58
00:02:30,170 --> 00:02:32,400
let's try to take this same approach and

59
00:02:32,400 --> 00:02:34,060
see if we can deal with general sum games.

60
00:02:34,060 --> 00:02:34,690
>> Okay.
