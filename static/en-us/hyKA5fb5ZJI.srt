1
00:00:00,000 --> 00:00:04,306
So, this is NVVP. It's a very powerful tool. It has a lot of functionality.

2
00:00:04,306 --> 00:00:07,980
We're only going to touch on it today. My real point is simply to point out to

3
00:00:07,980 --> 00:00:11,147
you that these tools exist and that even though in this class we're going to

4
00:00:11,147 --> 00:00:15,219
focus on understanding the concepts and being able to reason about these things from scratch,

5
00:00:15,219 --> 00:00:18,818
I do want you to understand that you don't have to always do this from scratch,

6
00:00:18,818 --> 00:00:20,922
that you should use the tools that are available.

7
00:00:20,922 --> 00:00:24,061
So the first thing we're going to do is pick an executable to run on.

8
00:00:24,061 --> 00:00:29,030
And so this is where we tell it what executable to run, we're going to run our transpose code.

9
00:00:29,030 --> 00:00:34,038
Before we do that, I'm actually going to go into the transpose code and comment

10
00:00:34,038 --> 00:00:40,010
out the section of it that is running the transpose serial kernel,

11
00:00:40,010 --> 00:00:44,914
that first kernel that we figured out. That one is so slow that I don't really want to spend any more time on that.

12
00:00:44,914 --> 00:00:50,654
And as you'll see, the profiler takes a little time to run the code several times and acquire statistics about it.

13
00:00:50,654 --> 00:00:58,944
So I comment that out and save it. I go back and I recompile my code, and now I'm ready to go back to NVVP,

14
00:00:58,944 --> 00:01:10,357
tell it what executable we're going to run, and here we go.

15
00:01:10,357 --> 00:01:17,074
It runs the program, it measures all of the calls to the CUDA API, things like malloc and memcpy,

16
00:01:17,074 --> 00:01:20,368
and it measures the time taken for every kernel.

17
00:01:20,368 --> 00:01:24,660
Here's the transpose parallel per row kernel. Let me zoom In on that.

18
00:01:24,660 --> 00:01:32,079
You can see that the parallel per row kernel started a 110 milliseconds into the executional programs,

19
00:01:32,079 --> 00:01:34,927
finished at 119 milliseconds into the program,

20
00:01:34,942 --> 00:01:40,456
ran a single block at a grid size of 111 so it launched a single block,

21
00:01:40,456 --> 00:01:47,068
that block had 1024 threads. You can get a bunch of other statistics from this.

22
00:01:47,068 --> 00:01:52,905
If we scroll to the right, we can see the much shorter time taken by our transpose per element kernel.

23
00:01:52,905 --> 00:01:58,384
I'll zoom In even further. And this one, as you see,

24
00:01:58,384 --> 00:02:04,046
ran in a larger grid of 64 by 64 blocks, each of which had 16 by 16 threads.

25
00:02:04,046 --> 00:02:08,416
Now, I'll make the point that we measured a shorter time than that.

26
00:02:08,416 --> 00:02:10,785
And that the times that you see in the profile are not going to

27
00:02:10,785 --> 00:02:16,207
match up exactly to the timings that we measured a little bit ago, when we were outside the profiler,

28
00:02:16,207 --> 00:02:19,000
and that's to be expected.
