1
00:00:00,290 --> 00:00:04,910
So last time we talked about how entropy can be used to measure the degree of

2
00:00:04,910 --> 00:00:07,020
skew of a probability distribution.

3
00:00:07,020 --> 00:00:10,480
And it's defined to be the expression here.

4
00:00:10,480 --> 00:00:15,250
The sum of pi times log of pi times minus 1.

5
00:00:15,250 --> 00:00:18,220
Now let's examine the properties of H of x.

6
00:00:19,310 --> 00:00:21,080
H of x has a minimum value of zero.

7
00:00:21,080 --> 00:00:27,130
This happens when the probability distribution is maximally skewed.

8
00:00:27,130 --> 00:00:30,320
And, by maximally skewed, I mean it looks like this.

9
00:00:30,320 --> 00:00:35,610
The probability that x will take on any of its values is zero, except for

10
00:00:35,610 --> 00:00:38,600
a single value which has a probability of one.

11
00:00:38,600 --> 00:00:42,450
This represents a state of absolute certainty about what value x is

12
00:00:42,450 --> 00:00:43,710
going to take on.

13
00:00:43,710 --> 00:00:47,890
Conversely, H of x takes on its maximum value

14
00:00:47,890 --> 00:00:51,830
when x's probability distribution is at its most random.

15
00:00:51,830 --> 00:00:56,780
Meaning, if it can take on k possible values.

16
00:00:56,780 --> 00:01:00,430
The probability that anyone of them will occur is one over k,

17
00:01:00,430 --> 00:01:05,150
thus as the distribution of x becomes more skewed it's entropy goes down
