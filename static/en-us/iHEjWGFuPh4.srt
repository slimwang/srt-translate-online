1
00:00:00,000 --> 00:00:03,000
Let's stop and review the 3 main forms of learning.

2
00:00:03,000 --> 00:00:05,000
We have supervised learning,

3
00:00:05,000 --> 00:00:07,000
in which the training set

4
00:00:07,000 --> 00:00:10,000
is a bunch of input/output pairs--

5
00:00:10,000 --> 00:00:15,000
X1,Y1; X2, Y2; et cetera--

6
00:00:15,000 --> 00:00:18,000
in which we try to produce a function:

7
00:00:18,000 --> 00:00:21,000
y equals f of x--

8
00:00:21,000 --> 00:00:24,000
and so the learning is producing this function, f.

9
00:00:24,000 --> 00:00:27,000
Then we have unsupervised learning,

10
00:00:27,000 --> 00:00:29,000
in which we're given just a set of data points--

11
00:00:29,000 --> 00:00:33,000
X1, X2, and so on--

12
00:00:33,000 --> 00:00:35,000
and each of these points, maybe, has many

13
00:00:35,000 --> 00:00:37,000
dimensions, many features.

14
00:00:37,000 --> 00:00:39,000
And what we're trying to learn is some patterns in that--

15
00:00:39,000 --> 00:00:42,000
some clusters of these data--

16
00:00:42,000 --> 00:00:45,000
or you could just say what we're trying to learn

17
00:00:45,000 --> 00:00:47,000
is a probability distribution

18
00:00:47,000 --> 00:00:49,000
or what's the probability that this

19
00:00:49,000 --> 00:00:52,000
random variable will have particular values;

20
00:00:52,000 --> 00:00:55,000
and learn something interesting from that.

21
00:00:55,000 --> 00:00:58,000
In this Unit, we're introducing the third type of learning--

22
00:00:58,000 --> 00:01:01,000
reinforcement learning--

23
00:01:01,000 --> 00:01:05,000
in which we have a sequence of action and state transitions.

24
00:01:05,000 --> 00:01:11,000
So: state and action, state and action--and so on.

25
00:01:11,000 --> 00:01:16,000
And at some point, we have some rewards associated with these.

26
00:01:16,000 --> 00:01:21,000
So there's a reward, and maybe not a reward for this state;

27
00:01:21,000 --> 00:01:23,000
and then another reward for this state--

28
00:01:23,000 --> 00:01:27,000
and the rewards are just scalar numbers, positive or negative numbers.

29
00:01:27,000 --> 00:01:29,000
What we're trying to learn here is:

30
00:01:29,000 --> 99:59:59,999
at optimal policy, what's the right thing to do in any of the states?
