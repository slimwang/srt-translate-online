1
00:00:01,300 --> 00:00:08,832
Hi, it's Peter. Sebastian's taking a rare vacation day today, so I'm going to be answering questions myself.

2
00:00:08,833 --> 00:00:15,599
We had a bunch of really great questions again from all you students out there, so let's go through some of them.

3
00:00:15,600 --> 00:00:20,666
The first one's from MRLED in London, and he asks,

4
00:00:20,667 --> 00:00:25,566
"What's the normal path for a professional in A.I. Assuming you come from university, what sorts of

5
00:00:25,567 --> 00:00:32,966
"roles in industry can be pursued? There's names like developer, games developer, computer scientist.

6
00:00:32,967 --> 00:00:38,599
"Are we more than just that?" And then related to that question is one from Sacha in Hamburg, saying,

7
00:00:38,600 --> 00:00:45,466
"I'm in love with A.I. However, given my age and lack of degree, it seems hopeless to get a job in A.I.

8
00:00:45,467 --> 00:00:49,466
"Is it too late for a mid-30s to jump onto the A.I. bandwagon?"

9
00:00:49,467 --> 00:00:55,732
And I guess I'd have to say that there are many paths. Another title besides the one mentioned would be research scientist,

10
00:00:55,733 --> 00:01:03,066
but usually that's for somebody with a Ph.D. I see the title "data scientist" showing up now more commonly.

11
00:01:03,067 --> 00:01:10,532
But I would say that most paths involve being a computer scientist or developer who has some A.I. expertise

12
00:01:10,533 --> 00:01:17,932
and gets to use that on the job, but doesn't define themselves only by being an A.I. developer.

13
00:01:17,933 --> 00:01:23,832
So you've got to be able to get the job done, no matter what the tools take, but it's great to have that A.I. expertise

14
00:01:23,833 --> 00:01:26,999
as part of your toolkit that you can apply to your job.

15
00:01:27,000 --> 00:01:35,699
I should also note that my colleague, Hal Varian at Google has stated that he thinks that the sexiest job

16
00:01:35,700 --> 00:01:42,932
in the next ten years will be statistician. And in some ways, statistician and machine learning person overlap

17
00:01:42,933 --> 00:01:46,899
quite a bit. So you're in good shape there.

18
00:01:46,900 --> 00:01:52,466
Let's see. The next question comes from New York City.

19
00:01:52,467 --> 00:01:57,532
"How has your experience of the continuous flow of data from quizzes and homeworks changed the way you teach

20
00:01:57,533 --> 00:02:03,199
"this course, and will it impact how you teach in-person courses?"

21
00:02:03,200 --> 00:02:08,265
Well, I've got to say we're doing our best just to keep up day-to-day, so we really haven't had a lot of time

22
00:02:08,267 --> 00:02:15,432
to do a complete quantitative analysis of all the data that we get--all the clickstreams and the answers to questions and so on.

23
00:02:15,433 --> 00:02:18,966
We'd like to do that; we haven't really had a chance yet.

24
00:02:18,967 --> 00:02:25,266
We've got sort of an impression overall from the quizzes and homeworks, and from all the discussion forums.

25
00:02:25,267 --> 00:02:31,099
It tells us a lot about what works and doesn't work. And overall, I guess one thing that comes through is

26
00:02:31,100 --> 00:02:38,432
when you have 1,000 times more students than you're used to, there are 1,000 times more ways to misunderstand

27
00:02:38,433 --> 00:02:43,799
and get some assumptions wrong. And we feel bad whenever there's confusion over that,

28
00:02:43,800 --> 00:02:50,999
so we're learning to be more careful. But still, I'd like to take this chance to say that the point of the quizzes

29
00:02:51,000 --> 00:02:56,799
and the homeworks is not really just to see if you can get 100% right. If you can, that's great.

30
00:02:56,800 --> 00:03:01,132
But the point of the quizzes is for you to think deeply about the problem.

31
00:03:01,133 --> 00:03:08,399
And in some ways, if you find the problem confusing and it forces you to think about it, then you've really done more work

32
00:03:08,400 --> 00:03:14,066
than somebody who doesn't see any ambiguity and answers right away.

33
00:03:14,067 --> 00:03:21,299
So if there are ambiguities, I apologize and I feel bad about that, and we'll use the learning we get from all of you

34
00:03:21,300 --> 00:03:25,899
to do a better job and try to eliminate those ambiguities.

35
00:03:25,900 --> 00:03:31,532
But if it makes you think about the problem more deeply and if you discover something that others didn't,

36
00:03:31,533 --> 00:03:38,999
then I'm not apologizing for that, because really what we're trying to do is to get you to engage with the material as much as we can.

37
00:03:39,000 --> 00:03:45,666
Next question is from Geneva, Switzerland. "What books would you recommend to read after the end of the class?"

38
00:03:45,667 --> 00:03:50,166
And I guess I would say there's a lot of recommendations; I don't want to go through all of them now.

39
00:03:50,167 --> 00:03:57,199
And I've written up some of them already, so you can search online for Peter Norvig's library, Google Books,

40
00:03:57,200 --> 00:04:01,632
and you'll see a list of books that I've pointed out there,

41
00:04:01,633 --> 00:04:04,466
and some of them I've actually written reviews for.

42
00:04:04,467 --> 00:04:08,566
And there are several lists and they're broken out by different categories within A.I.

43
00:04:08,567 --> 00:04:14,266
So it all depends on what you're interested in. Natural language, planning, vision, robotics, learning.

44
00:04:14,267 --> 00:04:19,065
All the different areas are there, and you can choose the ones you're most interested in.

45
00:04:19,067 --> 00:04:24,732
Next question is from Pambo in Buenos Aires. Says, "Hi, is there any relation between Markov decision processes

46
00:04:24,733 --> 00:04:27,366
"and game theory? They seem to have something in common."

47
00:04:27,367 --> 00:04:34,466
And that's a great question, and it's one that came up in the class at Stanford as well.

48
00:04:34,467 --> 00:04:39,332
We've shown several approaches to planning actions. We started out with state-based search,

49
00:04:39,333 --> 00:04:44,099
which works best for deterministic and centerless domains.

50
00:04:44,100 --> 00:04:49,999
And then we got into classical planning--that works with more abstract and complex representations.

51
00:04:50,000 --> 00:04:54,399
We added the ability to do sensing to that, to go beyond classical planning.

52
00:04:54,400 --> 00:04:59,866
We talked about MDPs, Markov decision processes, to deal with stochastic actions,

53
00:04:59,867 --> 00:05:02,466
and game theory to deal with adversaries.

54
00:05:02,467 --> 00:05:07,299
Now, each of these fields have historically developed separately, and they had their own vocabulary

55
00:05:07,300 --> 00:05:14,732
and their own methods of working. And they attack different parts of the complexity of environments.

56
00:05:14,733 --> 00:05:21,566
But when you start adding on, we go from classical planning and you start adding in stochastic

57
00:05:21,567 --> 00:05:25,899
and sensing and so on, then they all begin to merge. When you add all the features back in,

58
00:05:25,900 --> 00:05:28,432
they all arrive at the same place.

59
00:05:28,433 --> 00:05:33,866
And so the historical differences seem to dissolve a little.

60
00:05:33,867 --> 00:05:39,399
So the nomenclature is still different, and that makes it confusing--I apologize for that.

61
00:05:39,400 --> 00:05:43,966
And the different approaches have their different highlights. They are good in different things.

62
00:05:43,967 --> 00:05:50,666
So take for example MDPs, seem to be good for things like a robot navigating around a room.

63
00:05:50,667 --> 00:05:58,032
Where the robot could very easily get from one spot of the room to another, and could come to the same spot multiple times.

64
00:05:58,033 --> 00:06:04,332
So you could say that the state space is rather dense, in that it's easy to get from one place to another.

65
00:06:04,333 --> 00:06:11,999
Now, compare that to a game tree search or game theory. Take a game like the game of Go.

66
00:06:12,000 --> 00:06:18,599
We put a black stone down on one point, now we've really kind of partitioned the state.

67
00:06:18,600 --> 00:06:22,566
And so now we're only in the part of the state space where there's black on that point

68
00:06:22,567 --> 00:06:26,166
and not on the part of the state space where there's white.

69
00:06:26,167 --> 00:06:32,866
Now, it is possible in Go to flip from one to the other, you could capture that piece and then replace it with a white one.

70
00:06:32,867 --> 00:06:42,332
But that's rare. So really, in Go, you're exploring deep, long trees that don't have many circles back to the same position,

71
00:06:42,333 --> 00:06:45,632
whereas in robot navigation it's almost the opposite.

72
00:06:45,633 --> 00:06:53,066
So that's why we have different techniques, even though they're very overlapping in what they end up covering.

73
00:06:53,067 --> 00:06:57,266
I hope that maybe someday there'll be a unification of all these different techniques,

74
00:06:57,267 --> 00:07:03,499
and that we'll understand all the best parts of all of them and be able to express one in the notation of another

75
00:07:03,500 --> 00:07:09,932
and make it all work. And maybe one of you students out there will be the one who ends up doing it.

76
00:07:09,933 --> 00:07:14,266
Let's see. Another question about MDPs comes from Germany:

77
00:07:14,267 --> 00:07:19,899
"You showed some videos of robots when introducing MDPs. Actually, what are the stochastic actions?

78
00:07:19,900 --> 00:07:25,866
"I can't believe one really treats the movement of a robot in a stochastic way.

79
00:07:25,867 --> 00:07:29,066
"And even if so, how small or large did you use your grid?"

80
00:07:29,067 --> 00:07:37,299
So yes, the movement of real robots are in fact stochastic. But we exaggerated that in the examples we gave.

81
00:07:37,300 --> 00:07:43,266
So it would be very rare to have a robot that's so bad that 80% of the time it goes straight,

82
00:07:43,267 --> 00:07:48,366
but 10% of the time it goes in a 90-degree direction in the wrong way.

83
00:07:48,367 --> 00:07:52,232
So robots don't go off that bad, by 90 degrees.

84
00:07:52,233 --> 00:07:58,832
But if you ask a robot to go forward one meter, it's very rare that it would go forward exactly one meter.

85
00:07:58,833 --> 00:08:05,166
Maybe it'll go 95 centimeters or 106 centimeters. And maybe it won't go exactly straight ahead;

86
00:08:05,167 --> 00:08:09,332
Maybe it'll go five degrees to the right or two degrees to the left.

87
00:08:09,333 --> 00:08:15,699
So these actions are all stochastic, if not to the degree that we showed them.

88
00:08:15,700 --> 00:08:19,332
Should also say that there are stochastic changes to the environment.

89
00:08:19,333 --> 00:08:24,566
Say a person or another object might appear in front of the robot, forcing it to stop,

90
00:08:24,567 --> 00:08:31,466
and the percepts are very stochastic. Take a LIDAR rangefinder--these are very precise instruments

91
00:08:31,467 --> 00:08:38,466
and they can give measurements up to a few millimeters, but not every percep that they give back is accurate.

92
00:08:38,467 --> 00:08:45,166
Some of them are wildly off. And we have to use techniques like filtering to make sense of those.

93
00:08:45,167 --> 00:08:49,299
Let's see. The next question comes from Fargo, North Dakota.

94
00:08:49,300 --> 00:08:52,799
"When I tried to implement a reinforcement learning agent, I encountered an issue:

95
00:08:52,800 --> 00:08:57,732
"The agent takes too long to reach the absorbing state, and hence doesn't learn anything for a long time.

96
00:08:57,733 --> 00:09:02,099
"And sometimes it never stumbles upon the absorbing state. Any remedies?"

97
00:09:02,100 --> 00:09:08,532
Well, that's true. In problems where there are no rewards along the way, when there's only a final terminal state,

98
00:09:08,533 --> 00:09:12,699
and where the number of terminal states is small compared to the total number of states,

99
00:09:12,700 --> 00:09:19,632
it can take a really long time to learn anything. And one remedy is just try harder, run more steps,

100
00:09:19,633 --> 00:09:23,699
don't be afraid to do millions of steps rather than thousands of steps.

101
00:09:23,700 --> 00:09:32,799
And another is to use an initial policy that's not completely random, but is somehow guided towards these terminal states.

102
00:09:32,800 --> 00:09:41,399
And if you don't know where they are, then use an initial policy that's more exploratory to cover the full state,

103
00:09:41,400 --> 00:09:51,332
And then once you find your way, then you can try to bias the policy in that direction.

104
00:09:51,333 --> 00:10:03,066
Let's see. Here's a question, "Is the update in value iterations simultaneous, like the AIMA book says on 653,

105
00:10:03,067 --> 00:10:09,832
"or not? Like the video in 917 seems to imply." And that's a good question, and it turns out both approaches have been used,

106
00:10:09,833 --> 00:10:14,099
and they're called synchronous and asynchronous value iteration,

107
00:10:14,100 --> 00:10:22,232
and the initial theory is around the synchronous, and that's what the algorithm in the book is about.

108
00:10:22,233 --> 00:10:26,666
But you can also do it with asynchronous. It took a little bit longer to work out all the proofs,

109
00:10:26,667 --> 00:10:32,932
but asynchronous converges as well. So you have a choice.

110
00:10:32,933 --> 00:10:41,532
Here's one from Hamburg. "A utility function, U, resembles a heuristic function in being an estimate for the ways ahead,

111
00:10:41,533 --> 00:10:47,866
"whereas the notions of utility and costs are complements. Shouldn't utility be designed to never underestimate,

112
00:10:47,867 --> 00:10:52,366
"just as an admissible heuristic never overestimates?"

113
00:10:52,367 --> 00:11:00,099
Well, I would say that a utility function is not like a heuristic, in that a heuristic is something that you invent

114
00:11:00,100 --> 00:11:06,932
to help you solve the problem, where the utility is really given. You could think of it as part of the problem

115
00:11:06,933 --> 00:11:16,266
or you could think of it as really being part of the agent itself. It's built into what the agent wants, and when he wants it.

116
00:11:16,267 --> 00:11:21,366
So the "when he wants it" part or "when it wants it part" has to do with the discount factor.

117
00:11:21,367 --> 00:11:27,366
How much do you weigh off rewards now versus rewards later?

118
00:11:27,367 --> 00:11:30,732
But that's sort of an intrinsic part. It's not something that you can play around with.

119
00:11:30,733 --> 00:11:38,932
It's not an estimate. It's more of a definition of what the problem is.

120
00:11:38,933 --> 00:11:46,666
Here's a question. "I'm now inspired to learn more about POMDPs. Do you have any suggestions for books, articles, or exercises?"

121
00:11:46,667 --> 00:11:56,732
And that's from Zim in Chicago. I guess I started, and a lot of people really did, with an article in 1998 by Leslie Kaelbling,

122
00:11:56,733 --> 00:12:02,999
Mike Littman, and Tony Cassandra, called "Planning and Acting in Partially Observable Stochastic Domains."

123
00:12:03,000 --> 00:12:07,766
That was from the A.I. Journal. So that's a great place to start.

124
00:12:07,767 --> 00:12:20,000
Then you can look in the AIMA textbook and you can search online for POMDPs and there's a lot of information available.
