1
00:00:00,100 --> 00:00:05,814
Okay, so before we proceed I want to fill in these numbers again and rerun this

2
00:00:05,814 --> 00:00:10,585
program to get values that correspond to what you'll see on the Udacity IDE.

3
00:00:10,585 --> 00:00:13,577
Until now, as I've mentioned, I've been using my laptop but I want to--

4
00:00:13,577 --> 00:00:18,936
we're going to let you play around with this transpose code, and I want you to see on

5
00:00:18,936 --> 00:00:21,193
the screen numbers that are a little bit more comparable to what you'll get on

6
00:00:21,193 --> 00:00:25,732
the much bigger GPUs that Udacity is using versus the one that's in my laptop.

7
00:00:25,732 --> 00:00:30,949
Okay, so here's the code we've looked at before. This time it's running in the Udacity IDE.

8
00:00:30,949 --> 00:00:34,463
And what I want to do now is do a test run.

9
00:00:34,463 --> 00:00:36,629
And here's the results we get.

10
00:00:36,629 --> 00:00:40,410
So you can see that now transpose serial took 82.6 milliseconds,

11
00:00:40,410 --> 00:00:49,354
all the way down to transpose parallel per element tiled. which took about 0.13 milliseconds.

12
00:00:49,354 --> 00:00:54,104
So here I've written those results down again, and now we can go through and fill in the DRAM utilization.

13
00:00:54,750 --> 00:00:57,636
So you can see that we've got the same pattern.

14
00:00:57,636 --> 00:01:02,512
The serial version of the code, which runs in a single thread, takes 82.7 milliseconds.

15
00:01:02,512 --> 00:01:08,382
As we get more parallel, we start moving into single milliseconds, and then sub-millisecond.

16
00:01:08,398 --> 00:01:12,684
And by the time we get here, we're running quite well at 0.35 milliseconds.

17
00:01:12,684 --> 00:01:16,151
And sure enough, by tiling that code we

18
00:01:16,151 --> 00:01:20,215
improve on this a little bit, but now we're still spending a lot of time waiting at barriers.

19
00:01:20,215 --> 00:01:26,287
And by going to a smaller tile size we can get that all the way down to 0.13 milliseconds.

20
00:01:26,287 --> 00:01:29,578
Now at this point we're geting about5 43.5%

21
00:01:29,578 --> 00:01:34,761
of the theoretical peak limit of our bandwidth, and that's actually doing pretty good.

22
00:01:34,761 --> 00:01:38,944
There's one last optimization effect that we're not going to talk about at this lecture,

23
00:01:38,944 --> 00:01:42,645
but you're welcome, in fact encouraged, to go look it

24
00:01:42,645 --> 00:01:46,997
up in the CUDA C Best Practices Guide, or on many presentations that you can

25
00:01:46,997 --> 00:01:49,557
find online if you're motivated and curious.

26
00:01:49,557 --> 00:01:56,048
I decided that it's a fairly subtle effect and not too important. And that's shared memory bank conflicts.

27
00:01:56,048 --> 00:02:01,468
Essentially what this says is that shared memory is organized into banks, and

28
00:02:01,468 --> 00:02:07,699
depending on how you line up your thread accesses that the array of threads is

29
00:02:07,699 --> 00:02:10,061
making to the memory that's actually stored on the tile,

30
00:02:10,061 --> 00:02:16,155
you can end up spending a lot of time sort of replaying over and over.

31
00:02:16,155 --> 00:02:19,793
You saw that shared memory replays statistic much earlier

32
00:02:19,793 --> 00:02:23,739
when we were looking at the NVVP output, and that's what this is about.

33
00:02:23,739 --> 00:02:28,668
The fix is relatively simple, and as I said, we're not going to go into it here,

34
00:02:28,668 --> 00:02:31,942
but there's a version of this code that you can download and play with on Wiki.

35
00:02:31,942 --> 00:02:37,414
The fix is actually simply to reserve a slightly larger chunk of shared memory than you're actually

36
00:02:37,414 --> 00:02:41,504
going to use, and this has the effect of padding the shared memory and striping it across the

37
00:02:41,504 --> 00:02:45,504
banks, which can actually improve our run time even slightly more.
