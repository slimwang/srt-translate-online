1
00:00:00,720 --> 00:00:01,720
All right.
Welcome back.

2
00:00:01,720 --> 00:00:03,820
I hope you took a nice break there.

3
00:00:03,820 --> 00:00:07,238
As you can see, I'm, well, I'm totally,
actually, it's, it's another decade and

4
00:00:07,238 --> 00:00:08,240
I just kind of look the same.

5
00:00:09,680 --> 00:00:12,360
So what we've done is we've laid
out sort of the general idea

6
00:00:12,360 --> 00:00:16,730
of a Markov process, a Markov system,
and also a hidden Markov model.

7
00:00:16,730 --> 00:00:21,340
And what I want to do now is talk about
sort of three computational problems

8
00:00:21,340 --> 00:00:22,730
of hidden Markov models.

9
00:00:22,730 --> 00:00:27,160
And then we'll talk just about a simple
application of this to, computer vision.

10
00:00:28,620 --> 00:00:32,360
Also, just as a word of note,
I'm not going to go into the detail of

11
00:00:32,360 --> 00:00:35,480
different parts of, of lots of the math,
just one part of it and

12
00:00:35,480 --> 00:00:39,140
then the rest I'll, as they say,
I will leave it to the reader or

13
00:00:39,140 --> 00:00:43,050
the viewer, as an exercise to go get,
schooled up in it.

14
00:00:43,050 --> 00:00:47,550
Because, the details were less important
than knowing that it could be done and

15
00:00:47,550 --> 00:00:49,870
then it was applied to computer vision.

16
00:00:49,870 --> 00:00:54,080
All right so, we have a hidden Markov
model and we say that our hidden,

17
00:00:54,080 --> 00:00:57,830
hidden Markov model is a lambda,
is a triplet of A, B, and pi

18
00:00:57,830 --> 00:01:01,890
where A is the transition probability,
B your emission probabilities, and

19
00:01:01,890 --> 00:01:05,000
pi was just the distribution
where things started.

20
00:01:05,000 --> 00:01:08,200
Well given that, a HMM, given a machine,

21
00:01:08,200 --> 00:01:11,900
there are three computational
problems we can look at.

22
00:01:11,900 --> 00:01:13,450
And the first one is this.

23
00:01:13,450 --> 00:01:17,020
Basically, given some
observation sequence, so

24
00:01:17,020 --> 00:01:22,840
remember o1 through o cap T, are the
observed outputs, from 1 to cap T.

25
00:01:22,840 --> 00:01:25,020
So that's a particular sequence.

26
00:01:25,020 --> 00:01:30,560
You could ask what is the probability,
P of O, given that machine?

27
00:01:30,560 --> 00:01:34,550
That is, what's the likelihood that
that machine generating that particular

28
00:01:34,550 --> 00:01:35,740
sequence?

29
00:01:35,740 --> 00:01:40,140
And this is really the classification,
recognition, it says problem but

30
00:01:40,140 --> 00:01:42,020
actually how you use it for that, right?

31
00:01:42,020 --> 00:01:44,780
Assume I've got a couple different
h m m's, maybe three of them, and

32
00:01:44,780 --> 00:01:46,280
they each represent an activity.

33
00:01:46,280 --> 00:01:50,230
And let's assume that the prior of
which activity is going on are equal.

34
00:01:50,230 --> 00:01:50,980
All right?

35
00:01:50,980 --> 00:01:53,300
So then, once I've,
if I have the priors are equal,

36
00:01:53,300 --> 00:01:57,060
all I care about is the likelihood
of the data given the model.

37
00:01:57,060 --> 00:01:59,960
And the HMM would allow me to evaluate

38
00:01:59,960 --> 00:02:03,470
the likelihood of the observed
sequence for each of the given models.

39
00:02:03,470 --> 00:02:05,930
Whichever one was highest,
that would be the one I picked.

40
00:02:05,930 --> 00:02:08,788
That's a generative model
like we talked about before.

41
00:02:08,788 --> 00:02:12,950
So the evaluation problem is,
substance the key problem at run time.

42
00:02:12,950 --> 00:02:14,180
It's one we'll actually talk about.

43
00:02:15,686 --> 00:02:19,030
Another problem is what's
called decoding the sequence.

44
00:02:19,030 --> 00:02:23,380
So, remember you have
a sequence of observations, and

45
00:02:23,380 --> 00:02:27,100
the question is,
what is the single most likely

46
00:02:27,100 --> 00:02:32,100
sequence of states to have generated
that sequence of observations?

47
00:02:32,100 --> 00:02:36,370
So, it's not all possible ways these
observations could generate, it's just

48
00:02:36,370 --> 00:02:40,880
the single most likely sequence,
sometimes called the decoding problem.

49
00:02:40,880 --> 00:02:44,080
And sometimes you care about that,
sometimes you don't.

50
00:02:44,080 --> 00:02:46,190
When you do it basically
has to do with saying,

51
00:02:46,190 --> 00:02:49,740
well if I, if the states have
some particular meaning to me.

52
00:02:49,740 --> 00:02:51,310
Right?
Remember we talk about sunny, rainy,

53
00:02:51,310 --> 00:02:53,900
cloudy, so you know they might
have some specific meaning.

54
00:02:53,900 --> 00:02:59,160
I might want to say, okay here's what I
think the underlying state sequence, is,

55
00:02:59,160 --> 00:03:02,860
actually sometimes in,
genome work, etcetera.

56
00:03:02,860 --> 00:03:06,677
They actually want to know what
the underlying state sequence would be.

57
00:03:06,677 --> 00:03:09,290
But if you are actually just
doing it for recognition,

58
00:03:09,290 --> 00:03:12,180
then that's,
in some sense less important.

59
00:03:12,180 --> 00:03:16,680
And then the third problem, is in some
sense the most complicated problem.

60
00:03:16,680 --> 00:03:20,220
And that is, okay,
I give you a bunch of training examples,

61
00:03:20,220 --> 00:03:22,330
a bunch of these O sequences.

62
00:03:22,330 --> 00:03:23,430
Train model.

63
00:03:23,430 --> 00:03:27,920
Learn the Lambda that and the way you,
that you learn is you pick the one

64
00:03:27,920 --> 00:03:31,200
that maximizes the likelihood
of getting that observation.

65
00:03:31,200 --> 00:03:33,250
It's a maximum likelihood, method.

66
00:03:33,250 --> 00:03:35,810
It's actually called the EM,
Expectation Maximization,

67
00:03:35,810 --> 00:03:38,130
and we'll talk about
that just a little bit.

68
00:03:38,130 --> 00:03:40,710
We won't actually,
do the math of the, of that part.
