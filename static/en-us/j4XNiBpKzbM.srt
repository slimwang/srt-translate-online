1
00:00:00,000 --> 00:00:04,780
So CUDA DMA is a template library designed to make it easier to use shared memory

2
00:00:04,780 --> 00:00:06,476
while achieving high performance.

3
00:00:06,476 --> 00:00:12,783
Now to use CUDA DMA, programmers declare CUDA DMA objects for each shared memory buffer that needs to be loaded or stored,

4
00:00:12,783 --> 00:00:17,887
and the cool thing is that CUDA DMA let's you explicitly describe the transfer pattern for that data.

5
00:00:17,887 --> 00:00:23,365
So, for example, you might be transferring one long sequential trunk of memory,

6
00:00:23,365 --> 00:00:26,730
you might be transferring strided trunks of memory,

7
00:00:26,730 --> 00:00:29,588
or you might be doing sort of indirect access to memory,

8
00:00:29,588 --> 00:00:32,073
such as you would find in a sparse matrix representation.

9
00:00:32,073 --> 00:00:37,600
As with CUB, decoupling the transfer patterns from the actual processing that we're going to do on each

10
00:00:37,600 --> 00:00:41,472
thread to achieve that transfer pattern has several benefits and improves

11
00:00:41,472 --> 00:00:45,039
programmability because the code is now simpler.

12
00:00:45,039 --> 00:00:52,822
You packaged away all of that logic for doing the transfer separately from the actual compute in your kernel.

13
00:00:52,822 --> 00:00:57,885
It improves portability because you can have the CUDA DMA ninjas or the CUB ninjas

14
00:00:57,885 --> 00:01:03,394
develop the very best implementations of these various transfer patterns

15
00:01:03,394 --> 00:01:07,336
for your situation and package that all up in a library for you,

16
00:01:07,336 --> 00:01:10,765
and because those CUDA ninjas are good at what they do, you get high performance.

17
00:01:10,765 --> 00:01:13,370
You're going to achieve high DRAM memory bandwidth,

18
00:01:13,370 --> 00:01:17,772
you're going to hide the global memory latency for kernels that don't have a lot of occupancy,

19
00:01:17,772 --> 00:01:21,944
and hopefully this will lead to better compiler-generated code.

20
00:01:21,944 --> 00:01:28,619
And as I said, these benefits really accrue to both CUB, which tackles the whole circle from bringing

21
00:01:28,619 --> 00:01:34,316
data in from global memory and doing the computation on it, as well as CUDA DMA,

22
00:01:34,316 --> 00:01:40,567
which is just tackling the top part of that cycle where you're bringing memory into shared memory,

23
00:01:40,567 --> 00:01:43,000
and then you're going to do your own operations on it,

24
00:01:43,000 --> 00:01:46,203
so these benefits really accrue to both approaches.
