1
00:00:00,200 --> 00:00:03,790
Now, we are going to look at the mean square error and

2
00:00:03,790 --> 00:00:06,280
the bias variants trade off.

3
00:00:06,280 --> 00:00:10,960
Remember, we had the risk function given as this expression.

4
00:00:10,960 --> 00:00:17,030
The integral expression here denotes the expectation value of the loss function.

5
00:00:17,030 --> 00:00:24,920
Now, let's define y as the true value and f hat of x knot as the estimated

6
00:00:24,920 --> 00:00:31,020
value evaluated at x knot for this value of y which is the true value.

7
00:00:31,020 --> 00:00:37,570
We can now calculate the expectation value of the square error loss.

8
00:00:37,570 --> 00:00:40,740
The expectation value of the square error loss.

9
00:00:40,740 --> 00:00:45,870
Is given as the expectation value of this expression in here, which is the true

10
00:00:45,870 --> 00:00:53,840
value minus the estimated value at x not squared, evaluated at x not.

11
00:00:53,840 --> 00:00:56,630
We can now work out this expression.

12
00:00:56,630 --> 00:00:59,380
And you will get the following expression.

13
00:00:59,380 --> 00:01:03,220
This term here is the irreducible error.

14
00:01:03,220 --> 00:01:07,120
This term here is the square of the bias.

15
00:01:07,120 --> 00:01:10,620
And this term here is the variance.

16
00:01:10,620 --> 00:01:15,010
Now let us inspect each of these terms separately.

17
00:01:15,010 --> 00:01:19,080
Remember the irreducible error is this first term.

18
00:01:19,080 --> 00:01:24,990
It comes from the variance around the true mean inherent in the data.

19
00:01:24,990 --> 00:01:30,120
It is very difficult to get rid off the irreducible error, hence the name,

20
00:01:30,120 --> 00:01:32,920
through the modeling process.

21
00:01:32,920 --> 00:01:39,080
We have also seen how bias and variance, plays into how one can select a model.

22
00:01:39,080 --> 00:01:44,220
For non parametric models, like histograms and kernel density estimators.

23
00:01:44,220 --> 00:01:48,040
In lesson three, we discussed the role the bin size or

24
00:01:48,040 --> 00:01:53,250
bandwidth plays in determining the variance and bias of the model.

25
00:01:53,250 --> 00:01:57,800
Our goal is to minimize the square error loss.

26
00:01:57,800 --> 00:02:03,750
This, in turn, means that we are minimizing the bias term of the variance term.

27
00:02:03,750 --> 00:02:05,000
The problem is,

28
00:02:05,000 --> 00:02:09,810
minimizing both is not always possible due to the complexity of the model.

29
00:02:09,810 --> 00:02:15,200
We will look at this problem, thus we have a situation where we have

30
00:02:15,200 --> 00:02:21,190
to have a trade-off, between minimizing the bias square, or the variance turn.

31
00:02:21,190 --> 00:02:26,110
This is what is refered to as the bias variance trade-off.

32
00:02:26,110 --> 00:02:27,900
The goal in statistical and

33
00:02:27,900 --> 00:02:34,050
machine learning models is to minimize the loss function, and in turn, the risk.

34
00:02:34,050 --> 00:02:38,580
You may recall an introduction to machine learning class,

35
00:02:38,580 --> 00:02:45,230
how the cost function was defined and minimized to converge to a model.

36
00:02:45,230 --> 00:02:51,470
Notice it is simply the use of the least square loss function.

37
00:02:51,470 --> 00:02:56,710
Although we often choose the square loss in statistical learning,

38
00:02:56,710 --> 00:03:00,890
you must understand that you are in complete control of the way you

39
00:03:00,890 --> 00:03:04,750
choose your model through the choice of the loss function.

40
00:03:04,750 --> 00:03:07,860
Our decision is also based on how well we

41
00:03:07,860 --> 00:03:13,090
want the model to fit the data measured by the variance, versus how

42
00:03:13,090 --> 00:03:19,440
well we want the model to generalize to new data, which is measured by the bias.

43
00:03:19,440 --> 00:03:23,430
Remember, there's always the bias variance straight off.

44
00:03:23,430 --> 00:03:26,570
The cost function can be calculated.

45
00:03:26,570 --> 00:03:33,370
On the training data set, the validation data set, or the test data set.

46
00:03:33,370 --> 00:03:39,060
Minimizing each of these cost functions serve to optimize the model for

47
00:03:39,060 --> 00:03:40,510
different purposes.

48
00:03:40,510 --> 00:03:43,410
Let's take a look at the cost function calculated on

49
00:03:43,410 --> 00:03:47,140
the training set versus this test set.

50
00:03:47,140 --> 00:03:50,950
The following example is taken from a book named The Elements of

51
00:03:50,950 --> 00:03:52,410
Statistical Learning.

52
00:03:52,410 --> 00:03:56,440
Please see the instructor's notes for more details on the book.

53
00:03:56,440 --> 00:03:58,880
Let's take a look at the following diagram.

54
00:03:58,880 --> 00:04:03,780
What we have on the X-axis here, is model complexity.

55
00:04:03,780 --> 00:04:07,120
Think of it as the number of variables in your model.

56
00:04:07,120 --> 00:04:11,370
On the Y axis we have the prediction error.

57
00:04:11,370 --> 00:04:18,750
The red lines here, represent the error, calculated on the test data set.

58
00:04:18,750 --> 00:04:20,420
The blue lines here,

59
00:04:20,420 --> 00:04:25,950
represent the prediction errors calculated on the training data set.

60
00:04:25,950 --> 00:04:29,100
Notice how the error on the training data set

61
00:04:29,100 --> 00:04:33,430
reduces as the complexity of the model goes up.

62
00:04:33,430 --> 00:04:35,960
However, this is not the case for

63
00:04:35,960 --> 00:04:39,390
the prediction error calculated on the test data set.

64
00:04:39,390 --> 00:04:43,720
The solid curves here represent the expected values of all

65
00:04:43,720 --> 00:04:45,870
the prediction error curves.

66
00:04:45,870 --> 00:04:50,070
The model complexity can be thought of as the number of

67
00:04:50,070 --> 00:04:52,290
features in the model being used.

68
00:04:52,290 --> 00:04:55,210
Intuitively you may already have an idea.

69
00:04:55,210 --> 00:04:59,880
Of chow the number of feature can affect the way your generalization or

70
00:04:59,880 --> 00:05:02,320
prediction error can be affected.

71
00:05:02,320 --> 00:05:08,560
A less complex model with a low number of features will have low variance and

72
00:05:08,560 --> 00:05:13,230
high bias and we might end up under fitting the model.

73
00:05:13,230 --> 00:05:19,120
In contrast, a more complex model will have high number of features,

74
00:05:19,120 --> 00:05:22,610
resulting in high variance, a low bias.

75
00:05:22,610 --> 00:05:24,860
And thus, will be overfit.

76
00:05:24,860 --> 00:05:30,160
Remember, always, the tradeoff between the bias and the variance.

77
00:05:30,160 --> 00:05:34,760
In the next video, we will try to quantify the complexity of a model.

78
00:05:34,760 --> 00:05:36,470
And look at bounds on the training and

79
00:05:36,470 --> 00:05:39,710
testers that we can calculate on the samples.
