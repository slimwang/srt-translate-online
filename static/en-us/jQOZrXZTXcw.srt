1
00:00:00,450 --> 00:00:04,990
Let's apply this to face recognition and
it's referred to as using Eigenfaces and

2
00:00:04,990 --> 00:00:08,745
it comes from this paper by Matt Turk
and Alex Pentland, Sandy Pentland.

3
00:00:08,745 --> 00:00:12,000
It's Alex Pentland, but
his name is, he goes by Sandy.

4
00:00:12,000 --> 00:00:13,830
They were at the MIT
media lab at the time.

5
00:00:15,770 --> 00:00:18,870
And one of the reasons this paper,
besides the fact that it works so

6
00:00:18,870 --> 00:00:20,880
well, is just known so

7
00:00:20,880 --> 00:00:25,300
well, in my humble opinion, is because
of this term, Eigenfaces and face space.

8
00:00:25,300 --> 00:00:26,575
What a great, what a great phrase.

9
00:00:26,575 --> 00:00:29,049
In fact, I can show you the Eigenfaces,
I will in just a minute.

10
00:00:29,049 --> 00:00:30,640
All right.

11
00:00:30,640 --> 00:00:32,280
So, here's what they do.

12
00:00:32,280 --> 00:00:36,925
They assume that most face
images are going to lie on some

13
00:00:36,925 --> 00:00:41,730
low-dimensional subspace
in the big image space.

14
00:00:41,730 --> 00:00:46,220
Determine by, I don't know,
let's say, k eigenvectors.

15
00:00:46,220 --> 00:00:48,910
Okay?
K directions of maximum variance where k

16
00:00:48,910 --> 00:00:51,280
is going to be way smaller than d.

17
00:00:51,280 --> 00:00:53,190
So, d might be 10,000 or a million.

18
00:00:53,190 --> 00:00:55,217
K is going to be like 20 or 200.

19
00:00:55,217 --> 00:00:58,008
Both of those numbers are way
smaller than 10,000 or a million.

20
00:00:58,008 --> 00:01:01,591
So, what it does is they use PCA,
like I just showed you,

21
00:01:01,591 --> 00:01:05,190
to find the vectors, or
what are called the Eigenfaces.

22
00:01:05,190 --> 00:01:09,740
We'll look at them in a minute,
u1 through uk, that span, that subspace.

23
00:01:09,740 --> 00:01:10,830
Okay?
So, you take all your images,

24
00:01:10,830 --> 00:01:12,730
you find your eigenvectors.

25
00:01:12,730 --> 00:01:15,584
And now, what you're going to do,
and this is the really cool part,

26
00:01:15,584 --> 00:01:20,120
is you're going to represent your
face images in that data set

27
00:01:20,120 --> 00:01:25,150
as just the linear combination
of those eigenvectors.

28
00:01:25,150 --> 00:01:26,690
Or another way of saying it is,

29
00:01:26,690 --> 00:01:30,670
I'm just going to have the coefficients
of the 20, of the 20 eigenvectors.

30
00:01:30,670 --> 00:01:32,750
If it's 200, 200 eigenvectors.

31
00:01:32,750 --> 00:01:37,090
And I'm going to represent my
entire image, all 10,000 numbers,

32
00:01:37,090 --> 00:01:40,030
by just the coefficients
of these eigenvectors.

33
00:01:40,030 --> 00:01:43,473
And I multiply those coefficients
times the eigenvector, sum them up,

34
00:01:43,473 --> 00:01:44,778
that would be my new image.

35
00:01:44,778 --> 00:01:45,920
All right.

36
00:01:45,920 --> 00:01:48,120
So, it's a tremendous data reduction.

37
00:01:48,120 --> 00:01:49,980
So, take you through an example here.

38
00:01:49,980 --> 00:01:53,050
So, and some of these images come
directly from the old papers,

39
00:01:53,050 --> 00:01:55,350
some from some newer work,
but makes the same point.

40
00:01:55,350 --> 00:01:58,180
So, suppose I have some
training images X1 through XM.

41
00:01:58,180 --> 00:01:59,370
So, here's a picture of faces.

42
00:01:59,370 --> 00:02:02,260
Now, notice,
it's just the cropped part of the face.

43
00:02:02,260 --> 00:02:04,540
So, they're going to try to do
recognition just on the cropped

44
00:02:04,540 --> 00:02:05,150
part of the face.

45
00:02:05,150 --> 00:02:11,048
So, the first thing that's kind of cool
to look at is this is the mean image,

46
00:02:11,048 --> 00:02:12,620
mean face.

47
00:02:12,620 --> 00:02:15,820
Not the mean face,
the mean face, the average face.

48
00:02:15,820 --> 00:02:17,960
Now, nobody wants to
have the average face.

49
00:02:17,960 --> 00:02:18,810
Too bad.

50
00:02:18,810 --> 00:02:21,470
Whoever that is, he or
she has the average face.

51
00:02:21,470 --> 00:02:23,500
Why did I say he or she?

52
00:02:23,500 --> 00:02:26,560
Well, by definition, it's hard to tell.

53
00:02:26,560 --> 00:02:28,750
Right?
because it's an average over everybody.

54
00:02:28,750 --> 00:02:32,390
There is some interesting work
done on showing how average faces

55
00:02:32,390 --> 00:02:35,440
appear more beautiful than real faces.

56
00:02:35,440 --> 00:02:36,550
This was done a long time ago.

57
00:02:36,550 --> 00:02:38,450
It's just a study of
the appearance of beauty, anyway.

58
00:02:38,450 --> 00:02:41,730
It was interesting image processing
approach to thinking about,

59
00:02:41,730 --> 00:02:43,970
could you predict when a face
would be deemed beautiful.

60
00:02:43,970 --> 00:02:44,470
All right.

61
00:02:44,470 --> 00:02:45,127
You can imagine here's a question.

62
00:02:45,127 --> 00:02:48,694
Show a picture to a computer,
face image, part of the digression, and

63
00:02:48,694 --> 00:02:52,795
say on the average, on a scale of one to
ten, how likely are people to rate this

64
00:02:52,795 --> 00:02:55,363
thing as beautiful,
as this person is beautiful?

65
00:02:55,363 --> 00:02:56,727
And could you do that?

66
00:02:56,727 --> 00:03:00,537
I think these days using machine
learning might be a different approach.

67
00:03:00,537 --> 00:03:04,500
But at the time, there was work
done looking at average faces.

68
00:03:04,500 --> 00:03:05,830
Sure you wanted to know that.

69
00:03:05,830 --> 00:03:08,500
Anyway, oh, and so
what do you do with the mean face?

70
00:03:08,500 --> 00:03:10,690
Like what you should do with
a mean face, you throw it away.

71
00:03:10,690 --> 00:03:13,690
Well, what you really do is
subtract it out of your population.

72
00:03:13,690 --> 00:03:14,440
Isn't that great?

73
00:03:14,440 --> 00:03:16,750
We subtract all the [LAUGH] mean
faces out of the population.

74
00:03:16,750 --> 00:03:19,220
No, we subtract the mean
face from all the, and

75
00:03:19,220 --> 00:03:21,640
what we have left is our distribution.

76
00:03:21,640 --> 00:03:24,540
And then,
we start computing the eigenvectors.

77
00:03:24,540 --> 00:03:25,564
And here they are.
Okay?

78
00:03:25,564 --> 00:03:29,409
So, here's some top eigenvectors,
and one of the things you'll notice

79
00:03:29,409 --> 00:03:33,620
right away, fact, let's take
a look at this first eigenvector.

80
00:03:33,620 --> 00:03:35,150
Actually, the, the second one.

81
00:03:35,150 --> 00:03:37,300
Is that it's much brighter
on one side than the other,

82
00:03:37,300 --> 00:03:40,550
because some of the variation might just
be due to the lighting coming from one

83
00:03:40,550 --> 00:03:41,728
side or the other.

84
00:03:41,728 --> 00:03:44,660
But later, you'll have these
different eigenvectors,

85
00:03:44,660 --> 00:03:47,330
which look like these
kind of ghostly images.

86
00:03:47,330 --> 00:03:49,760
But these are eigenvectors.

87
00:03:49,760 --> 00:03:54,485
So, they look to you like pictures and
they are, but what they are is

88
00:03:54,485 --> 00:04:00,950
they're a 10,000 dimensional vector,
which is just an image.

89
00:04:00,950 --> 00:04:05,810
Remember, I can go between the image
space 10,000, right, and the images, so

90
00:04:05,810 --> 00:04:09,890
I'm just showing you these
eigenvectors as images.

91
00:04:09,890 --> 00:04:12,190
Okay?
And if I want to take a dot product

92
00:04:12,190 --> 00:04:14,580
of a real picture and these?

93
00:04:14,580 --> 00:04:15,140
That's easy.

94
00:04:15,140 --> 00:04:18,589
All I do is overlay them and multiply.

95
00:04:18,589 --> 00:04:20,130
Multiply every pixel and sum them up.

96
00:04:20,130 --> 00:04:21,940
because that's what a dot product is.

97
00:04:21,940 --> 00:04:23,439
So, when I show this to students,

98
00:04:23,439 --> 00:04:26,665
sometimes they don't understand how
come the images are eigenvectors.

99
00:04:26,665 --> 00:04:30,544
And that's because in image space,
that's what it means,

100
00:04:30,544 --> 00:04:31,920
a vector is an image.
