1
00:00:00,290 --> 00:00:04,500
So DEC-POMDP, I've written out
the expansion of what it stands for.

2
00:00:04,500 --> 00:00:07,170
Decentralized partially
observable Markov decision, and

3
00:00:07,170 --> 00:00:08,380
then I kind of ran out of space.

4
00:00:08,380 --> 00:00:09,340
>> Nah, the P doesn't matter.

5
00:00:09,340 --> 00:00:12,350
>> But you know that it's a POMDP,
decentralized POMDP, and so

6
00:00:12,350 --> 00:00:16,850
the quantities that define the DEC-POMDP
are very similar to the quantities that

7
00:00:16,850 --> 00:00:21,300
define a POMDP,
we've got a set of states, transitions,

8
00:00:21,300 --> 00:00:24,670
rewards, observations,
observation function.

9
00:00:24,670 --> 00:00:28,210
The major difference is that
actions are actually taken

10
00:00:28,210 --> 00:00:31,320
simultaneously by
a finite set of agents.

11
00:00:31,320 --> 00:00:34,890
There isn't just one driver
there is actually a whole,

12
00:00:34,890 --> 00:00:38,690
you know, too many emcees,
really, not enough mics.

13
00:00:38,690 --> 00:00:40,170
>> Well done.
>> There's a set of agents who

14
00:00:40,170 --> 00:00:43,500
actually get to have some say
over how the transitions works.

15
00:00:43,500 --> 00:00:46,300
So we're going to say capital I
is that finite set of agents,

16
00:00:46,300 --> 00:00:51,250
actions available to an agent
is A sub little i for agent I.

17
00:00:51,250 --> 00:00:54,740
And the transition
function is a function of

18
00:00:54,740 --> 00:00:58,200
the joint actions taken by all
the agents simultaneously.

19
00:00:58,200 --> 00:01:03,380
So instead of just going from state
to a next state through an action,

20
00:01:03,380 --> 00:01:08,940
this is actually, you could think of
it as a vector with one action for

21
00:01:08,940 --> 00:01:10,170
each of the agents in our set.

22
00:01:10,170 --> 00:01:10,770
>> All right.

23
00:01:10,770 --> 00:01:11,400
That makes sense.

24
00:01:11,400 --> 00:01:14,520
>> So that really is very
similar kind of quantity but

25
00:01:14,520 --> 00:01:16,550
we break up where
the actions are happening.

26
00:01:16,550 --> 00:01:18,520
They have to happen jointly.

27
00:01:18,520 --> 00:01:22,420
And the agents themselves, their
observations about the environment,

28
00:01:22,420 --> 00:01:25,370
are also distributed, in a sense.

29
00:01:25,370 --> 00:01:30,460
Agent i gets to gets to see something
that's a function of the agent and

30
00:01:30,460 --> 00:01:31,470
the state of the world.

31
00:01:31,470 --> 00:01:35,030
All right, so, this is a model.

32
00:01:35,030 --> 00:01:37,470
In this model,
all the agents are acting cooperatively,

33
00:01:37,470 --> 00:01:40,300
because there's one shared reward
function that everybody gets.

34
00:01:40,300 --> 00:01:42,900
If we actually split the reward
function so there's a different one for

35
00:01:42,900 --> 00:01:44,310
each of the agents, we get yet

36
00:01:44,310 --> 00:01:47,040
another model they're not
going to talk about caught up.

37
00:01:47,040 --> 00:01:50,150
Partially observable stochastic game or
POSG.

38
00:01:51,570 --> 00:01:52,420
>> POSG.

39
00:01:52,420 --> 00:01:53,120
>> POSG.

40
00:01:53,120 --> 00:01:54,500
>> I'm positive that
would be interesting.

41
00:01:54,500 --> 00:01:57,400
>> Yeah, yeah so we could POSG
this discussion at this point and

42
00:01:57,400 --> 00:01:58,305
have a side discussion.

43
00:01:58,305 --> 00:02:01,200
>> [LAUGH] More puns.

44
00:02:01,200 --> 00:02:03,585
The P stands for puns.

45
00:02:03,585 --> 00:02:05,500
>> [LAUGH]
>> So, this makes sense and

46
00:02:05,500 --> 00:02:09,759
sounds great as a formalism goes,
but I'm kind of struck.

47
00:02:09,759 --> 00:02:13,080
When you brought up the stochastic games
thing, I was thinking the whole time,

48
00:02:13,080 --> 00:02:15,720
this sounds like game theory stuff.

49
00:02:15,720 --> 00:02:16,450
>> Yeah, yeah.

50
00:02:16,450 --> 00:02:19,160
Right, it's sort of clearly game theory
when we have separate reward functions,

51
00:02:19,160 --> 00:02:22,970
because every agent has its
own personal interests.

52
00:02:22,970 --> 00:02:27,670
Right, so, the DEC-POMDP setup is
a kind of common interest game,

53
00:02:27,670 --> 00:02:32,010
might be a word for it, the notion that
there's agents acting independently, but

54
00:02:32,010 --> 00:02:34,480
they're all acting for the same reward.

55
00:02:34,480 --> 00:02:36,030
>> So,
we could model this as game theory, or

56
00:02:36,030 --> 00:02:37,740
is this some different
way of thinking about it?

57
00:02:37,740 --> 00:02:41,190
>> It's part way between game theory and
POMDPs, yeah.

58
00:02:41,190 --> 00:02:43,890
It has elements of both, and
in fact, I guess it subsumes both.

59
00:02:43,890 --> 00:02:47,780
So, you can represent any POMDP as
a DEC-POMDP with just one agent.

60
00:02:47,780 --> 00:02:52,500
And you can represent any common
interest game as a DEC-POMDP with

61
00:02:52,500 --> 00:02:56,120
complete observability, or
maybe no states, that sort of thing.
