1
00:00:00,000 --> 00:00:04,220
And if I can only run 1 block on that SM,

2
00:00:04,220 --> 00:00:08,170
then I'm getting a maximum of 1024 threads on that SM.

3
00:00:08,170 --> 00:00:12,992
So my occupancy is not as high as the SM could handle.

4
00:00:12,992 --> 00:00:17,098
You can get these vital statistics for the particular GPU you're programming to

5
00:00:17,098 --> 00:00:22,192
by looking it up in a CUDA reference manual or by making various CUDA calls on the GPU.

6
00:00:22,192 --> 00:00:26,611
And the CUDA SDK ships with a sample called deviceQuery—we saw it earlier—

7
00:00:26,611 --> 00:00:30,933
that conveniently collects all of this information into a single useful utility.

8
00:00:30,933 --> 00:00:37,250
Earlier we did a print out of the device query, the result of deviceQuery on my GPU.

9
00:00:37,250 --> 00:00:39,088
Let's look at that again.

10
00:00:39,088 --> 00:00:41,976
It prints out a lot of information.

11
00:00:41,976 --> 00:00:45,251
Earlier we were using this to estimate the total peak bandwidth

12
00:00:45,251 --> 00:00:48,223
by looking at the memory clock rate and the memory bus width.

13
00:00:48,223 --> 00:00:52,953
Now I want to draw your attention to the total amount of shared memory per block,

14
00:00:52,953 --> 00:00:55,842
the total number of registers per block,

15
00:00:55,842 --> 00:00:59,213
the maximum number of threads per multiprocessor or SM,

16
00:00:59,213 --> 00:01:01,702
the maximum number of threads per block.

17
00:01:01,702 --> 00:01:05,235
And here's the register and shared memory usage

18
00:01:05,235 --> 00:01:07,573
for our tiled transpose kernel.

19
00:01:07,573 --> 00:01:10,362
Going back into NVPP, we can see that we're launching

20
00:01:10,362 --> 00:01:14,763
a grid size of 32 x 32, a block size of 32 x 32,

21
00:01:14,763 --> 00:01:16,874
we're using 7 registers per thread,

22
00:01:16,874 --> 00:01:19,778
and we're requesting 4 kilobytes of shared memory per block.

23
00:01:19,778 --> 00:01:23,503
Given the register and shared memory usage for the that kernel we just saw

24
00:01:23,503 --> 00:01:28,655
and given the GPU statistics from deviceQuery running on my laptop that we just saw,

25
00:01:28,655 --> 00:01:31,800
how many thread blocks per SM can we run?

26
00:01:31,800 --> 00:01:34,546
And how many threads per SM can we run?

27
00:01:34,546 --> 00:01:37,797
And which resources are preventing us from running more?

28
00:01:37,797 --> 00:01:41,120
Are we limited by the maximum number of threads per SM

29
00:01:41,120 --> 00:01:43,902
or the maximum number of registers per SM,

30
00:01:43,902 --> 00:01:46,773
or the maximum amount of shared memory per SM,

31
00:01:46,773 --> 00:01:50,154
or by the maximum number of thread blocks per SM?
