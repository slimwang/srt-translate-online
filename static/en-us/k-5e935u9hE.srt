1
00:00:00,000 --> 00:00:02,000
Now to solve an MDP,

2
00:00:02,000 --> 00:00:06,000
we're trying to find a policy--pi of S--

3
00:00:06,000 --> 00:00:08,000
that's going to be our answer.

4
00:00:08,000 --> 00:00:10,000
The pi that we want--the optimal policy--

5
00:00:10,000 --> 00:00:13,000
is the one that's going to maximize

6
00:00:13,000 --> 00:00:15,000
the discounted, total Reward.

7
00:00:15,000 --> 00:00:17,000
So what we mean is:

8
00:00:17,000 --> 00:00:20,000
we want to take the sum over all Times

9
00:00:20,000 --> 00:00:23,000
into the future of the Reward

10
00:00:23,000 --> 00:00:25,000
that you get from starting out

11
00:00:25,000 --> 00:00:28,000
in the state that you're in, in time T--

12
00:00:28,000 --> 00:00:32,000
and then applying the policy to that state,

13
00:00:32,000 --> 00:00:35,000
and arriving at a new state, at time T plus 1.

14
00:00:35,000 --> 00:00:37,000
And so we want to maximize that sum--

15
00:00:37,000 --> 00:00:39,000
but the sum might be infinite

16
00:00:39,000 --> 00:00:41,000
and so, what we do is

17
00:00:41,000 --> 00:00:43,000
we take this value, Gamma,

18
00:00:43,000 --> 00:00:46,000
and raise it to the T power, saying

19
00:00:46,000 --> 00:00:49,000
we're going to count future Rewards less than

20
00:00:49,000 --> 00:00:52,000
current Rewards--and that way,

21
00:00:52,000 --> 00:00:55,000
we'll make sure that the sum total is bounded.

22
00:00:55,000 --> 00:00:58,000
So we want the policy that maximizes that result.

23
00:00:58,000 --> 00:01:00,000
If we figure out the utility of the state

24
00:01:00,000 --> 00:01:03,000
by solving the Markov Decision Process,

25
00:01:03,000 --> 00:01:07,000
then we have: the utility of any state, S,

26
00:01:07,000 --> 00:01:09,000
is equal to the maximum over all

27
00:01:09,000 --> 00:01:12,000
possible actions that we could take in S

28
00:01:12,000 --> 00:01:15,000
of the expected value of taking that action.

29
00:01:15,000 --> 00:01:17,000
And what's the expected value?

30
00:01:17,000 --> 00:01:21,000
Well, it's just the sum over all resulting states

31
00:01:21,000 --> 00:01:23,000
of the transition model--

32
00:01:23,000 --> 00:01:25,000
the probability that we get to that state,

33
00:01:25,000 --> 00:01:28,000
given from the start state, we take an action

34
00:01:28,000 --> 00:01:31,000
specified by the optimal policy

35
00:01:31,000 --> 00:01:34,000
times the utility of that resulting state.

36
00:01:34,000 --> 00:01:37,000
So--look at all possible actions;

37
00:01:37,000 --> 00:01:39,000
choose the best one--

38
00:01:39,000 --> 99:59:59,999
according to the expected, in terms of probability utility.
