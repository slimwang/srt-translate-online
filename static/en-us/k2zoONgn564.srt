1
00:00:00,170 --> 00:00:01,650
One other thing I want
to point out here.

2
00:00:01,650 --> 00:00:04,800
This is kind of cool and this is maybe
why potential functions are helpful and

3
00:00:04,800 --> 00:00:06,480
why it can help speed up learning.

4
00:00:06,480 --> 00:00:11,650
Eric Rewaref showed that the series
of updates that you get by running

5
00:00:11,650 --> 00:00:15,820
Q learning with some potential
function is actually exactly the same.

6
00:00:15,820 --> 00:00:19,200
As what you get by running Q learning
without a potential function, but

7
00:00:19,200 --> 00:00:22,940
with the Q-function initialized to what
the potential function would have been.

8
00:00:22,940 --> 00:00:25,680
>> So you get the same, in the sense
that, you get the same answer or

9
00:00:25,680 --> 00:00:28,080
you get the same in the sense that,
it takes the same amount of time?

10
00:00:28,080 --> 00:00:30,970
>> It goes through the same
series of action choices.

11
00:00:30,970 --> 00:00:33,790
So it's going to do
the same things forever.

12
00:00:33,790 --> 00:00:37,040
But yeah, the actual q functions that
you get are different in the two cases.

13
00:00:37,040 --> 00:00:38,710
>> Okay, well I meant the same
thing in terms of policy,

14
00:00:38,710 --> 00:00:41,200
which I guess had to be true.

15
00:00:41,200 --> 00:00:41,790
>> Yeah, but

16
00:00:41,790 --> 00:00:45,460
not only does it converge to the same
policy which we knew was true, but

17
00:00:45,460 --> 00:00:49,820
it's actually going to have the same
policy at every step door in learning.

18
00:00:49,820 --> 00:00:50,360
>> Right.

19
00:00:50,360 --> 00:00:52,900
So that makes sense, okay cool.

20
00:00:52,900 --> 00:00:56,560
>> Oh sure,
because psi is just a constant,

21
00:00:56,560 --> 00:00:58,890
really, with respect to any given state.

22
00:00:58,890 --> 00:01:00,690
>> Right, that's right,
which is having an impact,

23
00:01:00,690 --> 00:01:06,830
this shift impact on what the Q
values are converging toward, yeah.

24
00:01:06,830 --> 00:01:08,140
>> So that's kind of neat.

25
00:01:08,140 --> 00:01:10,570
>> Yeah, it's neat, and
it's a cute proof technique as well.

26
00:01:10,570 --> 00:01:15,040
So, I guess, one way of
interpreting this is yeah you know,

27
00:01:15,040 --> 00:01:16,120
potentials they're really cool.

28
00:01:16,120 --> 00:01:17,580
It's just like initialization.

29
00:01:17,580 --> 00:01:20,580
Another way to interpret this is,
who needs potentials?

30
00:01:20,580 --> 00:01:24,090
You can always get the same effect
by Q function initialization.

31
00:01:24,090 --> 00:01:25,390
>> Well, I think,
it says something else too.

32
00:01:25,390 --> 00:01:30,190
It says that whenever you randomly
initialize your Q function.

33
00:01:30,190 --> 00:01:31,460
You are, in fact,

34
00:01:31,460 --> 00:01:35,015
asserting a particular potential which
just cannot be the right thing to do.

35
00:01:35,015 --> 00:01:35,730
>> Mm-hm.

36
00:01:35,730 --> 00:01:36,790
Wait, so what do you mean by that?

37
00:01:36,790 --> 00:01:39,250
>> Well, this argues that random,
because sometimes,

38
00:01:39,250 --> 00:01:39,820
what are you going to do?

39
00:01:39,820 --> 00:01:40,609
You gotta start out somewhere.

40
00:01:40,609 --> 00:01:43,030
So you just randomly
initialize Q functions.

41
00:01:43,030 --> 00:01:45,200
You put random values
in every Q function.

42
00:01:45,200 --> 00:01:48,570
But that's like saying that, since
that's equivalent to having a potential

43
00:01:48,570 --> 00:01:52,070
function, you're saying that where you
randomly choose to put high values

44
00:01:52,070 --> 00:01:53,020
versus low values.

45
00:01:53,020 --> 00:01:56,440
And you're saying those
are places you ought to be, or

46
00:01:56,440 --> 00:01:58,550
places you ought to start
out trying to be anyway.

47
00:01:58,550 --> 00:02:01,620
So, that would argue against
random initialization.

48
00:02:01,620 --> 00:02:06,010
>> Because it matters, and so
if it matters, why be random?

49
00:02:06,010 --> 00:02:08,030
I mean, one reason to be
random even though it matters,

50
00:02:08,030 --> 00:02:11,310
is because you don't know [LAUGH]
If you always say give it 0, or

51
00:02:11,310 --> 00:02:13,540
if you always give it plus 1000 or
something like that,

52
00:02:13,540 --> 00:02:18,360
you're biasing it as well in a very
consistent way that's easy to fake out.

53
00:02:18,360 --> 00:02:20,225
Right?
It's easy to make up an example MVP

54
00:02:20,225 --> 00:02:22,510
where that's going to
be the wrong thing.

55
00:02:22,510 --> 00:02:26,050
Whereas, if it's random,
you know, maybe it'll work.

56
00:02:26,050 --> 00:02:30,170
>> My guess is though that the you know,
if you integrated over

57
00:02:30,170 --> 00:02:32,930
all the ones that were wrong versus
all the ones that were right for

58
00:02:32,930 --> 00:02:34,840
any given random thing,
for any given fixed MVP,

59
00:02:34,840 --> 00:02:37,470
if you didn't know what it was,
that number's going to be big.

60
00:02:37,470 --> 00:02:39,370
>> But I think it argues for

61
00:02:39,370 --> 00:02:43,101
you shouldn't be randomly
initializing Q functions.

62
00:02:43,101 --> 00:02:45,655
>> All right,
that's not an unreasonable thing to say.

63
00:02:45,655 --> 00:02:48,035
I mean, because basically,
you're injecting knowledge.

64
00:02:48,035 --> 00:02:50,160
And you should not inject noise.

65
00:02:50,160 --> 00:02:51,585
[LAUGH] You should inject knowledge.

66
00:02:51,585 --> 00:02:53,535
>> Right,
actually that's exact the way we put it.

67
00:02:53,535 --> 00:02:56,005
Knowledge and
noise don't both start within.

68
00:02:56,005 --> 00:02:56,855
>> Mm.

69
00:02:56,855 --> 00:02:57,730
>> Only noise does.

70
00:02:57,730 --> 00:03:00,915
>> [LAUGH]
>> That feels like a saying.

71
00:03:02,040 --> 00:03:07,520
Knowledge and noise both sound the same,
but knowledge starts with k, like okay.

72
00:03:07,520 --> 00:03:09,390
And noise starts with no, like N-O.

73
00:03:09,390 --> 00:03:12,910
>> Oh, I did not know that.

74
00:03:14,410 --> 00:03:15,850
Which is the beginning of knowledge.

75
00:03:15,850 --> 00:03:18,350
Okay, all right.
I fear that we are off track.

76
00:03:18,350 --> 00:03:23,050
So, the punchline of this was,
that well, your punchline was,

77
00:03:23,050 --> 00:03:25,930
you don't want to randomly
initialize your Q functions.

78
00:03:25,930 --> 00:03:28,730
My punchline was just that you
can actually inject various

79
00:03:28,730 --> 00:03:32,800
kinds of knowledge in here, and it
could have an impact on learning time.

80
00:03:32,800 --> 00:03:35,060
This doesn't actually show
that it has an impact, but

81
00:03:35,060 --> 00:03:37,580
empirically it often does.

82
00:03:37,580 --> 00:03:41,650
There are some results that actually
show that if you initialize

83
00:03:41,650 --> 00:03:45,448
your Q function optimistically,
but not too optimistically.

84
00:03:45,448 --> 00:03:49,190
You actually bound what the learning
kind is going to be in terms of that.

85
00:03:49,190 --> 00:03:54,110
But that's kind of a story for
another time.

86
00:03:54,110 --> 00:03:55,180
I think, at this point,

87
00:03:55,180 --> 00:03:57,570
we've said what we're going to
say about messing with rewards.

88
00:03:57,570 --> 00:03:59,810
So, maybe we should do
a what have we learned.

89
00:03:59,810 --> 00:04:01,730
>> Okay, I'm a big fan of that.
