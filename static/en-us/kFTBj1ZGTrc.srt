1
00:00:00,068 --> 00:00:02,937
So let's have a programming exercise with the CUB library.

2
00:00:02,937 --> 00:00:06,113
Here's a simple example of the CUB blockscan primitive, okay,

3
00:00:06,113 --> 00:00:09,919
and this is doing a prefix sum in a single block.

4
00:00:09,919 --> 00:00:14,785
And you've seen scan a lot by now; you know a lot of the intricacies of how it can implemented,

5
00:00:14,785 --> 00:00:20,621
and so here's an example of how easy this is to implement with high performance in the IDE.

6
00:00:20,621 --> 00:00:23,900
And there's a couple different things about this from some of the examples you've seen;

7
00:00:23,900 --> 00:00:29,800
again, we're focusing on a single block, because the whole point of CUB is to give you intra-block primitives:

8
00:00:29,800 --> 00:00:34,505
Code for writing your own thread block level code within CUDA kernels.

9
00:00:34,505 --> 00:00:39,210
So we're just going to time a single block, and we're going to measure something we haven't quite seen before.

10
00:00:39,210 --> 00:00:44,715
We're actually going to measure the scan throughput, and we're measuring that in clocks per element scanned.

11
00:00:44,715 --> 00:00:48,351
These are SM clocks: How many actual clock cycles the SM took

12
00:00:48,351 --> 00:00:53,565
averaged across the total number of elements that were scanned by this single thread block.

13
00:00:53,565 --> 00:00:58,628
You'll see the CUDA built-in clock in this kernel, and the way it's used is obvious.

14
00:00:58,628 --> 00:01:01,431
And what I want you to do is fill out this matrix.

15
00:01:01,431 --> 00:01:05,292
This is a performance matrix where we have the number of threads per block

16
00:01:05,292 --> 00:01:07,771
which is represented in the code with block threads,

17
00:01:07,771 --> 00:01:10,475
and the number of items processed per thread,

18
00:01:10,475 --> 00:01:13,207
which is represented in the code with items per thread.

19
00:01:13,207 --> 00:01:16,286
And I want you to go ahead and look at the matrix here

20
00:01:16,286 --> 00:01:19,880
and experiment with different values of block threads and items per thread,

21
00:01:19,880 --> 00:01:23,954
and try to figure out how these 2 items affect the scan throughput.

22
00:01:23,954 --> 00:01:29,325
It would be too much work to fill out this whole matrix, so let me just guide you to a few sweet spots.

23
00:01:29,325 --> 00:01:32,878
So much of the interesting action is going to happen along this diagonal.

24
00:01:32,878 --> 00:01:38,743
So for example, 1,024 threads each representing a single item,

25
00:01:38,743 --> 00:01:42,840
each in charge of scanning a single item per thread

26
00:01:42,840 --> 00:01:45,205
is the kind of thing you might code up naturally.

27
00:01:45,205 --> 00:01:48,296
It might be the first thing you try; it's the simplest to code up.

28
00:01:48,296 --> 00:01:56,593
And then try to analyze what happens when you do 512 by 2 threads, 256 by 4 threads, 128 by 8 threads.

29
00:01:56,593 --> 00:02:00,933
and then let's go ahead and complete the diagonal--won't take long at all.

30
00:02:00,933 --> 00:02:05,228
All the way down to 32 threads each responsible for 32 items

31
00:02:05,228 --> 00:02:08,229
and 16 threads each responsible for 64 items.

32
00:02:08,229 --> 00:02:14,039
And so what I'm going to have you do is fill in this diagonal and from the choices along this diagonal,

33
00:02:14,039 --> 00:02:17,469
check off the option that gives you the best performance;

34
00:02:17,469 --> 00:02:21,469
in other words, the highest scan throughput which corresponds to the fewest clocks per elements scanned.
