1
00:00:00,260 --> 00:00:04,160
So in particular, once we've
supplied a set of actions in a state

2
00:00:04,160 --> 00:00:08,500
representation, we would like
agents to do as well as they can,

3
00:00:08,500 --> 00:00:11,690
without necessarily
questioning what we gave them.

4
00:00:11,690 --> 00:00:13,810
So in this case of
temporal abstractions,

5
00:00:13,810 --> 00:00:19,190
again, hierarchical optimality is
the notion that the agent should

6
00:00:19,190 --> 00:00:24,120
do the best that it can,
respecting the high level,

7
00:00:24,120 --> 00:00:28,780
their temporal extract actions that
they're given without having to question

8
00:00:28,780 --> 00:00:32,729
whether it could do better by not
listening to that whole concept.

9
00:00:32,729 --> 00:00:37,300
>> Right and so maybe, it's the case
that we should include in our options,

10
00:00:37,300 --> 00:00:40,540
every single primitive action,
every single atomic action.

11
00:00:40,540 --> 00:00:42,170
Maybe we don't have to do that.

12
00:00:42,170 --> 00:00:45,200
It really just sort of depends upon
what you're trying to accomplish and

13
00:00:45,200 --> 00:00:46,000
what the trade-offs are.

14
00:00:46,000 --> 00:00:48,030
But we're making those
trade-offs all the time,

15
00:00:48,030 --> 00:00:50,130
even when we define
the problem in the first place.

16
00:00:50,130 --> 00:00:51,910
>> I guess that's fair.

17
00:00:51,910 --> 00:00:55,495
>> Right, but in the same way that
once we have all the machinery of

18
00:00:55,495 --> 00:01:00,370
SMDPs in place, we might as well just
pretend that we're dealing with MDPs for

19
00:01:00,370 --> 00:01:02,782
this kind of purposes of planning and
learning.

20
00:01:02,782 --> 00:01:05,440
We sort of should do that
with the set of options

21
00:01:05,440 --> 00:01:07,570
that we have in the set of
state features that we have.

22
00:01:07,570 --> 00:01:10,140
So that's a big win that we're
going to still get optimality.

23
00:01:10,140 --> 00:01:13,690
It'll be limited by the set of
options that we've constructed, but

24
00:01:13,690 --> 00:01:16,310
it was limited by the set of actions
that we had in the first place.

25
00:01:16,310 --> 00:01:18,354
There's other things that
sort of fall out from that.

26
00:01:18,354 --> 00:01:20,525
One, is state abstraction,
which we talked about before,

27
00:01:20,525 --> 00:01:24,740
that because we have these different
options, if we are lucky or

28
00:01:24,740 --> 00:01:29,280
clever we might be able to define
options that don't require that we pay

29
00:01:29,280 --> 00:01:33,440
attention to the entire state and pay
attention to everything that's going on.

30
00:01:33,440 --> 00:01:34,770
And if so, that can be a big win.

31
00:01:34,770 --> 00:01:39,941
But there's also something else and
I said this on the first slide,

32
00:01:39,941 --> 00:01:45,384
because we're able to jump ahead
from one doorway to another doorway,

33
00:01:45,384 --> 00:01:51,304
it allows us to avoid wandering around
in say, this northeast room, right?

34
00:01:51,304 --> 00:01:55,468
Which means, we get to avoid what I'll
call the boring parts of the mark off

35
00:01:55,468 --> 00:01:56,900
decision process.

36
00:01:56,900 --> 00:01:59,380
The parts of the state
space that don't matter.

37
00:01:59,380 --> 00:02:02,630
Because remember, fundamentally,
in order to do reinforcement learning,

38
00:02:02,630 --> 00:02:05,300
we have to worry about
exploration versus exploitation.

39
00:02:05,300 --> 00:02:08,120
But the options, because they
have these policies built in, and

40
00:02:08,120 --> 00:02:11,390
because they're trying to accomplish
very specific things, allow us to

41
00:02:11,390 --> 00:02:14,760
avoid having to pay attention to
large parts of the state space.

42
00:02:14,760 --> 00:02:17,760
So, they actually help
us with exploration.

43
00:02:17,760 --> 00:02:21,970
So, with the idea that we can focus our
decision making energy on the places

44
00:02:21,970 --> 00:02:26,210
where those decisions actually have an
impact, as opposed to stuff that really

45
00:02:26,210 --> 00:02:28,640
there's only one right answer,
we shouldn't be thinking about it.

46
00:02:28,640 --> 00:02:31,610
>> Right, so in fact, the word that you
that you mentioned as a phrase used

47
00:02:31,610 --> 00:02:32,680
there that I think is just perfect,

48
00:02:32,680 --> 00:02:37,790
which is, we need to focus on the places
where we can actually make decisions.

49
00:02:37,790 --> 00:02:41,580
And because options allow us to kind
of jump forward in time, it means,

50
00:02:41,580 --> 00:02:44,180
in some very important sense
from planning at that level,

51
00:02:44,180 --> 00:02:46,010
there are no decisions to happen here.

52
00:02:46,010 --> 00:02:49,030
We made the decision when we
decided to execute the option,

53
00:02:49,030 --> 00:02:50,360
go to the southern door.

54
00:02:50,360 --> 00:02:52,310
How it gets there, is it's business.

55
00:02:52,310 --> 00:02:53,429
We're just assuming that it works.

56
00:02:53,429 --> 00:02:56,992
In the same way, that when we
decided to execute the up action or

57
00:02:56,992 --> 00:02:58,001
the left action,

58
00:02:58,001 --> 00:03:02,990
we didn't worry about what was happening
while that action was being executed.

59
00:03:02,990 --> 00:03:04,430
We didn't think about it at all.

60
00:03:04,430 --> 00:03:06,190
Those are the decision points.

61
00:03:06,190 --> 00:03:08,010
That's what makes
the decision process and

62
00:03:08,010 --> 00:03:09,881
that's where we should be building
our computational energy.

63
00:03:09,881 --> 00:03:13,910
So, insofar as we can jump ahead in the
future, we can avoid making decisions,

64
00:03:13,910 --> 00:03:17,230
which means we avoid doing
exploration and that is a huge win.
