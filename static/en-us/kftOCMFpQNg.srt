1
00:00:00,260 --> 00:00:01,880
So that brings us to the end of the

2
00:00:01,880 --> 00:00:03,590
topics we are going to talk about in terms of

3
00:00:03,590 --> 00:00:05,290
neural nets. There's going to be some interesting stuff

4
00:00:05,290 --> 00:00:08,130
for you to do in terms of the homework where

5
00:00:08,130 --> 00:00:10,210
you'll be exposed to some other important concepts. But

6
00:00:10,210 --> 00:00:12,590
that's, that's all we're going to lecture about for now.

7
00:00:12,590 --> 00:00:17,300
So let's just remind ourselves what exactly we covered in

8
00:00:17,300 --> 00:00:20,020
the neural net section. So Charles what do you remember?

9
00:00:21,110 --> 00:00:25,020
>> I remember perceptrons. I remember.

10
00:00:25,020 --> 00:00:25,830
>> And

11
00:00:25,830 --> 00:00:28,960
perceptron was a threshold unit, a linear threshold

12
00:00:28,960 --> 00:00:31,310
unit, and we could put networks of them together.

13
00:00:31,310 --> 00:00:31,970
>> Yes.

14
00:00:31,970 --> 00:00:36,520
>> To produce any Boolean function. What else?

15
00:00:36,520 --> 00:00:38,970
Oh, we had a learning rule for perceptrons.

16
00:00:38,970 --> 00:00:43,830
>> Mm-hm. Which runs in finite time for linearly separable data sets.

17
00:00:43,830 --> 00:00:47,430
>> And we learned a general differentiable rule. Adding

18
00:00:47,430 --> 00:00:50,360
general we learned about propagation using a gradient set.

19
00:00:50,360 --> 00:00:51,460
>> And

20
00:00:51,460 --> 00:00:54,480
we talked a little bit about the, about the preference

21
00:00:54,480 --> 00:00:58,420
and restriction by c's of neural networks. Alright, til next time.

22
00:00:58,420 --> 00:00:59,180
>> See you Michael.
