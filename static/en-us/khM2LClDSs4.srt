1
00:00:00,250 --> 00:00:02,330
Hi Michael, so that covers Decision Trees

2
00:00:02,330 --> 00:00:04,000
>> Excellent.

3
00:00:04,000 --> 00:00:07,390
>> So, since you are the one who is listening,

4
00:00:07,390 --> 00:00:08,900
you get to tell me what we have learned today?

5
00:00:08,900 --> 00:00:11,780
>> Well, we learned about the Decision Tree representation,

6
00:00:15,750 --> 00:00:20,330
we learned the top down algorithm for inducing

7
00:00:20,330 --> 00:00:23,630
a Decision Tree. And we call that ID3

8
00:00:23,630 --> 00:00:26,110
>> All right. So we got a representation,

9
00:00:26,110 --> 00:00:28,150
we got a top down learning algorithm ID3.

10
00:00:28,150 --> 00:00:31,790
>> We learned about the, the expressiveness and the bias.

11
00:00:31,790 --> 00:00:35,370
>> Right. So those are 2 separate things, we learned about

12
00:00:35,370 --> 00:00:41,220
the sort of expressiveness. And we learned about the bias of ID3.

13
00:00:41,220 --> 00:00:48,510
>> And we gave one, so is this specific to ID3? We, we looked at one specific

14
00:00:48,510 --> 00:00:54,205
way of deciding on splits, which was to do this maximum information game.

15
00:00:54,205 --> 00:01:00,440
>> Right. So we talked about in general,um, what are good attributes or what

16
00:01:00,440 --> 00:01:04,250
are best attributes. So, information gain is

17
00:01:04,250 --> 00:01:06,830
one way of doing it. As one example.

18
00:01:06,830 --> 00:01:09,610
And, by the way, this notion of best attribute is something we'll end

19
00:01:09,610 --> 00:01:11,410
up returning to sometimes explicitly, and

20
00:01:11,410 --> 00:01:14,650
sometimes implicitly, throughout the entire course. And

21
00:01:14,650 --> 00:01:18,680
lastly, I feel like we talked about the problems with over-fitting and how

22
00:01:18,680 --> 00:01:24,570
in the Decision Tree context, you can prune back the tree to avoid over-fitting.

23
00:01:24,570 --> 00:01:28,720
>> Over-fitting is an issue. Over-fitting is always an issue and

24
00:01:28,720 --> 00:01:32,970
we came up with a couple of strategies for dealing with over-fitting

25
00:01:32,970 --> 00:01:37,030
in the context of Decision Trees. Okay! So we've learned everything

26
00:01:37,030 --> 00:01:39,160
there is to know about Decision Trees, there's nothing else to know.

27
00:01:39,160 --> 00:01:42,440
>> [LAUGH] Somehow I find that hard to believe.

28
00:01:42,440 --> 00:01:45,450
>> Yeah, there's a lot there and the students will get

29
00:01:45,450 --> 00:01:48,170
a chance to learn even more as they do the assignments.

30
00:01:48,170 --> 00:01:48,610
>> Cool.

31
00:01:48,610 --> 00:01:51,160
>> Excellent. All right Michael, thank you.

32
00:01:51,160 --> 00:01:53,250
>> Sure, look forward to the next chat.
