1
00:00:00,370 --> 00:00:04,650
okay, wait. So let me see if I can echo some of that back. So, it's almost as

2
00:00:04,650 --> 00:00:06,220
if what we're doing here is we're doing a

3
00:00:06,220 --> 00:00:10,800
transformation into a new space where feature selection can work.

4
00:00:10,800 --> 00:00:14,510
>> Exactly, and in fact, here's something kind of interesting for you.

5
00:00:14,510 --> 00:00:18,810
It turns out that if the ion value of some particular dimension

6
00:00:18,810 --> 00:00:22,590
is equal to zero, then it means it provides no information whatsoever

7
00:00:22,590 --> 00:00:25,410
in the original space. So, if I have a direction, if I have

8
00:00:25,410 --> 00:00:27,960
a dimension that has an eigen of zero, I can

9
00:00:27,960 --> 00:00:30,150
throw it away and it will not affect my reconstructioner.

10
00:00:30,150 --> 00:00:33,290
>> I'm trying to remember if that makes it irrelevant or useless.

11
00:00:33,290 --> 00:00:37,070
>> Well, it certainly makes it irrelevant. If there's no variance, that's the

12
00:00:37,070 --> 00:00:40,760
same thing as saying it has zero entropy, because it never changes. So it's

13
00:00:40,760 --> 00:00:44,580
irrelevant. Now whether it's useful or not, well, probably not, but it might

14
00:00:44,580 --> 00:00:48,005
be useful for something like, our simple,

15
00:00:48,005 --> 00:00:50,460
[INAUDIBLE] example that we used last time.

16
00:00:50,460 --> 00:00:50,460
>>

17
00:00:50,460 --> 00:00:50,720
Gotcha.

18
00:00:50,720 --> 00:00:53,190
>> Okay. So does this all make sense?

19
00:00:53,190 --> 00:00:55,300
>> So one question I have is in the example

20
00:00:55,300 --> 00:00:58,170
that we just kind of worked through there was a blob

21
00:00:58,170 --> 00:01:00,860
of data and when we drew the red line through

22
00:01:00,860 --> 00:01:05,120
the maximum variance direction it went through the. The xy origin.

23
00:01:05,120 --> 00:01:05,530
>> Um-huh.

24
00:01:05,530 --> 00:01:10,070
>> Does it have to? Is it necessarily the case that it's going to?

25
00:01:10,070 --> 00:01:12,070
Or you know is the algorithm restricted

26
00:01:12,070 --> 00:01:13,590
to have to put things through the origin?

27
00:01:13,590 --> 00:01:15,750
>> Well so my

28
00:01:15,750 --> 00:01:18,420
answer to you is that, that's actually a very complicated

29
00:01:18,420 --> 00:01:22,830
question. And in principle it, so to speak it, it

30
00:01:22,830 --> 00:01:26,220
doesn't really [LAUGH] matter. But in practice what people do

31
00:01:26,220 --> 00:01:29,010
and their doing something like PCA is they actually subtract the

32
00:01:29,010 --> 00:01:32,170
mean of the data. Or the central of the data

33
00:01:32,170 --> 00:01:33,880
from all the data points. So it ends up being

34
00:01:33,880 --> 00:01:38,370
centered around an origin, for the origin. But what this

35
00:01:38,370 --> 00:01:40,760
means is that you can then interprit what we're doing is

36
00:01:40,760 --> 00:01:43,350
finding maximum variants. As capturing correlation.

37
00:01:43,350 --> 00:01:44,510
>> Okay. That's helpful.

38
00:01:44,510 --> 00:01:46,930
>> And otherwise, if you don't do that, what you end up with is

39
00:01:46,930 --> 00:01:49,660
effectively a principle component that at least

40
00:01:49,660 --> 00:01:51,590
intuitively kind of captures the notion of where

41
00:01:51,590 --> 00:01:55,060
the origin should be, and it's not terribly helpful for what it is we're

42
00:01:55,060 --> 00:01:58,830
trying to do. So my answer is, no it doesn't, and yes it does.

43
00:01:58,830 --> 00:02:00,367
>> [LAUGH]

44
00:02:00,367 --> 00:02:01,650
>> Okay?

45
00:02:01,650 --> 00:02:01,770
>> Sure.

46
00:02:01,770 --> 00:02:04,190
>> Okay. So, that's, that's basically

47
00:02:04,190 --> 00:02:06,270
PCA. It's got all these neat properties.

48
00:02:06,270 --> 00:02:09,699
Let's sort of summarize them again. It's, well, actually.

49
00:02:09,699 --> 00:02:11,780
Let me add a couple beyond these. It is a

50
00:02:11,780 --> 00:02:14,420
global algorhythm. It does give you best reconstruction error. Which

51
00:02:14,420 --> 00:02:17,250
seems like a very nice thing to, thing to have.

52
00:02:17,250 --> 00:02:21,080
And, it tells you, which of the. New features

53
00:02:21,080 --> 00:02:23,680
that you get out are actually important with respect to

54
00:02:23,680 --> 00:02:27,190
this notion of reconstruction by simply looking at their corresponding

55
00:02:27,190 --> 00:02:31,370
eigenvalues. They also have a couple of other practical properties.

56
00:02:31,370 --> 00:02:34,130
In particular, it's very well studied. And what that means

57
00:02:34,130 --> 00:02:37,600
in this case is that there are very fast algorithms that

58
00:02:37,600 --> 00:02:41,160
do a very good job, even in weird cases. Even with

59
00:02:41,160 --> 00:02:44,520
large data sets, at least if they're appropriately sparse, of being

60
00:02:44,520 --> 00:02:46,710
able to compute these things. People have been working on this

61
00:02:46,710 --> 00:02:50,030
problem again, since before we were born Michael, and they've gotten

62
00:02:50,030 --> 00:02:53,690
really good at finding principle components even for what would be

63
00:02:53,690 --> 00:02:56,690
very difficult spaces. But this does lead me to a question

64
00:02:56,690 --> 00:02:59,840
though which is another question, which is okay, so

65
00:02:59,840 --> 00:03:01,900
you've got an algorithm that probably gives you the best

66
00:03:01,900 --> 00:03:05,730
reconstruction. But what does it have to do with classification?

67
00:03:05,730 --> 00:03:07,600
This is kind of like the question we had before

68
00:03:07,600 --> 00:03:11,440
about relevance versus usefulness. So we find a bunch

69
00:03:11,440 --> 00:03:15,770
of projections which are relevant in the sense that they

70
00:03:15,770 --> 00:03:18,880
allow you to do reconstruction. But it's not clear that

71
00:03:18,880 --> 00:03:21,840
if you threw away some of these projections, the ones

72
00:03:21,840 --> 00:03:24,090
with low eigenvalue, that even though you'd be

73
00:03:24,090 --> 00:03:27,510
able to reconstruct your. Original data is not

74
00:03:27,510 --> 00:03:30,180
clear that would help you do classification later,

75
00:03:30,180 --> 00:03:31,390
can you see how that would work out?

76
00:03:31,390 --> 00:03:33,900
>> Sure, I mean like, it just could be that that's

77
00:03:33,900 --> 00:03:37,050
not where the information is about what the labels ought to be.

78
00:03:37,050 --> 00:03:38,900
>> Right, so imagine for example that one

79
00:03:38,900 --> 00:03:42,380
of your original dimensions is in fact directly related

80
00:03:42,380 --> 00:03:43,680
to the label and all the rest are

81
00:03:43,680 --> 00:03:47,410
just, say goush and noise. But the, the variance

82
00:03:47,410 --> 00:03:51,170
of that particular direction is extremely small.

83
00:03:51,170 --> 00:03:52,850
>> It might end up throwing it away.

84
00:03:52,850 --> 00:03:55,580
>> It'll almost certainly end up throwing it away. Which means

85
00:03:55,580 --> 00:03:58,950
you'll end up with a bunch of random data that doesn't

86
00:03:58,950 --> 00:04:02,200
actually later help you do classification. And that's because this looks

87
00:04:02,200 --> 00:04:05,680
a lot like, if I can do the analogy, a filter method.

88
00:04:05,680 --> 00:04:06,700
>> I was going to say that, yeah.

89
00:04:06,700 --> 00:04:10,040
>> Oh. So this is kind of like filtering. And in fact it's going

90
00:04:10,040 --> 00:04:12,580
to turn out that the other two items we're looking at too are like

91
00:04:12,580 --> 00:04:15,150
filtering although their particular criterion might be

92
00:04:15,150 --> 00:04:17,579
more relevant. We'll see. Okay? Alright, so do

93
00:04:17,579 --> 00:04:20,160
you understand principal components analysis? Again, I'm not

94
00:04:20,160 --> 00:04:22,940
asking you to understand exactly how you would

95
00:04:22,940 --> 00:04:25,920
run a principal components analsyis algorithm. That

96
00:04:25,920 --> 00:04:28,020
stuff's in, in all the text, but just

97
00:04:28,020 --> 00:04:29,890
to sort of understand exactly what is is

98
00:04:29,890 --> 00:04:31,460
trying to do, what it's trying to accomplish.

99
00:04:31,460 --> 00:04:34,410
>> Yeah, I think so. So, it's, it's taking the data, it's finding

100
00:04:34,410 --> 00:04:37,650
a different set of axis, that are just like regular axis in that

101
00:04:37,650 --> 00:04:42,690
they're mutually orthogonal. But it lines up the varience of the data with those

102
00:04:42,690 --> 00:04:46,370
axis, so that we can drop the least significant ones, and that gives us a

103
00:04:46,370 --> 00:04:48,730
way to do. Feature selection. But the

104
00:04:48,730 --> 00:04:50,640
whole thing is a feature transformation algorithm, in

105
00:04:50,640 --> 00:04:54,010
the sense that it first moved the data around, to be able to do that.

106
00:04:54,010 --> 00:04:57,370
>> That's exactly right. So, you do, transformation into

107
00:04:57,370 --> 00:04:59,390
a new space, where you know how to do filtering.

108
00:04:59,390 --> 00:05:00,160
>> Got it.

109
00:05:00,160 --> 00:05:00,900
>> Excellent.
