1
00:00:00,230 --> 00:00:01,830
Alright Michael. So like I said, we're going

2
00:00:01,830 --> 00:00:03,240
to spend all this time trying to, to

3
00:00:03,240 --> 00:00:05,540
unpack this particular equation. And the first thing

4
00:00:05,540 --> 00:00:06,939
we need to do is we need to come

5
00:00:06,939 --> 00:00:08,930
up with another form of it that we

6
00:00:08,930 --> 00:00:11,360
might have some chance of actually understanding of

7
00:00:11,360 --> 00:00:12,950
actually getting through. So I want to use

8
00:00:12,950 --> 00:00:15,490
something called Bayes' rule. Do you remember Bayes' rule?

9
00:00:15,490 --> 00:00:16,400
>> I do.

10
00:00:16,400 --> 00:00:17,430
>> Okay, what's Bayes' Rule?

11
00:00:17,430 --> 00:00:18,915
>> The man with the Bayes makes the

12
00:00:18,915 --> 00:00:20,560
rule. Oh wait, no, that's the golden rule.

13
00:00:20,560 --> 00:00:21,020
>> That's right, no.

14
00:00:21,020 --> 00:00:23,950
>> The Bayes Rule, is, it relates, it, I

15
00:00:23,950 --> 00:00:25,340
don't know. I think of it as just letting

16
00:00:25,340 --> 00:00:27,650
you switch which thing is on which side of the bar.

17
00:00:27,650 --> 00:00:28,740
>> Okay, so.

18
00:00:28,740 --> 00:00:30,990
>> Do you want me to give the whole expression?

19
00:00:30,990 --> 00:00:32,060
>> Yeah, give me the whole expression.

20
00:00:32,060 --> 00:00:38,010
>> So if we're going to apply Bayes' Rule to the probability of h given D. We

21
00:00:38,010 --> 00:00:43,630
can move, turn it around and make it equal to the probably of D given H. And it

22
00:00:43,630 --> 00:00:45,020
would be great if we could just stop with

23
00:00:45,020 --> 00:00:46,860
that, but we can't. We have to now kind

24
00:00:46,860 --> 00:00:48,130
of put them in the same space. So, we

25
00:00:48,130 --> 00:00:50,420
multiply by the probability of H, and then we

26
00:00:50,420 --> 00:00:52,740
divide by the probability of D. And sometimes

27
00:00:52,740 --> 00:00:54,290
that's just a normalization and we don't have

28
00:00:54,290 --> 00:00:55,960
to worry about it too much. But that's,

29
00:00:55,960 --> 00:00:57,730
that's the bay, that's Bayes' rule right there.

30
00:00:57,730 --> 00:01:00,030
>> So this is Bayes' rule. And it actually is

31
00:01:00,030 --> 00:01:03,800
really easy to derive. It falls it follows directly from

32
00:01:03,800 --> 00:01:05,570
the chain rule in probability theory. Do you think it's

33
00:01:05,570 --> 00:01:09,070
worthwhile? Showing people that or just they should just accept it.

34
00:01:09,070 --> 00:01:11,360
>> Well, I mean, you could just, you might be able to just see it. Just,

35
00:01:11,360 --> 00:01:12,810
the, the thing on top of the, the

36
00:01:12,810 --> 00:01:15,710
normalization, the probability of D given h times probability

37
00:01:15,710 --> 00:01:18,710
of h. That's actually the probability of D and

38
00:01:18,710 --> 00:01:22,690
h together. Right. So the probability of h times the

39
00:01:22,690 --> 00:01:24,496
probability of d over h as you say also the

40
00:01:24,496 --> 00:01:28,577
chain rule basically the definition of conditional probability in conjunctions

41
00:01:28,577 --> 00:01:30,530
and if you move the probability of d over

42
00:01:30,530 --> 00:01:32,350
to the left hand side you can see we're really

43
00:01:32,350 --> 00:01:34,510
just saying the same thing two different ways. It's just

44
00:01:34,510 --> 00:01:36,710
the probability of h and d. So then we're done.

45
00:01:36,710 --> 00:01:38,650
>> No, that's right. So I can write down

46
00:01:38,650 --> 00:01:40,750
what you just said. And use different letters just

47
00:01:40,750 --> 00:01:42,230
to make it more confusing, so

48
00:01:42,230 --> 00:01:42,550
>> Oh good.

49
00:01:42,550 --> 00:01:46,800
>> You can point out that the probability of A and B, by the chain rule, is

50
00:01:46,800 --> 00:01:50,190
just the probability of A given B, times the

51
00:01:50,190 --> 00:01:54,880
probability of B. But because order doesn't matter, it's

52
00:01:54,880 --> 00:02:00,240
also the case that the probability of A and B. Is the probability of b given a

53
00:02:00,240 --> 00:02:03,490
times the probability of a. And that's just the

54
00:02:03,490 --> 00:02:05,820
chain rule. And so if these two quantities equal

55
00:02:05,820 --> 00:02:08,120
to one another's exactly what you say, I could say

56
00:02:08,120 --> 00:02:12,690
well, the probability of a given b is just the probability

57
00:02:12,690 --> 00:02:16,200
of b given a times the probability a divided by the

58
00:02:16,200 --> 00:02:19,550
probability of b. And that's exactly what we have over here.

59
00:02:19,550 --> 00:02:21,560
>> Good. So now that we've mastered that

60
00:02:21,560 --> 00:02:24,063
all your Bayes are belong to us. [LAUGH]

61
00:02:24,063 --> 00:02:26,470
>> How long have you been saying that?

62
00:02:26,470 --> 00:02:27,870
>> The...just, only about 3 or 4 minutes.

63
00:02:27,870 --> 00:02:30,960
>> [LAUGH] Fair enough. Okay, so we have Bayes's rule. And what's really nice

64
00:02:30,960 --> 00:02:33,890
about Bayes's rule is that while it's a very simple thing, it's

65
00:02:33,890 --> 00:02:37,180
also true. It follows directly from probability theory. But more importantly for

66
00:02:37,180 --> 00:02:40,770
machine learning, it gives us a handle to talk about. What it

67
00:02:40,770 --> 00:02:43,090
is we're exactly trying to do when we say we're trying to

68
00:02:43,090 --> 00:02:46,580
find the most probable hypothesis, given the data. So let's just take

69
00:02:46,580 --> 00:02:49,380
a moment to think about what all these terms mean. We know

70
00:02:49,380 --> 00:02:53,170
what this term here means. The, it's just the probability of some

71
00:02:53,170 --> 00:02:56,560
hypothesis given the data. But what do all these other terms mean?

72
00:02:56,560 --> 00:03:00,676
I want to start with this term, the probability of

73
00:03:00,676 --> 00:03:05,600
the data. It's really nothing more than your prior belief of

74
00:03:05,600 --> 00:03:08,350
seeing some particular set of data. Now, and as you point

75
00:03:08,350 --> 00:03:10,520
out, Michael, often it just ends up to be a normalizing

76
00:03:10,520 --> 00:03:13,330
term and typically does not matter, though we'll see a couple

77
00:03:13,330 --> 00:03:16,930
of cases where it does matter, helps us to, to sort

78
00:03:16,930 --> 00:03:19,490
of think about a few things. But generally speaking, whatever it

79
00:03:19,490 --> 00:03:21,560
is Since the only thing that we care about is the

80
00:03:21,560 --> 00:03:24,440
hypothesis, we're trying to find that, the

81
00:03:24,440 --> 00:03:26,080
probability of the data doesn't depend on the

82
00:03:26,080 --> 00:03:29,870
hypothesis, so typically we ignore it, but it's nice to just be clear about what

83
00:03:29,870 --> 00:03:33,480
it means. The other terms are a bit more interesting. They matter a little bit

84
00:03:33,480 --> 00:03:36,380
more. This term here, the probability is the

85
00:03:36,380 --> 00:03:39,010
probability of the data given the hypothesis right?

86
00:03:39,010 --> 00:03:41,310
>> Mm. Seems like learning backwards.

87
00:03:41,310 --> 00:03:43,640
>> It does seem like learning backwards but

88
00:03:43,640 --> 00:03:46,700
what's really nice about this quantity is that unlike

89
00:03:46,700 --> 00:03:50,150
the other quantity, the probability of the hypothesis given the data, it's

90
00:03:50,150 --> 00:03:54,250
actually, turns out to be pretty easy to think about the likelihood that

91
00:03:54,250 --> 00:03:57,000
we would see some data given that we were in a world

92
00:03:57,000 --> 00:03:59,430
where some hypothesis, h, is true. So there is a little bit of

93
00:03:59,430 --> 00:04:02,510
subtlety there and I, let me, let me unpack that subtlety a

94
00:04:02,510 --> 00:04:05,630
little bit. So we've been talking about the data if its sort of

95
00:04:05,630 --> 00:04:08,520
a thing that is floating out in air, but we know that the

96
00:04:08,520 --> 00:04:12,500
data is actually our training data. And it's a set of inputs and

97
00:04:12,500 --> 00:04:14,140
lets just say for the sake of argument we are

98
00:04:14,140 --> 00:04:17,320
going to do classification learning, it's a set of labels that

99
00:04:17,320 --> 00:04:20,440
are associated with those inputs. So just to drive the

100
00:04:20,440 --> 00:04:24,700
point home, I'm going to call those d's, little d's. And

101
00:04:24,700 --> 00:04:26,170
so our data is made up of a bunch of

102
00:04:26,170 --> 00:04:30,170
these training examples. And these training examples are whatever input that

103
00:04:30,170 --> 00:04:33,620
we get coming from a teacher, coming from ourselves, coming

104
00:04:33,620 --> 00:04:37,690
from nature, coming from somewhere and the associated label that goes

105
00:04:37,690 --> 00:04:42,700
along with them. So when you talk about the probability of the data given

106
00:04:42,700 --> 00:04:44,820
the hypothesis, what you're talking about, well,

107
00:04:44,820 --> 00:04:46,940
what's the likelihood that. Given that I've

108
00:04:46,940 --> 00:04:51,000
got all of these Xis and given that I'm living in a world where

109
00:04:51,000 --> 00:04:54,490
this particular hypothesis that I would see

110
00:04:54,490 --> 00:04:57,320
these particular labels. Does that make sense Michael?

111
00:04:57,320 --> 00:04:59,760
>> I see. Yeah, so, so I can imagine a

112
00:04:59,760 --> 00:05:03,240
more complicated kind of notation where, we're, we're kind of accepting

113
00:05:03,240 --> 00:05:07,070
the Xs as given. But the labels is what we are

114
00:05:07,070 --> 00:05:13,310
actually saying is something that we want to assigned probability to.

115
00:05:13,310 --> 00:05:16,438
>> Right so its not really that the x's matter in the sense

116
00:05:16,438 --> 00:05:21,105
that we are trying to understand those. What really mattes re the labels

117
00:05:21,105 --> 00:05:25,144
that are associated with them. And we will see an example of that

118
00:05:25,144 --> 00:05:28,310
in a moment. But I wanted to make sure that you get this subtled.

119
00:05:28,310 --> 00:05:31,490
>> So in a sense then I guess you're saying that the probability of D given H

120
00:05:31,490 --> 00:05:33,928
component, or, or quantity, is really like running

121
00:05:33,928 --> 00:05:36,350
the hypothesis. It's like, It's like labeling the data.

122
00:05:36,350 --> 00:05:39,040
>> Okay Michael, just to make sure we get this. Let's

123
00:05:39,040 --> 00:05:43,700
imagine we're in a universe, where the following hypothesis is true. It

124
00:05:43,700 --> 00:05:48,270
returns true, in exactly the cases where some input number X, is

125
00:05:48,270 --> 00:05:52,370
greater than or equal to 10 And it returns false otherwise. Okay?

126
00:05:52,370 --> 00:05:53,160
>> Yup.

127
00:05:53,160 --> 00:05:54,100
>> Okay. So

128
00:05:54,100 --> 00:05:57,740
here's a question for you. Let's say that our data was made up of exactly one

129
00:05:57,740 --> 00:06:03,490
point. And that value set x equal to 7. Okay? What is

130
00:06:03,490 --> 00:06:09,100
the probability that the label associated with 7. Would be true.

131
00:06:09,100 --> 00:06:11,800
>> Huh. So you're saying we're in a world

132
00:06:11,800 --> 00:06:14,370
where h is holding and that the h, h

133
00:06:14,370 --> 00:06:16,810
is being used to generate labels. So it wouldn't

134
00:06:16,810 --> 00:06:18,980
do that right? So, the probability ought to be zero.

135
00:06:18,980 --> 00:06:19,320
>> That's

136
00:06:19,320 --> 00:06:21,830
exactly right and what's the probability that it would

137
00:06:21,830 --> 00:06:25,200
be false? 1 minus 0 [LAUGH] which we'll call 1.

138
00:06:25,200 --> 00:06:26,780
>> Which we'll call 1. That's exactly right.

139
00:06:26,780 --> 00:06:28,590
So it's, it's just that simple. That, the

140
00:06:28,590 --> 00:06:30,990
probability of the data given the hypothesis, is

141
00:06:30,990 --> 00:06:33,150
really about, given a set of x's, what's

142
00:06:33,150 --> 00:06:35,550
the probability that I would see some particular

143
00:06:35,550 --> 00:06:38,390
label. Now, what's nice about that is, is,

144
00:06:38,390 --> 00:06:40,730
as you point out, is that, it's as

145
00:06:40,730 --> 00:06:44,190
if we're running the hypothesis. Well, given a hypothesis,

146
00:06:44,190 --> 00:06:47,890
it's really easy, or at least it's easier usually, to compute

147
00:06:47,890 --> 00:06:50,980
the probability of us seeing some labels. So, this quantity is

148
00:06:50,980 --> 00:06:54,920
a lot easier to figure out than the original quantity that

149
00:06:54,920 --> 00:06:57,730
we're looking for. The probability of the hypothesis, given the data.

150
00:06:57,730 --> 00:07:01,370
>> Yeah, I could see that. It's sort of reminding me a little

151
00:07:01,370 --> 00:07:06,490
bit of the Version Space, but I can't quite crystallize what the connection is.

152
00:07:06,490 --> 00:07:09,310
>> Well that's, it's good you bring that up. Because I, I think in a

153
00:07:09,310 --> 00:07:11,040
couple of seconds I'll give you an example

154
00:07:11,040 --> 00:07:12,700
that might really help you to see that. Okay?

155
00:07:12,700 --> 00:07:13,190
>> Okay.
