1
00:00:00,520 --> 00:00:05,050
One very powerful place that you can use regularization, is in regression.

2
00:00:05,050 --> 00:00:06,840
Regularization is a method for

3
00:00:06,840 --> 00:00:11,020
automatically penalizing the extra features that you use in your model.

4
00:00:11,020 --> 00:00:13,340
So, let me make this a little bit more concrete.

5
00:00:13,340 --> 00:00:17,140
There's a type of regularized regression called Lasso Regression.

6
00:00:17,140 --> 00:00:19,560
And, here's the rough formula for the Lasso Regression.

7
00:00:20,630 --> 00:00:22,350
A regular linear regression would say,

8
00:00:22,350 --> 00:00:27,640
I just want to minimize the sum of the squared errors in my fit.

9
00:00:27,640 --> 00:00:30,840
I want to minimize the distance between my fit, and

10
00:00:30,840 --> 00:00:35,210
any given data point, or the square of that distance.

11
00:00:35,210 --> 00:00:39,740
What Lasso Regression says is yeah, we want a small sum of squared error.

12
00:00:39,740 --> 00:00:42,890
But, in addition to minimizing the sum of the squared errors,

13
00:00:42,890 --> 00:00:45,720
I also want to minimize the number of features that I'm using.

14
00:00:45,720 --> 00:00:49,390
And, so I'm going to add in a second term here, in which I

15
00:00:49,390 --> 00:00:53,570
have a penalty parameter, and I have the coefficients of my regression.

16
00:00:53,570 --> 00:00:58,040
So, this is basically the term that describes how many features I'm using.

17
00:00:58,040 --> 00:01:00,200
So, here's the result of this formulation.

18
00:01:00,200 --> 00:01:05,030
When I'm performing my fit, I'm considering both the errors that come from that

19
00:01:05,030 --> 00:01:09,060
fit, and also the number of features that are being used.

20
00:01:09,060 --> 00:01:11,390
And, so let's say I'm comparing two different fits,

21
00:01:11,390 --> 00:01:12,890
that have different number of features in them.

22
00:01:13,930 --> 00:01:16,150
The one that has more features included,

23
00:01:16,150 --> 00:01:19,300
will almost certainly have a smaller sum of the squared error.

24
00:01:19,300 --> 00:01:22,640
because, it can fit more precisely to the points.

25
00:01:22,640 --> 00:01:25,400
But, I pay a penalty for using that extra feature.

26
00:01:25,400 --> 00:01:29,300
And, that comes in the second term with the, with the penalty term, and

27
00:01:29,300 --> 00:01:31,130
the coefficients of regression that I'm going to get for

28
00:01:31,130 --> 00:01:33,880
that additional feature that I'm using.

29
00:01:33,880 --> 00:01:36,660
And, so what this is saying is that the gain that I get,

30
00:01:36,660 --> 00:01:38,740
in terms of the, the precision,

31
00:01:38,740 --> 00:01:43,630
the goodness of fit of my regression, has to be a bigger gain than the, the loss

32
00:01:43,630 --> 00:01:47,730
that I take as a result of having that additional feature in my regression.

33
00:01:49,220 --> 00:01:52,820
So, this precisely formulates, in a mathematical way, the trade off between

34
00:01:52,820 --> 00:01:58,910
having small errors and having a simpler fit that's using fewer features.

35
00:01:58,910 --> 00:02:01,040
And, so what Lasso Regression does,

36
00:02:01,040 --> 00:02:04,970
is it automatically takes into account this penalty parameter.

37
00:02:04,970 --> 00:02:08,780
And, in so doing, it helps you actually figure out which features that

38
00:02:08,780 --> 00:02:12,040
are the ones that have the most important effect on your regression.

39
00:02:12,040 --> 00:02:15,010
And, once it's found those features, it can actually eliminate or

40
00:02:15,010 --> 00:02:19,500
set to zero, the coefficients for the features that basically don't help
