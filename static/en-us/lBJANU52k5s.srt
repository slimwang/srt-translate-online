1
00:00:00,000 --> 00:00:02,739
So let's recreate our programming model diagram.

2
00:00:02,739 --> 00:00:05,141
We've got threads, and we've got thread blocks.

3
00:00:05,141 --> 00:00:07,585
And really what is happening with these barriers

4
00:00:07,585 --> 00:00:12,307
is that the sync threads call is creating a barrier within a thread block.

5
00:00:12,307 --> 00:00:15,544
So all the threads within this thread block are going to get to this sync threads call,

6
00:00:15,544 --> 00:00:18,328
stop, and wait until they've all arrived and then proceed.

7
00:00:18,328 --> 00:00:22,389
And these thread blocks are organized in kernels; every kernel has a bunch of thread blocks.

8
00:00:22,389 --> 00:00:27,513
We haven't talked about this really, but there's an implicit barrier between kernels.

9
00:00:27,513 --> 00:00:30,771
So if I launch 2 kernels, one after another,

10
00:00:30,771 --> 00:00:35,402
then by default kernel a will finish before kernel b is allowed to start.

11
00:00:35,402 --> 00:00:39,042
So all of these threads will complete before any of these threads get launched.

12
00:00:39,042 --> 00:00:41,642
So when you add in our memory model that we've talked about,

13
00:00:41,642 --> 00:00:44,282
where every thread has access to its own local memory,

14
00:00:44,282 --> 00:00:49,184
to its block shared memory, and to the global memory shared by all threads and all kernels in the system,

15
00:00:49,184 --> 00:00:51,520
then what you've got is CUDA.

16
00:00:51,520 --> 00:00:54,500
At its heart, CUDA is a hierarchy of computation.

17
00:00:54,500 --> 00:00:56,767
That would the threads, thread blocks, and kernels

18
00:00:56,767 --> 00:01:00,294
with the corresponding hierarchy of memory spaces: local, shared, and global,

19
00:01:00,294 --> 00:01:02,866
and synchronization primitives:

20
00:01:02,866 --> 00:01:06,866
sync threads, barriers, and the implicit barrier in between 2 synchronous kernels.
