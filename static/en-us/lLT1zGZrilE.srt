1
00:00:00,280 --> 00:00:02,605
So let me briefly introduce here the idea of cache

2
00:00:02,605 --> 00:00:04,180
aware scheduling, when you have

3
00:00:04,180 --> 00:00:07,290
these multithreaded multi-core processors. And to

4
00:00:07,290 --> 00:00:10,800
make things concrete, let's assume that you have a pool of

5
00:00:10,800 --> 00:00:13,620
ready threads, and in this case I'm going to tell you that

6
00:00:13,620 --> 00:00:15,720
the pool of ready threads I have is 32. So I

7
00:00:15,720 --> 00:00:19,566
have 32 ready threads and I have a four way multi-core

8
00:00:19,566 --> 00:00:22,020
per CPU. Meaning that there are four cores in the CPU,

9
00:00:22,020 --> 00:00:25,360
and each core is four way hardware multithreaded. Or in other

10
00:00:25,360 --> 00:00:27,850
words, at any point of time the operating system

11
00:00:27,850 --> 00:00:31,310
can choose from this pool of ready threads, 16

12
00:00:31,310 --> 00:00:35,560
threads to be run on the processor. Because that's

13
00:00:35,560 --> 00:00:38,710
the number of hardware multi-threads that are available if

14
00:00:38,710 --> 00:00:41,250
you pool together all the four cores. So each

15
00:00:41,250 --> 00:00:45,075
core has four multi, hardware multi-threads, together they have

16
00:00:45,075 --> 00:00:50,520
1,600 threads that can be run on the CPU at any point of time. And the, the job

17
00:00:50,520 --> 00:00:52,960
of the operating scheduler is to pick from

18
00:00:52,960 --> 00:00:56,270
the available pool of ready threads, 16 candidates

19
00:00:56,270 --> 00:00:59,780
to be scheduled on the CPU. So how

20
00:00:59,780 --> 00:01:02,560
does the operating system choose the 16 threads

21
00:01:02,560 --> 00:01:08,210
to be run on the CPU at any point of time? What the operating system should

22
00:01:08,210 --> 00:01:11,740
try to do, is it should call schedule

23
00:01:11,740 --> 00:01:15,730
some number of cache frugal threads, and some number

24
00:01:15,730 --> 00:01:22,070
of cache hungry threads on the different course so that together the sum of all

25
00:01:22,070 --> 00:01:26,500
the cache hungriness of the 16 threads that

26
00:01:26,500 --> 00:01:29,080
are executing at any point of time in

27
00:01:29,080 --> 00:01:36,170
the CPU is less than the total size of the L2 cache. And as I said,

28
00:01:36,170 --> 00:01:40,980
L2 cache in this simple example, I gave you two levels of caching, a caching

29
00:01:40,980 --> 00:01:43,630
that is associated with each of these cores, and an

30
00:01:43,630 --> 00:01:47,220
L2 cache that is sitting outside of these cores, but

31
00:01:47,220 --> 00:01:49,830
it is, it is common to all the four cores.

32
00:01:49,830 --> 00:01:52,450
But of course, you can generalize this and say it is

33
00:01:52,450 --> 00:01:55,000
the last level cache, or in other words, you want to

34
00:01:55,000 --> 00:01:58,190
make sure that this, the universe of threads that are scheduled

35
00:01:58,190 --> 00:02:01,740
at any point of time, on the CPU, the sum

36
00:02:01,740 --> 00:02:06,230
total of the cache requirements of the universe of thread schedule

37
00:02:06,230 --> 00:02:09,830
on the processor, is less than the total capacity

38
00:02:09,830 --> 00:02:12,578
of the last level cache in the CPU. Because

39
00:02:12,578 --> 00:02:14,598
if it's missing the last level cache on the

40
00:02:14,598 --> 00:02:17,750
CPU, you're going outside the chip, out of memory,

41
00:02:17,750 --> 00:02:20,578
long latency operation, bad news. That's the thing that

42
00:02:20,578 --> 00:02:23,870
you're trying to do. So we're going to categorize threads

43
00:02:23,870 --> 00:02:27,935
as either cache frugal threads, or cache hungry threads.

44
00:02:27,935 --> 00:02:31,260
So cache frugal threads are one in, ones that require

45
00:02:31,260 --> 00:02:34,650
only a small portion of the cache to keep them

46
00:02:34,650 --> 00:02:36,940
happy. On the other hand, a cache hungry thread is

47
00:02:36,940 --> 00:02:39,640
one that requires a huge amount of cache space in

48
00:02:39,640 --> 00:02:43,100
order to keep it happy, meaning that the working set of

49
00:02:43,100 --> 00:02:45,980
cache hungry threads is much bigger than the working set

50
00:02:45,980 --> 00:02:49,080
of cache frugal threads. Now how do we know which

51
00:02:49,080 --> 00:02:52,500
threads are cache frugal and which threads are cache hungry?

52
00:02:52,500 --> 00:02:56,570
Well that's something that we can know only by profiling the

53
00:02:56,570 --> 00:02:59,310
execution of the threads overtime. So the assumption is

54
00:02:59,310 --> 00:03:02,420
that many of these threads get to run on

55
00:03:02,420 --> 00:03:05,400
the CPU over and over again, so overtime you

56
00:03:05,400 --> 00:03:09,070
can profile these threads and figure out whether a

57
00:03:09,070 --> 00:03:12,230
particular thread belongs to this category of cache frugal

58
00:03:12,230 --> 00:03:15,390
thread or this category of cache hungry thread. And

59
00:03:15,390 --> 00:03:18,520
the criterion that you want to use in picking the

60
00:03:18,520 --> 00:03:21,610
set of threads to be populated in the CPU

61
00:03:21,610 --> 00:03:25,920
at any point of time from the pool of available threads, is to make sure that

62
00:03:25,920 --> 00:03:28,970
the sum of the cache requirement of all

63
00:03:28,970 --> 00:03:30,710
the cache frugal threads is that there are

64
00:03:30,710 --> 00:03:35,470
N cache frugal threads and there are M cache hungry

65
00:03:35,470 --> 00:03:41,320
threads, then the cumulative cache requirement of all the threads

66
00:03:41,320 --> 00:03:46,700
put together is less than the total size of the L2 cache.

67
00:03:46,700 --> 00:03:49,380
And then I told you, we can generalize this L2 cache

68
00:03:49,380 --> 00:03:51,750
to the last level cache, that is the cache that is

69
00:03:51,750 --> 00:03:55,446
sitting at the last level inside the CPU beyond which you

70
00:03:55,446 --> 00:03:58,030
had to go out of the chip, go out to memory,

71
00:03:58,030 --> 00:04:01,950
and so that last level cache becomes the determinant in saying

72
00:04:01,950 --> 00:04:06,180
whether the size of that last level cache is within bounds

73
00:04:06,180 --> 00:04:09,500
of the cache requirements of all the threads that I want to

74
00:04:09,500 --> 00:04:12,100
schedule. So this is the set of threads that I want to pick,

75
00:04:12,100 --> 00:04:15,090
where, in this particular case, since the total

76
00:04:15,090 --> 00:04:17,700
number of hardware threads that I have available to

77
00:04:17,700 --> 00:04:19,910
me is 16, I want to make sure that

78
00:04:19,910 --> 00:04:23,330
M, the cache hungry threads, N, the cache frugal

79
00:04:23,330 --> 00:04:27,890
threads, is 16 and this inequality is satisfied

80
00:04:27,890 --> 00:04:30,630
as well. So that's what we want to shoot

81
00:04:30,630 --> 00:04:37,330
for in picking the set of threads to run on the processor at any point of time.

82
00:04:37,330 --> 00:04:40,070
I mentioned that we have to profile these

83
00:04:40,070 --> 00:04:43,090
threads, or monitor these threads as they're executing

84
00:04:43,090 --> 00:04:46,230
in order to figure out their cache occupancy

85
00:04:46,230 --> 00:04:49,060
over time so that we can categorize these

86
00:04:49,060 --> 00:04:52,590
threads as cache frugal or cache hungry, and

87
00:04:52,590 --> 00:04:56,450
the more information the scheduler has, the better

88
00:04:56,450 --> 00:05:00,570
decision it can take in terms of scheduling.

89
00:05:00,570 --> 00:05:02,400
Be we have to be careful about that.

90
00:05:02,400 --> 00:05:05,340
In order for the system to do this monitoring

91
00:05:05,340 --> 00:05:09,430
and profiling, clearly, the operating system has to lose some

92
00:05:09,430 --> 00:05:12,670
work in the middle of these threads doing useful

93
00:05:12,670 --> 00:05:17,200
work. And I always maintain that a good operating system

94
00:05:17,200 --> 00:05:19,670
gives you the resources that you need and gets

95
00:05:19,670 --> 00:05:22,270
out of the way very quickly. And so, you have

96
00:05:22,270 --> 00:05:24,770
to be very careful about the amount of time

97
00:05:24,770 --> 00:05:27,480
that the operating system takes in terms of doing this

98
00:05:27,480 --> 00:05:30,650
kind of monitoring and profiling, and this information is

99
00:05:30,650 --> 00:05:33,550
useful in scheduling decisions, but it should not be

100
00:05:33,550 --> 00:05:37,390
disrupting useful work that these guys have to do

101
00:05:37,390 --> 00:05:39,903
in the first place. Or in other words, the

102
00:05:39,903 --> 00:05:43,676
overhead for information gathering has to be kept minimal

103
00:05:43,676 --> 00:05:46,140
so that the OS does not consume too many

104
00:05:46,140 --> 00:05:49,605
cycles in doing this kind of overhead work accounting

105
00:05:49,605 --> 00:05:53,480
for making better decisions in, in terms of scheduling.
