1
00:00:00,137 --> 00:00:03,874
The computers we're using in this class are termed heterogeneous.

2
00:00:03,874 --> 00:00:08,023
They have two different processors in them, the CPU and the GPU.

3
00:00:08,023 --> 00:00:14,585
Now, if you write a plain C program, your code will only allow you to use the CPU to run your program.

4
00:00:14,585 --> 00:00:17,787
So how do we write code that will run on the GPU?

5
00:00:17,787 --> 00:00:20,092
That's where CUDA comes in.

6
00:00:20,092 --> 00:00:24,997
The CUDA programming model allows us to program both processors with one program

7
00:00:24,997 --> 00:00:28,170
so that we can use the power of the GPU in our programs.

8
00:00:28,170 --> 00:00:32,488
CUDA supports numerous languages, but in this class we're using C.

9
00:00:32,488 --> 00:00:37,160
Now, part of your CUDA program is plain C and it will run on your CPU.

10
00:00:37,160 --> 00:00:39,329
CUDA calls this the host.

11
00:00:39,329 --> 00:00:42,566
The other part of your problem will run on the GPU in parallel.

12
00:00:42,566 --> 00:00:46,485
It's also written in C but with some extensions that we use to express parallelism.

13
00:00:46,485 --> 00:00:49,673
The CUDA term for your GPU is the device.

14
00:00:49,673 --> 00:00:53,911
Then, the CUDA compiler will compile your program, split it into pieces

15
00:00:53,911 --> 00:00:58,614
that will run on the CPU and the GPU, and generate code for each.

16
00:00:58,614 --> 00:01:04,855
CUDA assumes that the device, the GPU, is a co-processor to the host, the CPU.

17
00:01:04,855 --> 00:01:11,064
It also assumes that both the host and the device have their own separate memories where they store data.

18
00:01:11,064 --> 00:01:14,593
In the systems we use in this class, both the CPU and the GPU

19
00:01:14,593 --> 00:01:18,432
have their own physical dedicated memory in the form of DRAM,

20
00:01:18,432 --> 00:01:22,934
with the GPU's memory typically being a very high performance block of memory.

21
00:01:22,934 --> 00:01:27,938
Now, in this relationship between CPU and GPU, the CPU is in charge.

22
00:01:27,938 --> 00:01:33,277
It runs the main program, and it sends directions to the GPU to tell it what to do.

23
00:01:33,277 --> 00:01:36,718
It's the part of the system that's responsible for the following.

24
00:01:36,718 --> 00:01:41,330
One, moving data from the CPU's memory to the GPU's memory.

25
00:01:41,330 --> 00:01:45,190
Two, moving data from the GPU back to the CPU.

26
00:01:45,190 --> 00:01:50,030
Now, in the C programming language, moving data from one place to another is called Memcpy.

27
00:01:50,030 --> 00:01:54,973
So, it makes sense that in CUDA, this command, either moving data from the CPU to the GPU

28
00:01:54,973 --> 00:01:58,875
or moving data from the GPU to the CPU, is called cudaMemcpy.

29
00:01:58,875 --> 00:02:05,285
Three, allocating memory on the GPU, and in C this command is Malloc, so in CUDA, it's cudaMalloc.

30
00:02:05,285 --> 00:02:10,155
And four, invoking programs on the GPU that compute things in parallel.

31
00:02:10,155 --> 00:02:12,261
These programs are called kernels.

32
00:02:12,261 --> 00:02:14,597
And, here's a lot of jargon in one phrase.

33
00:02:14,597 --> 00:02:18,566
We say that the host launches kernels on the device.
