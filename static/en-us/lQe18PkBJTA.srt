1
00:00:00,520 --> 00:00:01,600
I'm here with Kathleen Mullaney,

2
00:00:01,600 --> 00:00:04,019
who's the Vice President
of Careers here at Udacity.

3
00:00:04,019 --> 00:00:05,660
Kathleen, thank you so
much for talking to us today.

4
00:00:05,660 --> 00:00:06,750
>> Thank you, David.

5
00:00:06,750 --> 00:00:09,950
>> So I want to talk to Kathleen
about issues of gender in educational

6
00:00:09,950 --> 00:00:11,240
technology.

7
00:00:11,240 --> 00:00:14,700
So the first thing I'd ask is, there
is a trend in educational technology,

8
00:00:14,700 --> 00:00:16,860
that because the technology
industry is so dominated by men,

9
00:00:16,860 --> 00:00:21,440
the things we create in Ed Tech,
are often very oriented towards men.

10
00:00:21,440 --> 00:00:25,120
So what can we do to kind of try and
break that cycle?

11
00:00:25,120 --> 00:00:29,780
>> When I first started at Udacity back
when we were creating the first courses,

12
00:00:29,780 --> 00:00:33,080
we realized that this was actually
going to become quite prevalent.

13
00:00:33,080 --> 00:00:35,870
The first courses we
launched were CS 101,

14
00:00:35,870 --> 00:00:39,740
which is into to computer
programming in Python.

15
00:00:39,740 --> 00:00:42,030
The focus was how to
build a search engine.

16
00:00:42,030 --> 00:00:48,750
And the second course was an A.I. course
focused on how to program a robotic car.

17
00:00:48,750 --> 00:00:51,660
And those seem pretty agnostic.

18
00:00:51,660 --> 00:00:55,370
Now a car and
a search engine are not gender based.

19
00:00:55,370 --> 00:00:59,960
But, there is a tendency to view
those actually as more male oriented.

20
00:00:59,960 --> 00:01:02,880
Like cars in general
are more focused on men.

21
00:01:02,880 --> 00:01:07,440
And we realize that, actually just
choosing topics for the courses and

22
00:01:07,440 --> 00:01:10,970
the examples that we use in courses
could have a great effect on whether or

23
00:01:10,970 --> 00:01:15,350
not men or women would be attracted
to take the course to begin with.

24
00:01:15,350 --> 00:01:17,920
It's like one of the things that
we really focused on was trying,

25
00:01:17,920 --> 00:01:22,230
after we saw those first courses and
thought about how we want to market

26
00:01:22,230 --> 00:01:25,500
future courses, we actually took
a more broadened perspective.

27
00:01:25,500 --> 00:01:29,480
And we tried to actually make things
as agnostic as we possibly could, and

28
00:01:29,480 --> 00:01:30,740
had much more attention to it.

29
00:01:30,740 --> 00:01:34,130
So the first thing that I always tell
people is that you didn't need to be

30
00:01:34,130 --> 00:01:37,410
aware of the stereotypes that exist for
everybody.

31
00:01:37,410 --> 00:01:40,800
Including I have many stereotypes
of men and women myself,

32
00:01:40,800 --> 00:01:43,260
even though I am very well
educated on this topic.

33
00:01:43,260 --> 00:01:47,766
And so keeping those in mind, and
keeping them at your forefront when

34
00:01:47,766 --> 00:01:52,274
you're designing any kind of
materials it is crucially important,

35
00:01:52,274 --> 00:01:57,260
making sure that you're not playing
into any standard gender archetypes.

36
00:01:57,260 --> 00:02:01,306
So another example from
the History of Udacity is

37
00:02:01,306 --> 00:02:05,760
that we produce this course
on Tales from the Genome.

38
00:02:05,760 --> 00:02:07,820
So it was about genetics.

39
00:02:07,820 --> 00:02:13,470
And we had a male and
a female instructor, and

40
00:02:13,470 --> 00:02:18,750
the male was the expert, and
the female was the novice.

41
00:02:18,750 --> 00:02:21,180
And they played this really
nice dynamic on film,

42
00:02:21,180 --> 00:02:24,380
where the novice would
ask the expert questions.

43
00:02:24,380 --> 00:02:27,920
There were times,
though, in the filming,

44
00:02:27,920 --> 00:02:31,370
that we realized that we were
essentially portraying the female

45
00:02:31,370 --> 00:02:34,410
as this dumb person who didn't
know the answers to things.

46
00:02:34,410 --> 00:02:37,260
When really, she's very intelligent and
has her own background.

47
00:02:37,260 --> 00:02:39,290
She's just not a geneticist.

48
00:02:39,290 --> 00:02:41,230
And that was what we were
trying to portray there.

49
00:02:41,230 --> 00:02:43,730
We didn't want to say that one
person is smarter than the other,

50
00:02:43,730 --> 00:02:46,470
but this person just doesn't
have this expertise.

51
00:02:46,470 --> 00:02:49,850
And it was something that we
didn't actually realize until

52
00:02:49,850 --> 00:02:51,690
pretty far into filming the course.

53
00:02:51,690 --> 00:02:55,370
And we went through and
actually looked through every video and

54
00:02:55,370 --> 00:02:59,890
actually said, do we feel that this is
portraying women in a negative light?

55
00:02:59,890 --> 00:03:04,180
Or playing into these easy
archetypes that, films so

56
00:03:04,180 --> 00:03:05,890
often use, a woman in distress.

57
00:03:05,890 --> 00:03:06,480
She needs help.

58
00:03:06,480 --> 00:03:10,100
She needs to be saved by this
knight in shining armor.

59
00:03:10,100 --> 00:03:12,990
And we ended up cutting a lot
of content and changing it,

60
00:03:12,990 --> 00:03:16,680
because we realized we just don't want
to be portraying women in that light.

61
00:03:18,300 --> 00:03:23,000
Another example just to, again, to give
you an idea of how much this is very

62
00:03:23,000 --> 00:03:28,330
prevalent and how much we don't even
realize it, is that there's a video in

63
00:03:28,330 --> 00:03:33,240
one of our job rescript courses that had
a complaint from a student that said why

64
00:03:33,240 --> 00:03:38,590
are you playing into these gender roles?

65
00:03:38,590 --> 00:03:40,090
Haven't you heard of stereotype threat?

66
00:03:40,090 --> 00:03:42,020
If you haven't heard
of stereotype threat,

67
00:03:42,020 --> 00:03:45,290
that's where basically
reinforcing a stereotype

68
00:03:46,480 --> 00:03:51,440
in the context where a stereotype
is typically threatened.

69
00:03:51,440 --> 00:03:57,480
I guess that's the name, is more likely
to cause those issues than not, so, for

70
00:03:57,480 --> 00:04:03,090
example, women are told by society that
they're not as good at math as men are.

71
00:04:03,090 --> 00:04:06,780
And if you ask the woman to state her
gender before she takes a math test,

72
00:04:06,780 --> 00:04:09,290
she will do worse on that math test.

73
00:04:09,290 --> 00:04:13,070
And so, that's what stereotype is,
as an example.

74
00:04:13,070 --> 00:04:14,700
And what we realized is that,

75
00:04:14,700 --> 00:04:16,839
we were naming variables
in this JavaScript course.

76
00:04:16,839 --> 00:04:18,700
Completely what we thought was benign.

77
00:04:18,700 --> 00:04:22,930
But somebody saw that, you know,
we named one of the variables gal.

78
00:04:22,930 --> 00:04:26,230
The variable it was supposed to be
a hero so it was definitely portraying

79
00:04:26,230 --> 00:04:31,310
women in a positive light, but
pointing out that there are women

80
00:04:31,310 --> 00:04:36,250
in a situation where they may already
feel that they're not confident.

81
00:04:36,250 --> 00:04:40,780
May actually cause people to
think of their abilities as less,

82
00:04:40,780 --> 00:04:46,750
which I don't necessarily agree in this
scenario but just to point out the of

83
00:04:46,750 --> 00:04:50,810
the issues that may come up and how
much you do need to be aware of them.

84
00:04:50,810 --> 00:04:55,200
So I would say like the basic thing that
you can do is to educate yourself on

85
00:04:55,200 --> 00:04:59,380
on the dynamics that
we know exist today.

86
00:04:59,380 --> 00:05:02,900
So talking about stereotypes and
bias in particular.

87
00:05:02,900 --> 00:05:06,260
And not only with gender, though I don't
if we're talking about diversity at

88
00:05:06,260 --> 00:05:09,250
large, but
as with any underrepresented group.

89
00:05:09,250 --> 00:05:13,530
In computer science or
in technology in general.

90
00:05:13,530 --> 00:05:14,890
>> Yeah.
Do you know, and

91
00:05:14,890 --> 00:05:17,990
I assume your familiar with imposter
syndrome in this context as well, can

92
00:05:17,990 --> 00:05:20,430
you comment on that in the same kind of-
>> Yeah,

93
00:05:20,430 --> 00:05:22,300
it's very similar to stereotyping.

94
00:05:22,300 --> 00:05:24,220
They go hand in hand oftentimes.

95
00:05:24,220 --> 00:05:27,650
So imposter syndrome is that,
and I feel this all the time.

96
00:05:27,650 --> 00:05:30,460
I'm a high-level executive at this
company, and I still feel this.

97
00:05:30,460 --> 00:05:32,810
>> I have a PhD, and
I feel the exact same way.

98
00:05:32,810 --> 00:05:34,557
>> Yeah.
[LAUGH] It's like oh I don't

99
00:05:34,557 --> 00:05:36,300
have the answer to this yeah.

100
00:05:36,300 --> 00:05:41,100
It's basically you feel that
you don't have the expertise

101
00:05:41,100 --> 00:05:45,990
that you need to fulfill a position or
fulfill a job.

102
00:05:45,990 --> 00:05:49,900
And this is actually one of
the things that I mentor girls,

103
00:05:49,900 --> 00:05:54,070
especially the girls who code groups
that nobody knows what they're doing.

104
00:05:54,070 --> 00:05:55,390
Nobody, I mean it.

105
00:05:55,390 --> 00:05:58,450
Like I don't know any one person who
actually knows what they're doing all

106
00:05:58,450 --> 00:05:59,710
the time.

107
00:05:59,710 --> 00:06:03,300
And it's fun to actually at least be
in the start of environment because

108
00:06:03,300 --> 00:06:05,900
we try to raise that fact as
much as we possibly really can.

109
00:06:05,900 --> 00:06:08,950
I don't know if Sebastian liked
to mention his failure award.

110
00:06:08,950 --> 00:06:13,161
So if you try to break the company,
and we've succeeded at various points.

111
00:06:13,161 --> 00:06:14,372
>> [LAUGH] Yes we have.

112
00:06:14,372 --> 00:06:17,470
>> He rewards you with a bottle of wine,
a nice bottle of wine.

113
00:06:17,470 --> 00:06:20,680
And the whole point here is that
it's said to highlight this fact,

114
00:06:20,680 --> 00:06:23,270
I think at least,
to counter this imposter syndrome,

115
00:06:23,270 --> 00:06:28,170
that you're acting out of fear because
you think that you can't do something.

116
00:06:28,170 --> 00:06:31,140
Because you're not smart enough or
you don't have the right experience or

117
00:06:31,140 --> 00:06:35,640
the number of years of experience or
the specific background.

118
00:06:35,640 --> 00:06:38,420
A number of people at Udacity
work in computer science.

119
00:06:38,420 --> 00:06:40,350
They don't have computer
science degrees.

120
00:06:40,350 --> 00:06:44,030
A number of our students in particular
do not have computer science degrees but

121
00:06:44,030 --> 00:06:46,880
are looking to get into
the technical field.

122
00:06:46,880 --> 00:06:52,210
One of the most frequent questions
I receive is really focusing on

123
00:06:52,210 --> 00:06:57,000
how can I get a job here when
they say a CS degree is required.

124
00:06:57,000 --> 00:07:01,090
I have to counsel people on they say
this but that's really just this

125
00:07:01,090 --> 00:07:04,800
shortcut that people use to say
people just want a certain level of

126
00:07:04,800 --> 00:07:09,040
understanding of these topics or
a certain level of work experience.

127
00:07:09,040 --> 00:07:12,590
And time after time I talk to employers
and actually tried to assist this out.

128
00:07:12,590 --> 00:07:15,460
And they say, no,
we don't actually want assist really,

129
00:07:15,460 --> 00:07:18,540
we just really want to know
like do you know this stuff.

130
00:07:18,540 --> 00:07:21,240
And so that's what we focus on
Udacity its like know the stuff, and

131
00:07:21,240 --> 00:07:25,770
do it in a way that is more flexible and
it meet your needs rather than having to

132
00:07:25,770 --> 00:07:28,080
go back and
get a second degree often times.

133
00:07:28,080 --> 00:07:28,960
>> Yeah.
>> That

134
00:07:28,960 --> 00:07:30,697
was a very long answer to that question.

135
00:07:30,697 --> 00:07:31,770
[LAUGH]
>> That's all right.

136
00:07:31,770 --> 00:07:34,670
I ask on imposter syndrome because
we've done some interesting research on

137
00:07:34,670 --> 00:07:35,890
imposter syndrome in the OMS.

138
00:07:35,890 --> 00:07:39,990
And it's interesting that the research
came out that suggested that

139
00:07:39,990 --> 00:07:43,620
people who experience imposter
syndrome more strongly

140
00:07:43,620 --> 00:07:46,150
are less likely to take a chance on
an experimental program like this.

141
00:07:46,150 --> 00:07:47,170
>> Yes, exactly.
>> Because

142
00:07:47,170 --> 00:07:49,360
if it doesn't end up panning out,
it reflects poorly on me.

143
00:07:49,360 --> 00:07:51,570
And I'm already an imposter,
so now people recognize that.

144
00:07:51,570 --> 00:07:52,150
>> Yeah, yeah.
>> And so,

145
00:07:52,150 --> 00:07:54,330
I think combating that in
this program is really,

146
00:07:54,330 --> 00:07:56,920
really important because it
helps increase our diversity.

147
00:07:56,920 --> 00:07:59,780
Because the people who more often
experience that are the ones we

148
00:07:59,780 --> 00:08:01,590
really want in this program anyway.

149
00:08:01,590 --> 00:08:05,810
>> Yeah, have you talked about growth or
fixed mindsets in that context?

150
00:08:05,810 --> 00:08:08,480
>> We've talked about it a little
bit in one of the other lessons

151
00:08:08,480 --> 00:08:10,430
on motivation and meta cognition.

152
00:08:10,430 --> 00:08:11,480
So yeah.
>> Yeah, one of

153
00:08:11,480 --> 00:08:15,440
the best ways to counteract an imposter
syndrome is to have a growth mindset

154
00:08:15,440 --> 00:08:19,530
intervention to highlight that, you
know making mistakes helps you learn.

155
00:08:19,530 --> 00:08:21,440
It doesn't actually show
that you're stupid or

156
00:08:21,440 --> 00:08:24,310
that you don't have
the ability to do something.

157
00:08:24,310 --> 00:08:25,840
>> Yeah.
In fact, no one ever learns

158
00:08:25,840 --> 00:08:27,380
really anything except for by failure.

159
00:08:27,380 --> 00:08:28,140
>> Yeah exactly.

160
00:08:28,140 --> 00:08:31,000
>> So Kathleen will agree with me that
mindset is the one book that everyone

161
00:08:31,000 --> 00:08:31,758
should read I assume.

162
00:08:31,758 --> 00:08:33,010
>> Yep. Yep. [LAUGH]
>> And so

163
00:08:33,010 --> 00:08:34,760
we talk a lot about doing
this in Ed Tech and

164
00:08:34,760 --> 00:08:37,049
how to design educational technology
initiatives that do this.

165
00:08:37,049 --> 00:08:39,929
But many of our students probably
won't go into this field, but

166
00:08:39,929 --> 00:08:42,520
they'll go into fields that
suffer from these same issues.

167
00:08:42,520 --> 00:08:43,039
>> Yep, totally.
>> So

168
00:08:43,039 --> 00:08:44,400
just when they're on
the job in the future,

169
00:08:44,400 --> 00:08:47,340
what can they do to try and
improve diversity in their workplace,

170
00:08:47,340 --> 00:08:50,990
without being kind of pandering, or
some of the things we more often see?

171
00:08:50,990 --> 00:08:56,220
>> Sure, yeah, I guess the first
thing is that if you are not

172
00:08:56,220 --> 00:09:00,410
an underrepresented group that
doesn't mean you can't participate in

173
00:09:01,640 --> 00:09:06,770
trying to actually alleviate
the issues that these groups encounter.

174
00:09:07,960 --> 00:09:09,698
If you are a white male, sorry Dave.

175
00:09:09,698 --> 00:09:12,470
>> It's okay.
>> The things you can do are educate

176
00:09:12,470 --> 00:09:15,288
yourself and help promote
these issues to other people.

177
00:09:15,288 --> 00:09:18,390
I noticed actually when I first
brought up the issue of bias and

178
00:09:18,390 --> 00:09:23,410
stereotype with the company here, one of
the things that came up, is you're like,

179
00:09:23,410 --> 00:09:28,330
I am a white man, I feel like I can't
participate, because I'm the enemy.

180
00:09:28,330 --> 00:09:29,710
No, you're not the enemy,

181
00:09:29,710 --> 00:09:33,655
that's actually not what
we're trying to say here.

182
00:09:33,655 --> 00:09:37,080
A stereotype is a shortcut
in your cognitive process.

183
00:09:37,080 --> 00:09:39,850
It's something that you see something
over and over and over again, and

184
00:09:39,850 --> 00:09:41,620
your brain just makes a shortcut.

185
00:09:41,620 --> 00:09:45,010
And it helps you think about things.

186
00:09:45,010 --> 00:09:47,460
It actually helps you in
your life quite a bit.

187
00:09:47,460 --> 00:09:49,240
We're not saying that
all stereotypes are bad.

188
00:09:49,240 --> 00:09:51,630
You just need to understand
the ones that are,

189
00:09:51,630 --> 00:09:56,550
and the ones that result in
poor judgment or behavior.

190
00:09:56,550 --> 00:10:02,840
And a bias also is the shortcut,
but it's also an error.

191
00:10:02,840 --> 00:10:04,170
So if you talk about like in research.

192
00:10:04,170 --> 00:10:07,670
A bias is actually just
an error in measurement.

193
00:10:07,670 --> 00:10:10,270
And so what we want to do
is actually just again,

194
00:10:10,270 --> 00:10:14,130
the awareness of these things helps
cut down the effects of them.

195
00:10:14,130 --> 00:10:19,300
I don't know the exact statistic
right now, but a pretty big amount.

196
00:10:19,300 --> 00:10:26,350
And so educate yourself on the issues
that face people in these groups.

197
00:10:26,350 --> 00:10:31,530
So for the technology industry,
it's something about

198
00:10:31,530 --> 00:10:36,600
20 to 30% female on a good day and
if that's the case,

199
00:10:36,600 --> 00:10:41,500
then what are the things you can do
to actually get, to be more equal.

200
00:10:41,500 --> 00:10:44,570
And actually, probably educating
yourself on why that's even important.

201
00:10:44,570 --> 00:10:46,397
The point isn't just to have equality,

202
00:10:46,397 --> 00:10:48,340
the point is to have
diversity of thought.

203
00:10:48,340 --> 00:10:53,670
And you're not going to have diversity
of thought if everybody looks the same,

204
00:10:53,670 --> 00:10:58,180
has had the same education,
comes from the same type of family and

205
00:10:58,180 --> 00:11:01,050
from the same like economic background.

206
00:11:01,050 --> 00:11:02,110
So businesses,

207
00:11:02,110 --> 00:11:06,883
it's actually shown you know through
research again that businesses that

208
00:11:06,883 --> 00:11:11,210
have diverse employees are more
successful than ones that are not.

209
00:11:11,210 --> 00:11:13,984
So understanding like why
you actually want this and

210
00:11:13,984 --> 00:11:17,702
the more logical reasons rather than
the emotional ones are just like I

211
00:11:17,702 --> 00:11:19,670
don't want there to be inequality.

212
00:11:21,310 --> 00:11:24,990
And actually try to
move the needle based

213
00:11:24,990 --> 00:11:30,350
on logic rather than emotional
responses to the situation.

214
00:11:30,350 --> 00:11:32,330
>> Have you read Thinking Fast and Slow?

215
00:11:32,330 --> 00:11:33,250
>> Yeah it's a good book.

216
00:11:33,250 --> 00:11:34,610
>> If you all haven't heard of it,
it's a great book.

217
00:11:34,610 --> 00:11:36,080
I wouldn't have even
connected it to this, but

218
00:11:36,080 --> 00:11:37,500
I'm glad you mentioned about shortcuts.

219
00:11:37,500 --> 00:11:40,770
It talks a lot about how
stereotypes are just kind of

220
00:11:40,770 --> 00:11:43,500
easy ways to think that
we fall into because

221
00:11:43,500 --> 00:11:46,400
we can't think deeply about every
single thing we encounter all day long.

222
00:11:46,400 --> 00:11:46,930
>> Yeah, exactly.
>> But

223
00:11:46,930 --> 00:11:49,930
being aware of them is really
the quickest way to know

224
00:11:49,930 --> 00:11:50,770
when you're doing it badly.

225
00:11:50,770 --> 00:11:53,040
>> And not feeling like
you're doing something wrong.

226
00:11:53,040 --> 00:11:54,950
>> Yeah.
>> I noticed another example.

227
00:11:54,950 --> 00:11:56,460
Hopefully theses are helpful for
you guys.

228
00:11:58,170 --> 00:12:01,930
One of the stereotypes,
and again the biases,

229
00:12:01,930 --> 00:12:07,220
that comes in to play is that women are
more often judged by their character,

230
00:12:07,220 --> 00:12:13,750
so by how likable they are, and men
are more often judged by their skills.

231
00:12:13,750 --> 00:12:17,490
Which is really ridiculous, especially
when it comes into interviewing.

232
00:12:17,490 --> 00:12:19,040
And so
it's something that I work on very,

233
00:12:19,040 --> 00:12:23,720
very much with our Udacity students
in helping them apply for jobs.

234
00:12:23,720 --> 00:12:27,680
For the most part people
are not interviewing you for

235
00:12:27,680 --> 00:12:28,550
how likable you are.

236
00:12:28,550 --> 00:12:31,850
They're interviewing you because they
want you to perform a specific function

237
00:12:31,850 --> 00:12:33,170
in a company.

238
00:12:33,170 --> 00:12:36,400
And it's important for if you're
an interviewer to remember that and

239
00:12:36,400 --> 00:12:40,100
also as an interviewee because you
want to make sure that you're not giving

240
00:12:40,100 --> 00:12:44,940
any easy paths for an interviewer
to fall into and basically say, oh,

241
00:12:44,940 --> 00:12:46,740
I'm going to choose the shortcut.

242
00:12:46,740 --> 00:12:50,950
And so the things that you can actually
do are to enforce that you have

243
00:12:50,950 --> 00:12:55,870
the skills for a job, in your rhetoric
in your answers to interview questions.

244
00:12:55,870 --> 00:12:57,300
Highlight that you have the skills.

245
00:12:57,300 --> 00:13:00,160
Don't just smile a lot and be likeable.

246
00:13:00,160 --> 00:13:01,330
That's also an important trait.

247
00:13:01,330 --> 00:13:03,810
I'm not saying that that's
not important at all,

248
00:13:03,810 --> 00:13:07,510
but you can make it easier for
yourself and for

249
00:13:07,510 --> 00:13:13,310
the people who can easily be tracked
by stereotypes to come out on top.

250
00:13:13,310 --> 00:13:17,580
I know that when I was interviewing for
a candidate I was writing all of my

251
00:13:17,580 --> 00:13:21,440
feedback about this interview that I had
just completed and I was saying how she

252
00:13:21,440 --> 00:13:26,400
had a really sunny disposition and
I was like, what am I talking about?

253
00:13:26,400 --> 00:13:27,930
That has nothing to do with this job and
for me,

254
00:13:27,930 --> 00:13:30,920
once I realized that, that's when
I decided actually to start doing

255
00:13:30,920 --> 00:13:35,760
these talks here at Udacity because if
I, this is one of my favorite topics.

256
00:13:35,760 --> 00:13:39,080
If I'm so susceptible to this despite
the amount that I read about it,

257
00:13:39,080 --> 00:13:42,030
that I do my research,
that I try to help others and

258
00:13:42,030 --> 00:13:43,570
I'm still falling into these traps.

259
00:13:43,570 --> 00:13:47,420
We need to make it as prevalent so
that I have support.

260
00:13:47,420 --> 00:13:51,820
So I get a team of people to help
me actually do better myself.

261
00:13:51,820 --> 00:13:53,920
It's kind of like if you're
trying to lose weight.

262
00:13:53,920 --> 00:13:58,640
You're much more likely to
succeed if you have some social

263
00:13:58,640 --> 00:14:01,460
pressure to help you
with achieving your goal.

264
00:14:01,460 --> 00:14:02,160
>> Yeah absolutely.

265
00:14:02,160 --> 00:14:05,050
>> So if you wife was to say,
David you're trying to lose weight, so

266
00:14:05,050 --> 00:14:09,570
let's make sure we're not keeping
cookies in the house, or whatever.

267
00:14:09,570 --> 00:14:11,170
You're going to have more support.

268
00:14:11,170 --> 00:14:11,900
And so-
>> Yeah exactly.

269
00:14:11,900 --> 00:14:12,700
Accountability.

270
00:14:12,700 --> 00:14:14,730
>> Exactly.
So this accountability is important if

271
00:14:14,730 --> 00:14:18,230
we're able to help each other learn
about these things and be active,

272
00:14:18,230 --> 00:14:22,120
rather than passive about
the efforts to improve them.

273
00:14:22,120 --> 00:14:25,970
Personally, I think that's what
would lead to more success.

274
00:14:25,970 --> 00:14:27,590
>> Yeah, absolutely, I agree completely.

275
00:14:27,590 --> 00:14:30,650
Do you have any last thoughts you
want to share with our students, as they

276
00:14:30,650 --> 00:14:33,660
prepare to embark on their projects,
or maybe wrap up their projects?

277
00:14:33,660 --> 00:14:37,740
>> Yeah, I guess one of the things that
I wish I had understood a little sooner,

278
00:14:37,740 --> 00:14:42,410
was really think about,
if you're designing a course or

279
00:14:42,410 --> 00:14:45,700
a product, think about your learner or
your user.

280
00:14:45,700 --> 00:14:49,660
And really understand who that person
is, or who that group of people are.

281
00:14:49,660 --> 00:14:52,310
And do your best to limit it,
because making a product for

282
00:14:52,310 --> 00:14:53,290
everybody will never win.

283
00:14:54,530 --> 00:14:57,210
And really try to reach that person.

284
00:14:57,210 --> 00:15:01,990
I mean I think once we were
able to actually figure out

285
00:15:01,990 --> 00:15:05,150
some of the individuals that we were
reaching with the Udacity courses.

286
00:15:05,150 --> 00:15:08,470
We got a lot better about creating
content that fit their needs.

287
00:15:10,360 --> 00:15:13,750
I think really just focus on your user,
and make sure that

288
00:15:13,750 --> 00:15:18,680
you know who that person is, so that
you are solving a problem for them, or

289
00:15:18,680 --> 00:15:22,570
you're helping them achieve whatever
goal that they're trying to achieve.

290
00:15:22,570 --> 00:15:23,110
>> Make sense.
>> Yeah.

291
00:15:23,110 --> 00:15:23,700
>> Well great.

292
00:15:23,700 --> 00:15:25,770
Thank you so much for
talking to us today, Kathleen.

293
00:15:25,770 --> 00:15:26,770
>> Thanks for having me.

294
00:15:26,770 --> 00:15:27,852
Good luck with your projects.

295
00:15:27,852 --> 00:15:31,126
[SOUND]
>> [LAUGH]
