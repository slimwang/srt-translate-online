1
00:00:00,000 --> 00:00:02,923
So the analyze stage is really all about profiling the whole application,

2
00:00:02,923 --> 00:00:05,552
looking not just at the kernels that you intend to parallelize

3
00:00:05,552 --> 00:00:07,742
but looking at the whole thing and trying to figure out,

4
00:00:07,742 --> 00:00:10,576
where can this application benefit from parallelization,

5
00:00:10,576 --> 00:00:13,866
and how much can you expect to benefit?

6
00:00:13,866 --> 00:00:17,233
So once you've decided that there's a region of the code that you need to parallelize,

7
00:00:17,233 --> 00:00:19,998
of course there's different approaches to doing this.

8
00:00:19,998 --> 00:00:22,766
Certainly no code is better than the code you don't have to write at all.

9
00:00:22,766 --> 00:00:25,241
So if you can find that there's a parallel library for the GPU

10
00:00:25,241 --> 00:00:30,272
that already does exactly what you want, then great, you're done. You can simply call out to that library.

11
00:00:30,272 --> 00:00:33,881
Sometimes you have a lot of code and you want to be able to instrument it,

12
00:00:33,881 --> 00:00:37,451
you want to do a minimal amount of work to get a little bit of parallelization,

13
00:00:37,451 --> 00:00:41,194
and there's an approach for this called Directives.

14
00:00:41,194 --> 00:00:43,862
The most well known one on the CPU is called OpenMP.

15
00:00:43,862 --> 00:00:48,294
There's another cross-platform standard called OpenACC which has emerged,

16
00:00:48,294 --> 00:00:50,962
which ACC stands for accelerator.

17
00:00:50,962 --> 00:00:56,555
This is sort of an extension of OpenMP to encompass the ideas of accelerators like the GPU.

18
00:00:56,555 --> 00:00:59,102
And so if you're looking at GPU programming, OpenACC

19
00:00:59,102 --> 00:01:02,467
is a really lightweight way to experiment with it.

20
00:01:02,467 --> 00:01:06,841
But of course sometimes what you want to do is actually go in and write a parallel routine,

21
00:01:06,841 --> 00:01:11,602
and naturally that's what we've been focusing on throughout this course using CUDA C++.

22
00:01:11,602 --> 00:01:15,110
So of course we're going to focus here.

23
00:01:15,110 --> 00:01:18,410
So assuming that you're going to be coding something up from scratch for this purpose,

24
00:01:18,410 --> 00:01:22,364
the next choice is how to pick an algorithm. And this is a huge deal.

25
00:01:22,364 --> 00:01:26,185
This is your real chance to make a huge improvement. Pick the right algorithm.

26
00:01:26,185 --> 00:01:30,542
So all the bit twiddling optimization in the world won't gain you nearly as much

27
00:01:30,542 --> 00:01:34,373
as choosing the right sort of fundamentally parallel-friendly algorithm.

28
00:01:34,373 --> 00:01:36,474
I'll give a couple of simple examples in this unit,

29
00:01:36,474 --> 00:01:38,643
and you'll see more examples in Unit 6.

30
00:01:38,643 --> 00:01:40,552
There's no recipe for picking the right algorithm.

31
00:01:40,552 --> 00:01:44,457
What you need to do is sort of think deeply about what is the parallelism in your problem.

32
00:01:44,457 --> 00:01:47,693
We'll try to give you a few examples of that so that you have a little practice.

33
00:01:47,693 --> 00:01:50,397
So once you've decided how to parallelize your algorithm,

34
00:01:50,397 --> 00:01:55,336
then you want to optimize the code, and there will be sort of a cycle between these.

35
00:01:55,336 --> 00:01:58,296
As you'll see in the example, you try a parallelization,

36
00:01:58,296 --> 00:02:02,539
study how well it does, suggest some changes that might suggest a way

37
00:02:02,539 --> 00:02:05,912
that you approach the parallelization differently.

38
00:02:05,912 --> 00:02:10,447
So we're really talking about profile-driven optimization, by which I mean measure it.

39
00:02:10,447 --> 00:02:16,380
Measure, measure, measure, measure how fast things are going and use that to base your decisions.

40
00:02:16,380 --> 00:02:19,651
Don't just sort of take a guess at what's going to work well and what doesn't.

41
00:02:19,651 --> 00:02:21,591
And finally, the deploy step.

42
00:02:21,591 --> 00:02:24,169
So look, in this class we do these little homeworks,

43
00:02:24,169 --> 00:02:28,201
and they're fairly self-contained so that you can get them done in a reasonable amount of time.

44
00:02:28,201 --> 00:02:32,729
So the process of actually deploying a GPU-accelerated code into real use

45
00:02:32,729 --> 00:02:34,771
is not going to come up a lot in this class,

46
00:02:34,771 --> 00:02:37,617
so consider this just sort of free software engineering advice.

47
00:02:37,617 --> 00:02:42,242
When you're working on a real code, it's a really, really bad idea to optimize in a vacuum.

48
00:02:42,242 --> 00:02:46,180
And what I mean by this is that you can easily spend tons of time

49
00:02:46,180 --> 00:02:49,272
adding tons of unnecessary complexity to your code,

50
00:02:49,272 --> 00:02:53,177
speeding up a kernel way past the point where it's no longer a bottleneck.

51
00:02:53,177 --> 00:02:58,378
And it's just fundamentally a good idea to push any improvements through to the end code.

52
00:02:58,378 --> 00:03:01,296
You're making it real by doing that.

53
00:03:01,296 --> 00:03:04,701
And making it real as soon as possible ensures that you're running and profiling

54
00:03:04,701 --> 00:03:07,181
and parallelizing and optimizing real workloads.

55
00:03:07,181 --> 00:03:11,617
And the other thing to remember is that even small improvements are useful.

56
00:03:11,617 --> 00:03:16,473
If you make your code 50% faster or 2 times faster or 4 times faster,

57
00:03:16,473 --> 00:03:19,677
that's useful and you can push it out to the users of your code;

58
00:03:19,677 --> 00:03:21,990
you can start employing it and making it real,

59
00:03:21,990 --> 00:03:26,572
even if you think that there's a factor of 20 times speedup in the wings.

60
00:03:26,572 --> 00:03:31,695
The advice here again is be disciplined, analyze what you're doing,

61
00:03:31,695 --> 00:03:34,298
decide how you're going to parallelize it,

62
00:03:34,298 --> 00:03:38,429
decide how you're going to optimize it by studying that code

63
00:03:38,429 --> 00:03:41,639
and measuring it using profile-driven optimization,

64
00:03:41,639 --> 00:03:46,936
and finally, be sure to deploy frequently. So deploy early and often.

65
00:03:46,936 --> 00:03:50,386
Remember that this whole thing is intended to be a cycle.
