1
00:00:00,910 --> 00:00:02,580
Alright so here's how we're going to look at this. So as

2
00:00:02,580 --> 00:00:07,110
you may recall, in this housing example. If we look at different

3
00:00:07,110 --> 00:00:09,840
degrees of polynomials and how well they fit the data. Let's

4
00:00:09,840 --> 00:00:13,150
look at the training error. The per example training error. So how

5
00:00:13,150 --> 00:00:15,812
far off is it for each of the data points? And

6
00:00:15,812 --> 00:00:19,396
as we increase the degree of the polynomial from constant to linear

7
00:00:19,396 --> 00:00:22,276
to quadratic and all the way up to, when this case

8
00:00:22,276 --> 00:00:25,920
order six, the error's always falling. As you go up, you have

9
00:00:25,920 --> 00:00:28,080
more ability to fit the data, closer and closer and

10
00:00:28,080 --> 00:00:30,930
closer, right? because, each of these models is, is nested

11
00:00:30,930 --> 00:00:33,220
inside the other. We can always go back. If the

12
00:00:33,220 --> 00:00:37,070
zero fits best and I give you six degrees of freedom,

13
00:00:37,070 --> 00:00:40,420
you can still fit the zero. So, that's what happens

14
00:00:40,420 --> 00:00:42,240
with the training error, but now let's use this idea

15
00:00:42,240 --> 00:00:44,600
of cross validation to say what if we split the

16
00:00:44,600 --> 00:00:51,190
data up into chunks and have each chunk being predicted by

17
00:00:51,190 --> 00:00:53,170
the, the rest of the data? Train on the rest of the

18
00:00:53,170 --> 00:00:56,610
data, predict on the chunk. Repeat that for all the different chunks

19
00:00:56,610 --> 00:01:01,660
and average together. So, so I actually did that. And this is

20
00:01:01,660 --> 00:01:05,330
what I got with the cross validation error. So there's a I

21
00:01:05,330 --> 00:01:07,440
don't know there's a couple of interesting things to note about this

22
00:01:07,440 --> 00:01:09,810
plot. So that we see, we have this red plot that is

23
00:01:09,810 --> 00:01:13,640
constantly falling and the blue plot which is the cross validation error

24
00:01:13,640 --> 00:01:16,360
starts out a little bit higher than the, the red plot that's

25
00:01:16,360 --> 00:01:19,210
got higher error. So, why do you think that is Charles?

26
00:01:19,210 --> 00:01:20,620
>> Well that makes sense right?

27
00:01:20,620 --> 00:01:23,500
because we're actually training to minimize error.

28
00:01:23,500 --> 00:01:27,420
We're actually trying to minimize error on the training set. So the parts we

29
00:01:27,420 --> 00:01:30,190
aren't looking at, you're more likely to have some error with. That makes sense

30
00:01:30,190 --> 00:01:32,450
if you'd have a little bit more error on the data you haven't seen.

31
00:01:32,450 --> 00:01:35,990
>> Right, so, good. So, so, in the, on the, this red curve. We're

32
00:01:35,990 --> 00:01:39,720
actually predicting predicting all the different data

33
00:01:39,720 --> 00:01:41,910
points using all of those same data points.

34
00:01:41,910 --> 00:01:44,530
So it is using all the data to predict that data. This

35
00:01:44,530 --> 00:01:47,140
blue point, which is really only a little bit higher in this case,

36
00:01:47,140 --> 00:01:50,590
is using, in this particular case I used all but one of the

37
00:01:50,590 --> 00:01:54,842
examples to predict the remaining example. But it doesn't have that example when

38
00:01:54,842 --> 00:01:58,050
it's, when it's doing its fitting. So it's really predicting on a new

39
00:01:58,050 --> 00:02:00,732
example that it hasn't seen. And so of course you'd expect it to

40
00:02:00,732 --> 00:02:03,440
be a little bit worse. In this particular case, the averages are all

41
00:02:03,440 --> 00:02:06,870
pretty much the same so there's not a big difference. But now, let's,

42
00:02:06,870 --> 00:02:08,680
let's look at what happens as we start to increase the

43
00:02:08,680 --> 00:02:11,750
degree, we've got the ability to fit this data better and

44
00:02:11,750 --> 00:02:15,332
better and better, and, in fact, down at you know say,

45
00:02:15,332 --> 00:02:18,980
three and four, they're actually pretty close in terms of their

46
00:02:18,980 --> 00:02:23,630
ability to, to, to fit these examples. And then what's great,

47
00:02:23,630 --> 00:02:26,640
what's really interesting is what happens is now we start to

48
00:02:26,640 --> 00:02:28,890
give it more, the ability to fit the data closer and

49
00:02:28,890 --> 00:02:32,042
closer. And by the time we get up to, to order six

50
00:02:32,042 --> 00:02:34,870
polynomial, even though the error on the training set is

51
00:02:34,870 --> 00:02:37,470
really low, the error on this, on this cross validation

52
00:02:37,470 --> 00:02:41,320
error, the error that you, that you're measuring by predicting

53
00:02:41,320 --> 00:02:45,480
the examples that you haven't seen, is really high. And this

54
00:02:45,480 --> 00:02:48,710
is beautiful this, this inverted u, is, is exactly what

55
00:02:48,710 --> 00:02:50,820
you tend to see in these kinds of cases. That

56
00:02:50,820 --> 00:02:53,780
the error decreases as you have more power and then

57
00:02:53,780 --> 00:02:57,080
it starts to increase as you use too much [LAUGH] of

58
00:02:57,080 --> 00:03:00,280
that power. Does that make sense to you?

59
00:03:00,280 --> 00:03:03,740
>> It does make sense, so. The, the problem is that as

60
00:03:03,740 --> 00:03:05,930
we give it more and more power we're able to fit the

61
00:03:05,930 --> 00:03:09,530
data. But as it gets more and more and more power it

62
00:03:09,530 --> 00:03:16,260
tends to over fit the training data at the expense of future generalization.

63
00:03:16,260 --> 00:03:18,760
>> Right. So that's exactly how we, we referred to this is this

64
00:03:18,760 --> 00:03:22,020
sort of idea that if you don't give yourself enough degrees of freedom,

65
00:03:22,020 --> 00:03:25,250
you don't give yourself a model class that's powerful enough you

66
00:03:25,250 --> 00:03:27,680
will underfit the data. You won't be able to model what's actually

67
00:03:27,680 --> 00:03:30,440
going on and there'll be a lot of error. But if you

68
00:03:30,440 --> 00:03:33,610
give yourself too much you can overfit the data. You can actually

69
00:03:33,610 --> 00:03:36,850
start to model the error and it generalizes very poorly to

70
00:03:36,850 --> 00:03:41,410
unseen examples. And somewhere in between is kind of the goldilocks zone.

71
00:03:41,410 --> 00:03:42,750
Where we're not underfitting, and we're

72
00:03:42,750 --> 00:03:45,290
not overfitting. We're fitting just right.

73
00:03:45,290 --> 00:03:47,200
And that's the point that we really want to find. We want to

74
00:03:47,200 --> 00:03:50,820
find the model that fits the data without overfitting, and not underfitting.

75
00:03:50,820 --> 00:03:52,790
>> So what was the answer on the, housing exam?

76
00:03:52,790 --> 00:03:55,440
>> Well, so, it seems pretty clear in this,

77
00:03:55,440 --> 00:03:57,990
in this plot that it's somewhere, it's either three or

78
00:03:57,990 --> 00:03:59,660
four. It turns out, if you look at the

79
00:03:59,660 --> 00:04:02,700
actual numbers, three and four are really close. But three

80
00:04:02,700 --> 00:04:04,380
is a little bit lower. So three is actually

81
00:04:04,380 --> 00:04:06,190
the thing that fits it the best. And, in fact,

82
00:04:06,190 --> 00:04:08,430
if you look at what four does. It fits the

83
00:04:08,430 --> 00:04:12,340
data by more or less zeroing out the, the quartic

84
00:04:12,340 --> 00:04:14,550
term, right? It doesn't really use the, this power.

85
00:04:14,550 --> 00:04:16,779
>> Oh, but that's interesting. So that means

86
00:04:16,779 --> 00:04:19,940
it, it barely uses the, the, the extra degree

87
00:04:19,940 --> 00:04:22,079
of freedom you give it. But even using it

88
00:04:22,079 --> 00:04:24,880
a little bit, it still does worse than generalization.

89
00:04:24,880 --> 00:04:26,850
>> Just a tiny bit worse.

90
00:04:26,850 --> 00:04:26,910
>> Huh.

91
00:04:26,910 --> 00:04:27,750
>> Yup exactly so.

92
00:04:27,750 --> 00:04:29,560
>> That's actually kind of cool.
