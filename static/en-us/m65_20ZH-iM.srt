1
00:00:00,070 --> 00:00:01,460
>> So, let's look at the last quantity that

2
00:00:01,460 --> 00:00:03,930
we haven't talked about so far. And that is the

3
00:00:03,930 --> 00:00:08,109
probability of the hypothesis. Well, just like the probability of

4
00:00:08,109 --> 00:00:10,810
D is the prior on the data, this is in

5
00:00:10,810 --> 00:00:15,120
fact your prior on the hypothesis. So, just like

6
00:00:15,120 --> 00:00:18,420
the probability of D is a prior on the data.

7
00:00:18,420 --> 00:00:21,840
The probability of H is a prior on a particular

8
00:00:21,840 --> 00:00:25,430
hypothesis drawn from the hypothesis space. So in other words,

9
00:00:25,430 --> 00:00:30,320
in encapsulates our prior belief that one hypothesis is likely

10
00:00:30,320 --> 00:00:33,400
or unlikely compared to other hypotheses. So in fact what's

11
00:00:33,400 --> 00:00:35,720
really neat about this from a sort of AI point

12
00:00:35,720 --> 00:00:38,690
of view is that the prior ,as its called, is in

13
00:00:38,690 --> 00:00:42,032
fact our domain knowledge. So if every angle that we've

14
00:00:42,032 --> 00:00:45,106
seen so far, everything that we've said there's always some

15
00:00:45,106 --> 00:00:47,960
place where we stick in out domain knowledge. Are prior

16
00:00:47,960 --> 00:00:50,800
belief about the way the world works. Whether that's a similarity

17
00:00:50,800 --> 00:00:56,660
metric for Knn It, it's something about which

18
00:00:56,660 --> 00:00:58,710
features might be important, so we care about high

19
00:00:58,710 --> 00:01:01,540
information gain and decision trees, or our belief

20
00:01:01,540 --> 00:01:04,819
about the, the structure of a neural network. Those

21
00:01:04,819 --> 00:01:07,410
are prior beliefs, those are, that represents the

22
00:01:07,410 --> 00:01:10,180
main knowledge. And here in Bayesian Learning, here in

23
00:01:10,180 --> 00:01:12,580
this notion of, of Bayes' Rule, all of our

24
00:01:12,580 --> 00:01:16,280
prior knowledge sits here in the probability or prior

25
00:01:16,280 --> 00:01:19,320
probability over the hypotheses. Does that all make sense?

26
00:01:19,320 --> 00:01:22,630
>> Yeah its really interesting I guess. So we talked about things like

27
00:01:22,630 --> 00:01:24,880
kernels and similarity functions as ways

28
00:01:24,880 --> 00:01:26,850
of capturing this kind of domain knowledge.

29
00:01:26,850 --> 00:01:29,710
And I guess, I guess what its saying is that its maybe tending

30
00:01:29,710 --> 00:01:32,540
to prefer or assign higher probability to

31
00:01:32,540 --> 00:01:35,460
hypothesis that group things a certain way.

32
00:01:35,460 --> 00:01:37,760
>> exactly right. So, in fact, when you use something

33
00:01:37,760 --> 00:01:41,240
like Euclidian distance in KNN, what you're saying is,'Well,

34
00:01:41,240 --> 00:01:45,430
points that are closer together ought to have, similar labels, and so,

35
00:01:45,430 --> 00:01:49,010
we would believe any hypothesis that puts points that are physically close to

36
00:01:49,010 --> 00:01:52,950
one another to have similar outputs, we would say, are more likely than

37
00:01:52,950 --> 00:01:56,230
ones that put points that are very close together to have different outputs.

38
00:01:56,230 --> 00:01:56,590
>> Neat.

39
00:01:56,590 --> 00:01:58,880
>> So let me just mention one last thing

40
00:01:58,880 --> 00:02:01,590
before I give you a quiz, okay? So, see

41
00:02:01,590 --> 00:02:02,590
if this makes sense, I'm a see if you

42
00:02:02,590 --> 00:02:06,460
really understand Bayes' rule. So let's imagine that I wanted

43
00:02:06,460 --> 00:02:11,460
to know under what circumstances the, probability of a hypothesis, given the

44
00:02:11,460 --> 00:02:15,050
data, goes up. What on the right side of the equation would

45
00:02:15,050 --> 00:02:17,430
you expect to change, go up or go down, or stay the

46
00:02:17,430 --> 00:02:21,690
same, that would influence whether the probability of a hypothesis goes up.

47
00:02:21,690 --> 00:02:24,220
>> So the probability of the hypothesis given

48
00:02:24,220 --> 00:02:26,560
the data, what could make that combined quantity

49
00:02:26,560 --> 00:02:29,080
go up, so one is looking at the

50
00:02:29,080 --> 00:02:31,330
right hand side, the probability of the hypothesis,

51
00:02:31,330 --> 00:02:32,900
so, so if you have a hypothesis that has

52
00:02:32,900 --> 00:02:35,600
a higher prior, has, is more likely to be

53
00:02:35,600 --> 00:02:37,520
a good one. Before you see the data then

54
00:02:37,520 --> 00:02:39,940
that would raise it after you see the data too.

55
00:02:39,940 --> 00:02:40,180
>> Right.

56
00:02:40,180 --> 00:02:46,230
>> And I guess the probability of the data given the hypothesis should

57
00:02:46,230 --> 00:02:49,700
go up. Oh, which is kind of like accuracy. It's kind of like

58
00:02:49,700 --> 00:02:52,480
saying that if you pick a hypothesis that does a better job of

59
00:02:52,480 --> 00:02:56,980
labeling the data, then also your probability of the hypothesis will go up.

60
00:02:56,980 --> 00:02:58,580
>> Right. Anything else?

61
00:02:58,580 --> 00:03:01,000
>> I guess the probability of the data going

62
00:03:01,000 --> 00:03:03,810
down. But that's not really a change from the hypothesis.

63
00:03:03,810 --> 00:03:05,630
>> Right. But it is true that if those

64
00:03:05,630 --> 00:03:08,680
goes down, then the probability in the hypothesis can and

65
00:03:08,680 --> 00:03:09,860
the data will go up. But as you point

66
00:03:09,860 --> 00:03:13,870
out, it's not connected to the hypothesis directly. And I'll

67
00:03:13,870 --> 00:03:15,620
write in equation for you in, in just a

68
00:03:15,620 --> 00:03:17,500
moment that'll kind of make that, I think, a little

69
00:03:17,500 --> 00:03:19,650
bit clearer. Okay, but you got all this, right? So

70
00:03:19,650 --> 00:03:21,780
I think you understand it. So we got Bayes' Rule.

71
00:03:21,780 --> 00:03:23,880
And, notice what we've done. We've gone from this sort

72
00:03:23,880 --> 00:03:25,710
of general notion of saying we need to find the best

73
00:03:25,710 --> 00:03:29,120
hypothesis, to actually coming up with an equation, that sort

74
00:03:29,120 --> 00:03:32,010
of makes explicit what we mean by that. That what we

75
00:03:32,010 --> 00:03:34,730
care about is the probability of some hypothesis given the

76
00:03:34,730 --> 00:03:38,040
data. That's what we mean by best. And that, that can

77
00:03:38,040 --> 00:03:41,700
be further thought as, the probablity of us seeing, some labels

78
00:03:41,700 --> 00:03:46,820
on some data, given hypothesis. Times the probability of the hypothesis,

79
00:03:46,820 --> 00:03:49,880
even without any data whatsoever, normalized by the

80
00:03:49,880 --> 00:03:52,675
probability of the data. So let's play around with

81
00:03:52,675 --> 00:03:54,130
Bayes' rules a little bit and make certain that

82
00:03:54,130 --> 00:03:55,580
we all, we all kind of get it. Okay?

83
00:03:55,580 --> 00:03:56,090
>> Sure.

84
00:03:56,090 --> 00:03:56,510
>> Okay.
