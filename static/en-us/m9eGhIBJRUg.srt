1
00:00:00,000 --> 00:00:03,214
Finally, let's talk about optimizations at the whole system level,

2
00:00:03,214 --> 00:00:06,986
including the interactions that happen between the host and the GPU.

3
00:00:06,986 --> 00:00:11,357
So on most systems the CPU and the GPU are on different chips,

4
00:00:11,357 --> 00:00:17,127
and they communicate through something called a PCI Express bus, or PCIe for short.

5
00:00:17,127 --> 00:00:20,867
Today, for example, most systems are PCI Express Gen2,

6
00:00:20,867 --> 00:00:26,263
which can, in practice, get about 6 gigabytes per second maximum in either direction.

7
00:00:26,263 --> 00:00:30,946
PCI can transfer memory that has been page locked, or pinned,

8
00:00:30,946 --> 00:00:35,547
and it keeps a special chunk of pinned host memory set aside for this purpose.

9
00:00:35,547 --> 00:00:40,183
So when you have a chunk of memory that you want to copy over to the device memory on the GPU,

10
00:00:40,183 --> 00:00:42,687
CUDA first has to copy it into the staging area

11
00:00:42,687 --> 00:00:46,597
before it can then transfer it over the PCI Express bus onto the GPU.

12
00:00:46,597 --> 00:00:50,329
This extra copy here increases the total time taken by the memory transfer.

13
00:00:50,329 --> 00:00:53,703
Instead, you can use the cudaHostMalloc function

14
00:00:53,703 --> 00:00:56,462
to allocate pinned host memory in the first place,

15
00:00:56,462 --> 00:01:00,505
such that the memory you've got sitting on the host is already ready to copy directly over

16
00:01:00,505 --> 00:01:03,006
without this additional staging step.

17
00:01:03,006 --> 00:01:05,809
Or you can use the cudaHostRegister command

18
00:01:05,809 --> 00:01:09,115
to take some data you've already allocated and pin it,

19
00:01:09,115 --> 00:01:12,606
again avoiding the extra copy to take it into GPU memory.

20
00:01:12,606 --> 00:01:14,698
So that's why you care.

21
00:01:14,698 --> 00:01:18,946
Memory transfers from the host to device can go faster if you pin the memory first.

22
00:01:18,946 --> 00:01:23,195
And if you're transferring pinned memory, you can use the asynchronous memory transfer,

23
00:01:23,195 --> 00:01:29,096
cudaMemcpyAsync, which let's the CPU keep working while the memory transfer completes.

24
00:01:29,096 --> 00:01:31,072
How does that work?

25
00:01:31,072 --> 00:01:33,572
Well with cudaMemcpy, which you have been using until now,

26
00:01:33,572 --> 00:01:37,027
you make a call to cudaMemcpy, you pass it some information,

27
00:01:37,027 --> 00:01:39,138
like the pointers and the size and bytes

28
00:01:39,138 --> 00:01:42,732
and whether this is a transfer from device to host or host to device.

29
00:01:42,732 --> 00:01:45,122
And then there's a semicolon at the end of that statement.

30
00:01:45,122 --> 00:01:49,122
And with cudaMemcpy the CPU blocks until the transfer completes.

31
00:01:49,122 --> 00:01:53,559
With cudaMemcpyAsync, when it hits the semicolon it just keeps going.

32
00:01:53,559 --> 00:01:58,305
This kicks off an asynchronous memory transfer that continues to go on

33
00:01:58,305 --> 00:02:03,375
while control returns to the CPU and the CUDA program continues executing.

34
00:02:03,375 --> 00:02:07,643
In other words, the CPU can keep getting worked on while this memory transfers.

35
00:02:07,643 --> 00:02:10,074
So to control that asynchronous interaction,

36
00:02:10,074 --> 00:02:14,849
we have to introduce the next topic in host GPU interaction, and that's streams.
