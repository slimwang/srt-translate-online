1
00:00:00,420 --> 00:00:01,970
So one of the things that goes wrong, when you try to

2
00:00:01,970 --> 00:00:07,120
actually run gradient descent on a complex network with a lot of data

3
00:00:07,120 --> 00:00:09,240
is that you can get stuck in these local minima and then

4
00:00:09,240 --> 00:00:11,580
you start to wonder, boy is there some other way that I can

5
00:00:11,580 --> 00:00:14,010
optimize these weights. I'm trying to find a set of weights for

6
00:00:14,010 --> 00:00:19,880
the neural network Well that what? That, that tries to ,minimize error on

7
00:00:19,880 --> 00:00:22,490
the training set. And so, gradient decent is one way to do

8
00:00:22,490 --> 00:00:25,720
it, and it can get stuck, but there's other kinds of advanced optimization

9
00:00:25,720 --> 00:00:29,290
methods that become very appropriate here. And in fact, there's

10
00:00:29,290 --> 00:00:32,159
a lot of people in machine learning who think of optimization

11
00:00:32,159 --> 00:00:34,490
and learning as kind of being the same thing. That, that

12
00:00:34,490 --> 00:00:36,510
what you're really trying to do in any kind of learning

13
00:00:36,510 --> 00:00:40,300
problem is solve this, this high order, very difficult optimization

14
00:00:40,300 --> 00:00:44,490
problem to figure out what the The learned ,representation needs to

15
00:00:44,490 --> 00:00:47,770
be. So, I need to mention in passing some kinds of

16
00:00:47,770 --> 00:00:50,835
advanced methods that people have brought to bear, there's things like

17
00:00:50,835 --> 00:00:53,780
,uh,using momentum terms in the gradient,

18
00:00:53,780 --> 00:00:56,860
which basically, where the idea in momentum

19
00:00:56,860 --> 00:00:59,250
is, as we're doing gradient decent, so let's imagine this is our error

20
00:00:59,250 --> 00:01:01,950
surface, we don't want to get stick on this ball here, we want to

21
00:01:01,950 --> 00:01:04,420
kind of pass all the way through it to get to this ball, so maybe

22
00:01:04,420 --> 00:01:07,470
we need to just continue in the direction we've been going. So, instead

23
00:01:07,470 --> 00:01:10,120
of You know think of it as a kind of physical analogy. Instead

24
00:01:10,120 --> 00:01:12,670
of just, just going to the bottom of this hill and getting stuck,

25
00:01:12,670 --> 00:01:15,890
it can kind of bounce out and pop over and come to, what might be

26
00:01:15,890 --> 00:01:19,010
a lower, minima, later. There's a lot of

27
00:01:19,010 --> 00:01:21,490
work in using higher order derivatives to, to better

28
00:01:21,490 --> 00:01:24,450
optimize things instead of just thinking about the,

29
00:01:24,450 --> 00:01:26,910
way that individual weights change the error function to

30
00:01:26,910 --> 00:01:31,000
look at combinations of weights. Hamiltonions and whatnot.

31
00:01:31,000 --> 00:01:33,930
There's various ideas for randomized optimization, which we're going

32
00:01:33,930 --> 00:01:37,340
to get to in a sister course, that can

33
00:01:37,340 --> 00:01:40,840
be applied to, to, to make things more robust.

34
00:01:40,840 --> 00:01:44,310
And sometimes it's worth thinking, you know what, we don't really want to just

35
00:01:44,310 --> 00:01:47,810
minimize the error on the training set, we may actually want to have some

36
00:01:47,810 --> 00:01:53,210
kind of penalty for using, using a structure that's too complex. I mean this,

37
00:01:53,210 --> 00:01:56,790
this ,uh, when did we, when did we see something like this before Charles?

38
00:01:56,790 --> 00:02:00,470
>> When we were doing regression, and we were talking about over fitting.

39
00:02:00,470 --> 00:02:03,020
>> So right. That's right. It came up in

40
00:02:03,020 --> 00:02:05,980
regression but something similar will also happen in the decision

41
00:02:05,980 --> 00:02:06,540
tree section.

42
00:02:06,540 --> 00:02:08,470
>> Sure. We, we had a, we had a issue with

43
00:02:08,470 --> 00:02:11,280
decision trees where if we had, we let the tree grow too

44
00:02:11,280 --> 00:02:15,330
much to explain every little quirk in the data. You'd over fit,

45
00:02:15,330 --> 00:02:17,970
and so ,we came up with a lot of ways of dealing

46
00:02:17,970 --> 00:02:23,340
with that, like pruning. No going too far deeply into the tree.

47
00:02:23,340 --> 00:02:25,630
You can either do that by filling out the tree and then

48
00:02:25,630 --> 00:02:28,270
backing up so you only have a little bit of small error

49
00:02:28,270 --> 00:02:31,350
Or by stopping once you've reached some sort of threshold as you

50
00:02:31,350 --> 00:02:34,410
grow the tree out. That's really the same

51
00:02:34,410 --> 00:02:36,810
as giving some kind of penalty for complexity.

52
00:02:36,810 --> 00:02:38,770
>> Yes, exactly, right. So complexity in the

53
00:02:38,770 --> 00:02:40,440
tree setting has to do with the size

54
00:02:40,440 --> 00:02:43,880
of the tree, in regression it had to do with the order of the polynomial. What

55
00:02:43,880 --> 00:02:47,210
do you suppose it would mean in the neural net setting? And, and how would you

56
00:02:47,210 --> 00:02:50,820
predict, what negative attributes it might have. So,

57
00:02:50,820 --> 00:02:53,550
what's, what's a more or less complex network?

58
00:02:53,550 --> 00:02:56,470
>> Well, there's two things you can do

59
00:02:56,470 --> 00:02:58,020
with networks, you can add more and more

60
00:02:58,020 --> 00:03:01,630
nodes, and you can add more and more layers.

61
00:03:01,630 --> 00:03:03,620
>> Good. So, right. So if we, the more

62
00:03:03,620 --> 00:03:05,980
nodes that we put into network, the more complicated

63
00:03:05,980 --> 00:03:08,640
the mapping becomes from input to output, the more

64
00:03:08,640 --> 00:03:11,180
local minima we get, the more we have the ability

65
00:03:11,180 --> 00:03:13,910
to actually model the noise, which brings up exactly

66
00:03:13,910 --> 00:03:16,770
the same overfitting issues. It turns out there's another one

67
00:03:16,770 --> 00:03:18,740
that's actually really interesting in the neural net setting

68
00:03:18,740 --> 00:03:21,470
which, I think didn't occur to people In the early

69
00:03:21,470 --> 00:03:24,220
days but it became clear and clear over time,

70
00:03:24,220 --> 00:03:27,870
which is that ,you can also have a complex network,

71
00:03:27,870 --> 00:03:31,740
just because the numbers, the weights, are very large. So

72
00:03:31,740 --> 00:03:33,850
same number of weights, same number of nodes, same number

73
00:03:33,850 --> 00:03:37,610
of layers, but larger numbers often leads to more complex.

74
00:03:38,610 --> 00:03:41,930
Network and the possibility of overfitting. And so ,sometimes we

75
00:03:41,930 --> 00:03:43,680
want to penalize a network not just by giving it

76
00:03:43,680 --> 00:03:46,750
fewer nodes or layers but also by keeping the numbers

77
00:03:46,750 --> 00:03:49,510
in a reasonable range. Does that, does that make sense?

78
00:03:49,510 --> 00:03:50,280
>> Makes perfect sense.
