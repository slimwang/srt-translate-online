1
00:00:00,000 --> 00:00:03,007
The other major thing that you can do to minimize the time that your program

2
00:00:03,007 --> 00:00:06,212
spends in its memory accesses is what's called coalescing.

3
00:00:06,212 --> 00:00:09,079
We want to coalesce your accesses to global memory.

4
00:00:09,079 --> 00:00:10,547
Let me explain what that means.

5
00:00:10,547 --> 00:00:15,187
Whenever a thread on the GPU reads or writes global memory,

6
00:00:15,187 --> 00:00:18,070
it always accesses a large chunk of memory at once.

7
00:00:18,070 --> 00:00:22,694
Even if that thread only needs to read or write a small subset of the data in that large chunk.

8
00:00:22,694 --> 00:00:26,265
But if other threads are making similar axises at the same time

9
00:00:26,265 --> 00:00:29,779
then the GPU can exploit that and reuse this larger chunk

10
00:00:29,779 --> 00:00:33,336
for all of the threads that're trying to access that memory.

11
00:00:33,336 --> 00:00:39,645
This means the GPU is at its most efficient when threads read or write contiguous global memory locations.

12
00:00:39,645 --> 00:00:42,236
We say such an access pattern is coalesced.

13
00:00:42,236 --> 00:00:45,116
In this example every thread is reading or writing from a chunk of memory

14
00:00:45,116 --> 00:00:48,619
that's basically given by the index of the thread plus some offset.

15
00:00:48,619 --> 00:00:51,320
This is a coalesced access. This is good.

16
00:00:51,320 --> 00:00:55,281
You'll get very high performance on a memory read or memory write in this setting.

17
00:00:55,281 --> 00:01:01,514
In this example every adjacent thread is accessing every other memory location,

18
00:01:01,514 --> 00:01:03,765
and so this is not coalesced.

19
00:01:03,765 --> 00:01:07,057
We would call this strided because there is a stride

20
00:01:07,057 --> 00:01:10,614
between every threads access and this pattern is not so good.

21
00:01:10,614 --> 00:01:14,798
If you think about it, the way that I drew this dotted line here sort of implies

22
00:01:14,798 --> 00:01:20,672
that the GPU in this case was accessing memory in chunks of five memory locations.

23
00:01:20,672 --> 00:01:24,009
If I were to just draw out the next five memory locations,

24
00:01:24,009 --> 00:01:28,532
you could see that here, to service the needs of four threads making a request

25
00:01:28,532 --> 00:01:33,318
each to an adjacent memory location, I was able to service that with a single memory transaction.

26
00:01:33,318 --> 00:01:34,485
This dotted line.

27
00:01:34,485 --> 00:01:40,270
Whereas in this case the same four threads are striding across memory,

28
00:01:40,270 --> 00:01:46,639
and I actually need to pull in 2 memory transactions to these chunks of memory in order to service that.

29
00:01:46,639 --> 00:01:50,081
I'm going to get half of the speed of out of my global memory here.

30
00:01:50,081 --> 00:01:53,682
You can probably see that the larger the stride between my threads,

31
00:01:53,682 --> 00:01:57,223
the more total memory transactions I'm going to have to do

32
00:01:57,223 --> 00:01:59,357
and the lower my performance will get.

33
00:01:59,357 --> 00:02:03,474
At the limit you can get to a place where each thread is accessing spots

34
00:02:03,474 --> 00:02:06,298
so far in memory or so unrelated to each other in memory

35
00:02:06,298 --> 00:02:09,570
that every single thread gets its own memory transaction.

36
00:02:09,570 --> 00:02:13,237
This, as you can imagine, will lead to pretty bad memory performance from the memory system.

37
00:02:13,237 --> 00:02:16,005
We'll talk more about memory optimizations later.

38
00:02:16,005 --> 00:02:18,739
For now, just know that global memory is going to be fastest

39
00:02:18,739 --> 00:02:22,739
when successive threads read or write adjacent locations in a continuous stretch of memory.
