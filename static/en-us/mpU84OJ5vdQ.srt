1
00:00:00,240 --> 00:00:04,090
Okay, so what we have here, again, is pseudocode for

2
00:00:04,090 --> 00:00:06,700
our K-NN algorithm. And I'm sort of writing it as

3
00:00:06,700 --> 00:00:09,150
like, a function. So, you're going to be given some

4
00:00:09,150 --> 00:00:12,560
training data D, that's the little x, y points, x

5
00:00:12,560 --> 00:00:14,110
y one, x y 2, x y 3, so on

6
00:00:14,110 --> 00:00:17,530
and so fourth. You're given some kind of distance metric

7
00:00:17,530 --> 00:00:22,460
or similarity function. And this is important because this represents

8
00:00:22,460 --> 00:00:25,450
the domain knowledge as I think we, we've already said.

9
00:00:25,450 --> 00:00:28,640
You get some number of neighbors that you care about,

10
00:00:28,640 --> 00:00:31,140
k, hence the k and n, which also, by the

11
00:00:31,140 --> 00:00:34,620
way, represents domain knowledge. Tells you something about how many

12
00:00:34,620 --> 00:00:36,300
neighbors you think you should have. And then are given

13
00:00:36,300 --> 00:00:39,230
some particular new query point and I want to output

14
00:00:39,230 --> 00:00:42,970
some kind of answer, some label, some value. So the

15
00:00:42,970 --> 00:00:46,590
K nn algorithm is remarkably simple given these things you

16
00:00:46,590 --> 00:00:50,480
simply find a set of nearest neighbors such that they

17
00:00:50,480 --> 00:00:53,830
are the K closest to your query point.

18
00:00:53,830 --> 00:00:57,950
>> Okay. I'm sort of processing this. So the, the data

19
00:00:57,950 --> 00:01:01,610
the capital D. Are those pairs and there's a set of pairs?

20
00:01:01,610 --> 00:01:02,150
>> Yes.

21
00:01:02,150 --> 00:01:08,650
>> Ok. And k smallest distances. So this NN this is a set?

22
00:01:08,650 --> 00:01:09,150
>> Yes.

23
00:01:09,150 --> 00:01:11,970
>> And it consists of all the elements in

24
00:01:11,970 --> 00:01:15,910
the data that are closest to the query point?

25
00:01:15,910 --> 00:01:19,130
>> Yep. And the so the query point is a parameter of that. Okay.

26
00:01:19,130 --> 00:01:22,180
Yeah. Alright. I think I. Oh. And then it's, then the so it's just return.

27
00:01:22,180 --> 00:01:23,600
>> Yeah, so we haven't figured out what

28
00:01:23,600 --> 00:01:25,090
to return. So there's two separate cases we've been

29
00:01:25,090 --> 00:01:27,690
talking about so far. One is where, we're doing

30
00:01:27,690 --> 00:01:31,500
classification, and one is where we're doing regression. So,

31
00:01:31,500 --> 00:01:33,360
a question for you would be, what do you

32
00:01:33,360 --> 00:01:35,940
think we should when we're doing classification? Sort of,

33
00:01:35,940 --> 00:01:37,980
what we were doing before on the map. What

34
00:01:37,980 --> 00:01:39,620
will be a way of returning a proper label?

35
00:01:39,620 --> 00:01:40,920
>> So you want to

36
00:01:40,920 --> 00:01:43,520
label, not a, like a weight on a label or something like that?

37
00:01:43,520 --> 00:01:44,580
>> No. I want a label. You have to

38
00:01:44,580 --> 00:01:46,420
produce an answer. You have to commit to something Michael.

39
00:01:46,420 --> 00:01:49,900
>> Alright. Can I commit to more than one thing?

40
00:01:49,900 --> 00:01:50,400
>> Nope.

41
00:01:51,720 --> 00:01:55,460
>> Okay. So I would say that a reasonable thing to do

42
00:01:55,460 --> 00:01:59,480
there would be. Did we get Ys associated with the things in NN?

43
00:01:59,480 --> 00:02:00,020
>> Yeap.

44
00:02:00,020 --> 00:02:02,590
>> So I would go with they should vote.

45
00:02:03,800 --> 00:02:05,900
>> I like that. I think that's a good one,

46
00:02:05,900 --> 00:02:08,220
so we'll simply vote and what does it mean to vote?

47
00:02:08,220 --> 00:02:13,540
>> It means, let's see, so feel like there would be a way to represent

48
00:02:13,540 --> 00:02:16,690
it in terms of NN, the set. Like do you want me to write it formally?

49
00:02:16,690 --> 00:02:17,320
>> No.

50
00:02:17,320 --> 00:02:21,820
>> Oh, then I would just say The closest point.

51
00:02:21,820 --> 00:02:25,370
Whichever yi is most frequent among the closest points wins.

52
00:02:25,370 --> 00:02:27,870
>> Yeah. Right. So you want to find a,

53
00:02:27,870 --> 00:02:31,520
a vote of basically a vote of the yi's,

54
00:02:31,520 --> 00:02:35,130
that are apart of the neighborhood set. And you take the plurality.

55
00:02:35,130 --> 00:02:39,160
>> Plurality I see. So it's whichever one occurs the most.

56
00:02:39,160 --> 00:02:39,830
>> Right.

57
00:02:39,830 --> 00:02:40,960
>> What if there's ties?

58
00:02:40,960 --> 00:02:42,870
>> It's the mode. The mode. Right.

59
00:02:42,870 --> 00:02:42,960
>> Right.

60
00:02:42,960 --> 00:02:45,520
>> Mmmm. I love mode.

61
00:02:45,520 --> 00:02:48,440
>> What if they're ties? That's a good point. Well, if they are

62
00:02:48,440 --> 00:02:51,440
ties among the output, then you're just going to have to pick one.

63
00:02:51,440 --> 00:02:51,570
>> OK.

64
00:02:51,570 --> 00:02:53,200
>> And there's lots of ways you might do that.

65
00:02:53,200 --> 00:02:57,380
You might say, well, I'll take the one. That is

66
00:02:57,380 --> 00:03:00,360
say, most commonly represented in the data

67
00:03:00,360 --> 00:03:03,280
period. Or I'll just randomly pick each

68
00:03:03,280 --> 00:03:06,130
time, or any number of ways you might, you c an imagine doing that.

69
00:03:06,130 --> 00:03:07,790
>> The one that's first alphabetically.

70
00:03:07,790 --> 00:03:09,200
>> The one that's first lexicographically?

71
00:03:09,200 --> 00:03:10,080
>> Hm.

72
00:03:10,080 --> 00:03:11,890
>> What about in the regression case?

73
00:03:11,890 --> 00:03:15,288
>> Okay. So in the regression case our y-is are numbers.

74
00:03:15,288 --> 00:03:19,610
>> Uh-huh. And we have the closest Yi's, so

75
00:03:19,610 --> 00:03:22,790
we have a bunch of those numbers and it

76
00:03:22,790 --> 00:03:25,330
seems like [LAUGH] if you have a pile of numbers and have to return

77
00:03:25,330 --> 00:03:28,410
one, a standard thing to do would be to take the average, or the mean.

78
00:03:28,410 --> 00:03:31,910
>> Yeah. Now let's just simply take the mean

79
00:03:31,910 --> 00:03:33,980
of the Yi's, and at least there, you don't

80
00:03:33,980 --> 00:03:37,250
have to worry about a tie. That's right. Though,

81
00:03:37,250 --> 00:03:38,850
I guess, you know. We didn't really deal with

82
00:03:38,850 --> 00:03:40,860
the question of what happens if there's more than

83
00:03:40,860 --> 00:03:42,710
k small. It's, like, what if they're all exactly

84
00:03:42,710 --> 00:03:44,600
the same distance? All n of them are exactly

85
00:03:44,600 --> 00:03:46,780
the same distance. So which are the k closest?

86
00:03:46,780 --> 00:03:47,810
>> Well, there's lots

87
00:03:47,810 --> 00:03:50,950
of things you could do there. I guess what I would suggest doing, is,

88
00:03:50,950 --> 00:03:54,800
take the, If you have more than k that are close, that are closest

89
00:03:54,800 --> 00:03:57,990
because you have a bunch of ties, in terms of the distance. Just take

90
00:03:57,990 --> 00:04:01,930
all of them. Get the smallest number greater than or equal to k. Okay.

91
00:04:01,930 --> 00:04:03,170
>> That seem reasonable?

92
00:04:03,170 --> 00:04:06,240
>> Yeah, I think that's what college rankings do.

93
00:04:06,240 --> 00:04:10,410
>> Actually, that is what college rankings do, and then they, yeah,

94
00:04:10,410 --> 00:04:12,880
that's exactly what college rankings do. So, let's do that. We know

95
00:04:12,880 --> 00:04:16,070
that college rankings make sense. [LAUGH]. Yeah,

96
00:04:16,070 --> 00:04:19,110
those are, they're scientifically proven to be,

97
00:04:19,110 --> 00:04:19,529
>> Youths.

98
00:04:19,529 --> 00:04:21,760
>> scary, scary to people in colleges.

99
00:04:21,760 --> 00:04:23,380
>> That's exactly right. So, here's what we've

100
00:04:23,380 --> 00:04:24,950
got, Michael. So, all we do is we take

101
00:04:24,950 --> 00:04:26,490
the training data. We have some notion of

102
00:04:26,490 --> 00:04:28,350
similarity or distance. We have a notion of the

103
00:04:28,350 --> 00:04:30,250
number of neighbors that we care about. We

104
00:04:30,250 --> 00:04:32,880
have a query point, we find the K closest

105
00:04:32,880 --> 00:04:35,470
to one, you know breaking ties accordingly. And then

106
00:04:35,470 --> 00:04:37,910
we basically average in some way, in a way

107
00:04:37,910 --> 00:04:39,520
that make sense for classification, in a way they make sense

108
00:04:39,520 --> 00:04:42,730
for regression and we are done. It's a very simple algorithm,

109
00:04:42,730 --> 00:04:45,920
but some of that's because a lot of decisions are being

110
00:04:45,920 --> 00:04:50,100
left up to the designer. The distance metric. The number k,

111
00:04:50,100 --> 00:04:53,670
how you're going to break ties. Exactly how you choose to implement

112
00:04:53,670 --> 00:04:57,890
voting. Exactly how you choose to implement the mean or the

113
00:04:57,890 --> 00:05:00,490
average operation that shows how to do here. And you could

114
00:05:00,490 --> 00:05:03,300
put a bunch of different things here and you end up in,

115
00:05:04,500 --> 00:05:07,860
completely, you could end up with completely different answer. Mm.

116
00:05:07,860 --> 00:05:10,110
>> By the way, one thing that you might do, just to give

117
00:05:10,110 --> 00:05:13,750
you an example of just, how much range there is here. Is rather than

118
00:05:13,750 --> 00:05:17,250
doing a simple vote by counting, you could do a vote that is say,

119
00:05:17,250 --> 00:05:22,190
weighted by how far away you are. So we could have a weighted vote.

120
00:05:22,190 --> 00:05:23,230
>> Uh-huh.

121
00:05:23,230 --> 00:05:24,380
>> That might help us with ties.

122
00:05:24,380 --> 00:05:26,400
>> That could help with ties. Yeah.

123
00:05:26,400 --> 00:05:29,360
>> You could do a weighted average. Yes, right.

124
00:05:29,360 --> 00:05:34,580
So, you're basically saying that the y values that correspond to x values that

125
00:05:34,580 --> 00:05:38,240
are closer to the query point have more of an influence on the mean.

126
00:05:38,240 --> 00:05:39,580
>> Which makes some sense, right?

127
00:05:39,580 --> 00:05:41,450
>> No, I think it makes a lot of sense!

128
00:05:41,450 --> 00:05:44,310
>> So, how would you weight that? What would you do?

129
00:05:44,310 --> 00:05:46,610
>> I would weight it by the similarity.

130
00:05:46,610 --> 00:05:50,170
>> Right, so well in this case, the similarity is we have a distance value

131
00:05:50,170 --> 00:05:54,880
similarity, so You would have to, you know, weight it by something like one over

132
00:05:54,880 --> 00:05:55,460
the distance.

133
00:05:55,460 --> 00:05:59,400
>> Oh I see. Okay. That seems like a hack.

134
00:05:59,400 --> 00:06:01,880
>> Sure but it's a hack that sort of makes sense.

135
00:06:01,880 --> 00:06:02,990
>> Okay.

136
00:06:02,990 --> 00:06:05,280
>> Okay. So anyway. Simple algorithim. Lots

137
00:06:05,280 --> 00:06:07,280
and lots of decisions to make here. All

138
00:06:07,280 --> 00:06:11,610
of which could in principle have a pretty big effect. And so, in order to see

139
00:06:11,610 --> 00:06:16,150
that, I want to do two quizzes that I hope get to heart of this and

140
00:06:16,150 --> 00:06:17,520
maybe give us a little bit of insight

141
00:06:17,520 --> 00:06:20,370
into how some of these decisions might matter

142
00:06:20,370 --> 00:06:24,310
on the one hand, and exactly just how simple

143
00:06:24,310 --> 00:06:26,770
or not simple this algorithm turns out to be. Okay?

144
00:06:26,770 --> 00:06:27,170
>> Awesome.
