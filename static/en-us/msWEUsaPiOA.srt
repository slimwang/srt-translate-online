1
00:00:00,260 --> 00:00:02,500
Let's consider the following
decision problem.

2
00:00:02,500 --> 00:00:09,710
From an initial state, we have a choice
of three actions, a, b, and c.

3
00:00:09,710 --> 00:00:14,370
The immediate reward associated
with each of these actions is zero.

4
00:00:14,370 --> 00:00:16,980
But these three actions will
take us to different states with

5
00:00:16,980 --> 00:00:20,200
different possible rewards
available from these states.

6
00:00:20,200 --> 00:00:25,030
Action a takes us to state S1,
from which our only

7
00:00:25,030 --> 00:00:30,320
action is to loop at each
step back into state S1,

8
00:00:30,320 --> 00:00:36,530
accruing a reward of P1,
which is a parameter, each time.

9
00:00:36,530 --> 00:00:39,203
If we take action b
at the initial state,

10
00:00:39,203 --> 00:00:44,090
we will spend the rest of the game
alternating between states S2 and S4.

11
00:00:44,090 --> 00:00:50,830
We will gain a reward of P2 each
time we move from state S2 to S4.

12
00:00:50,830 --> 00:00:55,720
And we will receive a reward
of P3 on the return trip.

13
00:00:55,720 --> 00:01:00,190
And if we take action c initially,
we will move into state S3,

14
00:01:00,190 --> 00:01:05,920
from which our only next action is to
move into state S5, attaining no reward.

15
00:01:05,920 --> 00:01:06,460
After that,

16
00:01:06,460 --> 00:01:12,560
we repeatedly loop in state S5,
attaining a reward of P4 at each step.

17
00:01:12,560 --> 00:01:16,830
Suppose the game has an independent
probability, of ending after each step?

18
00:01:16,830 --> 00:01:19,760
And, let's call this probability,
1 minus gamma, so

19
00:01:19,760 --> 00:01:23,830
that the effective discount factor for
the scenario is gamma.

20
00:01:23,830 --> 00:01:30,423
Given the parameters P1, P2, P3 and
P4, our problem is to determine for

21
00:01:30,423 --> 00:01:34,988
which values of gamma we
prefer the initial action a,

22
00:01:34,988 --> 00:01:41,300
the initial action b, and for which
gamma we prefer the initial action c.

23
00:01:41,300 --> 00:01:46,205
We will solve this problem by setting up
a Markov decision process encapsulating

24
00:01:46,205 --> 00:01:48,090
this state diagram in Burlap.

25
00:01:48,090 --> 00:01:51,356
We will then run value iteration
procedures on the MDP for

26
00:01:51,356 --> 00:01:54,840
different values of gamma in
order to answer this question.
