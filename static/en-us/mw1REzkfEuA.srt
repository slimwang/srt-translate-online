1
00:00:00,390 --> 00:00:02,303
Why is this transformation a good idea?

2
00:00:02,303 --> 00:00:03,974
We still have to share the information

3
00:00:03,974 --> 00:00:07,443
for a source object up here across all these threads.

4
00:00:07,443 --> 00:00:09,746
That doesn't change from our previous implementation.

5
00:00:09,746 --> 00:00:14,211
But now we don't have to share any destination object information at all,

6
00:00:14,211 --> 00:00:17,986
because 1 thread is completely responsible for 1 destination object,

7
00:00:17,986 --> 00:00:21,956
so we can load that object's information directly into that thread's local storage.

8
00:00:21,956 --> 00:00:26,861
And we don't have to communicate between threads to sum up individual forces.

9
00:00:26,861 --> 00:00:29,494
So it used to be we had P different threads here,

10
00:00:29,494 --> 00:00:33,170
and we had to communicate between those P threads to add up all these forces.

11
00:00:33,170 --> 00:00:36,800
Now because we only have 1 thread that's responsible for all these forces,

12
00:00:36,800 --> 00:00:39,806
we don't have to do any communication between threads at all.

13
00:00:39,806 --> 00:00:44,244
That thread can just accumulate all of the partial results in its local storage.

14
00:00:44,244 --> 00:00:46,913
So the result is a faster implementation overall.

15
00:00:46,913 --> 00:00:52,250
Dave discussed this technique generically in the last unit, Unit 5, and he used the term privatization.

16
00:00:52,250 --> 00:00:53,920
So in making this transformation,

17
00:00:53,920 --> 00:01:00,223
is the amount of parallelism now increased, decreased, or kept constant from the previous implementation?
