1
00:00:00,000 --> 00:00:03,000
You just learned about an exciting clustering algorithm

2
00:00:03,000 --> 00:00:07,000
that's really easy to implement called k-means.

3
00:00:07,000 --> 00:00:09,000
To give you the algorithm in pseudocode,

4
00:00:09,000 --> 00:00:15,000
initially we select k cluster centers at random and then we repeat.

5
00:00:15,000 --> 00:00:21,000
In a corresponding step, we correspond all the data points to the nearest cluster center,

6
00:00:21,000 --> 00:00:26,000
and then we calculate the new cluster center by the mean of the corresponding data points.

7
00:00:26,000 --> 00:00:30,000
We repeat this until nothing changes any more.

8
00:00:30,000 --> 00:00:37,000
Now special care has to be taken if a cluster center becomes empty--that means no data point is associated.

9
00:00:37,000 --> 00:00:43,000
In which case, we just restart cluster centers at random that have no corresponding points.

10
00:00:43,000 --> 00:00:46,000
Empty cluster centers restart at random.

11
00:00:46,000 --> 00:00:54,000
This algorithm is known to converge to a locally optimal clustering of data ponts.

12
00:00:54,000 --> 00:00:58,000
The general clustering problem is known to be NP-hard.

13
00:00:58,000 --> 00:01:03,000
So a locally optimal solution, in a way, is the best we can hope for.

14
00:01:03,000 --> 00:01:06,000
Now let me talk about problems with k-means.

15
00:01:06,000 --> 00:01:09,000
First we need to know k, the number of cluster centers.

16
00:01:09,000 --> 00:01:10,000
As I mentioned, the local minimum.

17
00:01:10,000 --> 00:01:16,000
For example, for 4 data points like this and 2 cluster centers that happen to be just over here,

18
00:01:16,000 --> 00:01:20,000
with the separation line like this there would be no motion of k means.

19
00:01:20,000 --> 00:01:25,000
Even though moving one over here and one over there would give a better solution.

20
00:01:25,000 --> 00:01:28,000
There's a general problem of high dimensionality of the space

21
00:01:28,000 --> 00:01:32,000
that is not dissimilar from the way k-nearest neighbor suffers from high dimensionality.

22
00:01:32,000 --> 00:01:35,000
And then there's lack of a mathematical basis.

23
00:01:35,000 --> 00:01:38,000
Now if you're a partitioner, you might not care about a mathematical basis.

24
00:01:38,000 --> 00:01:42,000
But for the sake of this class, let's just care about it.

25
00:01:42,000 --> 00:01:45,000
So here's a first quiz for k-means.

26
00:01:45,000 --> 00:01:50,000
Given the following two cluster centers, C1 and C2,

27
00:01:50,000 --> 99:59:59,999
click on exactly those points that are associated with C1 and not with C2.
