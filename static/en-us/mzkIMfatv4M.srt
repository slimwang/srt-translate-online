1
00:00:00,315 --> 00:00:01,940
Alright, you know the issue that we want to make sure

2
00:00:01,940 --> 00:00:04,610
that we think about each time we introduce a new kind

3
00:00:04,610 --> 00:00:08,900
of supervised learning representation is to ask what its preference bias

4
00:00:08,900 --> 00:00:11,330
is. So Charles, can you remind us what preference bias is?

5
00:00:11,330 --> 00:00:14,410
>> Like restriction bias tells you what it is you

6
00:00:14,410 --> 00:00:17,430
are able to represent. Preference bias tells you something about the

7
00:00:17,430 --> 00:00:20,750
algorithm that you are using to learn. That tells you,

8
00:00:20,750 --> 00:00:26,270
given two representations, why I would prefer one over the other.

9
00:00:26,270 --> 00:00:28,290
So, perhaps you think back what we talked

10
00:00:28,290 --> 00:00:34,560
about with decision trees, we preferred trees where nodes

11
00:00:34,560 --> 00:00:37,360
near the top had high information gain We preferred

12
00:00:37,360 --> 00:00:40,520
correct trees. We preferred trees that were shorter to

13
00:00:40,520 --> 00:00:43,240
ones that were longer unnecessarily and so on and

14
00:00:43,240 --> 00:00:45,610
so forth. So that actually brings up a point

15
00:00:45,610 --> 00:00:47,890
here which is, we haven't actually chosen an algorithm.

16
00:00:47,890 --> 00:00:51,570
We talked about how derivatives work, how back propagation

17
00:00:51,570 --> 00:00:56,470
works, but you missed telling me one very important thing, which is how do

18
00:00:56,470 --> 00:00:59,420
we start? You tell me how to update the weights but, how do I

19
00:00:59,420 --> 00:01:01,450
start out with the weights? Do they all start at zero? Do they all

20
00:01:01,450 --> 00:01:04,260
start out at one? How do you usually set the weights in the beginning?

21
00:01:04,260 --> 00:01:07,530
>> Yes indeed. We did not talk about that, that's, it's

22
00:01:07,530 --> 00:01:10,420
really important. You can't run this algorithm without initializing the weights

23
00:01:10,420 --> 00:01:13,090
to something. Right? We did talk about how you update the

24
00:01:13,090 --> 00:01:16,970
weights but they don't just you know, just start undefined and you,

25
00:01:16,970 --> 00:01:18,850
you can't just update something that's undefined. So we

26
00:01:18,850 --> 00:01:21,660
have to set the initial weights to something. So pretty

27
00:01:21,660 --> 00:01:27,300
typical thing for people to do, is small, random,

28
00:01:27,300 --> 00:01:31,410
values. So why do you suppose we want random values?

29
00:01:31,410 --> 00:01:34,650
>> Because we have no particular reason to pick one set of values over

30
00:01:34,650 --> 00:01:36,040
another. So you start somewhere in the

31
00:01:36,040 --> 00:01:38,830
space. Probably helps us to avoid local minimum.

32
00:01:38,830 --> 00:01:42,820
>> Yea kind of. I mean there's also the issue of Well

33
00:01:42,820 --> 00:01:46,700
if we run the algorithm multiple times if we get stuck, we like

34
00:01:46,700 --> 00:01:49,900
it not to get stuck exactly there again, if do, if you run it

35
00:01:49,900 --> 00:01:53,690
again. So it gives some variability, which is a helpful thing in avoiding

36
00:01:53,690 --> 00:01:55,490
local minimal. And what do you suppose,

37
00:01:55,490 --> 00:01:57,250
it's important to start with small values.

38
00:01:57,250 --> 00:02:02,840
>> Well you just said. In our discussion before that if the weights get

39
00:02:02,840 --> 00:02:04,990
really big that can sometimes lead to

40
00:02:04,990 --> 00:02:08,758
over fitting, because it let's you represent arbitrarily

41
00:02:08,758 --> 00:02:09,470
complex functions.

42
00:02:09,470 --> 00:02:11,290
>> Good. And so, and what is that

43
00:02:11,290 --> 00:02:13,160
tell us about what the preference bias is then?

44
00:02:13,160 --> 00:02:17,580
>> Well if we start out with small random values. That means

45
00:02:17,580 --> 00:02:22,540
we are starting out with low complexity. So that means we prefer Simpler

46
00:02:22,540 --> 00:02:27,180
explanations to more complex explanations. And of course the usual stuff like

47
00:02:27,180 --> 00:02:29,980
we prefer, correct answers to incorrect answers, and so on and so forth.

48
00:02:29,980 --> 00:02:33,920
>> > So, you'd say that neural-nets implement, or maybe we should

49
00:02:33,920 --> 00:02:40,330
say, that neural networks implement a kind of bias that says Prefer correct over

50
00:02:40,330 --> 00:02:44,080
incorrect but all things being equal, the simpler explanation, is preferred.

51
00:02:44,080 --> 00:02:46,360
>> Well, if you have the right algorithm. If the algorithm starts

52
00:02:46,360 --> 00:02:49,940
with small, random values and tries to stop, you know, when you

53
00:02:49,940 --> 00:02:54,160
start over-fitting Then you, cause you're going to start out with the simpler

54
00:02:54,160 --> 00:02:57,620
explanations first before you allow your weights to grow. so you, about that.

55
00:02:57,620 --> 00:02:59,280
>> So this

56
00:02:59,280 --> 00:03:02,240
reminiscent of the principal that is known as Occan's

57
00:03:02,240 --> 00:03:05,240
razor which is often stated as entities should not be

58
00:03:05,240 --> 00:03:08,940
multiplied unnecessarily. And given that we're working with neural

59
00:03:08,940 --> 00:03:14,230
networks, there's a lot of unnecessary multiplication that happens. [LAUGH]

60
00:03:14,230 --> 00:03:18,490
But, in fact, this actually is referring to exactly what we've been

61
00:03:18,490 --> 00:03:21,420
talking about. So this unnecessarily is,

62
00:03:21,420 --> 00:03:24,230
one interpretation of this is that, "Well,

63
00:03:24,230 --> 00:03:26,840
when is it necessary?" It's necessary if you're getting better

64
00:03:26,840 --> 00:03:30,870
explanatory power, you're fitting your data better. So Unnecessarily would

65
00:03:30,870 --> 00:03:32,680
mean, well we're not doing any better at fitting the

66
00:03:32,680 --> 00:03:35,610
data. If we're not doing any better at fitting the data,

67
00:03:35,610 --> 00:03:41,010
then we should not multiply entities. And multiply here means

68
00:03:41,010 --> 00:03:44,340
make more complex. So don't make something more complex unless

69
00:03:44,340 --> 00:03:46,700
you're getting better error, or if two things have similar

70
00:03:46,700 --> 00:03:50,070
error Choose the simpler one, use the one that's less complex.

71
00:03:50,070 --> 00:03:54,480
That has been shown to, if you mathematize this and you use it in

72
00:03:54,480 --> 00:03:57,240
the context of supervised learning, that we're

73
00:03:57,240 --> 00:04:01,640
going to get better generalization error with simpler hypotheses.
