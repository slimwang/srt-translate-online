1
00:00:00,430 --> 00:00:05,100
Now we have the kernel function given as K of x minus xn over h.

2
00:00:05,100 --> 00:00:09,810
Let's say we have total number of points, capital N,

3
00:00:09,810 --> 00:00:14,470
then the density estimate is given as a function of x.

4
00:00:15,520 --> 00:00:19,570
Here's the normalization factor given by the number of points

5
00:00:19,570 --> 00:00:25,490
inside the box with side h and dimension D.

6
00:00:25,490 --> 00:00:31,690
And we sum all the kernels from one to N for each xn.

7
00:00:31,690 --> 00:00:36,950
Here we can get the expectation value from the kernel density estimate.

8
00:00:36,950 --> 00:00:40,960
It's all right if you can't follow along the entire math here.

9
00:00:40,960 --> 00:00:44,070
The idea is to take the expectation value

10
00:00:44,070 --> 00:00:47,790
of the probability density derived from the kernel.

11
00:00:47,790 --> 00:00:50,750
Which is one over Ndh to the d.

12
00:00:51,760 --> 00:00:54,140
And the sum of the kernel and

13
00:00:54,140 --> 00:00:57,970
the expectation function has been taken inside the sum.

14
00:00:57,970 --> 00:01:02,310
Once we work this out, you can see that the expectation value

15
00:01:02,310 --> 00:01:07,780
is a convolution of the kernel, with the original probability density.

16
00:01:07,780 --> 00:01:11,570
We won't get into more details about the expectation value here.

17
00:01:11,570 --> 00:01:15,330
But we can discuss in the forum why this form is important.

18
00:01:16,590 --> 00:01:23,000
The kernel function we have defined here is known as the Parzen window kernel.

19
00:01:23,000 --> 00:01:26,489
This method is used in signal processing problems.

20
00:01:26,489 --> 00:01:33,793
You will also notice that the smoothness of this PDF depends on the value of h.

21
00:01:33,793 --> 00:01:38,010
We will now look at another kernel known as the Gaussian kernel.
