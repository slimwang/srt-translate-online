1
00:00:00,300 --> 00:00:02,690
So a distributed system is a collection of

2
00:00:02,690 --> 00:00:05,800
Nodes which are interconnected by a Local Area

3
00:00:05,800 --> 00:00:09,210
Network or a Wide Area Network and this

4
00:00:09,210 --> 00:00:13,050
Local Area Network may be implemented using a

5
00:00:13,050 --> 00:00:17,240
twisted pair, coaxial cable and optical fiber And

6
00:00:17,240 --> 00:00:19,070
if it is a Wide Area Network, it

7
00:00:19,070 --> 00:00:22,900
could be implemented using a satellite communication microwave

8
00:00:22,900 --> 00:00:25,650
links and so on. And the media access

9
00:00:25,650 --> 00:00:29,750
protocols that may be available for communication of

10
00:00:29,750 --> 00:00:32,360
these nodes on a Local Area Network or

11
00:00:32,360 --> 00:00:35,200
a Wide Area Network, maybe ATM or Ethernet

12
00:00:35,200 --> 00:00:36,890
and so on and so forth. That's sort of

13
00:00:36,890 --> 00:00:40,430
the picture of What a distributed system is,

14
00:00:40,430 --> 00:00:44,930
number one. Number two, there's no physical memory that

15
00:00:44,930 --> 00:00:47,650
is shared between nodes of the distributed system.

16
00:00:47,650 --> 00:00:50,790
So the only way nodes can communicate with one

17
00:00:50,790 --> 00:00:53,600
another is by sending messages on the local

18
00:00:53,600 --> 00:00:55,850
area network to one another. So there is

19
00:00:55,850 --> 00:00:58,740
no shared memory for communication. Between the nodes

20
00:00:58,740 --> 00:01:03,430
of the distributed system. No physical memory for communication

21
00:01:03,430 --> 00:01:05,519
between the nodes of the distributed system. And

22
00:01:05,519 --> 00:01:08,680
the third property is the fact the even

23
00:01:08,680 --> 00:01:11,610
computation time, that is the time it takes

24
00:01:11,610 --> 00:01:15,720
on a single node to do some meaningful processing,

25
00:01:15,720 --> 00:01:17,540
that computation time is what we are calling

26
00:01:17,540 --> 00:01:20,430
as the event computation time. That is Te.

27
00:01:20,430 --> 00:01:23,590
And a node may also communicate With other

28
00:01:23,590 --> 00:01:25,800
nodes in the system and that's what we're calling

29
00:01:25,800 --> 00:01:31,370
as communication time or the messaging time, TM. And the third property

30
00:01:31,370 --> 00:01:36,240
of the distributed system is that the time for communication between

31
00:01:36,240 --> 00:01:41,840
nodes in the system, TM. Is much more significantly larger

32
00:01:41,840 --> 00:01:44,480
than the event communication. So these are the three

33
00:01:44,480 --> 00:01:47,640
properties which I would like to think of to make

34
00:01:47,640 --> 00:01:49,540
sure that we have a shared understanding of what

35
00:01:49,540 --> 00:01:52,540
we mean by distributed systems. That they are connected by

36
00:01:52,540 --> 00:01:55,070
some sort of local area network or wide area

37
00:01:55,070 --> 00:01:59,790
network. A collection of nodes. And their own physically shared

38
00:01:59,790 --> 00:02:03,590
memory, so the only communication, only way they can communicate

39
00:02:03,590 --> 00:02:06,860
with one another is via messages that are sent between

40
00:02:06,860 --> 00:02:09,900
the nodes using the local area network. And the

41
00:02:09,900 --> 00:02:13,220
third property, is the fact that the message communication

42
00:02:13,220 --> 00:02:17,130
time is significantly larger And even computation time that

43
00:02:17,130 --> 00:02:20,000
happens on a single node. You probably remember a good

44
00:02:20,000 --> 00:02:23,120
friend, Leslie Lamport. I introduced you to him when

45
00:02:23,120 --> 00:02:26,160
we talked about parallel systems, and I said that we

46
00:02:26,160 --> 00:02:29,240
will see him again. In parallel systems, he's the

47
00:02:29,240 --> 00:02:32,770
one who gave us the notion of sequential consistency, and

48
00:02:32,770 --> 00:02:35,510
the same person that we're going to be

49
00:02:35,510 --> 00:02:38,150
talking about in this lecture, Leslie Lamport. And

50
00:02:38,150 --> 00:02:41,080
in particular LAN port has a definition for

51
00:02:41,080 --> 00:02:43,660
a distributed system and the definition of a

52
00:02:43,660 --> 00:02:46,930
distributed system verbatim goes like this. A system

53
00:02:46,930 --> 00:02:51,940
is distributed if the message transmission time, Tm,

54
00:02:51,940 --> 00:02:55,730
is not negligible to the time between events

55
00:02:55,730 --> 00:02:57,790
in a single process. Cause there's a time

56
00:02:57,790 --> 00:02:59,460
between events in a single process, there's a

57
00:02:59,460 --> 00:03:03,690
message transmission time, and so the definition that Lesley

58
00:03:03,690 --> 00:03:06,250
Lamport gives is that a system is distributed

59
00:03:06,250 --> 00:03:10,070
is a message transmission time tm, is not negligible

60
00:03:10,070 --> 00:03:12,830
compared to the time between events in a

61
00:03:12,830 --> 00:03:16,960
single process. What is the implication of this definition?

62
00:03:16,960 --> 00:03:20,240
Interestingly, even a cluster is a distributed system by

63
00:03:20,240 --> 00:03:22,820
this definition. We've been talking about clusters a lot

64
00:03:22,820 --> 00:03:25,420
when we discussed parallel systems and I told you

65
00:03:25,420 --> 00:03:28,490
that clusters are the work horses of data centers

66
00:03:28,490 --> 00:03:31,760
today. Even a cluster is a distributed system by

67
00:03:31,760 --> 00:03:35,140
this definition because processors have become blazingly fast, so

68
00:03:35,140 --> 00:03:38,070
the event computation has shrunk quite a bit. On

69
00:03:38,070 --> 00:03:41,190
the other hand the message communication time is also

70
00:03:41,190 --> 00:03:44,350
becoming better but not as fast as the computation

71
00:03:44,350 --> 00:03:47,990
time that happens on a single processor and therefore even

72
00:03:47,990 --> 00:03:50,130
on a cluster which is all contained in a

73
00:03:50,130 --> 00:03:53,590
single rack in a data center, the message transmission

74
00:03:53,590 --> 00:03:57,280
time is significantly more than The event time. And

75
00:03:57,280 --> 00:03:59,080
so even a cluster is a distributed system by

76
00:03:59,080 --> 00:04:03,790
this definition. The importance of this inequality is in

77
00:04:03,790 --> 00:04:06,900
the design of algorithms that are going to span the

78
00:04:06,900 --> 00:04:09,150
nodes of the network. What we want to make sure,

79
00:04:09,150 --> 00:04:13,500
because the message transmission time is so significantly larger

80
00:04:13,500 --> 00:04:16,279
than the event computation time on a single

81
00:04:16,279 --> 00:04:21,630
node In structuring applications that run on distributed nodes

82
00:04:21,630 --> 00:04:23,770
of a system like this, one has to be

83
00:04:23,770 --> 00:04:27,390
very careful to make sure that the computation time

84
00:04:27,390 --> 00:04:30,000
in the algorithms that you're designing is significantly

85
00:04:30,000 --> 00:04:32,450
more than the communication time. Otherwise, we are not

86
00:04:32,450 --> 00:04:35,550
going to reap the benefits of parallelism. If, most

87
00:04:35,550 --> 00:04:39,160
of the time you're communicating. That's, the reason why,

88
00:04:39,160 --> 00:04:42,490
this definition of distributed system is extremely important.
