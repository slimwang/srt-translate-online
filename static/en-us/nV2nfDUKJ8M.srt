1
00:00:00,300 --> 00:00:02,830
SMDP, by the way, theres an entire
literature about semi-Markov

2
00:00:02,830 --> 00:00:06,510
decision processes well beyond
the sort of scope of this lesson.

3
00:00:06,510 --> 00:00:08,950
But from our point of view what's
really neat about this, and

4
00:00:08,950 --> 00:00:11,230
this comes up again and
again when you use SMDPs,

5
00:00:11,230 --> 00:00:15,716
is that once we define an SMDP and
the appropriate Bellman operator for

6
00:00:15,716 --> 00:00:18,490
it, we can basically just
pretend that it's an MDP.

7
00:00:18,490 --> 00:00:20,164
Because at bottom it is an MDP and

8
00:00:20,164 --> 00:00:23,026
these expectations allow to
ground out in that in MDP.

9
00:00:23,026 --> 00:00:24,923
And so we can just pretend we're
always dealing with an MDP.

10
00:00:24,923 --> 00:00:27,590
And we only have to deal with
the fact that it's a SMDP.

11
00:00:27,590 --> 00:00:29,850
It just works,
I take an option is just an action.

12
00:00:29,850 --> 00:00:34,413
It just takes time and
isn't atomic but whatever.

13
00:00:34,413 --> 00:00:36,190
I might as well treat them
as if they were atomic.

14
00:00:36,190 --> 00:00:37,799
I just let it run and it comes back.

15
00:00:37,799 --> 00:00:40,628
And when it comes back it will
have accumulated for me all of

16
00:00:40,628 --> 00:00:44,260
the reward that I would accumulate if
I were living in the underlying MDP.

17
00:00:44,260 --> 00:00:48,796
And it will discount the future in
exactly the right way after I've

18
00:00:48,796 --> 00:00:49,853
taken k steps.

19
00:00:49,853 --> 00:00:50,805
>> Whoo-hoo!

20
00:00:50,805 --> 00:00:53,610
>> [LAUGH] Yes, exactly.

21
00:00:53,610 --> 00:00:57,435
So this is a lot of sort of
equation writing and deltas and

22
00:00:57,435 --> 00:01:01,880
delta raised to powers and expectations
that allows us to say, you know what?

23
00:01:01,880 --> 00:01:03,890
We're still dealing
with MDPs with options.

24
00:01:03,890 --> 00:01:05,410
Everything we knew before works.

25
00:01:05,410 --> 00:01:09,480
>> Okay, that's a relief, I guess, that
we didn't have to throw all that out and

26
00:01:09,480 --> 00:01:10,194
start afresh.

27
00:01:10,194 --> 00:01:13,270
>> Yeah, and
again I mean it's actually important.

28
00:01:13,270 --> 00:01:16,608
For me this seemed like a subtle
point for a long time but

29
00:01:16,608 --> 00:01:18,650
it's actually not that subtle.

30
00:01:18,650 --> 00:01:21,308
It's just an SMDP is just an MDP and

31
00:01:21,308 --> 00:01:26,808
where we have to keep track of the fact
that our actions are not atomic and

32
00:01:26,808 --> 00:01:31,250
accumulate reward and
accumulate all the transitions.

33
00:01:31,250 --> 00:01:34,232
But really, that's all captured
very nicely by this expectation.

34
00:01:34,232 --> 00:01:37,435
It just means we have to hide things
like the discount factors inside of

35
00:01:37,435 --> 00:01:38,480
our functions.

36
00:01:38,480 --> 00:01:40,190
Whereas before with the MDP we could

37
00:01:40,190 --> 00:01:42,110
stick them outside of
the transition functions.

38
00:01:42,110 --> 00:01:45,955
>> Okay, well then I'm not quite
getting that name, I think.

39
00:01:45,955 --> 00:01:50,410
Semi-Markov decision process sounds
like it's kind of Markov but

40
00:01:50,410 --> 00:01:52,280
also kind of not Markov.

41
00:01:52,280 --> 00:01:54,530
>> Yeah, that's right, it's semi-Markov.

42
00:01:54,530 --> 00:01:56,097
>> But this seems Markov, right?

43
00:01:56,097 --> 00:02:00,826
I mean, underneath it's just a whole
bunch of standard MDP-type transitions.

44
00:02:00,826 --> 00:02:02,025
>> Yeah, that's exactly right.

45
00:02:02,025 --> 00:02:04,973
But what's allowing it, but
at the level of the options right,

46
00:02:04,973 --> 00:02:08,039
it's not Markov because you have
to know how much time has passed.

47
00:02:10,740 --> 00:02:14,840
>> I see, I see, so where, right,
the state that we end up in

48
00:02:14,840 --> 00:02:18,850
could actually depend on how much time
we've spent, how much time has elapsed.

49
00:02:18,850 --> 00:02:21,280
>> In fact it will, in general, right?

50
00:02:21,280 --> 00:02:23,940
Because if I do it in one step I'm
going to end up in some state.

51
00:02:23,940 --> 00:02:26,690
I can't reach the states that
are more than one step away.

52
00:02:26,690 --> 00:02:29,120
If I take ten steps then
I can reach more states.

53
00:02:29,120 --> 00:02:31,196
So the state I end up
in does depend upon k.

54
00:02:31,196 --> 00:02:34,150
It depends upon how long
the option was executed.

55
00:02:34,150 --> 00:02:34,780
And in general,

56
00:02:34,780 --> 00:02:39,160
since beta is a probability over states,
it could happen at any time.

57
00:02:39,160 --> 00:02:42,290
>> I see, so it's got elements of
Markov and elements of not Markov.

58
00:02:42,290 --> 00:02:46,340
So elements of Markov include
things like well, we can

59
00:02:46,340 --> 00:02:49,860
think about the probability distribution
over where we're going to end up.

60
00:02:49,860 --> 00:02:51,470
It really is Markov.

61
00:02:51,470 --> 00:02:54,050
But we have to be much
more careful when we

62
00:02:54,050 --> 00:02:58,140
actually sum up rewards because
the time that has lapsed is variable.

63
00:02:58,140 --> 00:03:03,010
And so we have to actually take into
consideration how many steps have passed

64
00:03:03,010 --> 00:03:04,240
to do that.

65
00:03:04,240 --> 00:03:06,770
>> Right, and in fact is
that's a very important point.

66
00:03:06,770 --> 00:03:08,670
And I'll sort of point
out two things here.

67
00:03:08,670 --> 00:03:12,570
One is when we have a process
that is not Markovian,

68
00:03:12,570 --> 00:03:16,170
often we can turn into, in fact we
can turn basically any non-Markovian

69
00:03:16,170 --> 00:03:19,880
process into a Markovian process by just
keeping track of enough history, right?

70
00:03:20,890 --> 00:03:22,780
And history is, often time is enough.

71
00:03:22,780 --> 00:03:25,130
Sometimes you actually want to
know what states you visited but

72
00:03:25,130 --> 00:03:27,020
often time is enough.

73
00:03:27,020 --> 00:03:30,190
And so we can turn a non-Markovian
thing into a Markovian thing.

74
00:03:30,190 --> 00:03:32,350
And in some ways that's what
we're doing here, right?

75
00:03:32,350 --> 00:03:37,050
We are keeping track of the places
that we saw in between.

76
00:03:37,050 --> 00:03:38,430
But at the level of the SMDP,

77
00:03:38,430 --> 00:03:42,200
at the level of the options,
I can ignore the MDP and

78
00:03:42,200 --> 00:03:46,600
pretend that the SMDP is an MDP and
that these are really atomic actions.

79
00:03:46,600 --> 00:03:52,160
But in reality they aren't, which is
why R and F actually depend upon k.

80
00:03:52,160 --> 00:03:54,600
It actually matters how
many steps you took.

81
00:03:54,600 --> 00:03:58,090
And that affects the expectation.

82
00:03:58,090 --> 00:04:02,170
And in fact, if you look at R I am
actually accumulating all the reward.

83
00:04:02,170 --> 00:04:06,100
So I'm taking something that's
non-Markovian because it depends upon

84
00:04:06,100 --> 00:04:08,080
the actual rewards that I see,

85
00:04:08,080 --> 00:04:10,389
which depends upon the actual
states that I visit.

86
00:04:11,570 --> 00:04:14,690
And I'm gathering them
with me as I go along.

87
00:04:14,690 --> 00:04:18,450
So that when I pop out a level it
goes back to looking Markovian again.

88
00:04:18,450 --> 00:04:19,300
>> Cool.

89
00:04:19,300 --> 00:04:25,400
>> Right, so an SMDP, it's an MDP and
I don't have to think about it anymore.

90
00:04:25,400 --> 00:04:27,840
>> I feel like I'm getting
the kind of math of this.

91
00:04:27,840 --> 00:04:31,641
But it's still, it's still
a bit of an abstraction to me.

92
00:04:31,641 --> 00:04:36,790
Can we get into some kind of an example
to to practice with these ideas?

93
00:04:36,790 --> 00:04:39,374
>> So yes, first I want to point
out that I appreciate the pun,

94
00:04:39,374 --> 00:04:40,887
using the word abstraction here.

95
00:04:40,887 --> 00:04:42,187
>> Thanks.
>> Very good, I like that.

96
00:04:42,187 --> 00:04:44,830
But I think the answer is both yes and
no.

97
00:04:44,830 --> 00:04:48,020
I'm going to go through an example

98
00:04:48,020 --> 00:04:50,290
that I think might help you
think a little bit about this.

99
00:04:50,290 --> 00:04:53,220
But I also think that actually working
out all these expectations would take us

100
00:04:53,220 --> 00:04:53,970
a long time.

101
00:04:53,970 --> 00:04:56,337
And it's a really good and
useful exercise,

102
00:04:56,337 --> 00:04:58,836
which is why it should
probably be an exercise.

103
00:04:58,836 --> 00:05:00,330
We'll let the students do it.

104
00:05:00,330 --> 00:05:03,550
>> I mean, I don't know, it seems like a
cool exercise in dynamic programming and

105
00:05:03,550 --> 00:05:04,590
algorithm design.

106
00:05:04,590 --> 00:05:08,050
>> But Michael, you think everything is
a cool exercise in dynamic programming.

107
00:05:08,050 --> 00:05:09,250
>> Well that may be, but

108
00:05:09,250 --> 00:05:12,480
that's because everything is a cool
exercise in dynamic programming.

109
00:05:12,480 --> 00:05:13,590
>> Yes, that's an excellent point, and

110
00:05:13,590 --> 00:05:14,810
why we're going to move
on to the next slide.
