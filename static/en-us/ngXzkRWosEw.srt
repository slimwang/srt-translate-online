1
00:00:00,000 --> 00:00:03,000
Now, before we get into the math of reinforcement learning,

2
00:00:03,000 --> 00:00:05,000
let's review MDPs--

3
00:00:05,000 --> 00:00:09,000
which are, of course, Markov Decision Processes.

4
00:00:09,000 --> 00:00:13,000
An MDP consists of a set of states--

5
00:00:13,000 --> 00:00:16,000
S is an element of the state, S;

6
00:00:16,000 --> 00:00:18,000
a set of actions--

7
00:00:18,000 --> 00:00:26,000
A is an element of the actions that are available in each of the states, S.

8
00:00:26,000 --> 00:00:28,000
And we're going to distinguish a Start state,

9
00:00:28,000 --> 00:00:30,000
which we'll call S-zero,

10
00:00:30,000 --> 00:00:34,000
and then we need a transition function that says:

11
00:00:34,000 --> 00:00:37,000
How does the world evolve as we take actions in the world?

12
00:00:37,000 --> 00:00:39,000
And we can denote that by

13
00:00:39,000 --> 00:00:45,000
the probability that we get a Result state, S prime--

14
00:00:45,000 --> 00:00:49,000
given that we start in state, S,

15
00:00:49,000 --> 00:00:50,000
and apply action, A.

16
00:00:50,000 --> 00:00:52,000
That's a probability distribution

17
00:00:52,000 --> 00:00:54,000
because the world is stochastic.

18
00:00:54,000 --> 00:00:56,000
The same result doesn't happen every time,

19
00:00:56,000 --> 00:00:58,000
when we do the same action,

20
00:00:58,000 --> 00:01:00,000
so we have this probability distribution.

21
00:01:00,000 --> 00:01:03,000
In some notations, you'll see:

22
00:01:03,000 --> 00:01:08,000
T of S, A, S--for the transition function.

23
00:01:08,000 --> 00:01:10,000
And then, in addition to the transition,

24
00:01:10,000 --> 00:01:12,000
we need a reward function--

25
00:01:12,000 --> 00:01:14,000
which we'll denote R.

26
00:01:14,000 --> 00:01:16,000
Sometimes that's over the whole triplet--

27
00:01:16,000 --> 00:01:19,000
the reward that you get from starting in one state,

28
00:01:19,000 --> 00:01:22,000
taking an action, and arriving at another state;

29
00:01:22,000 --> 00:01:26,000
sometimes we only need to talk about the result state.

30
00:01:26,000 --> 00:01:29,000
So in the 4 by 3 Grid World, for example,

31
00:01:29,000 --> 00:01:31,000
we don't care how you got to this state--

32
00:01:31,000 --> 00:01:33,000
it's just, when you get to one of the states

33
00:01:33,000 --> 00:01:35,000
in the upper right, you get a plus 1 or minus 1 reward.

34
00:01:35,000 --> 00:01:38,000
And similarly, in a game like backgammon--

35
00:01:38,000 --> 00:01:40,000
when you win or lose,

36
00:01:40,000 --> 00:01:42,000
you get a positive or negative reward.

37
00:01:42,000 --> 00:01:44,000
It doesn't matter what move you took to win or lose.

38
00:01:44,000 --> 99:59:59,999
And so that's all there is to MDPs.
