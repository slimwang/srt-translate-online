1
00:00:00,420 --> 00:00:03,910
As it happens when we split based on speed limit we get

2
00:00:03,910 --> 00:00:07,540
perfect purity of the branches that we make as a result.

3
00:00:07,540 --> 00:00:10,510
So our information gain is going to be equal to 1.

4
00:00:10,510 --> 00:00:13,290
We started out with an entropy of 1.

5
00:00:13,290 --> 00:00:14,970
At the end we had an entropy of 0.

6
00:00:14,970 --> 00:00:16,800
So the information gain is going to be 1.

7
00:00:16,800 --> 00:00:19,380
So this is the best information gain that we can have,

8
00:00:19,380 --> 00:00:21,070
definitely this is where we want to make a split.

9
00:00:22,120 --> 00:00:25,500
And just to sketch out the decision tree it would look something like this.

10
00:00:25,500 --> 00:00:28,910
Where when we look at samples where the speed limit is in effect,

11
00:00:28,910 --> 00:00:31,660
so these first two rows where the, the answer for

12
00:00:31,660 --> 00:00:35,940
speed limit is yes, then we get all of our slow examples over there.

13
00:00:35,940 --> 00:00:41,130
On the other side when there's no speed limit, everything is going to be fast.

14
00:00:41,130 --> 00:00:44,540
So this has just been a very simple calculation.

15
00:00:44,540 --> 00:00:46,570
It still took us a while to get through, but

16
00:00:46,570 --> 00:00:48,440
I hope you have a little bit of a better sense for

17
00:00:48,440 --> 00:00:52,480
what information gain is in decision trees and why it's so important.

18
00:00:52,480 --> 00:00:56,540
So, it's calculations like this that the decision tree is figuring out when it

19
00:00:56,540 --> 00:00:57,200
does the training.

20
00:00:57,200 --> 00:01:00,400
It looks at all the training examples, all of the different features that

21
00:01:00,400 --> 00:01:03,710
are available to it, and it uses this information gain

22
00:01:03,710 --> 00:01:08,370
criterion in deciding which variables to split on, and how to make the splits.
