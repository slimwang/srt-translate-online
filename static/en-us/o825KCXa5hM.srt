1
00:00:00,130 --> 00:00:02,216
To analyze a cache-oblivious algorithm,

2
00:00:02,216 --> 00:00:04,420
we're going to need a model
of how a cache works.

3
00:00:04,420 --> 00:00:06,400
Here's the model I want you to assume,

4
00:00:06,400 --> 00:00:08,800
which we sometimes call
the ideal cache model.

5
00:00:08,800 --> 00:00:10,687
Start by assuming that slow and

6
00:00:10,687 --> 00:00:13,890
fast memory are divided into
blocks of size L words.

7
00:00:13,890 --> 00:00:16,210
This L is the same as the transfer size.

8
00:00:16,210 --> 00:00:21,200
We'll refer to one of these blocks in
fast memory or cache as a cache line.

9
00:00:21,200 --> 00:00:25,220
So you have a machine in which fast
memory is being managed automatically.

10
00:00:25,220 --> 00:00:27,340
Consider your algorithm or program.

11
00:00:27,340 --> 00:00:31,300
As it runs, it issues sequences
of load and store operations.

12
00:00:31,300 --> 00:00:34,890
These loads and stores reference
addresses in slow memory.

13
00:00:34,890 --> 00:00:35,776
For this lesson,

14
00:00:35,776 --> 00:00:39,510
let's assume that the algorithm
issues these operations sequentially.

15
00:00:39,510 --> 00:00:41,680
Now, consider some load operation.

16
00:00:41,680 --> 00:00:44,122
Suppose it reads from
an address call it a and

17
00:00:44,122 --> 00:00:47,690
wants to load the value into a register,
call it r.

18
00:00:47,690 --> 00:00:52,290
The hardware will check to see if
a copy of a is already in fast memory.

19
00:00:52,290 --> 00:00:55,490
If it's there, then it returns the value
and completes the store operation,

20
00:00:55,490 --> 00:00:57,560
which writes to register r.

21
00:00:57,560 --> 00:00:59,690
Let's refer to this case as a cache hit,

22
00:00:59,690 --> 00:01:02,580
because the value that
we want is in cache.

23
00:01:02,580 --> 00:01:06,250
If the value we want is not in cache,
then it's a cache miss.

24
00:01:06,250 --> 00:01:09,697
In this case, the hardware grabs
the value from slow memory, but

25
00:01:09,697 --> 00:01:11,460
also stashes a copy in the cache.

26
00:01:11,460 --> 00:01:15,510
Keep in mind that the hardware has
to transfer an entire cache line.

27
00:01:15,510 --> 00:01:20,363
Now, which L consecutive words around
the address a get transferred depends

28
00:01:20,363 --> 00:01:21,640
on how a is aligned.

29
00:01:21,640 --> 00:01:26,640
So, what about a store from say
a register s to a memory address b?

30
00:01:26,640 --> 00:01:29,500
It will behave kind of
like a load operation.

31
00:01:29,500 --> 00:01:32,681
There's a copy of b in cache,
then it's a cache hit and

32
00:01:32,681 --> 00:01:34,350
we update the cached value.

33
00:01:34,350 --> 00:01:38,000
Otherwise, there's no copy of b in
the cache and it's a cache miss.

34
00:01:38,000 --> 00:01:41,760
The hardware would load the block
from slow memory into cache.

35
00:01:41,760 --> 00:01:46,390
In other words, a store miss like
a load miss causes a memory transfer.

36
00:01:46,390 --> 00:01:49,710
So those were the basics of load and
store operations.

37
00:01:49,710 --> 00:01:52,670
Here's the next assumption
in the ideal cache model.

38
00:01:52,670 --> 00:01:54,880
Cache is fully associative.

39
00:01:54,880 --> 00:01:56,420
So, what does that mean?

40
00:01:56,420 --> 00:02:00,850
Remember that a cache consists of
a set of cache lines or cache blocks.

41
00:02:00,850 --> 00:02:05,505
Now suppose you load a new block from
slow memory, full associativity means

42
00:02:05,505 --> 00:02:09,590
that this block is allowed to go
into any block or line of the cache.

43
00:02:09,590 --> 00:02:14,170
Now you may know about set associative
caches and direct-mapped caches.

44
00:02:14,170 --> 00:02:17,770
If you do, then you know the real
caches typically don't implement full

45
00:02:17,770 --> 00:02:19,120
associativity.

46
00:02:19,120 --> 00:02:21,547
Rather, they implement
one of these schemes,

47
00:02:21,547 --> 00:02:24,338
which has the effect of
restricting the possible cache

48
00:02:24,338 --> 00:02:26,840
lines that are given memory
address can go into.

49
00:02:26,840 --> 00:02:29,630
Full associativity says,
you can ignore this restriction.

50
00:02:29,630 --> 00:02:32,309
It's a simplifying
assumption that will make

51
00:02:32,309 --> 00:02:35,545
our ideal cache model more
powerful than real cache is.

52
00:02:35,545 --> 00:02:39,625
Now at some point, the cache will
be full of previously used values.

53
00:02:39,625 --> 00:02:41,153
To make room for new values,

54
00:02:41,153 --> 00:02:45,277
the hardware will need to choose
some line to kick out or to evict.

55
00:02:45,277 --> 00:02:49,562
If the value being evicted hasn't been
written to main memory yet, because say,

56
00:02:49,562 --> 00:02:53,677
it was a store hit previously, then
that will cause another memory transfer.

57
00:02:53,677 --> 00:02:57,137
I'll refer to those transfers
as store-evictions.

58
00:02:57,137 --> 00:03:00,057
So if we have to kick something out,
what do we kick out?

59
00:03:00,057 --> 00:03:03,864
That leads us to the next assumption
of the ideal cache model,

60
00:03:03,864 --> 00:03:05,337
optimal replacement.

61
00:03:05,337 --> 00:03:09,538
Optimal replacement means that
the hardware managing the cache actually

62
00:03:09,538 --> 00:03:10,627
knows the future.

63
00:03:10,627 --> 00:03:14,580
In particular,
the hardware knows all future accesses.

64
00:03:14,580 --> 00:03:17,630
It looks at all the blocks
currently in the cache and then

65
00:03:17,630 --> 00:03:22,010
evicts the one that will be accessed
the most distantly in the future.

66
00:03:22,010 --> 00:03:26,170
At first glance, this might strike
you as being extremely idealistic or

67
00:03:26,170 --> 00:03:27,330
optimistic.

68
00:03:27,330 --> 00:03:29,936
But in fact,
we'll do an analysis of just how

69
00:03:29,936 --> 00:03:32,760
powerful this assumption
really is in a moment.

70
00:03:32,760 --> 00:03:36,100
Let's do a quick summary of all the
assumptions of the ideal cache model.

71
00:03:36,100 --> 00:03:38,900
We'll model the program as
issuing a sequence of load and

72
00:03:38,900 --> 00:03:41,420
store operations to slow memory.

73
00:03:41,420 --> 00:03:44,060
The hardware manages
the z words of cache,

74
00:03:44,060 --> 00:03:47,350
which is divided into lines
of size L words each.

75
00:03:47,350 --> 00:03:51,790
These Z over L cache lines
are sometimes called cache blocks.

76
00:03:51,790 --> 00:03:54,150
As in the conventional I/O model,

77
00:03:54,150 --> 00:03:58,880
slow memory to cache transfers will
happen in lines or blocks of size L.

78
00:03:58,880 --> 00:04:02,186
If the value for some slow memory
address is already in cache,

79
00:04:02,186 --> 00:04:03,210
it's a cache hit.

80
00:04:03,210 --> 00:04:05,630
And otherwise, it's a cache miss.

81
00:04:05,630 --> 00:04:08,630
The cache will assume
it's fully associated.

82
00:04:08,630 --> 00:04:11,007
Lastly, when we need
to evict a cache line,

83
00:04:11,007 --> 00:04:13,590
we'll assume an optimal
replacement policy.

84
00:04:13,590 --> 00:04:16,630
This policy sees the future.

85
00:04:16,630 --> 00:04:18,130
One final point.

86
00:04:18,130 --> 00:04:22,000
Remember that in the conventional I/O
model, we counted memory transfers.

87
00:04:22,000 --> 00:04:24,590
In the ideal cache model,
we do the same thing.

88
00:04:24,590 --> 00:04:28,597
The number of transfers is really equal
to the number of misses plus the number

89
00:04:28,597 --> 00:04:29,840
of store-evictions.

90
00:04:29,840 --> 00:04:31,492
Now I think is a good time to see,

91
00:04:31,492 --> 00:04:34,380
if you really understand how
an ideal cache might work.
