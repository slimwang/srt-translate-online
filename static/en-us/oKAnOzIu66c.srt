1
00:00:00,140 --> 00:00:02,700
So I just want to quickly also
now just review the whole

2
00:00:03,770 --> 00:00:08,778
process of how scale-invariant
features are computed SIFTs.

3
00:00:08,778 --> 00:00:12,110
First we want to find the orientation,
compute the test orientation for

4
00:00:12,110 --> 00:00:13,860
each keypoint region.

5
00:00:13,860 --> 00:00:17,762
And then we want to actually use
the method to kind of find the keypoints

6
00:00:17,762 --> 00:00:21,390
itself and we use local image
gradients at selected scale, and

7
00:00:21,390 --> 00:00:24,010
rotation to describe
each keypoint region.

8
00:00:25,590 --> 00:00:28,010
Here's an example of
invariant local features.

9
00:00:28,010 --> 00:00:30,760
I mean,
here are two examples of the same car.

10
00:00:30,760 --> 00:00:35,490
So here for example, this region
here is matched to this region here.

11
00:00:35,490 --> 00:00:38,570
It's the same car, but
of course, we notice

12
00:00:38,570 --> 00:00:41,960
this patch is exactly the same even with
orientation and all that kind of stuff.

13
00:00:41,960 --> 00:00:44,680
And it finds others
regions like the similar.

14
00:00:44,680 --> 00:00:46,630
This region here is matched to this.

15
00:00:46,630 --> 00:00:51,330
This part here is matched, well it's
available here but there's no match here

16
00:00:51,330 --> 00:00:54,520
because these two here are either
too small or not visible.

17
00:00:54,520 --> 00:00:57,743
The bigger patch here again,
has the same characteristics on both.

18
00:00:57,743 --> 00:01:01,389
So one of the biggest advantage of SIFT
is that basically image content is

19
00:01:01,389 --> 00:01:03,903
transformed into local
feature coordinates that

20
00:01:03,903 --> 00:01:08,168
are invariant to translation, rotation,
scale, and other imaging parameters.

21
00:01:08,168 --> 00:01:12,320
And therefore allows us to
take two pairs of images with

22
00:01:12,320 --> 00:01:16,390
similar appearances of objects and
stuff like that.

23
00:01:16,390 --> 00:01:19,550
And it starts finding the features that
might have seen and do different things,

24
00:01:19,550 --> 00:01:21,680
and that allows us to do the matching.

25
00:01:21,680 --> 00:01:23,120
And that becomes an important part.

26
00:01:23,120 --> 00:01:25,070
So let's say I have these two images.

27
00:01:25,070 --> 00:01:28,780
First, I can run the whole process
again to detect all the features.

28
00:01:28,780 --> 00:01:31,430
Here it found all the features
in these two images.

29
00:01:31,430 --> 00:01:33,070
This is of course the common regions.

30
00:01:33,070 --> 00:01:35,020
These are two images next to each other.

31
00:01:35,020 --> 00:01:38,360
Now of course, what I basically
after I have found the features,

32
00:01:38,360 --> 00:01:39,160
I want to match them.

33
00:01:39,160 --> 00:01:40,040
So here basically,

34
00:01:40,040 --> 00:01:43,570
it find and matches that these
are similar between these two.

35
00:01:43,570 --> 00:01:47,760
And once I match them,
I can also then register them together.

36
00:01:47,760 --> 00:01:51,400
And if I registered, then I can use
that to align the two images together,

37
00:01:51,400 --> 00:01:52,450
which I'm not actually showing.

38
00:01:52,450 --> 00:01:54,940
I'm just basically showing that
these two features are now

39
00:01:54,940 --> 00:01:56,410
matches between these two images.
