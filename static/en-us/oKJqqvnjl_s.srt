1
00:00:00,260 --> 00:00:04,010
So we have threads at the user level, we have threads at the kernel level.

2
00:00:04,010 --> 00:00:07,610
We will now see what are some of the interactions that are necessary in

3
00:00:07,610 --> 00:00:10,410
order to efficiently manage threads.

4
00:00:10,410 --> 00:00:12,540
Consider we have a multithreaded process.

5
00:00:12,540 --> 00:00:16,420
And let's say that process has four user-level threads.

6
00:00:16,420 --> 00:00:19,910
However, the process is such that, at any given point of time,

7
00:00:19,910 --> 00:00:23,460
the actual level of concurrency is just two.

8
00:00:23,460 --> 00:00:25,590
Basically, if you look at the process,

9
00:00:25,590 --> 00:00:29,910
it always happens that two of its user-level threads are waiting on I/O, and

10
00:00:29,910 --> 00:00:32,470
then some other two are actually executing.

11
00:00:32,470 --> 00:00:36,430
So, if our operating system has a limit on the number of kernel threads that it

12
00:00:36,430 --> 00:00:42,060
can support, it would be nice if the user-level process actually said,

13
00:00:42,060 --> 00:00:43,950
I just really need two threads.

14
00:00:43,950 --> 00:00:48,380
So when the process starts, the kernel will first give it, let's say, a default

15
00:00:48,380 --> 00:00:52,440
number of kernel-level threads and the accompanying lightweight threads.

16
00:00:52,440 --> 00:00:54,100
And let's say that is one.

17
00:00:54,100 --> 00:00:58,600
Then the process will request additional kernel-level threads, and the way it's

18
00:00:58,600 --> 00:01:03,670
done is that the kernel now supports a system call called set_concurrency.

19
00:01:03,670 --> 00:01:07,930
In response to this system call the kernel will create additional threads and

20
00:01:07,930 --> 00:01:10,510
it will allocate those to this process.

21
00:01:10,510 --> 00:01:14,230
Now lets consider this scenario in which the two user-level threads that were

22
00:01:14,230 --> 00:01:17,690
mapped on the underlying kernel-level threads block.

23
00:01:17,690 --> 00:01:22,960
They needed to perform some I/O operation and then they were basically

24
00:01:22,960 --> 00:01:27,560
moved on the wait queue that's associated with that particular I/O event.

25
00:01:27,560 --> 00:01:30,120
So the kernel level threads are blocked as well.

26
00:01:30,120 --> 00:01:33,790
Now let's say we have a situation in which the two user-level threads that

27
00:01:33,790 --> 00:01:38,080
were running on kernel level threads issued an I/O request, and

28
00:01:38,080 --> 00:01:40,040
now have to wait for that to complete.

29
00:01:40,040 --> 00:01:41,190
So it's a blocking I/O.

30
00:01:41,190 --> 00:01:44,950
What that means is that the kernel-level threads themselves,

31
00:01:44,950 --> 00:01:47,140
they're also blocked on that I/O operation.

32
00:01:47,140 --> 00:01:51,900
They're waiting in a queue somewhere in the kernel for that I/O event to occur.

33
00:01:51,900 --> 00:01:55,350
Now we have a situation where the process as a whole is blocked,

34
00:01:55,350 --> 00:01:59,440
because it only had two kernel-level threads, both of them are blocked, and

35
00:01:59,440 --> 00:02:03,700
there are user-level threads that are ready to run and make progress.

36
00:02:03,700 --> 00:02:07,810
The reason why this is happening is because the user-level library doesn't know

37
00:02:07,810 --> 00:02:09,320
what is happening in the kernel,

38
00:02:09,320 --> 00:02:13,610
it doesn't know that the kernel threads are about to block.

39
00:02:13,610 --> 00:02:16,990
What would have really been useful is if the kernel had

40
00:02:16,990 --> 00:02:22,520
notified the user-level library before it blocks the kernel-level threads.

41
00:02:22,520 --> 00:02:25,850
And then the user-level library can look at its run queue,

42
00:02:25,850 --> 00:02:29,320
it can see that it has multiple runnable user-level threads, and,

43
00:02:29,320 --> 00:02:33,700
in response, can let the kernel know, so, call a system call

44
00:02:33,700 --> 00:02:37,620
to request more kernel-level threads or lightweight processes.

45
00:02:37,620 --> 00:02:40,915
Now in response to this call, the kernel can allocate an extra

46
00:02:40,915 --> 00:02:44,905
kernel-level thread, and the library can start scheduling the remaining

47
00:02:44,905 --> 00:02:48,590
user-level threads onto the associated lightweight process.

48
00:02:48,590 --> 00:02:53,220
At a later time when the I/O operation completes, at some point the kernel will

49
00:02:53,220 --> 00:02:57,960
notice that one of the kernel-level threads is pretty much constantly idle,

50
00:02:57,960 --> 00:03:02,050
because we said that that's the natural state of this particular application.

51
00:03:02,050 --> 00:03:05,310
So maybe the kernel can tell the kernel-level library that, you no

52
00:03:05,310 --> 00:03:09,220
longer have access to this kernel-level thread, so you can't schedule on it.

53
00:03:09,220 --> 00:03:12,240
By going through these examples you realize that

54
00:03:12,240 --> 00:03:15,630
both the user-level library doesn't know what's happening in the kernel, but

55
00:03:15,630 --> 00:03:19,440
also the kernel doesn't know what's happening at the user level.

56
00:03:19,440 --> 00:03:22,390
Both of these fact cause for some problems.

57
00:03:22,390 --> 00:03:26,680
To correct for these issues, we saw how in the Solaris threading implementation,

58
00:03:26,680 --> 00:03:30,300
they introduced certain system calls and special signals that

59
00:03:30,300 --> 00:03:35,430
can be used to pass or request certain things among these two layers.

60
00:03:35,430 --> 00:03:38,042
And basically this is how the kernel-level and

61
00:03:38,042 --> 00:03:41,510
the user-level thread management interact and coordinate.
