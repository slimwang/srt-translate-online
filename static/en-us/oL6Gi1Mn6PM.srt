1
00:00:00,130 --> 00:00:02,620
Let's suppose that x and y are numerical.

2
00:00:04,050 --> 00:00:06,340
Do x and y have anything to do with each other?

3
00:00:07,930 --> 00:00:10,450
Let's try to pose the question in a more precise way.

4
00:00:11,740 --> 00:00:13,260
You've been collecting records and

5
00:00:13,260 --> 00:00:17,870
so, you can form a joint probability table between x and y.

6
00:00:17,870 --> 00:00:23,330
So we can say in an absolute sense, in a prior probability sense.

7
00:00:23,330 --> 00:00:26,710
What degree of certainty do we have about x?

8
00:00:26,710 --> 00:00:30,600
If x takes on the values x1 or x2 we can ask,

9
00:00:30,600 --> 00:00:33,700
what's the probability with which it takes on those values?

10
00:00:33,700 --> 00:00:37,190
So, from the table here we know we can just evaluate those numbers.

11
00:00:37,190 --> 00:00:40,301
They're just 0.29 and 0.71.

12
00:00:40,301 --> 00:00:44,420
Now let's suppose that we know y is stuck at y1.

13
00:00:44,420 --> 00:00:48,810
Does this make us more certain at all about what x is?

14
00:00:48,810 --> 00:00:53,850
Specifically what's the conditional probability that x is x1 or

15
00:00:53,850 --> 00:00:56,630
x2, given that we know y is fixed at y1.

16
00:00:56,630 --> 00:01:00,144
Well again we can just directly evaluate it from the table.

17
00:01:00,144 --> 00:01:03,508
It's 0.16 and 0.84.

18
00:01:03,508 --> 00:01:07,340
So it seems that knowing that y is stuck at y1 has told us

19
00:01:07,340 --> 00:01:09,750
something about the value of x, right?

20
00:01:09,750 --> 00:01:11,340
Intuitively we know that.

21
00:01:11,340 --> 00:01:15,440
It caused the probability to skew a little bit to one side.

22
00:01:15,440 --> 00:01:17,300
In this case, towards x2.

23
00:01:17,300 --> 00:01:21,020
It turns out there's an expression to measure the degree of

24
00:01:21,020 --> 00:01:23,540
skew of a probability distribution.

25
00:01:23,540 --> 00:01:25,970
This expression is called entropy.

26
00:01:25,970 --> 00:01:31,600
Specifically, let's suppose that a random variable x takes on the values x1,

27
00:01:31,600 --> 00:01:33,320
x2 though xk.

28
00:01:33,320 --> 00:01:39,530
The entropy for the probability distribution of x is labeled h of x.

29
00:01:39,530 --> 00:01:45,470
And it is defined as minus p1 times log of p1 minus p2 times log of p2 and

30
00:01:45,470 --> 00:01:47,360
so on through pk.

31
00:01:47,360 --> 00:01:52,460
Otherwise written as minus 1 times the sum of pi log pi.

32
00:01:52,460 --> 00:01:56,090
Now the base of the logarithm isn't necessarily important.

33
00:01:56,090 --> 00:02:01,790
If the base is base two, then, the units of entropy is called bits.

34
00:02:01,790 --> 00:02:05,640
If the base is e, then the units are called nets.

35
00:02:05,640 --> 00:02:08,530
Traditionally in computer science, we use base two,

36
00:02:08,530 --> 00:02:11,600
but, it's not really super important.

37
00:02:11,600 --> 00:02:15,540
Now to see what's behind the expression h of x.

38
00:02:16,960 --> 00:02:21,580
And why this formula was chosen over other options for formulas and much more

39
00:02:21,580 --> 00:02:25,540
details about information theory in general, see the instructor's notes.
