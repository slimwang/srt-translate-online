1
00:00:00,110 --> 00:00:02,510
So what did we learn about
exploring exploration, Charles?

2
00:00:02,510 --> 00:00:04,366
>> Well, we actually learned a lot, but

3
00:00:04,366 --> 00:00:07,530
really I think you can summarize
it in just a couple things.

4
00:00:07,530 --> 00:00:11,850
We learned about bandits,
which is really about stochasticity and

5
00:00:11,850 --> 00:00:12,950
randomness.

6
00:00:12,950 --> 00:00:17,560
And in particular,
we learned about how we can

7
00:00:17,560 --> 00:00:22,010
estimate what we know using Hoeffding
bounds and maybe some union bounds.

8
00:00:22,010 --> 00:00:22,530
>> So, yeah.

9
00:00:22,530 --> 00:00:25,031
In particular we employed
Hoeffding bound,

10
00:00:25,031 --> 00:00:29,506
u nion bound to convince ourselves that
we have a sufficiently accurate estimate

11
00:00:29,506 --> 00:00:33,452
that we can get near optimal reward
in these kind of non-sequential but

12
00:00:33,452 --> 00:00:35,500
stochastic decision problems.

13
00:00:35,500 --> 00:00:39,097
>> Right, and that's really important
because a lot of the time you don't know

14
00:00:39,097 --> 00:00:41,088
what to do because you're not certain,
or

15
00:00:41,088 --> 00:00:43,321
how certain you really
are about what you know.

16
00:00:43,321 --> 00:00:46,779
And what the Hoefding and union bounds
tell us is how certain we really are and

17
00:00:46,779 --> 00:00:48,736
so we can know when
we're certain enough.

18
00:00:48,736 --> 00:00:50,485
>> Good, all right so
we have a stochastic world,

19
00:00:50,485 --> 00:00:52,910
we want to learn how it works so
that we can get near optimal reward.

20
00:00:52,910 --> 00:00:53,870
So we can optimize.

21
00:00:53,870 --> 00:00:55,042
>> Right, so that was the first thing.

22
00:00:55,042 --> 00:01:00,039
And then the second thing is we learned,
well, we learned about Rmax, really,

23
00:01:00,039 --> 00:01:03,724
and how Rmax works specifically
with deterministic MDPs.

24
00:01:03,724 --> 00:01:07,486
>> Right, and so that was about
optimism in the face of uncertainty and

25
00:01:07,486 --> 00:01:11,380
how it would cause us to explore or
discover new stuff about the world,

26
00:01:11,380 --> 00:01:12,898
kind of by planning ahead and

27
00:01:12,898 --> 00:01:16,730
trying to get to states where
new information could be gained.

28
00:01:16,730 --> 00:01:18,520
>> Right, well what it really says,
it's really,

29
00:01:18,520 --> 00:01:21,880
we believe the grass is always greener
until we get there and learn otherwise.

30
00:01:21,880 --> 00:01:22,500
>> All right, great.

31
00:01:22,500 --> 00:01:24,640
This let us deal with
sequential decision making,

32
00:01:24,640 --> 00:01:27,010
this let us deal with
stochastic decision making.

33
00:01:27,010 --> 00:01:28,020
>> And then we combine them.

34
00:01:28,020 --> 00:01:33,280
>> Right, where we used the bandit
idea to estimate the noisy parameters,

35
00:01:33,280 --> 00:01:35,740
the transition probabilities and
so forth, and

36
00:01:35,740 --> 00:01:39,450
we used the Rmax idea to make sure
that we visited things enough so

37
00:01:39,450 --> 00:01:41,390
that we could get those
accurate estimates.

38
00:01:41,390 --> 00:01:44,530
>> Right, so
Rmax basically remained Rmax, and

39
00:01:44,530 --> 00:01:47,753
the bandit stuff helped us with
the transition probabilities, so

40
00:01:47,753 --> 00:01:49,460
that we knew when we
actually believed them.

41
00:01:49,460 --> 00:01:52,990
So, they were kind of,
this isn't really true, but

42
00:01:52,990 --> 00:01:54,940
they were kind of deterministic enough.

43
00:01:54,940 --> 00:01:59,100
So we knew enough to know what
the transition model is, and

44
00:01:59,100 --> 00:02:01,395
once you know what the transition model
is you can just kind of go from there,

45
00:02:01,395 --> 00:02:04,040
because then you can just estimate
the rewards and do the right thing.

46
00:02:04,040 --> 00:02:08,090
>> Now this notion of, the way that
learning happens in the bandit

47
00:02:08,090 --> 00:02:10,590
here where we actually kind of
distinguish between known and

48
00:02:10,590 --> 00:02:13,420
unknown, can actually be generalized

49
00:02:13,420 --> 00:02:16,375
even beyond the kind of general
MDPs that we're talking about here.

50
00:02:17,580 --> 00:02:21,076
And I don't think we're going to get
a chance to talk about this, but

51
00:02:21,076 --> 00:02:25,200
the idea of KWIK learning is learning
transition probabilities using methods

52
00:02:25,200 --> 00:02:26,615
that know what they know.

53
00:02:26,615 --> 00:02:30,230
And if it can distinguish between
known and unknown, it can associate

54
00:02:30,230 --> 00:02:33,750
optimism with the unknown and then we
can make guarantees on how uniquely and

55
00:02:33,750 --> 00:02:36,690
efficiently near optimal
behavior can be learned.

56
00:02:36,690 --> 00:02:39,460
So this is kind of a learning
framework that my students and

57
00:02:39,460 --> 00:02:44,470
I have been studying to try to
generalize beyond learning in

58
00:02:44,470 --> 00:02:48,480
tabular MDPs to be able to generalize
between transition probabilities and

59
00:02:48,480 --> 00:02:49,390
different parts of the MDP.

60
00:02:49,390 --> 00:02:50,244
>> Well that all makes sense to me.

61
00:02:50,244 --> 00:02:51,781
So then the only question is,

62
00:02:51,781 --> 00:02:54,918
how does this help us do
practical reinforcement learning?

63
00:02:54,918 --> 00:02:56,845
>> All right, so
that pretty much ends this.

64
00:02:56,845 --> 00:03:00,145
>> [LAUGH]
>> Well, okay, so just briefly,

65
00:03:00,145 --> 00:03:04,310
I find that Rmax is actually a really
effective algorithm to use in practice.

66
00:03:04,310 --> 00:03:07,230
We don't set C,
we don't set that parameter of how often

67
00:03:07,230 --> 00:03:10,730
we need to see things to be what
the theory says we should set it to.

68
00:03:10,730 --> 00:03:13,330
We usually set it to something much,
much smaller than that.

69
00:03:13,330 --> 00:03:17,550
And we tried it by hand, strike a trade
off between making it too small,

70
00:03:17,550 --> 00:03:21,230
in which case it might actually not
learn near optimal behavior, and

71
00:03:21,230 --> 00:03:25,281
making it too big, in which case the
learner actually has to sniff around and

72
00:03:25,281 --> 00:03:28,690
visit many, many, many, many state
action pairs over and over again.

73
00:03:28,690 --> 00:03:32,390
So, Rmax itself does seems to
be a pretty effective algorithm.

74
00:03:32,390 --> 00:03:34,490
It makes very good use of the data.

75
00:03:34,490 --> 00:03:37,850
It's just we can't quite use it in
the theoretically specified way.

76
00:03:37,850 --> 00:03:39,160
>> Okay, well that's good enough for me.

77
00:03:39,160 --> 00:03:40,150
Seems pretty practical.

78
00:03:40,150 --> 00:03:40,740
Okay, so we're done.

79
00:03:40,740 --> 00:03:41,790
So, are we done with the class?

80
00:03:41,790 --> 00:03:42,290
What's next?

81
00:03:42,290 --> 00:03:43,620
>> We have more stuff!

82
00:03:43,620 --> 00:03:45,000
We have what's next, next.

83
00:03:45,000 --> 00:03:46,650
>> Oh cool, well I look forward to that.
