1
00:00:00,230 --> 00:00:03,730
There's a work out of Stanford and
UC Berkeley

2
00:00:03,730 --> 00:00:08,780
on using reinforcement learning to
actually fly model helicopter tricks.

3
00:00:08,780 --> 00:00:09,800
>> Like what?

4
00:00:09,800 --> 00:00:12,310
>> So it turns out that if you,
you're flying a model helicopter,

5
00:00:12,310 --> 00:00:13,870
I've tried to do this.

6
00:00:13,870 --> 00:00:15,230
Turns out to be really hard to do.

7
00:00:15,230 --> 00:00:19,920
Part of it is you have to coordinate
the blades in the blades in the Blades.

8
00:00:19,920 --> 00:00:25,360
>> Yeah, and part of it is that you
can do crazy things with helicopters,

9
00:00:25,360 --> 00:00:26,130
if you know how to do it.

10
00:00:26,130 --> 00:00:28,680
You can make them fly upside down,
you can make them swing back and

11
00:00:28,680 --> 00:00:32,549
forth, you can make them kind
of roll over and over and over.

12
00:00:32,549 --> 00:00:34,420
You can do all sorts of really,
well I don't know,

13
00:00:34,420 --> 00:00:36,390
you can do all sorts of amazing things,
but

14
00:00:36,390 --> 00:00:40,030
professional stunt flying helicopter
people can do amazing things.

15
00:00:40,030 --> 00:00:43,760
What the reinforcement learning work was
about is trying to get a system to learn

16
00:00:43,760 --> 00:00:45,980
to do those same kinds of tricks.

17
00:00:45,980 --> 00:00:48,170
>> And they were able to do
this with real helicopters or

18
00:00:48,170 --> 00:00:49,340
real model helicopters?

19
00:00:49,340 --> 00:00:51,800
>> A real model helicopter,
not a simulator, but

20
00:00:51,800 --> 00:00:54,380
an actual Mini helicopter.

21
00:00:54,380 --> 00:00:55,050
>> Okay, so that's cool.

22
00:00:55,050 --> 00:00:59,450
So you sort of convinced me that burlap
and repository, that's sort of a good.

23
00:00:59,450 --> 00:01:00,158
Okay, I buy that.

24
00:01:00,158 --> 00:01:01,767
But I do want to point out
something that you said,

25
00:01:01,767 --> 00:01:02,876
that I thought was interesting.

26
00:01:02,876 --> 00:01:04,608
You talked about simulators and

27
00:01:04,608 --> 00:01:08,140
that maybe what we really want to
do is work in the real world.

28
00:01:08,140 --> 00:01:11,370
But it occurs to me that at
least one of those examples,

29
00:01:11,370 --> 00:01:12,980
the simulator is the real world.

30
00:01:12,980 --> 00:01:14,540
So, games, right?

31
00:01:14,540 --> 00:01:16,460
So if you have games like Backgammon,

32
00:01:16,460 --> 00:01:19,920
if you have simulations that
people actually interact with

33
00:01:21,020 --> 00:01:24,330
then that is actually the real problem
itself, that is the real world.

34
00:01:24,330 --> 00:01:27,129
But if I'm playing Pac-Man and
I've got a simulator for Pac-Man,

35
00:01:27,129 --> 00:01:28,950
that's the same thing as having Pac-Man.

36
00:01:28,950 --> 00:01:31,740
And if I can learn to play Pac-Man,
I've learn to play Pac-Man.

37
00:01:31,740 --> 00:01:33,200
There's not really a simulator involved.

38
00:01:33,200 --> 00:01:36,517
>> I would make a distinction between
grid world type simulators where

39
00:01:36,517 --> 00:01:38,380
they were developed specifically for

40
00:01:38,380 --> 00:01:40,948
reinforcement learning
researchers to work with.

41
00:01:40,948 --> 00:01:44,134
Versus, as you say, the kinds of
simulators that people actually interact

42
00:01:44,134 --> 00:01:46,638
with when they're doing things
like playing video games.

43
00:01:46,638 --> 00:01:50,421
>> And so, video games actually is
a really great intermediate step between

44
00:01:50,421 --> 00:01:54,480
real physical world stuff and
things that can be run in simulation.

45
00:01:54,480 --> 00:01:57,320
And actually there's been some
terrific work in that space.

46
00:01:57,320 --> 00:01:58,190
>> Really, like what?

47
00:01:58,190 --> 00:02:00,790
>> Well, one of the things that
I'm very excited about recently is

48
00:02:00,790 --> 00:02:05,180
a group called Deep Mind, which
they're very good at naming things.

49
00:02:05,180 --> 00:02:06,770
Have a system for

50
00:02:06,770 --> 00:02:10,020
actually doing what they call deep
reinforcement learning, deep q learning.

51
00:02:10,020 --> 00:02:14,920
Where they are actually trying to
learn behavior in Atari video games.

52
00:02:14,920 --> 00:02:19,110
Video games from the 80s, where they
actually are using Atari emulators,

53
00:02:19,110 --> 00:02:21,870
the actual games as people
actually play them.

54
00:02:21,870 --> 00:02:25,540
Taking the information that's
available on the screen as input and

55
00:02:25,540 --> 00:02:27,390
sending joystick commands as output, and

56
00:02:27,390 --> 00:02:30,270
learning to play actually at
the level of human players.

57
00:02:30,270 --> 00:02:32,950
>> Wait,
they're as good as human players?

58
00:02:32,950 --> 00:02:35,740
>> They are as good on
average as human players.

59
00:02:35,740 --> 00:02:37,980
In some of the games, they're
actually much better than people.

60
00:02:37,980 --> 00:02:41,710
Games like Breakout where you
have to bounce a ball all around.

61
00:02:41,710 --> 00:02:42,340
>> Okay.
>> And

62
00:02:42,340 --> 00:02:45,480
then there's other games where they're
actually much much worse than people.

63
00:02:45,480 --> 00:02:49,120
Games like Frost Bite where you have
to jump around on a bunch of ice floes.

64
00:02:49,120 --> 00:02:50,390
It seems very difficult for

65
00:02:50,390 --> 00:02:52,780
the system to actually learn
what to do in that game.

66
00:02:52,780 --> 00:02:57,070
>> Okay, so their heads in an oven,
their feet is in ice water.

67
00:02:57,070 --> 00:02:58,248
But on average, they're comfortable.

68
00:02:58,248 --> 00:02:59,408
>> I see what you're saying.

69
00:02:59,408 --> 00:03:02,698
So, your point is that it's not
playing just like a person.

70
00:03:02,698 --> 00:03:05,197
It's playing better and
worse than people, so

71
00:03:05,197 --> 00:03:08,456
on average you can say that it's
kind of playing like people.

72
00:03:08,456 --> 00:03:10,200
>> Mm-hm, but still that's kind of cool.

73
00:03:10,200 --> 00:03:11,220
>> I think it's really amazing and

74
00:03:11,220 --> 00:03:15,980
I think it's that particular data set
is itself really interesting, because

75
00:03:15,980 --> 00:03:20,220
there's 40 some odd Atari video games
that are all built in the same system.

76
00:03:20,220 --> 00:03:22,850
You can use the same inputs, and
the same outputs, and the same learning

77
00:03:22,850 --> 00:03:25,310
algorithm, but you can put in different
games to see what it will do.

78
00:03:25,310 --> 00:03:28,656
>> Okay, so it is just like Weka and
it's just like UCI, except here,

79
00:03:28,656 --> 00:03:32,590
Weka is the burlap algorithms or
whatever, reinforcement algorithms.

80
00:03:32,590 --> 00:03:36,850
And UCI, instead of being input output
pairs are sort of the real world.

81
00:03:36,850 --> 00:03:40,090
>> Yeah, and there's some work that's
being done in collecting data from

82
00:03:40,090 --> 00:03:40,590
the real world.

83
00:03:40,590 --> 00:03:44,778
For example, medical diagnosis or
online education.

84
00:03:44,778 --> 00:03:45,998
>> I like that,
we should do something like that.

85
00:03:45,998 --> 00:03:46,614
>> [LAUGH] For

86
00:03:46,614 --> 00:03:51,630
trying to figure out what the right way
of interacting with people online is.

87
00:03:51,630 --> 00:03:53,830
People have collected some of that data
and there's reinforcement learning

88
00:03:53,830 --> 00:03:55,440
algorithms that are being
run at that data.

89
00:03:55,440 --> 00:03:59,370
It's not the full reinforcement learning
problem, because you don't actually,

90
00:03:59,370 --> 00:04:02,830
the learning system doesn't actually
have to decide how to collect the data,

91
00:04:02,830 --> 00:04:04,260
it's already been collected for it.

92
00:04:04,260 --> 00:04:06,950
But it is really using real data and
making decisions based on that and

93
00:04:06,950 --> 00:04:09,340
I think that's a really
promising direction for

94
00:04:09,340 --> 00:04:10,970
reinforcement learning
to go in the future.

95
00:04:10,970 --> 00:04:12,926
>> Okay, I like it.
So I got two things out of that.

96
00:04:12,926 --> 00:04:16,048
The first thing is reinforcement
learning really can be used in the real

97
00:04:16,048 --> 00:04:17,504
world, it has been, it can be and

98
00:04:17,504 --> 00:04:19,757
there's a nice bright future for
it that's good.

99
00:04:19,757 --> 00:04:21,168
And the second thing is I was right,

100
00:04:21,168 --> 00:04:23,220
there was a bunch of stuff
we hadn't talked about.

101
00:04:23,220 --> 00:04:25,855
>> Yeah, okay, all right,
that's a valid point.

102
00:04:25,855 --> 00:04:27,955
>> Okay, was there anything else
you want to wrap up with anything?

103
00:04:27,955 --> 00:04:31,735
>> Well, I feel like we should say maybe
a little bit about the current direction

104
00:04:31,735 --> 00:04:33,105
in the future of reinforcement learning.

105
00:04:33,105 --> 00:04:36,365
So, one of the things I think is really
neat is that it's having an impact

106
00:04:36,365 --> 00:04:38,675
not just on engineering and
computer science, but

107
00:04:38,675 --> 00:04:42,935
also on the behavioral sciences and
the neurosciences.

108
00:04:42,935 --> 00:04:43,710
>> How so?

109
00:04:43,710 --> 00:04:46,730
>> Well so, the sorts of things that we
think about reinforcement learning being

110
00:04:46,730 --> 00:04:50,070
good for, are an agent it's
interacting with some environment,

111
00:04:50,070 --> 00:04:51,250
it's making decisions.

112
00:04:51,250 --> 00:04:54,030
It's trying to figure out what to do,
it's trying to be happy.

113
00:04:54,030 --> 00:04:57,610
And so you could argue that animals and
people are trying to do exactly those

114
00:04:57,610 --> 00:05:00,980
sorts of things,
that we are reinforcement learners.

115
00:05:00,980 --> 00:05:02,980
>> Sure animals do it, people do it.

116
00:05:02,980 --> 00:05:03,748
>> Yeah.
>> Birds do it.

117
00:05:03,748 --> 00:05:06,350
>> Educated fleas have been known
to do reinforcement learning.

118
00:05:06,350 --> 00:05:06,850
>> Right, okay.

119
00:05:06,850 --> 00:05:10,240
>> And so, the interesting thing about
that is the Nora scientists when they're

120
00:05:10,240 --> 00:05:11,060
trying to think about, okay.

121
00:05:11,060 --> 00:05:13,929
Well, how can we understand the
algorithms that essentially that people

122
00:05:13,929 --> 00:05:14,750
are running.

123
00:05:14,750 --> 00:05:17,380
They're turning to the reinforcement
learning field as a source of

124
00:05:17,380 --> 00:05:21,100
inspiration for the structure of
the algorithm, how they can be set up.

125
00:05:21,100 --> 00:05:23,390
What sorts of things seem to work and

126
00:05:23,390 --> 00:05:27,334
not work and they're actually looking
for evidence of these in real brain.

127
00:05:27,334 --> 00:05:28,796
>> I like that.
I like that a lot.

128
00:05:28,796 --> 00:05:30,245
And you know what I
really like about that?

129
00:05:30,245 --> 00:05:32,550
That it takes us back to the beginning
of the machine learning class.

130
00:05:32,550 --> 00:05:34,960
It takes us back to the beginning of
the reinforcement learning class,

131
00:05:34,960 --> 00:05:37,980
where we tried to emphasize the point
that reinforcement learning

132
00:05:37,980 --> 00:05:40,730
is not just a set of algorithms,
it's not just an approach.

133
00:05:40,730 --> 00:05:43,340
It's actually a way of
thinking about problems.

134
00:05:43,340 --> 00:05:46,588
And so really this whole class
has been about solving problems.

135
00:05:46,588 --> 00:05:48,535
And I feel like we have
solved some problems.

136
00:05:48,535 --> 00:05:51,150
>> Great, I think that's
a wonderful note to end things on,

137
00:05:51,150 --> 00:05:52,110
what do you think?

138
00:05:52,110 --> 00:05:54,140
>> No, I think this is really valuable,
thanks so much.

139
00:05:54,140 --> 00:05:57,020
>> All right well,
thank you very much, Michael.

140
00:05:57,020 --> 00:05:59,400
I appreciate it and
I will see you in the next class.

141
00:06:01,610 --> 00:06:02,110
Bye.
