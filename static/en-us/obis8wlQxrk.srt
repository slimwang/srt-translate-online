1
00:00:00,260 --> 00:00:03,320
So we've determined that we want to
figure out which set of parameters

2
00:00:03,320 --> 00:00:06,700
provide the best predictions for our
output variable, but how can we do that?

3
00:00:07,900 --> 00:00:10,590
We'll use an algorithm
called gradient descent.

4
00:00:10,590 --> 00:00:13,390
First, we need to define
some cost function

5
00:00:13,390 --> 00:00:14,970
which we'll call J of big theta.

6
00:00:16,290 --> 00:00:20,340
I'm going to use big theta here to
represent our entire set of theta's and

7
00:00:20,340 --> 00:00:22,860
I'll use this notation throughout
the rest of this lesson.

8
00:00:22,860 --> 00:00:25,530
The cost function is meant to
provide a measure of how well our

9
00:00:25,530 --> 00:00:28,880
current set of thetas does at
modeling the observed data.

10
00:00:28,880 --> 00:00:31,060
So we want to minimize
the cost functions value.

11
00:00:32,340 --> 00:00:35,500
As we discussed just a moment ago,
when we're doing linear regression,

12
00:00:35,500 --> 00:00:39,390
our cost function J of theta can simply
measure the sum of the squares of

13
00:00:39,390 --> 00:00:42,050
the differences between our
predicted and observed values.
