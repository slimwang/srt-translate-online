1
00:00:00,000 --> 00:00:03,000
The definition of the expected sum of future

2
00:00:03,000 --> 00:00:06,000
possible discounted reward that it has given you

3
00:00:06,000 --> 00:00:10,000
allows me to define a value function.

4
00:00:10,000 --> 00:00:13,000
For each status, my value of the state

5
00:00:13,000 --> 00:00:17,000
is the expected sum of future discounted reward

6
00:00:17,000 --> 00:00:21,000
provided that I start in state S,

7
00:00:21,000 --> 00:00:23,000
then I execute policy pi.

8
00:00:23,000 --> 00:00:26,000
This expression looks really complex,

9
00:00:26,000 --> 00:00:28,000
but it really means something really simple,

10
00:00:28,000 --> 00:00:30,000
which is suppose we start in the state over here,

11
00:00:30,000 --> 00:00:34,000
and you get +100 over here, -100 over here.

12
00:00:34,000 --> 00:00:38,000
And suppose for now, every other state costs you -3.

13
00:00:38,000 --> 00:00:41,000
For any possible policy that assigns actions to

14
00:00:41,000 --> 00:00:44,000
the non-absorbing states, you can now

15
00:00:44,000 --> 00:00:47,000
simulate the agent for quite a while and compute empirically

16
00:00:47,000 --> 00:00:52,000
what is the average reward that is being received

17
00:00:52,000 --> 00:00:54,000
until you finally hit a goal state.

18
00:00:54,000 --> 00:00:57,000
For example, for the policy that you like,

19
00:00:57,000 --> 00:01:00,000
the value would, of course, for any state

20
00:01:00,000 --> 00:01:02,000
depend on how much you make progress towards the goal,

21
00:01:02,000 --> 00:01:04,000
or whether you bounce back and forth.

22
00:01:04,000 --> 00:01:06,000
In fact, in this state over here, you might bounce down

23
00:01:06,000 --> 00:01:08,000
and have to do the loop again.

24
00:01:08,000 --> 00:01:11,000
But there's a well defined expectation

25
00:01:11,000 --> 00:01:14,000
over any possible execution of the policy pi

26
00:01:14,000 --> 00:01:17,000
that is generic to each state and each policy pi.

27
00:01:17,000 --> 00:01:19,000
That's called a value.

28
00:01:19,000 --> 00:01:21,000
And value functions are absolutely essential to MDP,

29
00:01:21,000 --> 00:01:25,000
so the way we're going to plan is we're going to iterate

30
00:01:25,000 --> 00:01:28,000
and compute value functions, and it will turn out

31
00:01:28,000 --> 99:59:59,999
that by doing this, we're going to find better and better policies as well.
