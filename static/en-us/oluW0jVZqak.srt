1
00:00:00,360 --> 00:00:02,440
So, yeah, so next time we're
going to dive into algorithms and

2
00:00:02,440 --> 00:00:05,210
talk about temporal difference learning,
so why don't we just sort of summarize

3
00:00:05,210 --> 00:00:08,630
the philosophical stuff that
we talked about just now.

4
00:00:08,630 --> 00:00:10,020
>> Okay.
>> So what have we learned?

5
00:00:10,020 --> 00:00:11,880
>> So,
I think we learned a bunch of stuff, but

6
00:00:11,880 --> 00:00:14,540
we really learned three
classes of things.

7
00:00:15,640 --> 00:00:18,671
We learned sort of what
reinforcement learning is.

8
00:00:18,671 --> 00:00:20,560
>> All right,
that's one class of things.

9
00:00:20,560 --> 00:00:25,070
>> Yeah, and as you wrote up there, we
realize RL is about agents interacting

10
00:00:25,070 --> 00:00:29,190
with environments and getting rewards
and figuring out what the world is.

11
00:00:29,190 --> 00:00:33,300
And by the way, as a part of that,
we learned just how frustrating it is

12
00:00:33,300 --> 00:00:36,550
to actually do reinforcement learning
rather than just solving an MDP.

13
00:00:36,550 --> 00:00:37,830
>> I agree with that.

14
00:00:37,830 --> 00:00:40,400
>> Okay, the second thing we learned or

15
00:00:40,400 --> 00:00:45,430
we talked about was a description of
what a policy is and the different

16
00:00:45,430 --> 00:00:50,270
ways you might go around solving these
RL problems and evaluating them.

17
00:00:50,270 --> 00:00:52,300
What does it mean to have a good policy?

18
00:00:53,340 --> 00:00:55,880
And then the third class
of things that we learned

19
00:00:55,880 --> 00:00:58,350
was about evaluating learners.

20
00:00:58,350 --> 00:01:01,730
And we realized that there's lots of
different ways to evaluate learners just

21
00:01:01,730 --> 00:01:05,810
like there's lots of different ways
to evaluate policies and plans.

22
00:01:05,810 --> 00:01:09,830
You can talk about truncating them or
doing things that are infinite.

23
00:01:09,830 --> 00:01:12,390
We can talk about whether
a learning is fast,

24
00:01:12,390 --> 00:01:15,290
whether a learner is efficient
in a different sense like with

25
00:01:15,290 --> 00:01:18,260
sample complexity whether it
actually returns good values.

26
00:01:18,260 --> 00:01:22,860
If you think about it just in terms of
it's output or how it got to its output.

27
00:01:22,860 --> 00:01:23,390
>> Right.
And so

28
00:01:23,390 --> 00:01:26,600
we're going to make some choices so that
we have concrete algorithms that we can

29
00:01:26,600 --> 00:01:28,630
present and
analyses that we can present.

30
00:01:28,630 --> 00:01:31,810
But it's worth keeping in mind that some
of these things are kind of arbitrary.

31
00:01:31,810 --> 00:01:33,320
I mean, they're justified.

32
00:01:33,320 --> 00:01:35,970
But there's other things that
would also be justified.

33
00:01:35,970 --> 00:01:38,180
>> That seems fair in
machine learning All right,

34
00:01:38,180 --> 00:01:39,570
well that all make sense to me Michael.

35
00:01:39,570 --> 00:01:43,640
So, I guess we'll continue
this conversation next week.

36
00:01:43,640 --> 00:01:44,620
>> Sounds good.

37
00:01:44,620 --> 00:01:45,870
>> All right, have a good day.
