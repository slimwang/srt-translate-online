1
00:00:00,190 --> 00:00:03,550
All right Michael, so here's boosting in pseudo code. Okay,

2
00:00:03,550 --> 00:00:05,180
let me read it out to you then you can tell

3
00:00:05,180 --> 00:00:08,480
me if it makes sense. So we're given a training set.

4
00:00:08,480 --> 00:00:11,720
It's made up of a bunch of xi, yr pairs. You

5
00:00:11,720 --> 00:00:15,180
know, x is your input and y is your output.

6
00:00:15,180 --> 00:00:18,350
And for reasons that'll be clear in a moment All of

7
00:00:18,350 --> 00:00:22,340
your labels are either minus one or plus one. Where minus

8
00:00:22,340 --> 00:00:25,240
one means not in class or plus one means you're in

9
00:00:25,240 --> 00:00:28,260
a class. So this is a binary classification task. That make sense?

10
00:00:28,260 --> 00:00:29,360
>> So far.

11
00:00:29,360 --> 00:00:32,140
>> Okay. And then what you're going to do is,

12
00:00:32,140 --> 00:00:34,545
you're going to loop at every time step, let's call it

13
00:00:34,545 --> 00:00:39,180
lower-case t. From the first time step one, to some big

14
00:00:39,180 --> 00:00:41,290
time in the future. We'll just call it capital T and

15
00:00:41,290 --> 00:00:42,980
not worry about where it comes from right now. The

16
00:00:42,980 --> 00:00:46,240
first thing you're going to do is you're going to construct a distribution.

17
00:00:46,240 --> 00:00:47,840
And I'll tell you how in a minute, Michael. Okay, so,

18
00:00:47,840 --> 00:00:50,260
so, don't worry about it. And we'll just call that D

19
00:00:50,260 --> 00:00:52,470
sub T. So, this is your distribution over your

20
00:00:52,470 --> 00:00:57,220
examples at some particular time T. Given that distribution, I

21
00:00:57,220 --> 00:00:59,350
want you to find a weak classifier. I want your

22
00:00:59,350 --> 00:01:03,430
weak learner to output some hypothesis. Let's call that epsilon

23
00:01:03,430 --> 00:01:05,450
sub T. The hypothesis that gets produced to that

24
00:01:05,450 --> 00:01:09,560
time step. And that hypothesis should have some small error.

25
00:01:09,560 --> 00:01:11,750
Let's call that error Epsilon sub T, because it's a

26
00:01:11,750 --> 00:01:15,310
small number. Such that it does well on the training

27
00:01:15,310 --> 00:01:19,000
set, given the particular distribution. So, I'm just rewriting

28
00:01:19,000 --> 00:01:22,060
my notion of error from, the other side of the

29
00:01:22,060 --> 00:01:24,750
screen. So there are times we want you to find

30
00:01:24,750 --> 00:01:27,190
a weak classifier. That is, we want you to call

31
00:01:27,190 --> 00:01:30,310
some weak learner that returns some hypothesis. Lets call

32
00:01:30,310 --> 00:01:33,140
it h sub t that has a small error. Lets

33
00:01:33,140 --> 00:01:36,870
call that epsilons of t. Which is to say that

34
00:01:36,870 --> 00:01:40,390
the probability of it being wrong that is disagreeing with

35
00:01:40,390 --> 00:01:44,670
the training label is small, with respect to the underlying distribution.

36
00:01:44,670 --> 00:01:48,230
>> So just to be clear there, the epsilon could

37
00:01:48,230 --> 00:01:51,560
be as big as slightly less than a half. Right? It

38
00:01:51,560 --> 00:01:54,200
doesn't have to be teeny, tiny. It could actually be,

39
00:01:54,200 --> 00:01:56,100
almost a half. But it can't be bigger than a half.

40
00:01:56,100 --> 00:01:58,870
>> That's right. And, and no matter what happens.

41
00:01:58,870 --> 00:02:01,480
Or even equal to a half. but, you know,

42
00:02:01,480 --> 00:02:03,390
we can assume, although it doesn't matter for the

43
00:02:03,390 --> 00:02:05,540
algorithm that the learner is going to return the best one

44
00:02:05,540 --> 00:02:08,120
that it can. With some error. But regardless, it's

45
00:02:08,120 --> 00:02:11,050
going to have, it's going to satisfy the requirements

46
00:02:11,050 --> 00:02:14,040
of a weak learner, and all I've done is

47
00:02:14,040 --> 00:02:16,310
copy this notion of error over to here. Ok?

48
00:02:17,370 --> 00:02:18,500
>> Awesome!

49
00:02:18,500 --> 00:02:21,320
>> Ok. So, you're going to do that and you'll do that a whole bunch of times

50
00:02:21,320 --> 00:02:25,315
steps, constantly finding hypothesis at each time step

51
00:02:25,315 --> 00:02:31,060
h sub t with small error epsilon sub t constantly making new distributions,

52
00:02:31,060 --> 00:02:33,020
and then eventually, you're going to output some

53
00:02:33,020 --> 00:02:35,930
final hypothesis. Which, I haven't told you yet how

54
00:02:35,930 --> 00:02:37,750
you're going to to get the final hypothesis. But that's

55
00:02:37,750 --> 00:02:39,770
the high level bit. Look at your training data,

56
00:02:39,770 --> 00:02:44,110
construct distributions, find a week classifier with low error.

57
00:02:44,110 --> 00:02:45,930
Keep doing that you have a bunch of them

58
00:02:45,930 --> 00:02:48,490
and then combine them somehow into some final hypothesis.

59
00:02:48,490 --> 00:02:51,250
And that's high level of algorithm for boosting, okay?

60
00:02:51,250 --> 00:02:53,380
>> Okay, but you've left out the two, two really

61
00:02:53,380 --> 00:02:55,880
important things, even the part from how you find we,

62
00:02:55,880 --> 00:02:58,430
weak classifier, which is where do we get this

63
00:02:58,430 --> 00:03:01,380
distribution and where do we get this, this final hypothesis?

64
00:03:01,380 --> 00:03:03,640
>> Right, so let me do that for you right now.
