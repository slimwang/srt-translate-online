1
00:00:00,025 --> 00:00:04,360
Dyna-Q is an algorithm
developed by Rich Sutton

2
00:00:04,360 --> 00:00:09,900
intended to speed up learning or
model convergence for Q learning.

3
00:00:09,900 --> 00:00:13,560
Remember that Q learning is model free.

4
00:00:13,560 --> 00:00:18,760
Meaning that it does not rely on T or R.

5
00:00:18,760 --> 00:00:22,415
T being, our transition matrix and

6
00:00:22,415 --> 00:00:26,380
R being our reward function,
it doesn't know it.

7
00:00:26,380 --> 00:00:27,810
Q learning does not know T or R.

8
00:00:29,010 --> 00:00:34,060
Dyna ends up becoming a blend of
model free and model based methods.

9
00:00:34,060 --> 00:00:35,850
Here's how it works.

10
00:00:35,850 --> 00:00:38,950
So let's first consider
plain old Q learning.

11
00:00:38,950 --> 00:00:41,490
Dyna is really in
addition to Q learning.

12
00:00:41,490 --> 00:00:43,540
So let's start with
the regular Q learning and

13
00:00:43,540 --> 00:00:46,010
then look at how we add Dyna into it.

14
00:00:46,010 --> 00:00:50,770
So we initialize our Q table and
then we begin iterating.

15
00:00:50,770 --> 00:00:55,680
We observe s, we execute action a and

16
00:00:55,680 --> 00:00:59,990
then we have observe our new
state s prime and our reward R

17
00:01:01,220 --> 00:01:06,380
within updater our Q table with
this experience tubal and repeat.

18
00:01:08,220 --> 00:01:11,610
And we do that over and over and over
again, interacting with the world and

19
00:01:11,610 --> 00:01:14,640
our Q table becomes better and better.

20
00:01:14,640 --> 00:01:19,680
So when we augment Q
learning with Dyna-Q.

21
00:01:19,680 --> 00:01:24,810
We had three new components,
the first is that we

22
00:01:24,810 --> 00:01:30,650
add some logic that enables
us to learn models of T and

23
00:01:30,650 --> 00:01:37,100
R then for lack of a better term
we halucinate an experience.

24
00:01:37,100 --> 00:01:41,060
So rather than interacting with
the real world like we do appear with

25
00:01:41,060 --> 00:01:45,070
the Q learning part and
this is expensive by the way.

26
00:01:45,070 --> 00:01:49,580
We hallucinate these experiences,
update our queue table and

27
00:01:49,580 --> 00:01:54,540
repeat this many times,
maybe hundreds of times.

28
00:01:54,540 --> 00:02:00,700
This operation is very cheap compared
to interacting with the real world.

29
00:02:00,700 --> 00:02:05,490
So we can leverage
the experience we gain here

30
00:02:05,490 --> 00:02:10,280
from an interaction with the real world,
but then update our model

31
00:02:10,280 --> 00:02:15,710
more completely before we step out and
interact with the real world again.

32
00:02:15,710 --> 00:02:21,050
After we've iterated enough times
down here maybe 100 maybe 200 times.

33
00:02:21,050 --> 00:02:26,340
Then we return back up here and resume
our interaction with the real world.

34
00:02:26,340 --> 00:02:31,030
The key thing here is that for
each experience with the real world,

35
00:02:31,030 --> 00:02:35,310
we have maybe 100 or
200 updates of our model here.

36
00:02:35,310 --> 00:02:38,630
Let's look at each of these components
in a little more detail now.

37
00:02:38,630 --> 00:02:41,168
So in this part where
we update our model.

38
00:02:41,168 --> 00:02:47,511
What we really want to do is
find new values for T and R.

39
00:02:47,511 --> 00:02:51,900
The point where we update our
model includes the following.

40
00:02:51,900 --> 00:02:56,430
We want to update T, so I'm calling
it T prime for the moment, which

41
00:02:56,430 --> 00:03:01,270
represents our transition matrix and
we want to update our reward function.

42
00:03:01,270 --> 00:03:06,173
Now, remember that T is
the probability that if we

43
00:03:06,173 --> 00:03:11,558
are in state s and
we take action a will end up in s prime.

44
00:03:11,558 --> 00:03:17,678
And r is our expected reward if we
are in state s and we take action a.

45
00:03:17,678 --> 00:03:21,299
I'm going to show you in
a moment how we'll update T and

46
00:03:21,299 --> 00:03:24,440
R Here's how we
hallucinate an experience.

47
00:03:25,480 --> 00:03:30,820
First, we randomly select an s,
second, we randomly select an a,

48
00:03:30,820 --> 00:03:34,360
so our state and
action are chosen totally at random.

49
00:03:35,420 --> 00:03:40,500
Then we infer our new state s prime,
by looking at T.

50
00:03:40,500 --> 00:03:46,880
And we infer a reward, our immediate
reward R by looking at big R or R table.

51
00:03:47,940 --> 00:03:51,830
So, now we've got s, a, s prime and r.

52
00:03:51,830 --> 00:03:56,680
Or a complete experience tuple and
we can update our Q table using that.

53
00:03:56,680 --> 00:03:59,370
So, the Q table update
is our final step and

54
00:03:59,370 --> 00:04:02,970
this is really all there
is to die in a queue.

55
00:04:02,970 --> 00:04:07,110
We've added these three sections and
what's missing and I'll get to in just

56
00:04:07,110 --> 00:04:13,440
a moment is how do we update our
model of t and our model of r?
