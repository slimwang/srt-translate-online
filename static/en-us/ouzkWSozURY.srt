1
00:00:00,380 --> 00:00:02,450
So in order to elaborate on all of that,

2
00:00:02,450 --> 00:00:06,130
Michael, I want to ask
you a simple question.

3
00:00:06,130 --> 00:00:07,310
And here it is.

4
00:00:07,310 --> 00:00:10,210
What makes reinforcement learning hard?

5
00:00:10,210 --> 00:00:12,790
>> I'm going to say
living on the streets.

6
00:00:12,790 --> 00:00:14,270
>> Makes reinforcement learning hard?

7
00:00:14,270 --> 00:00:15,840
>> Yeah, it sort of hardens it.

8
00:00:15,840 --> 00:00:19,580
>> Let me let me ask that
question slightly differently.

9
00:00:19,580 --> 00:00:20,110
>> Okay.

10
00:00:20,110 --> 00:00:24,470
>> From a computational point of view,
from an algorithmic point of view,

11
00:00:24,470 --> 00:00:26,150
what makes reinforcement learning hard?

12
00:00:26,150 --> 00:00:29,400
>> So well there's a lot of
things we've been talking about.

13
00:00:29,400 --> 00:00:32,530
There's the notion of delayed
reward is a very tricky thing.

14
00:00:32,530 --> 00:00:35,600
>> I like that one, I like that one
enough for me to write it down.

15
00:00:35,600 --> 00:00:36,700
>> You're not going to wait?

16
00:00:36,700 --> 00:00:37,210
>> No, I'm not going to wait,

17
00:00:37,210 --> 00:00:41,030
I'm just going to write it down right
now because it's ironic that way.

18
00:00:41,030 --> 00:00:44,210
>> And a related issue well
there's two pieces in that.

19
00:00:44,210 --> 00:00:45,680
There's a delay and there's a reward.

20
00:00:45,680 --> 00:00:49,050
The reward part is like unlike
say supervised learning

21
00:00:49,050 --> 00:00:52,810
reinforcement learning is trying to do
its job based on very weak feedback.

22
00:00:52,810 --> 00:00:57,770
That it's only told how it's doing and
not necessarily what it should be doing.

23
00:00:57,770 --> 00:01:01,730
>> Right which makes it very different
from normal function approximation,

24
00:01:01,730 --> 00:01:03,790
in the way that we think
about in supervised learning.

25
00:01:03,790 --> 00:01:04,519
Okay, what else?

26
00:01:04,519 --> 00:01:06,590
>> If we do try to use something
like supervised learning,

27
00:01:06,590 --> 00:01:10,140
one thing that makes the function
approximation hard is

28
00:01:10,140 --> 00:01:12,160
the idea that it's kind
of a moving target.

29
00:01:12,160 --> 00:01:16,640
We tend to base our estimates
on our current estimates,

30
00:01:16,640 --> 00:01:21,470
as opposed to anything kind of
legitimately valid in the world.

31
00:01:21,470 --> 00:01:24,050
Right, so doing estimates
based on estimates is, well,

32
00:01:24,050 --> 00:01:25,070
like averaging averages.

33
00:01:27,810 --> 00:01:30,250
>> Okay, I want to say
bootstrapping is the issue.

34
00:01:30,250 --> 00:01:31,770
>> Yeah I agree with that.

35
00:01:31,770 --> 00:01:33,700
Bootstrapping is sort of a problem.

36
00:01:33,700 --> 00:01:39,060
I think I would probably summarize that
as the problem with the need to do

37
00:01:39,060 --> 00:01:40,060
good exploration.

38
00:01:40,060 --> 00:01:42,920
And that's a little different
from the normal supervised

39
00:01:42,920 --> 00:01:45,360
learning set up where you've got
a bunch of training examples.

40
00:01:45,360 --> 00:01:49,550
Here because you only have estimates
in the reinforcement learning context,

41
00:01:49,550 --> 00:01:52,940
you have to do a bunch of extra
things to actually figure

42
00:01:52,940 --> 00:01:55,560
out how to get good estimates so
that you can actually do good learning.

43
00:01:55,560 --> 00:02:00,180
So one more thing I think sort of worth
mentioning that makes this sort of hard

44
00:02:00,180 --> 00:02:03,420
and it's kind of very basic kind
of computational problems, right.

45
00:02:03,420 --> 00:02:06,330
You know when we describe computation
complexity we talk about it in terms

46
00:02:06,330 --> 00:02:06,860
of big o.

47
00:02:06,860 --> 00:02:09,090
And the question is big o of what?

48
00:02:09,090 --> 00:02:10,370
So what's the what here?

49
00:02:10,370 --> 00:02:13,170
What's the in,
in our big o that makes ROR?

50
00:02:13,170 --> 00:02:14,540
>> Well there's a lot of them.

51
00:02:14,540 --> 00:02:15,070
>> Like what?

52
00:02:15,070 --> 00:02:17,560
>> Well I mean if we're
thinking about sort of M.V.P.

53
00:02:17,560 --> 00:02:21,841
style reinforcement learning then
the size of the state space is a big o.

54
00:02:21,841 --> 00:02:23,920
>> Mm hm.

55
00:02:23,920 --> 00:02:26,530
>> The size of the temporal
window in some sense,

56
00:02:26,530 --> 00:02:28,440
like how how delayed is the reward.

57
00:02:28,440 --> 00:02:29,550
>> Mm hm.
>> There's

58
00:02:29,550 --> 00:02:31,640
issues on what could be
the actions as well.

59
00:02:31,640 --> 00:02:34,160
There's lots of things that could scale.

60
00:02:34,160 --> 00:02:38,120
But I guess what makes state super hard
is that you can very easily specify

61
00:02:38,120 --> 00:02:42,410
a problem that has a state space
with an exponential number of states

62
00:02:42,410 --> 00:02:46,710
that is still relatively simple, but
algorithms that don't generalize

63
00:02:46,710 --> 00:02:50,230
between states are not going to be able
to learning them very effectively.

64
00:02:50,230 --> 00:02:51,250
>> I think all of what you said is true.

65
00:02:51,250 --> 00:02:53,920
Some of it, I think, is kind of captured
not necessarily as explicitly in

66
00:02:53,920 --> 00:02:55,130
the first two bullet points.

67
00:02:55,130 --> 00:02:57,940
But I'd like to focus on states and
actions.

68
00:02:57,940 --> 00:03:02,630
The problem is that the learning that
we need to do grows super linearly

69
00:03:02,630 --> 00:03:07,070
in general, we aren't very, very careful
and smart in the number of states and

70
00:03:07,070 --> 00:03:08,660
the number of actions.

71
00:03:08,660 --> 00:03:12,150
And so we really need to have kind
of clever ways of thinking about

72
00:03:12,150 --> 00:03:15,270
learning across multiple states and
perhaps multiple actions.

73
00:03:15,270 --> 00:03:17,940
And in fact when you talked
about function approximation and

74
00:03:17,940 --> 00:03:21,810
generalization, I would claim that
at least in sort of this discussion,

75
00:03:21,810 --> 00:03:25,580
we could talk about that as a way of
dealing with generalizing overstates,

76
00:03:25,580 --> 00:03:29,910
that you did function
approximation over value functions.

77
00:03:29,910 --> 00:03:34,560
Right, so here you tried one way of
dealing with the problem of scale

78
00:03:34,560 --> 00:03:36,650
when it comes to states.

79
00:03:36,650 --> 00:03:39,400
Is well,
if I learn something about one state,

80
00:03:39,400 --> 00:03:43,110
I want to somehow have that teach me
something about a bunch of other states.

81
00:03:43,110 --> 00:03:46,870
And then I won't have to visit every
single state an infinite number of

82
00:03:46,870 --> 00:03:49,460
times, and take every action
an infinite number of times.

83
00:03:49,460 --> 00:03:51,380
And that's really good,
that's really important, right, and

84
00:03:51,380 --> 00:03:56,930
the trick here is I can see many, many
states by just seeing a single state and

85
00:03:56,930 --> 00:04:00,980
that's what function approximation over
the value functions actually gives me.

86
00:04:00,980 --> 00:04:02,430
Agreed?
>> Yeah I don't know that I'd say it

87
00:04:02,430 --> 00:04:05,800
quite that way, but
that we learn about many states

88
00:04:05,800 --> 00:04:09,120
when we learn about other states or
something like that.

89
00:04:09,120 --> 00:04:11,900
But yeah, that's absolutely the way
I was thinking about function

90
00:04:11,900 --> 00:04:14,570
approximation is helping us to
kind of fill in the gaps for

91
00:04:14,570 --> 00:04:16,959
states that we haven't
visited nearly as often.

92
00:04:16,959 --> 00:04:20,690
>> So in fact, I could be aggressive and
ask you the question,

93
00:04:20,690 --> 00:04:22,950
why did you do all those function
approximation over states but

94
00:04:22,950 --> 00:04:25,840
you didn't do any function
approximation over actions, Michael?

95
00:04:25,840 --> 00:04:26,948
>> That would be aggressive
>> And

96
00:04:26,948 --> 00:04:32,430
the [LAUGH] I don't have
a wonderful answer to that.

97
00:04:32,430 --> 00:04:37,120
I mean that it's tradition, in the sense
that a lot of the MDPs that people have

98
00:04:37,120 --> 00:04:40,530
been working with really do try to keep
the action space relatively small and

99
00:04:40,530 --> 00:04:44,250
expand in the state space but that's
not really that good of an answer.

100
00:04:44,250 --> 00:04:49,130
I think action, large actions bases are
really important tom we just don't have

101
00:04:49,130 --> 00:04:50,510
as good ways to deal with them.

102
00:04:50,510 --> 00:04:54,650
For one thing the computing the max
ends up being hard in q learning.

103
00:04:54,650 --> 00:04:57,510
>> Right so in fact what I'm going
to try to talk about in the rest of

104
00:04:57,510 --> 00:05:00,610
the lesson is trying to do To
generalizations of the sort of

105
00:05:00,610 --> 00:05:02,360
generalization that you
are talking about and

106
00:05:02,360 --> 00:05:06,200
think about how we can learn
about actions and how we can

107
00:05:06,200 --> 00:05:10,690
abstract over actions in the same way
that you are abstracting over states.

108
00:05:10,690 --> 00:05:12,760
>> That seems great and
a little bit meta.

109
00:05:12,760 --> 00:05:13,570
>> A little bit meta.

110
00:05:13,570 --> 00:05:17,190
And so that's why the lessons
generalizing over generalizing and

111
00:05:17,190 --> 00:05:20,220
the whole point in the end I'm
going to claim other than just sort of

112
00:05:20,220 --> 00:05:23,640
it's interesting in and of itself is
that it's going to allow us to scale.

113
00:05:23,640 --> 00:05:26,960
So all of these tricks ultimately
have a kind of practical use

114
00:05:26,960 --> 00:05:29,100
including function approximation
of allowing us to scale.
