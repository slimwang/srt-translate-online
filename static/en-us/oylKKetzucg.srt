1
00:00:00,000 --> 00:00:03,007
So, here's a bunch of threads executing instructions over time.

2
00:00:03,007 --> 00:00:07,342
Now these threads all belong to the same warp, and in all Nvidia GPU's today,

3
00:00:07,342 --> 00:00:12,117
a warp has 32 threads. Here's another warp where all the threads take the else clause,

4
00:00:12,117 --> 00:00:13,735
so there's no problems with this.

5
00:00:13,735 --> 00:00:16,754
All of these threads execute all of their instructions in lock step,

6
00:00:16,754 --> 00:00:21,466
but here's a warp where some threads take the if the branch and some take the else branch.

7
00:00:21,466 --> 00:00:25,147
And because the threads in a warp can only execute a single instruction at a time,

8
00:00:25,147 --> 00:00:29,295
the hardware automatically deactivates some of the threads, executes the red instructions,

9
00:00:29,295 --> 00:00:33,055
then flips which threads are activated and executes the blue instructions.

10
00:00:33,055 --> 00:00:37,465
And as you can see, if this is time, the code is going to take longer to execute overall.

11
00:00:37,465 --> 00:00:41,364
Some of our threads are wasting time sitting idle while their workmates execute.

12
00:00:41,364 --> 00:00:47,100
We say that the threads have diverged during this section of code and would refer to this as 2-way branch divergence.

13
00:00:47,100 --> 00:00:49,844
If I'd written a nested if-else statement,

14
00:00:49,844 --> 00:00:53,036
I could have 4-way branch divergence and could take even longer to execute.

15
00:00:53,036 --> 00:00:58,146
So, as a quiz, what is the maximum branch divergence penalty that a CUDA thread block with

16
00:00:58,146 --> 00:01:04,786
1024 threads could experience when executing a kernel? Is it a 2x slow down, a 4x slowdown, or so forth?
