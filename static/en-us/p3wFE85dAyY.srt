1
00:00:00,360 --> 00:00:01,710
Now, it would be great to see for

2
00:00:01,710 --> 00:00:05,590
yourself that these embeddings
are clustering together as you'd expect.

3
00:00:05,590 --> 00:00:08,360
One way to see it,
is by doing a nearest neighbor lookup

4
00:00:08,360 --> 00:00:10,800
of the words that are closest
to any given word.

5
00:00:12,030 --> 00:00:15,820
Another way is to try to reduce the
dimensionality of the embedding space

6
00:00:15,820 --> 00:00:20,280
down to two dimensions, and to plug
the two dimensional representation.

7
00:00:20,280 --> 00:00:21,670
If you do that the native way, for

8
00:00:21,670 --> 00:00:25,000
example using PCA,
you basically get a mush.

9
00:00:25,000 --> 00:00:27,780
You lose too much
information in the process.

10
00:00:27,780 --> 00:00:31,390
What you need is a way of projecting
that preserves the neighborhood

11
00:00:31,390 --> 00:00:33,260
structure of your data.

12
00:00:33,260 --> 00:00:36,850
Things that are close in the embedding
space should remain close to the ends,

13
00:00:36,850 --> 00:00:40,020
things that are far should
be far away from each other.

14
00:00:40,020 --> 00:00:43,310
One very effective technique that
does exactly that is called t-SNE.

15
00:00:43,310 --> 00:00:46,250
You'll get to play with this
visualization technique in

16
00:00:46,250 --> 00:00:46,820
the assignment
