1
00:00:00,260 --> 00:00:05,060
Okay Michael, so
I have brought back yet another slide,

2
00:00:05,060 --> 00:00:09,150
slightly modified, from the discussion
on options and constraints.

3
00:00:09,150 --> 00:00:11,260
Do you recognize it, you remember this?

4
00:00:11,260 --> 00:00:14,530
>> Well, it's got the Pac-Man thing
on it, and it has the four options or

5
00:00:14,530 --> 00:00:16,250
whatever that we just talked about.

6
00:00:16,250 --> 00:00:18,490
And it has the beginnings
of a stoplight.

7
00:00:18,490 --> 00:00:22,310
>> Right, exactly right, so in fact I
told you a little about this experiment

8
00:00:22,310 --> 00:00:24,330
that we did, where we asked people
to come up with these options.

9
00:00:24,330 --> 00:00:25,880
But there was a second
part to the experiment.

10
00:00:25,880 --> 00:00:27,047
There are two buttons,

11
00:00:27,047 --> 00:00:29,737
this is me attempting to draw
what a button looks like.

12
00:00:29,737 --> 00:00:31,837
There's a green button and
there's a red button.

13
00:00:31,837 --> 00:00:35,695
And what you're being asked to do is
to watch Pac-Man over here run around

14
00:00:35,695 --> 00:00:38,944
executing actions including
these options you came up with.

15
00:00:38,944 --> 00:00:42,470
And then occasionally you're supposed
to hit a green or red button.

16
00:00:42,470 --> 00:00:44,430
And what do you think that the green and
the red stand for?

17
00:00:44,430 --> 00:00:47,330
>> Probably green is Pac-Man
did something good, and

18
00:00:47,330 --> 00:00:49,310
the red is the Pac-Man did
something not so good.

19
00:00:49,310 --> 00:00:50,070
>> Right, that's exactly right.

20
00:00:50,070 --> 00:00:51,350
So this is supposed to mean good.

21
00:00:51,350 --> 00:00:53,990
And this is supposed to mean bad,
just like you would expect.

22
00:00:53,990 --> 00:00:55,500
We had people do this, and they did it.

23
00:00:55,500 --> 00:00:57,910
They would say that's good,
that's bad, that's bad, that's bad.

24
00:00:57,910 --> 00:00:59,820
That's good, that's good, that's good,
that's good, that's good, that's good,

25
00:00:59,820 --> 00:01:02,020
that's good, that's bad,
and so on and so forth.

26
00:01:02,020 --> 00:01:04,510
Now, here's my question to you.

27
00:01:04,510 --> 00:01:06,535
Given that people have done all of this,
and

28
00:01:06,535 --> 00:01:09,947
they've given us lots of data on
whether Pac-Man did well or did poorly.

29
00:01:09,947 --> 00:01:12,887
What would be the obvious thing
to do with that information?

30
00:01:12,887 --> 00:01:15,680
>> The obvious thing would be,
it seems like the right thing.

31
00:01:15,680 --> 00:01:19,390
Which is to,
if you treat the good as a plus one and

32
00:01:19,390 --> 00:01:23,130
the bad as a minus one, and
have the agent just maximize for reward.

33
00:01:23,130 --> 00:01:25,500
That should basically be
reinforcement learning.

34
00:01:25,500 --> 00:01:26,335
>> Right, and
that makes a lot of sense, right?

35
00:01:26,335 --> 00:01:27,933
So since we do reinforcement learning,

36
00:01:27,933 --> 00:01:30,427
we'd want to turn this into
a reinforcement learning problem.

37
00:01:30,427 --> 00:01:32,889
And we would say well,
every time you said something was good,

38
00:01:32,889 --> 00:01:35,737
I'm going to treat that as a reward
of say one, it doesn't really matter.

39
00:01:35,737 --> 00:01:39,297
And bad say is something like that,
and you get a reward for

40
00:01:39,297 --> 00:01:42,577
say, being in a state in
taking a particular action.

41
00:01:42,577 --> 00:01:44,177
And that's a perfectly
reasonable thing to do.

42
00:01:44,177 --> 00:01:48,207
And in fact this ends up being a way
to get human beings to help us to

43
00:01:48,207 --> 00:01:50,457
automatically do reward shaping.

44
00:01:50,457 --> 00:01:52,757
And in fact you can show
this works pretty well.

45
00:01:52,757 --> 00:01:55,523
And again,
there's been a lot of work on this,

46
00:01:55,523 --> 00:01:59,810
we have provided some papers on this for
the students to read about.

47
00:01:59,810 --> 00:02:02,290
And you do better, as you might imagine,

48
00:02:02,290 --> 00:02:05,220
by treating these things as reward
shaping than if you didn't.

49
00:02:05,220 --> 00:02:07,440
Because really they're hints,
as you move along,

50
00:02:07,440 --> 00:02:10,139
about where you ought to be and
the kinds of things you have to do.

51
00:02:10,139 --> 00:02:11,830
But remember what I said before,

52
00:02:11,830 --> 00:02:16,010
that human beings actually
are trying to tell you something.

53
00:02:16,010 --> 00:02:18,474
And they might be trying to tell you
something different from what you

54
00:02:18,474 --> 00:02:19,077
want them to be.

55
00:02:19,077 --> 00:02:21,746
So, if I think back to what you just
said, what you kind of said is,

56
00:02:21,746 --> 00:02:23,857
well look it's a reinforcement
learning problem.

57
00:02:23,857 --> 00:02:26,057
Reinforcement learning
problems need rewards, and so

58
00:02:26,057 --> 00:02:27,950
we should convert these
things into rewards.

59
00:02:27,950 --> 00:02:31,440
Well, it turns out that in practice,
when you actually talk to humans,

60
00:02:31,440 --> 00:02:34,380
and work out what they do,
they're not actually doing this.

61
00:02:34,380 --> 00:02:36,140
When they say good and they say bad,

62
00:02:36,140 --> 00:02:39,901
they're not thinking in terms of
actual rewards, plus one, minus one.

63
00:02:39,901 --> 00:02:43,850
Or plus ten and minus nine,
and these kinds of things.

64
00:02:43,850 --> 00:02:46,550
Because of course they're not
reinforcement learning experts,

65
00:02:46,550 --> 00:02:47,770
they're not thinking about rewards.

66
00:02:47,770 --> 00:02:50,400
They're actually giving you
a different kind of information.

67
00:02:50,400 --> 00:02:52,330
So here, let me just ask you.

68
00:02:52,330 --> 00:02:54,390
If I told you in English,

69
00:02:54,390 --> 00:03:00,520
that you should hit the green button
whenever the agent does something right.

70
00:03:00,520 --> 00:03:03,640
And the red button when
the agent does something wrong,

71
00:03:03,640 --> 00:03:08,280
then what kind of information am I
conveying, other than a reward value?

72
00:03:08,280 --> 00:03:08,950
>> Well, one way to put it,

73
00:03:08,950 --> 00:03:12,620
is your you're giving commentary
on the behavior or the policy.

74
00:03:12,620 --> 00:03:15,550
>> That's exactly right,
that really in fact, it turns out,

75
00:03:15,550 --> 00:03:18,090
you could convert these
things into rewards.

76
00:03:18,090 --> 00:03:20,350
>> But you could also convert them into
something that's a bit more direct.

77
00:03:20,350 --> 00:03:23,170
You can just say well you're
actually telling me that in this

78
00:03:23,170 --> 00:03:26,210
particular state,
I should in fact take this action.

79
00:03:26,210 --> 00:03:29,703
Or that in this particular state,
I should not take this action.

80
00:03:29,703 --> 00:03:32,950
And this is direct policy advice,
as opposed to reward advice.

81
00:03:32,950 --> 00:03:36,770
And so maybe, what we're really doing,
is not reward shaping, or

82
00:03:36,770 --> 00:03:40,460
what humans are doing is not reward
shaping, it's actually a policy shaping.
