1
00:00:00,020 --> 00:00:07,473
Let's look at the code again. This is our tile per element kernel. And this syncthreads is the problem.

2
00:00:07,473 --> 00:00:15,397
Remember we have a thread block of K by K threads, and at the moment, K is set to 32.

3
00:00:15,397 --> 00:00:18,659
So, we have a thread block of 1024 threads,

4
00:00:18,659 --> 00:00:22,352
and if you think about it, each thread is doing very little work.

5
00:00:22,352 --> 00:00:27,997
It's reading a single word into shared memory, and then it's waiting around at the barrier

6
00:00:27,997 --> 00:00:32,390
until all the other threads get done with their job, and then it's writing a single word to memory.

7
00:00:32,390 --> 00:00:36,772
So most of the threads in this block are spending most

8
00:00:36,772 --> 00:00:41,606
of their time waiting on the syncthreads call, waiting on this barrier for everybody else to get there.

9
00:00:41,606 --> 00:00:45,227
And the more threads in the block, the more time on average they spend waiting.

10
00:00:45,227 --> 00:00:49,334
So for a simple kernel like this, most threads spend a large chunk of their lifetime

11
00:00:49,334 --> 00:00:53,672
simply waiting at this barrier until the last few threads complete their read operations.

12
00:00:53,672 --> 00:00:56,776
So, what can we do to reduce the average wait time per thread?

13
00:00:56,776 --> 00:00:59,196
Should we eliminate the syncthreads call?

14
00:00:59,196 --> 00:01:02,639
Should we reduce the number of threads per thread block?

15
00:01:02,639 --> 00:01:05,425
Or should we increase the number of threads per thread block,

16
00:01:05,425 --> 00:01:08,819
or perhaps increase the number of thread blocks per SM?
