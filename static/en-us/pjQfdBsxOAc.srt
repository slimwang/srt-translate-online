1
00:00:00,280 --> 00:00:00,860
Okay, Michael.

2
00:00:00,860 --> 00:00:03,700
So, we did supervised learning and

3
00:00:03,700 --> 00:00:05,870
we did unsupervised learning
in randomized optimization.

4
00:00:05,870 --> 00:00:07,580
>> Yep.
>> And the last section of the course

5
00:00:07,580 --> 00:00:09,080
was on reinforcement learning.

6
00:00:09,080 --> 00:00:09,630
>> Nice.

7
00:00:09,630 --> 00:00:11,530
>> Now there's a lot of
stuff that we didn't cover

8
00:00:11,530 --> 00:00:12,610
in that section of the course.

9
00:00:12,610 --> 00:00:15,100
Which sort of makes me sad,
because that's what I do for a living.

10
00:00:15,100 --> 00:00:17,130
I do reinforcement learning,
a little bit of game theory.

11
00:00:17,130 --> 00:00:19,980
You do a lot of game theory and
a lot of reinforcement learning.

12
00:00:19,980 --> 00:00:21,430
And there were a bunch of
things that we didn't cover.

13
00:00:21,430 --> 00:00:24,285
Can you think of anything in
particular you wish you did mention?

14
00:00:24,285 --> 00:00:26,430
>> Well I think the thing that most
people want to know about when

15
00:00:26,430 --> 00:00:29,030
they hear about reinforcement
learning is function approximation.

16
00:00:29,030 --> 00:00:31,577
The idea of can you apply
this idea to something

17
00:00:31,577 --> 00:00:34,410
other than a three by three
grid world [LAUGH] right?

18
00:00:34,410 --> 00:00:36,820
So to do that you often have to use

19
00:00:36,820 --> 00:00:39,540
the ideas that we talked about in
the other sections of the class.

20
00:00:39,540 --> 00:00:44,090
So use ideas from supervised learning to
learn something about the environment

21
00:00:44,090 --> 00:00:45,510
that you are acting in.

22
00:00:45,510 --> 00:00:49,136
>> Or even things from the unsupervised
learning section of the class.

23
00:00:49,136 --> 00:00:50,059
>> Like feature selection.

24
00:00:50,059 --> 00:00:51,161
>> Like feature selection.

25
00:00:51,161 --> 00:00:55,526
And in fact, the notion of function
approximation is sort of a a special

26
00:00:55,526 --> 00:00:57,590
case, maybe, the right term.

27
00:00:57,590 --> 00:01:02,230
Attraction, a particular state of
attraction, action attraction,

28
00:01:03,920 --> 00:01:07,770
this sort of notion of being able to
learn about one part of the space and

29
00:01:07,770 --> 00:01:09,260
have it teach you about
other parts of the space.

30
00:01:09,260 --> 00:01:09,930
>> Generalization?

31
00:01:09,930 --> 00:01:12,468
>> Generalization,
in general, that's a big

32
00:01:12,468 --> 00:01:16,024
part of making reinforcement
learning work in the real world.

33
00:01:16,024 --> 00:01:17,878
Speaking of which, there's POMDPs.

34
00:01:17,878 --> 00:01:20,370
We didn't talk about POMDPs-
>> Yeah, I get asked about that a lot.

35
00:01:20,370 --> 00:01:23,210
So, should I say what it is?

36
00:01:23,210 --> 00:01:26,280
>> People on the street walk up to you
and say POMDPs what's that all about?

37
00:01:26,280 --> 00:01:27,740
>> It's surprising.

38
00:01:27,740 --> 00:01:30,710
>> So
why don't you explain what POMDPs are?

39
00:01:30,710 --> 00:01:33,190
>> Usually it's after class,
when I'm talking about something else.

40
00:01:33,190 --> 00:01:34,060
>> Oh, well that I believe.

41
00:01:34,060 --> 00:01:36,830
>> So partially observable Markov
decision processes, right.

42
00:01:36,830 --> 00:01:39,830
So when we talked about without the PO.

43
00:01:39,830 --> 00:01:41,420
MDP, yeah, all right.

44
00:01:41,420 --> 00:01:46,410
So when you talk about MDPs,
the agent always has complete

45
00:01:46,410 --> 00:01:48,530
information about what
the current state is.

46
00:01:48,530 --> 00:01:50,410
That's the state is what you're
using to decide what to do.

47
00:01:50,410 --> 00:01:52,760
The policy maps state to action.

48
00:01:52,760 --> 00:01:56,740
But in reality, you don't really
have complete state information.

49
00:01:56,740 --> 00:01:58,720
If you're a helicopter and
you're flying through the air,

50
00:01:58,720 --> 00:02:01,740
you have to decide what to do based
on what you can sense right now.

51
00:02:01,740 --> 00:02:04,920
And there may be even uncertainty about
what's actually going on around you.

52
00:02:04,920 --> 00:02:05,980
>> Right.
>> And so,

53
00:02:05,980 --> 00:02:09,180
you can't just use state information,
because you don't have it.

54
00:02:09,180 --> 00:02:14,170
In the POMDP world, there's a separation
between what the actual world has

55
00:02:14,170 --> 00:02:18,260
as its current state, and
what the decision maker knows to be

56
00:02:18,260 --> 00:02:20,810
its perception of the state,
and those can be out of whack.

57
00:02:20,810 --> 00:02:23,950
>> And that makes sense, I mean we live
in a world where we don't know what

58
00:02:23,950 --> 00:02:28,290
is happening for every atom in the
universe that might have actually met.

59
00:02:28,290 --> 00:02:30,840
So you know there's a whole other
class of problems that I wish

60
00:02:30,840 --> 00:02:32,070
we could have talked
about that we didn't.

61
00:02:32,070 --> 00:02:34,755
And those are all the ones
that involve humans.

62
00:02:34,755 --> 00:02:35,360
>> Yes.
>> So

63
00:02:35,360 --> 00:02:39,110
a lot of game theory is mathematical and
abstract.

64
00:02:39,110 --> 00:02:41,219
But really it's about trying
to understand human behavior.

65
00:02:42,230 --> 00:02:42,890
Right.
In fact there's been

66
00:02:42,890 --> 00:02:46,920
a bunch of work done on bringing in
behavior into the game theory and

67
00:02:46,920 --> 00:02:51,710
game theory into behavior and
marrying economics, game theory,

68
00:02:51,710 --> 00:02:54,770
sociology, marketing, and-
>> Psychology, right, yeah.

69
00:02:54,770 --> 00:02:57,798
I mean, behavioral game theory
is one of the names for that.

70
00:02:57,798 --> 00:03:00,890
Though neuroeconomics also comes up
as a word that just seems like it

71
00:03:00,890 --> 00:03:01,590
shouldn't be a word.

72
00:03:01,590 --> 00:03:03,450
>> Yeah, that feels kind of made up.

73
00:03:03,450 --> 00:03:05,665
Except of course-
>> I will pay you in brain cells.

74
00:03:05,665 --> 00:03:08,360
>> [LAUGHTER] I'll get those brains.

75
00:03:08,360 --> 00:03:10,615
>> That's the currency that zombies use.

76
00:03:10,615 --> 00:03:13,810
>> [LAUGHTER] Okay, so there's a lot
about that with people with game theory,

77
00:03:13,810 --> 00:03:16,620
and even in reinforcement learning,
there's been a huge move.

78
00:03:16,620 --> 00:03:18,610
Actually, dating back decades, but

79
00:03:18,610 --> 00:03:21,560
really has exploded over
the last four or five years.

80
00:03:21,560 --> 00:03:24,180
>> Well in fact, what is the name of
school that you're in at Georgia Tech?

81
00:03:24,180 --> 00:03:25,930
>> The School of Interactive Computing.

82
00:03:25,930 --> 00:03:28,240
>> Yeah. That's it. Interaction turned
out to be a really important thing to

83
00:03:28,240 --> 00:03:28,970
some people.

84
00:03:28,970 --> 00:03:31,510
>> Right.
So, a lot of my work and reinforcement

85
00:03:31,510 --> 00:03:34,990
learning, in particular, has been
about bringing people into the loop.

86
00:03:34,990 --> 00:03:37,932
Learning from people,
watching what people do in

87
00:03:37,932 --> 00:03:42,780
order to do feature selection, or state
abstraction, or just simple learning.

88
00:03:42,780 --> 00:03:44,420
>> Some of my best friends
care about people.

89
00:03:45,490 --> 00:03:46,794
>> Are you saying I'm your best friend,
Michael?

90
00:03:46,794 --> 00:03:48,609
[SOUND] Very good, very good.

91
00:03:48,609 --> 00:03:50,612
Okay, so
there's a whole bunch of stuff there.

92
00:03:50,612 --> 00:03:52,066
[CROSSTALK]
>> No, I said some of my best friends.

93
00:03:52,066 --> 00:03:53,990
>> [LAUGH]
>> You are some of my best friends.

94
00:03:53,990 --> 00:03:54,770
>> I am some of your best friends?

95
00:03:54,770 --> 00:03:57,950
>> I don't know.
But the point is that there's

96
00:03:57,950 --> 00:04:00,710
this interesting line that happens
when you start thinking about

97
00:04:00,710 --> 00:04:04,080
how these learning systems are
interacting with people either once they

98
00:04:04,080 --> 00:04:07,450
start to behave in the world or
just during the learning process itself.

99
00:04:07,450 --> 00:04:09,995
>> Right, because in fact,
in practice that's how we teach it.

100
00:04:09,995 --> 00:04:11,595
So obviously we can talk
about this forever.

101
00:04:11,595 --> 00:04:12,155
>> Yeah.

102
00:04:12,155 --> 00:04:14,515
I'll just mention one,
let me just mention one buzzword.

103
00:04:14,515 --> 00:04:16,204
>> Okay.
>> So, inverse reinforcement learning is

104
00:04:16,204 --> 00:04:17,654
one that I particularly like.

105
00:04:17,654 --> 00:04:21,035
We talked about reinforcement learning
in the class, where you take a reward

106
00:04:21,035 --> 00:04:24,318
function and an interaction with
an environment and you create behavior.

107
00:04:24,318 --> 00:04:29,140
An inverse reinforcement learning
goes in the other direction.

108
00:04:29,140 --> 00:04:32,850
No, you start from
observations of behavior,

109
00:04:32,850 --> 00:04:36,310
interacting in an environment that some
expert is doing, and you try to guess

110
00:04:36,310 --> 00:04:38,960
what the reward function was that,
that expert would have been using.

111
00:04:38,960 --> 00:04:40,990
>> Or a reward function that's
consistent with that behavior.

112
00:04:40,990 --> 00:04:42,100
>> Exactly.
because you can't know,

113
00:04:42,100 --> 00:04:47,000
really how rewarding is it to say,
get groaned at when you make a pun.

114
00:04:47,000 --> 00:04:49,560
But you can tell that it's
obviously more rewarding than not.

115
00:04:49,560 --> 00:04:50,460
>> Right.

116
00:04:50,460 --> 00:04:53,470
But reward function, and reward function
times seven is basically the same thing.

117
00:04:53,470 --> 00:04:55,680
>> Yeah, because you can scale them and

118
00:04:55,680 --> 00:04:57,130
it doesn't change what
the behavior looks like.

119
00:04:57,130 --> 00:04:57,720
That's right.

120
00:04:57,720 --> 00:05:03,690
So what we take away from this is some
representation of the motivations,

121
00:05:03,690 --> 00:05:07,990
desires, of the individual who
demonstrated the behavior, that we can

122
00:05:07,990 --> 00:05:10,890
then transfer into new environments and
get good behavior out of that.

123
00:05:10,890 --> 00:05:12,130
>> You know,
I think it's broader than that.

124
00:05:12,130 --> 00:05:18,100
It's even broader than that, which is
really you can think of this whole

125
00:05:18,100 --> 00:05:22,470
learning framework as kind
of programming framework.

126
00:05:22,470 --> 00:05:23,430
Software engineering.

127
00:05:23,430 --> 00:05:29,710
Where reinforcement signals are the
mechanism by which you program the agent

128
00:05:29,710 --> 00:05:34,660
in order to get some particular behavior
as opposed to simple function calls.

129
00:05:34,660 --> 00:05:36,640
>> I like that topic,

130
00:05:36,640 --> 00:05:38,950
in fact, I think we should write
a grant proposal about that.

131
00:05:38,950 --> 00:05:39,920
>> Let's write a grant proposal.

132
00:05:39,920 --> 00:05:41,843
Maybe a paper in triple x.

133
00:05:41,843 --> 00:05:42,820
>> Nice.

134
00:05:42,820 --> 00:05:43,720
>> Let's do that.

135
00:05:43,720 --> 00:05:46,880
So look I think it's obvious we can talk
about reinforcement learning forever

136
00:05:46,880 --> 00:05:49,120
even though we didn't spend a lot
of time on it in the class.

137
00:05:49,120 --> 00:05:51,920
Because there's all these basic things
you really have to get about supervisory

138
00:05:51,920 --> 00:05:52,720
and unsupervised.

139
00:05:52,720 --> 00:05:54,940
>> But how can we get all
this information out there?

140
00:05:54,940 --> 00:05:56,100
>> I can think of one way.

141
00:05:56,100 --> 00:05:59,140
>> Mm-hm.
Direct brain interface.

142
00:05:59,140 --> 00:06:00,560
>> No.
>> Okay.

143
00:06:00,560 --> 00:06:01,860
>> I mean yes, but no.

144
00:06:01,860 --> 00:06:02,650
>> Okay.

145
00:06:02,650 --> 00:06:05,740
>> We could do another class.

146
00:06:05,740 --> 00:06:07,520
>> You mean, like together?

147
00:06:07,520 --> 00:06:10,790
>> Yes, we could do a whole class on
reinforcement learning, game theory and

148
00:06:10,790 --> 00:06:11,910
explore all these ideas.

149
00:06:11,910 --> 00:06:14,130
>> But who would even
want to come to such a class?

150
00:06:14,130 --> 00:06:15,971
>> Everyone because we're going to
make it required [CROSSTALK]

151
00:06:15,971 --> 00:06:16,488
>> Oh!

152
00:06:16,488 --> 00:06:17,956
That's great!

153
00:06:17,956 --> 00:06:18,690
>> Mm-hm.

154
00:06:18,690 --> 00:06:19,420
>> Let's do that.

155
00:06:19,420 --> 00:06:19,920
>> Let's do that.

156
00:06:19,920 --> 00:06:21,950
>> Let's make a class on
reinforcement learning.

157
00:06:21,950 --> 00:06:27,120
>> Yes, we'll do a nice three hour
course on reinforcement learning and

158
00:06:27,120 --> 00:06:29,480
game theory, and we can come back and
do all the same thing.

159
00:06:29,480 --> 00:06:30,390
We can use the same editor.

160
00:06:32,010 --> 00:06:34,720
>> Yeah, though it's very
possible that he'll cut this out.

161
00:06:34,720 --> 00:06:35,885
If we even talk about it.

162
00:06:35,885 --> 00:06:38,290
>> [LAUGH] No we'll use the same edit,
push car will be there.

163
00:06:38,290 --> 00:06:39,890
It'll, we'll get the band back together.

164
00:06:39,890 --> 00:06:42,080
>> Get the band back together again,
that sounds really fun.

165
00:06:42,080 --> 00:06:43,520
And the groupies can be the same.

166
00:06:43,520 --> 00:06:45,390
>> Yes.
We can have exactly the same set.

167
00:06:45,390 --> 00:06:46,760
So, you want to do it?

168
00:06:47,800 --> 00:06:50,010
>> I think I would.

169
00:06:50,010 --> 00:06:51,310
>> Let's shake on it.

170
00:06:51,310 --> 00:06:52,170
>> Let's do this thing.

171
00:06:52,170 --> 00:06:52,790
>> Done.

172
00:06:52,790 --> 00:06:53,330
>> Alright.

173
00:06:53,330 --> 00:06:55,400
>> Well, then,
see you in the next class.

174
00:06:55,400 --> 00:06:56,260
>> Yeah, we'll get started next week.
