1
00:00:00,280 --> 00:00:01,770
So this is a concept that we're going to be

2
00:00:01,770 --> 00:00:04,700
able to apply in lots of different settings when we have

3
00:00:04,700 --> 00:00:08,160
infinite hypothesis classes. And this is really the fundamental way that

4
00:00:08,160 --> 00:00:11,080
it's used except usually, there's kind of a more of a

5
00:00:11,080 --> 00:00:15,200
technical sounding definition. This notion of labeling in all possible

6
00:00:15,200 --> 00:00:19,190
ways is usually termed shattering. So this quantity that we're talking

7
00:00:19,190 --> 00:00:22,220
about here, this, this size of the largest set of inputs

8
00:00:22,220 --> 00:00:26,440
that the hypothesis space can shatter, is called the VC dimension.

9
00:00:26,440 --> 00:00:28,310
>> What does VC stand for?

10
00:00:28,310 --> 00:00:32,500
>> VC stands for Vapnik - Chervonenkis which is a

11
00:00:32,500 --> 00:00:35,462
pair of actual people. So that, you know, really smart

12
00:00:35,462 --> 00:00:38,190
insightful guys that put together this notion of a definition

13
00:00:38,190 --> 00:00:40,120
and what they did is they can relate the VC

14
00:00:40,120 --> 00:00:42,840
dimension of a class to the amount of data that

15
00:00:42,840 --> 00:00:45,560
you need to be able to learn effectively in that

16
00:00:45,560 --> 00:00:49,010
class. S, as long as this dimensionality is finite. Even

17
00:00:49,010 --> 00:00:51,540
if the hypothesis class is infiniite. We are going to be

18
00:00:51,540 --> 00:00:55,900
able to say things about how much data we need to learn.

19
00:00:55,900 --> 00:00:58,750
So, that's, that's really cool. It really connects things up beautifully. So, I

20
00:00:58,750 --> 00:01:01,860
think what would be a really useful exercise now is to look at

21
00:01:01,860 --> 00:01:06,160
various kinds of hypothesis classes. And for us to measure the VC dimension.

22
00:01:06,160 --> 00:01:07,290
>> Okay, sounds like fun.
