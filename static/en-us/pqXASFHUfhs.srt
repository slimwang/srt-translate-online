1
00:00:00,210 --> 00:00:03,060
Well, what you're doing in order to make
that work and what you end up doing in

2
00:00:03,060 --> 00:00:05,160
supervised learning and
functions approximation in general,

3
00:00:05,160 --> 00:00:07,920
is you make some fundamental
assumptions about the world, right?

4
00:00:07,920 --> 00:00:11,560
You decide that you have a well-behaved
function that is consistent with

5
00:00:11,560 --> 00:00:15,480
the data that you're given, and with
that, you're able to generalize, and

6
00:00:15,480 --> 00:00:18,120
in fact that is the fundamental
problem in machine learning.

7
00:00:18,120 --> 00:00:19,760
It is generalization.

8
00:00:19,760 --> 00:00:23,055
Now what's behind all of is
I'm going to claim, Michael,

9
00:00:23,055 --> 00:00:27,030
you jump in whenever you disagree is
>> I disagree sorry to soon, go ahead.

10
00:00:27,030 --> 00:00:28,520
>> is bias and in particular

11
00:00:28,520 --> 00:00:29,652
>> bias
>> inductive bias.

12
00:00:29,652 --> 00:00:30,496
>> Inductive bias.

13
00:00:30,496 --> 00:00:32,595
>> Right, so all of machine learning or

14
00:00:32,595 --> 00:00:37,360
certainly supervised learning is about
induction, as opposed to deduction.

15
00:00:37,360 --> 00:00:39,070
>> I see.
Induction of course being a problem of

16
00:00:39,070 --> 00:00:41,820
going from examples to
a more general rule.

17
00:00:41,820 --> 00:00:44,700
>> Right, specifics to generalities.

18
00:00:44,700 --> 00:00:46,160
By contrast, deduction is?

19
00:00:47,730 --> 00:00:48,250
>> Be the opposite.

20
00:00:48,250 --> 00:00:51,300
It would be going from a general
rule to specific instances,

21
00:00:51,300 --> 00:00:53,290
basically like reasoning.

22
00:00:53,290 --> 00:00:54,190
>> Right, and in fact,

23
00:00:54,190 --> 00:00:56,900
a lot of AI in the beginning
was about deductive reasoning,

24
00:00:56,900 --> 00:01:00,200
about logic programming, those sorts of
things, where you have certain rules,

25
00:01:00,200 --> 00:01:03,400
and you deduce only those things that
follow immediately from those rules.

26
00:01:03,400 --> 00:01:06,030
So for example,
you'd have something like A implies B.

27
00:01:06,030 --> 00:01:08,200
That's a rule in the universe.

28
00:01:08,200 --> 00:01:09,650
And then I tell you A.

29
00:01:09,650 --> 00:01:12,540
So if you A implies B is a rule in
the universe, and I tell you A,

30
00:01:12,540 --> 00:01:14,540
then you also know?

31
00:01:14,540 --> 00:01:16,310
>> That A implies B.

32
00:01:16,310 --> 00:01:17,930
>> And therefore you can infer that?

33
00:01:17,930 --> 00:01:18,480
>> And A. B. >> B.

34
00:01:18,480 --> 00:01:23,270
You have A implies B,
you have A, that implies B.

35
00:01:23,270 --> 00:01:26,890
>> Okay.
>> That's what we just said.

36
00:01:28,230 --> 00:01:29,040
But what,
>> that's deduction.

37
00:01:29,040 --> 00:01:31,680
>> That's deduction, but
we just did was not deduction.

38
00:01:31,680 --> 00:01:35,480
Before then when I asked you one, one,
two, four, three, nine, four sixteen and

39
00:01:35,480 --> 00:01:36,230
so forth.

40
00:01:36,230 --> 00:01:37,310
>> Right,
>> we did induction.

41
00:01:37,310 --> 00:01:38,090
>> That was induction.

42
00:01:38,090 --> 00:01:41,900
>> Induction is more about
did the sun rise yesterday?

43
00:01:41,900 --> 00:01:43,610
>> yes.
>> Did the sun rise the day before that?

44
00:01:43,610 --> 00:01:45,090
>> yes.
>> Did the sun rise the day before that?

45
00:01:46,880 --> 00:01:48,080
>> I slept in.

46
00:01:48,080 --> 00:01:49,250
Did the sun rise the day before that?

47
00:01:49,250 --> 00:01:49,860
>> Yes.

48
00:01:49,860 --> 00:01:50,360
>> Yes.

49
00:01:50,360 --> 00:01:51,520
So the sun has risen every day.

50
00:01:51,520 --> 00:01:52,810
Is the sun going to rise tomorrow?

51
00:01:53,870 --> 00:01:54,740
>> I sure hope so.

52
00:01:54,740 --> 00:01:57,030
>> We all hope so, and
we all act like it does,

53
00:01:57,030 --> 00:02:00,010
because if it doesn't, then there are a
whole bunch of other things we ought to

54
00:02:00,010 --> 00:02:02,290
be doing besides sitting in this
studio and having this interview.

55
00:02:02,290 --> 00:02:03,481
>> I think we should warn the plants.

56
00:02:03,481 --> 00:02:05,701
>> [LAUGH] I don't think
the plants are going to care.

57
00:02:05,701 --> 00:02:07,050
>> They are.
They really need sun.

58
00:02:07,050 --> 00:02:09,389
I think we all need sun, Mike.

59
00:02:09,389 --> 00:02:11,200
>> Eh.
>> So, the idea there is

60
00:02:11,200 --> 00:02:14,310
induction is crucial, and
the inductive bias is crucial.

61
00:02:14,310 --> 00:02:16,650
We'll talk about all of this in,
in the course.

62
00:02:16,650 --> 00:02:17,680
>> Kay.
>> But that's kind of a fundamental

63
00:02:17,680 --> 00:02:19,930
notion behind supervised learning and
machine learning in general.

64
00:02:19,930 --> 00:02:20,500
>> I agree with that.

65
00:02:20,500 --> 00:02:21,110
>> Agreed?
>> Yeah.

66
00:02:21,110 --> 00:02:22,080
>> Al lright, so we're on the same page.

67
00:02:22,080 --> 00:02:23,260
So that's supervised learning.

68
00:02:23,260 --> 00:02:25,820
Supervised learning, you can talk about
it in these high muckity muck ways, but

69
00:02:25,820 --> 00:02:27,990
at the end of the day,
it's function approximation.

70
00:02:27,990 --> 00:02:30,090
It's figuring out how to take
a bunch of training examples and

71
00:02:30,090 --> 00:02:33,050
coming up with some function
that generalizes beyond

72
00:02:33,050 --> 00:02:33,660
the data that you see.

73
00:02:33,660 --> 00:02:35,590
>> So, why wouldn't you call
it function induction, then?

74
00:02:37,175 --> 00:02:38,770
>> because someone said
supervised learning first.

75
00:02:38,770 --> 00:02:39,280
Well, there is a-

76
00:02:39,280 --> 00:02:39,850
>> No, no, no, no, no.
Sorry.

77
00:02:39,850 --> 00:02:43,297
You said supervised learning is function
approximation and I want to say,

78
00:02:43,297 --> 00:02:46,680
why don't you say supervised
learning is function induction.

79
00:02:46,680 --> 00:02:48,210
>> As opposed to function approximation?

80
00:02:48,210 --> 00:02:48,960
>> Yeah.

81
00:02:48,960 --> 00:02:49,470
>> Okay.

82
00:02:49,470 --> 00:02:51,840
It's a
>> Approximate function induction.

83
00:02:53,230 --> 00:02:55,353
>> Or induction of approximate, of.

84
00:02:55,353 --> 00:02:56,061
>> Approximate functions?

85
00:02:56,061 --> 00:02:56,960
>> Approximate functions,
something like that, yeah.

86
00:02:56,960 --> 00:02:59,160
>> You don't want to induce
an approximate function,

87
00:02:59,160 --> 00:03:00,600
you want to induce the actual function.

88
00:03:00,600 --> 00:03:01,970
>> Yeah, but sometimes you can't,
>> Yeah.

89
00:03:01,970 --> 00:03:06,254
>> Because sometimes you think
it's quadratic, but it's not.

90
00:03:06,254 --> 00:03:07,900
>> I have that as a plaque on my wall.

91
00:03:07,900 --> 00:03:09,370
>> You do?

92
00:03:09,370 --> 00:03:09,962
>> No.

93
00:03:09,962 --> 00:03:10,682
>> Yeah I didn't think so.

94
00:03:10,682 --> 00:03:11,970
Okay so that's supervised learning
