1
00:00:00,080 --> 00:00:03,400
To see what the RAID 0
reliability looks like,

2
00:00:03,400 --> 00:00:09,160
let's assume that f is a failure
rate for a single disk.

3
00:00:09,160 --> 00:00:13,690
A failure rate is really how
many failures do we expect for

4
00:00:13,690 --> 00:00:17,220
a single working disk
to have in a second.

5
00:00:17,220 --> 00:00:20,800
Obviously, this number
will be extremely small.

6
00:00:20,800 --> 00:00:24,350
Usually we can assume that the failure
rate is constant over time,

7
00:00:24,350 --> 00:00:29,070
that is, every second, if the desk is
working at the beginning of the second,

8
00:00:29,070 --> 00:00:32,790
it has the same failure
rate during that second.

9
00:00:32,790 --> 00:00:35,500
For a single disk then, the MTTF,

10
00:00:35,500 --> 00:00:40,630
the mean time to failure,
will be 1 over f.

11
00:00:40,630 --> 00:00:44,780
So, for example, if the disk has one
millionth of a failure per second,

12
00:00:44,780 --> 00:00:48,830
then the MTTF will be 1 million seconds.

13
00:00:48,830 --> 00:00:54,090
For disks, the entity f is also
sometimes called mean time

14
00:00:54,090 --> 00:00:59,690
to data loss, or how long until
we lose some data on this disk.

15
00:00:59,690 --> 00:01:02,740
And for a single disk,
the mean time to data loss

16
00:01:02,740 --> 00:01:07,030
is the same as the MTTF of one disk.

17
00:01:07,030 --> 00:01:09,750
Now, let's look at what happens when we

18
00:01:09,750 --> 00:01:13,310
have N such disks in
a RAID 0 configuration.

19
00:01:13,310 --> 00:01:17,560
It turns out that the failure
rate when we have N working disks

20
00:01:17,560 --> 00:01:23,200
is equal to N times
the failure rate of one disk.

21
00:01:23,200 --> 00:01:24,580
This is because, for

22
00:01:24,580 --> 00:01:29,810
example, if we can expect one millionth
of a failure per working disk per second

23
00:01:29,810 --> 00:01:34,670
then we can expect one failure per
million working disks per second.

24
00:01:34,670 --> 00:01:37,870
So if we start the second
with a million working disks,

25
00:01:37,870 --> 00:01:41,150
chances are that one of them
will fail during the second.

26
00:01:41,150 --> 00:01:45,760
If we start the second with
half a million working disks,

27
00:01:45,760 --> 00:01:49,780
on average half of a single disk
will fail per second, and so on.

28
00:01:50,950 --> 00:01:57,250
It also turns out that the mean time to
the first failure among N working disks

29
00:01:57,250 --> 00:02:02,810
has the same distribution as the first
failure for the single disk, so

30
00:02:02,810 --> 00:02:09,419
we get that MTTF for
N disks in a RAID 0 configuration,

31
00:02:09,419 --> 00:02:14,800
where MTTF is really the mean
time to data loss in this case,

32
00:02:14,800 --> 00:02:20,890
is equal to the MTTF of
one disk divided by N.

33
00:02:20,890 --> 00:02:25,460
So if you have two disks,
the MTTF of the RAID 0 array with two

34
00:02:25,460 --> 00:02:30,230
disks will be half of
the MTTF of a single disk.

35
00:02:30,230 --> 00:02:33,050
A single disk typically
will have the MTTF

36
00:02:33,050 --> 00:02:37,020
on the order of ten-ish
years to maybe 100 years.

37
00:02:37,020 --> 00:02:41,550
So if we have too many disks in a raid
zero configuration, we can reduce

38
00:02:41,550 --> 00:02:46,170
the MTTF, so the average disk will not
fail in the five or six years that we

39
00:02:46,170 --> 00:02:50,850
actually end up using it, but if we have
two or three or four such disks then we

40
00:02:50,850 --> 00:02:55,120
can expect the array to fail at least
once until we actually get rid of it.
