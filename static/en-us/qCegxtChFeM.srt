1
00:00:00,250 --> 00:00:02,610
Alright, so, remember, Charles, we were talking about

2
00:00:02,610 --> 00:00:06,540
three different kinds of ways of choosing the

3
00:00:06,540 --> 00:00:08,840
inputs to the learner. Okay? So what were

4
00:00:08,840 --> 00:00:10,705
they again? We just looked at two of them.

5
00:00:10,705 --> 00:00:15,480
>> So the learner chooses examples. The teacher, hopefully a nice

6
00:00:15,480 --> 00:00:20,450
one, chooses examples, and then there was the case with the

7
00:00:20,450 --> 00:00:22,950
examples are given to us by nature. And I guess there

8
00:00:22,950 --> 00:00:25,267
was a fourth one, which is. That a mean teacher gives

9
00:00:25,267 --> 00:00:27,490
it to us but I, you know I don't think that's all

10
00:00:27,490 --> 00:00:30,283
that interesting. I tend to think of nature as a mean teacher.

11
00:00:30,283 --> 00:00:32,107
>> Well not only that but in the, at least in

12
00:00:32,107 --> 00:00:35,014
the mistake bound setting that we just looked at again the, the

13
00:00:35,014 --> 00:00:38,320
learner was robust against any choice of where the inputs were coming

14
00:00:38,320 --> 00:00:41,455
from. So mean teacher it would've it would've done just as well.

15
00:00:41,455 --> 00:00:42,424
>> That's a fine point.

16
00:00:42,424 --> 00:00:45,103
>> It doesn't matter when it makes its mistakes, it's only

17
00:00:45,103 --> 00:00:48,080
going to make a fixed number of them no matter what.

18
00:00:48,080 --> 00:00:48,260
>> Okay.

19
00:00:48,260 --> 00:00:50,270
>> So, we've kind of dealt with three

20
00:00:50,270 --> 00:00:52,880
of these but we haven't really talked about the, the

21
00:00:52,880 --> 00:00:56,000
nature chooses case yet. And that's in some ways the most

22
00:00:56,000 --> 00:00:59,850
interesting and relevant and in other ways the most complicated

23
00:00:59,850 --> 00:01:02,810
because you have to really take into consideration this space of

24
00:01:02,810 --> 00:01:05,760
possible distributions. I think now though we're in a pretty

25
00:01:05,760 --> 00:01:09,020
good situation in terms of being able to define some important

26
00:01:09,020 --> 00:01:11,430
terms and we're going to use those terms to get

27
00:01:11,430 --> 00:01:13,800
a handle on this question of what happens when nature chooses.

28
00:01:13,800 --> 00:01:15,090
>> Okay, that sounds reasonable.

29
00:01:16,190 --> 00:01:19,130
>> So computational complexity, we talked about. The, the, how much

30
00:01:19,130 --> 00:01:22,070
computational effort is going to be needed for a learner to convert

31
00:01:22,070 --> 00:01:25,830
to the answer. Sample complexity in the case of a batch,

32
00:01:25,830 --> 00:01:28,730
that is to say, we have a training set. Is how large

33
00:01:28,730 --> 00:01:31,270
is that training set need to be for the learner to

34
00:01:31,270 --> 00:01:35,090
be able to create successful hypothesis. And that's in the batch setting.

35
00:01:35,090 --> 00:01:37,740
In the online setting, we have this notion of a mistake

36
00:01:37,740 --> 00:01:41,180
bound. How many misclassifications can the learner make over an infinite run?

37
00:01:41,180 --> 00:01:41,870
>> Mind

38
00:01:41,870 --> 00:01:43,230
if I ask you a question? You said

39
00:01:43,230 --> 00:01:46,580
something I thought pretty interesting. How, for computational complexity,

40
00:01:46,580 --> 00:01:48,660
you said how much computational effort is needed

41
00:01:48,660 --> 00:01:51,210
for a learner to converge To the right answer.

42
00:01:51,210 --> 00:01:53,890
Is that the, is that a requirement when

43
00:01:53,890 --> 00:01:56,960
you talk about computational complexity? Or is just that

44
00:01:56,960 --> 00:01:58,350
you need to know how much computational effort

45
00:01:58,350 --> 00:02:01,110
is needed for a learner to converge to something?

46
00:02:01,110 --> 00:02:03,910
>> Well, so if it's converging to something and we don't care what

47
00:02:03,910 --> 00:02:06,960
it's converging to, then it's really easy to have an algorithm with low

48
00:02:06,960 --> 00:02:08,820
computational complexity, right? It's just like

49
00:02:08,820 --> 00:02:10,850
return garbage. It runs in constant time.

50
00:02:10,850 --> 00:02:11,260
>> Mm hm.

51
00:02:11,260 --> 00:02:14,160
>> So, yeah, it's in the context of actually

52
00:02:14,160 --> 00:02:17,310
solving the problem, that computational complexity is most interesting.

53
00:02:17,310 --> 00:02:18,610
>> Well, I was going to say, what if a set

54
00:02:18,610 --> 00:02:21,609
of hypothesis that I'm willing to entertain, doesn't include the true concept.

55
00:02:22,990 --> 00:02:25,330
>> good. Right. So in some sense maybe

56
00:02:25,330 --> 00:02:26,820
in that case what we're trying to find is

57
00:02:26,820 --> 00:02:29,900
the best hypothesis in the hypothesis class. But we

58
00:02:29,900 --> 00:02:32,520
can still ask about computational complexity in that case.

59
00:02:32,520 --> 00:02:35,550
So, I was saying successful hypothesis here. By that I

60
00:02:35,550 --> 00:02:38,780
meant, you know, whatever it is that the problem demands that

61
00:02:38,780 --> 00:02:41,230
we return and if it's a hypothesis class that's very

62
00:02:41,230 --> 00:02:43,970
limited we might just return the best thing in that class.

63
00:02:43,970 --> 00:02:45,090
>> Okay.

64
00:02:45,090 --> 00:02:47,120
>> But the important thing here is that we're not going

65
00:02:47,120 --> 00:02:48,880
to talk about computational [LAUGH] complexity,

66
00:02:48,880 --> 00:02:50,060
for the most part. We're going to

67
00:02:50,060 --> 00:02:54,070
focus on sample complexity for the time being. And that's really

68
00:02:54,070 --> 00:02:57,460
the relevant concept when we're talking about the idea of nature choosing.

69
00:02:57,460 --> 00:02:57,810
>> I believe you
