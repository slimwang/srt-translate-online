1
00:00:00,330 --> 00:00:08,050
Given a behavior spec for a protocol say TCP/IP, can a stack be synthesized

2
00:00:08,050 --> 00:00:14,200
as a composition of the ensemble micro protocols? Given 60

3
00:00:14,200 --> 00:00:19,500
micro protocols in the ensemble suite, there are way too many combinations for a

4
00:00:19,500 --> 00:00:26,110
brute force approach. Instead. In the ensemble approach, the user heuristic

5
00:00:26,110 --> 00:00:29,736
algorithm for synthesizing the stack given

6
00:00:29,736 --> 00:00:34,342
the desired behavioral spec and designer's knowledge

7
00:00:34,342 --> 00:00:40,320
of the micro protocols. The result is a protocol stack which is functional, but

8
00:00:40,320 --> 00:00:43,389
not optimized for performance. Of course, as

9
00:00:43,389 --> 00:00:46,489
operating system designers, we're always worried about

10
00:00:46,489 --> 00:00:49,470
perf, performance. And we naturally have to

11
00:00:49,470 --> 00:00:51,410
think about how to optimize the protocol

12
00:00:51,410 --> 00:00:53,210
stack, so that it is not only

13
00:00:53,210 --> 00:00:57,110
functional, but also performant. In particular, the

14
00:00:57,110 --> 00:01:00,520
fact that we've assembled this protocol stack

15
00:01:00,520 --> 00:01:02,860
like Lego blocks. Putting together all these

16
00:01:02,860 --> 00:01:06,500
micro protocols leads to layering and layering

17
00:01:06,500 --> 00:01:09,690
leads to inefficiencies. Now this is where

18
00:01:09,690 --> 00:01:14,100
the analogy to VLSI component based design,

19
00:01:14,100 --> 00:01:16,680
breaks down. And the reason is because in

20
00:01:16,680 --> 00:01:19,280
VLSI component based design, even though we

21
00:01:19,280 --> 00:01:23,040
are building a complex chi, like a CPU.

22
00:01:23,040 --> 00:01:27,140
By putting together components, the components just

23
00:01:27,140 --> 00:01:29,370
fit together very nicely. There is, there is

24
00:01:29,370 --> 00:01:32,800
no inefficiency in going between these components.

25
00:01:32,800 --> 00:01:36,600
But in software components unfortunately, they have interfaces.

26
00:01:36,600 --> 00:01:39,058
And interfaces usually mean that, that there

27
00:01:39,058 --> 00:01:42,780
is well defined boundaries between these components and

28
00:01:42,780 --> 00:01:45,490
so to cross the component boundary, you may have

29
00:01:45,490 --> 00:01:49,350
to copy parameters at here to interface specifications of the

30
00:01:49,350 --> 00:01:52,660
components and so on. All of those things leads to

31
00:01:52,660 --> 00:01:57,200
inefficiencies and this where the VLSI component based design idea

32
00:01:57,200 --> 00:01:59,380
breaks down little bit when we just put together

33
00:01:59,380 --> 00:02:02,130
the components of software in order to build the, the

34
00:02:02,130 --> 00:02:04,660
large scale system. So we have to do the extra

35
00:02:04,660 --> 00:02:08,144
work that is needed in order to optimize the component-based

36
00:02:08,144 --> 00:02:11,650
design so that it can perform well. Fortunately there

37
00:02:11,650 --> 00:02:14,760
are several sources of optimization that are possible. For

38
00:02:14,760 --> 00:02:19,740
instance I mentioned that OCaml has implicit garbage collection.

39
00:02:19,740 --> 00:02:22,870
Now it is good that it has implicit garbage collection

40
00:02:22,870 --> 00:02:25,620
as a fall-back. But, maybe we don't want to use

41
00:02:25,620 --> 00:02:26,890
it all the time and we want to be

42
00:02:26,890 --> 00:02:29,620
explicit about how we manage our own memory that

43
00:02:29,620 --> 00:02:33,310
can be more efficient, that is a source of optimization.

44
00:02:33,310 --> 00:02:37,930
I mentioned that OCaml has ability for doing

45
00:02:37,930 --> 00:02:41,150
marshaling and unmarshaling of arguments to go across

46
00:02:41,150 --> 00:02:45,140
layers, but is also a very good thing to do, in order to have a component-based

47
00:02:45,140 --> 00:02:48,460
design. But, when you're going across layers, these

48
00:02:48,460 --> 00:02:51,550
things can add overheads and this is another

49
00:02:51,550 --> 00:02:54,800
source of optimization that is possible by avoiding

50
00:02:54,800 --> 00:02:58,790
these marshalling and unmarshalling across the layers but collapsing

51
00:02:58,790 --> 00:03:02,320
the layers. Another opportunity that exists especially in networking

52
00:03:02,320 --> 00:03:05,020
systems, is the fact that there's going to be

53
00:03:05,020 --> 00:03:08,580
communication and computation. If you think about TCPIP Protocol,

54
00:03:08,580 --> 00:03:11,400
it has to necessarily buffer the packets that it's sending

55
00:03:11,400 --> 00:03:14,180
out. Because if the packets are lost for some

56
00:03:14,180 --> 00:03:17,100
reason, it may have to re-transmit it. This is again,

57
00:03:17,100 --> 00:03:21,420
a situation where we can overlap this buffering which

58
00:03:21,420 --> 00:03:24,310
is computation on the node that is sending the packet

59
00:03:24,310 --> 00:03:26,940
with the actual transmission. So, we can overlap that.

60
00:03:26,940 --> 00:03:29,880
That's another opportunity for optimization.

61
00:03:29,880 --> 00:03:31,980
Another opportunity is compressing the

62
00:03:31,980 --> 00:03:35,380
header, especially when we have this layering, at every

63
00:03:35,380 --> 00:03:38,660
layer it might add a new header specific to that

64
00:03:38,660 --> 00:03:41,230
layer, and that may have common fields for instance,

65
00:03:41,230 --> 00:03:43,960
size of the packet or check sum for the packet

66
00:03:43,960 --> 00:03:46,230
and so on. Those are common things that you can

67
00:03:46,230 --> 00:03:49,340
eliminate when we are going across these layers. So the

68
00:03:49,340 --> 00:03:53,090
header compression is another possibility for optimization. Another thing

69
00:03:53,090 --> 00:03:56,280
that we always have to worry about, is making

70
00:03:56,280 --> 00:04:00,100
sure that the code that we execute fits in

71
00:04:00,100 --> 00:04:03,260
the caches and this has been something that we've talked

72
00:04:03,260 --> 00:04:05,620
about all along when we talked about single node

73
00:04:05,620 --> 00:04:09,500
systems to parallel systems. That locality enhancement, making sure that

74
00:04:09,500 --> 00:04:12,450
the working set of whatever code that is executing

75
00:04:12,450 --> 00:04:15,310
on a processor fits in the cache is very important.

76
00:04:15,310 --> 00:04:19,269
So, this again is an opportunity by identifying common

77
00:04:19,269 --> 00:04:22,440
code path across the different layers of the protocol

78
00:04:22,440 --> 00:04:26,780
stack, and co-locating that common code path across the

79
00:04:26,780 --> 00:04:29,610
layers. In order to make sure that we can

80
00:04:29,610 --> 00:04:33,170
enhance locality for processing is another source of optimization

81
00:04:33,170 --> 00:04:35,190
that is possible. This is all good. So there

82
00:04:35,190 --> 00:04:37,500
are lots of opportunities for optimization but do it

83
00:04:37,500 --> 00:04:40,470
by hand manually, that's tedious. So how do we

84
00:04:40,470 --> 00:04:44,790
automate the process of optimization so that we don't have to do it manually?
