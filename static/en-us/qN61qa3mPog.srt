1
00:00:00,220 --> 00:00:04,230
So first we want to do
is basically compute H,

2
00:00:04,230 --> 00:00:09,120
but loop until we get
the most popular H.

3
00:00:09,120 --> 00:00:09,750
How to we start?

4
00:00:09,750 --> 00:00:12,770
Well, select four feature
points at random.

5
00:00:12,770 --> 00:00:15,760
Compute the homography
between all of them, exactly.

6
00:00:15,760 --> 00:00:17,784
But then start looking for

7
00:00:17,784 --> 00:00:23,310
the sum of squared differences
between the inlier for the new one.

8
00:00:23,310 --> 00:00:28,040
And, basically, the one that's computed,
with respect to some threshold.

9
00:00:28,040 --> 00:00:30,410
Keep the largest set of inliers.

10
00:00:30,410 --> 00:00:34,610
And then re-compute the H estimate
within all of the inliers.

11
00:00:34,610 --> 00:00:38,210
And the basic idea here is that there
are more inliers than outliers.

12
00:00:38,210 --> 00:00:40,070
Well, that doesn't really help.

13
00:00:40,070 --> 00:00:41,290
The idea really is,

14
00:00:41,290 --> 00:00:47,670
is which outliers are wrong from a set
that is actually much more correct.

15
00:00:47,670 --> 00:00:49,930
So, giving more consensus or

16
00:00:49,930 --> 00:00:55,040
more power, more popularity to the ones
that actually are matching each other

17
00:00:55,040 --> 00:00:58,180
is what the key is, not that there
are a number of samples more.

18
00:00:58,180 --> 00:00:59,650
But there could be a lot of samples, but

19
00:00:59,650 --> 00:01:03,620
it's going to go choose the one that are
actually more similar to each other and

20
00:01:03,620 --> 00:01:04,970
that'll be the one that
we want to look for.
