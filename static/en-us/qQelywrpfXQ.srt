1
00:00:00,000 --> 00:00:02,939
Today we're going to talk about optimizing GPU programs.

2
00:00:02,939 --> 00:00:08,645
Now the whole reason we want to use a parallel platform like the GPU is to solve problems faster,

3
00:00:08,645 --> 00:00:13,483
and in turn the reason we might want to solve problems faster could be simply because

4
00:00:13,483 --> 00:00:17,821
we want to a solve problems faster. Or more often it's because we want to solve bigger problems,

5
00:00:17,821 --> 00:00:24,092
or solve more problems. So the good news is that it's often the case that the first initial port

6
00:00:24,092 --> 00:00:28,832
of a problem gets a speed up, assuming that you got a parallel problem to begin with,

7
00:00:28,832 --> 00:00:33,303
and in my experience, this is actually when a lot of GPU programmers get hooked.

8
00:00:33,303 --> 00:00:35,508
You know, over the weekend they go home and out of curiosity

9
00:00:35,508 --> 00:00:39,129
they try porting some piece of their existing CPU code to the GPU,

10
00:00:39,129 --> 00:00:41,810
and they get a nice speed up, a 5x or 8x speed up.

11
00:00:41,810 --> 00:00:44,347
And that's, that's what sort of gets them hooked and makes them realize they

12
00:00:44,347 --> 00:00:46,916
could put some more effort into this and get a bigger speed up.

13
00:00:46,916 --> 00:00:51,628
So if you have a naturally parallel problem, it's often the case that that first initial

14
00:00:51,628 --> 00:00:55,825
Cuda port will get you good speed up, and that's cool.

15
00:00:55,825 --> 00:01:01,560
But, you know, by definition GPU programmers care about performance, that's why they're using the GPU.

16
00:01:01,560 --> 00:01:06,268
That means they often want to spend additional effort to maximize the speed up beyond that first initial try.

17
00:01:06,268 --> 00:01:09,238
So in this unit we're going to talk about how to optimize GPU programs.

18
00:01:09,238 --> 00:01:12,407
So cast your mind back to unit 2,

19
00:01:12,407 --> 00:01:17,122
when we talked for a little bit about some basic principles of efficient GPU programming.

20
00:01:17,122 --> 00:01:19,844
So check which of these principles accurately correspond to things

21
00:01:19,844 --> 00:01:22,511
we talked about in unit 2 about efficient GPU programming.

22
00:01:22,511 --> 00:01:25,817
Do we want to decrease arithmetic intensity?

23
00:01:25,817 --> 00:01:30,221
Do we want to decrease the time spent on memory operations per thread?

24
00:01:30,221 --> 00:01:33,198
Do we want to coalesce global memory accesses?

25
00:01:33,198 --> 00:01:39,672
Do we want to do fewer memory operations per thread? Do we want to avoid thread divergence?

26
00:01:39,672 --> 00:01:42,467
Do we want to move all data to shared memory?
