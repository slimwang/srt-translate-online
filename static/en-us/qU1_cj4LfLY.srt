1
00:00:00,210 --> 00:00:04,770
So here's the way that a lasso regression can actually make a regression

2
00:00:04,770 --> 00:00:07,620
simpler in terms of the number of features that it's using.

3
00:00:07,620 --> 00:00:12,110
So what it does is for features that don't help the regression results enough,

4
00:00:12,110 --> 00:00:15,820
it can set the coefficient of those features to a very small value,

5
00:00:15,820 --> 00:00:17,360
potentially to zero.

6
00:00:17,360 --> 00:00:20,740
So imagine that I have up to four features that are available to me,

7
00:00:20,740 --> 00:00:24,520
x1 through x4, and I don't know which ones are the most powerful.

8
00:00:24,520 --> 00:00:26,650
What are the ones that I actually want to be using?

9
00:00:26,650 --> 00:00:29,120
I don't want to use all of these if I don't have to.

10
00:00:29,120 --> 00:00:33,380
And that m1 through m4 are the coefficients of regression that I get for

11
00:00:33,380 --> 00:00:36,918
these features when I actually perform the fit.

12
00:00:36,918 --> 00:00:40,050
So in an ordinary multivariate regression,

13
00:00:40,050 --> 00:00:42,310
I would get a result that looks something like this.

14
00:00:42,310 --> 00:00:45,280
It will use all of the features that I make available to it and

15
00:00:45,280 --> 00:00:48,184
it will assign to each one a coefficient of regression.

16
00:00:48,184 --> 00:00:52,100
What lasso regression will do, is it will try adding them in one at a time.

17
00:00:52,100 --> 00:00:54,720
And if the new feature doesn't improve the fit enough to

18
00:00:54,720 --> 00:00:58,910
outweigh the penalty term of including that feature, then it won't be added.

19
00:00:58,910 --> 00:01:01,870
And by not added, it just means that it sets the coefficient to zero.

20
00:01:01,870 --> 00:01:04,060
So let's suppose that x1 and

21
00:01:04,060 --> 00:01:07,560
x2 are two features that are really critical to include in our regression.

22
00:01:07,560 --> 00:01:11,660
They really describe the pattern with a low of power.

23
00:01:11,660 --> 00:01:14,880
But that x3 and x4 don't improve the fit by very much.

24
00:01:14,880 --> 00:01:18,610
They might improve it by a little bit, but we could just be fitting to noise.

25
00:01:18,610 --> 00:01:21,133
Then what will happen is that the coefficients for

26
00:01:21,133 --> 00:01:23,124
those two features will be set to zero.

27
00:01:23,124 --> 00:01:27,750
So m3 will be set to 0 and m4 will be set to 0.

28
00:01:27,750 --> 00:01:30,850
Which effectively means that these two features become irrelevant in the fit,

29
00:01:30,850 --> 00:01:33,780
and the only thing that's left is x1 and x2,

30
00:01:33,780 --> 00:01:37,360
the original two features that we wanted to have anyway.

31
00:01:37,360 --> 00:01:40,730
The power of regularization, the power of having this penalty term for

32
00:01:40,730 --> 00:01:44,440
the extra features, is that it can automatically do this selection for you.
