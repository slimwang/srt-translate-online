1
00:00:00,450 --> 00:00:03,190
But first,
I want to introduce you to another idea,

2
00:00:03,190 --> 00:00:06,010
it's the idea of one
by one convolutions.

3
00:00:06,010 --> 00:00:10,550
You might wonder, why might one ever
want to use one by one convolutions?

4
00:00:10,550 --> 00:00:13,880
They're not really looking at a patch
of the image, just that one pixel.

5
00:00:15,070 --> 00:00:17,380
Look at the classic convolution setting.

6
00:00:17,380 --> 00:00:22,140
It's basically a small classifier for
a patch of the image, but

7
00:00:22,140 --> 00:00:24,617
it's only a linear classifier.

8
00:00:24,617 --> 00:00:28,606
But if you add a one by one convolution
in the middle, suddenly you have

9
00:00:28,606 --> 00:00:33,910
a mini neural network running over the
patch instead of a linear classifier.

10
00:00:33,910 --> 00:00:37,560
Interspersing your convolutions
with one by one convolutions

11
00:00:37,560 --> 00:00:40,920
is a very inexpensive way to
make your models deeper and

12
00:00:40,920 --> 00:00:45,250
have more parameters without
completely changing their structure.

13
00:00:45,250 --> 00:00:48,350
They're also very cheap,
because if you go through the math,

14
00:00:48,350 --> 00:00:50,110
they're not really convolutions at all.

15
00:00:50,110 --> 00:00:54,670
They're really just matrix multiplies,
and they have relatively few parameters.

16
00:00:54,670 --> 00:00:59,034
I mention all of this, average pooling
and one by one convolutions because I

17
00:00:59,034 --> 00:01:02,923
want to talk about a general strategy
that has been very successful at

18
00:01:02,923 --> 00:01:05,379
creating covnets that
are both smaller and

19
00:01:05,379 --> 00:01:09,088
better than covnets that simply
use a pyramid of convolutions.
