1
00:00:00,350 --> 00:00:02,710
So what are some examples
of good Kernels?

2
00:00:03,730 --> 00:00:04,615
That become generals.

3
00:00:04,615 --> 00:00:05,640
[LAUGH] Anyway.

4
00:00:05,640 --> 00:00:06,689
All right.
Fine.

5
00:00:06,689 --> 00:00:11,410
Well, the simplest kernel is actually
just the original linear space.

6
00:00:11,410 --> 00:00:11,960
Right?

7
00:00:11,960 --> 00:00:12,880
So if.

8
00:00:12,880 --> 00:00:16,180
K of xi,
xj is just the dot product of xi, xj.

9
00:00:16,180 --> 00:00:18,350
That's what I was showing you before for
the Dalal and

10
00:00:18,350 --> 00:00:20,330
Triggs Pedestrian Detection.

11
00:00:20,330 --> 00:00:22,190
Right?
You just have a great big long feature

12
00:00:22,190 --> 00:00:25,940
vector and it, I don't even have to map
it into a higher dimensional space.

13
00:00:25,940 --> 00:00:28,970
I'm just going to assume
that that's going to work.

14
00:00:28,970 --> 00:00:31,520
Now and
the number of dimensions is what?

15
00:00:31,520 --> 00:00:34,050
It's just N, where N was
the length of the feature vector.

16
00:00:34,050 --> 00:00:36,170
So that's the lifting space.

17
00:00:36,170 --> 00:00:38,690
Now something that I'm not going to

18
00:00:40,130 --> 00:00:44,900
teach here even in some
higher dimensional space.

19
00:00:44,900 --> 00:00:48,850
So when you pick, your system might
not be completely linearly separable.

20
00:00:48,850 --> 00:00:50,250
Right?

21
00:00:50,250 --> 00:00:54,770
And there's ways of learning support
vector machines in that situation.

22
00:00:54,770 --> 00:00:56,640
I use what's called Slack Variables.

23
00:00:56,640 --> 00:00:58,410
For those of you take
a machine learning class,

24
00:00:58,410 --> 00:01:01,770
you learn a whole lot more about
how SVMs and Slack Variables work.

25
00:01:01,770 --> 00:01:03,690
But basically what it does is it says,

26
00:01:03,690 --> 00:01:06,630
let me try to find
the maximum margin possible.

27
00:01:06,630 --> 00:01:07,280
Allowing for

28
00:01:07,280 --> 00:01:11,210
the fact that some of my points
are going to end up on the wrong side.

29
00:01:11,210 --> 00:01:12,010
Okay.

30
00:01:12,010 --> 00:01:13,340
So that, that's how you handle that.

31
00:01:13,340 --> 00:01:16,420
So even if you have this great
big long feature vector,

32
00:01:16,420 --> 00:01:18,760
if things aren't totally
linear separable, you're okay.

33
00:01:18,760 --> 00:01:21,830
So that's, when you have
a large input feature vector,

34
00:01:21,830 --> 00:01:24,620
often its the case that
people use linear SVMs.

35
00:01:24,620 --> 00:01:27,020
So heres another kernel.

36
00:01:27,020 --> 00:01:27,910
Okay.
That people use, and

37
00:01:27,910 --> 00:01:29,230
its very commonly used.

38
00:01:29,230 --> 00:01:33,950
And it's a Gaussian kernel, sometimes
called RBF, Radial Bases Function.

39
00:01:33,950 --> 00:01:39,670
So it's a kernel where just as,
as point moves away from

40
00:01:39,670 --> 00:01:44,343
the support vecrum as xj moves away from
xi it just falls off like a Gaussian.

41
00:01:44,343 --> 00:01:47,520
Just like a Gaussian bump.

42
00:01:47,520 --> 00:01:50,090
Now, you might wonder, I did.

43
00:01:51,450 --> 00:01:53,700
How is that exactly a kernel?

44
00:01:53,700 --> 00:01:58,515
Well, and, and also because
that means that there is some

45
00:01:58,515 --> 00:02:04,840
space in which this exponent,
exponential, is a docked product.

46
00:02:04,840 --> 00:02:06,140
Okay?

47
00:02:06,140 --> 00:02:09,288
And I realize I had learned this
before and I had forgotten it.

48
00:02:09,288 --> 00:02:14,270
That this dimensionality of that
space is essentially infinite.

49
00:02:14,270 --> 00:02:17,400
And there are a couple of ways of,
of thinking about this.

50
00:02:17,400 --> 00:02:21,880
But the one that's the simplest is
it turns out you can write that

51
00:02:21,880 --> 00:02:26,270
exponentiation or
that decay exponential this way.

52
00:02:26,270 --> 00:02:28,600
Okay.
So here is my exponential the sigma

53
00:02:28,600 --> 00:02:31,500
square rule, sigma square equals one.

54
00:02:31,500 --> 00:02:37,060
And notice this is just some particular
value, so you just, the magnitude of x.

55
00:02:37,060 --> 00:02:38,560
This is magnitude of x dot.

56
00:02:38,560 --> 00:02:41,020
So for some x and x dot, it's fixed.

57
00:02:41,020 --> 00:02:45,510
But notice that you're
taking dot products,

58
00:02:45,510 --> 00:02:48,320
and you're taking
an infinite sum of them.

59
00:02:48,320 --> 00:02:50,860
So it's the dot product,
raised to this j power.

60
00:02:50,860 --> 00:02:51,590
Let me remove that.

61
00:02:51,590 --> 00:02:52,190
Right?

62
00:02:52,190 --> 00:02:57,790
So, it's the sum of, and the these terms
come out because they're constant.

63
00:02:57,790 --> 00:03:01,660
It's the sum of this dot
product raised to the j power,

64
00:03:01,660 --> 00:03:03,640
j equal to 0 equal to infinity.

65
00:03:03,640 --> 00:03:08,950
Do you remember that e to the x was
an infinite exponential series?

66
00:03:08,950 --> 00:03:12,490
Right?
So, way back in Mrs. McGillicutty's you,

67
00:03:12,490 --> 00:03:14,550
maybe you didn't learn this in
McGillicutty's Algebra class.

68
00:03:14,550 --> 00:03:18,170
Maybe you learned it in Miss Thompson's
Calculus class about infinite series

69
00:03:18,170 --> 00:03:23,830
and, right, that an exponential, e to
the x squared, was x plus, x squared

70
00:03:23,830 --> 00:03:26,776
over two factorial plus x cubed over
three factorial, in an infinite sum.

71
00:03:26,776 --> 00:03:29,535
That's, see the factorial
here Here on the bottom.

72
00:03:29,535 --> 00:03:32,115
That's related to the,
to the sum that's going on.

73
00:03:32,115 --> 00:03:36,705
So the idea is that this Gaussian
kernel actually has infinite dimension

74
00:03:36,705 --> 00:03:40,435
in its lifting space, but you know what,
we don't actually have to do that.

75
00:03:40,435 --> 00:03:42,955
We can just use the Gaussian kernel.
