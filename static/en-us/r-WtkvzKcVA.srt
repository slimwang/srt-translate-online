1
00:00:00,068 --> 00:00:05,390
So, by just naively having 10,000 threads incrementing 10 array elements, we got the wrong answer.

2
00:00:05,390 --> 00:00:09,218
One solution might be to sprinkle barriers throughout the code that I just showed you.

3
00:00:09,218 --> 00:00:12,495
But another solution is something called atomic memory operations.

4
00:00:12,495 --> 00:00:16,040
Atomic memory operations, or atomics for short, are special

5
00:00:16,040 --> 00:00:19,758
instructions that the GPU implements specifically for this problem.

6
00:00:19,758 --> 00:00:21,627
And, there's a bunch of atomics.

7
00:00:21,627 --> 00:00:23,925
You can get the list in the programing guide, but some of

8
00:00:23,925 --> 00:00:30,131
the examples that you might use are atomic add, atomic min, atomic xor, and so forth.

9
00:00:30,131 --> 00:00:38,106
A particularly interesting one is called atomic CAS, which stands for compare and swap.

10
00:00:38,106 --> 00:00:41,639
Let's look at how these work. Okay, so here's our code again.

11
00:00:41,639 --> 00:00:46,983
And this time, instead of calling it increment naive, let's call a new function increment atomic.

12
00:00:46,983 --> 00:00:49,036
This kernel is almost exactly the same.

13
00:00:49,036 --> 00:00:51,663
Every thread computes what thread it is,

14
00:00:51,663 --> 00:00:55,651
and then it mods that by the size of the array to figure out where it's going to write.

15
00:00:55,651 --> 00:01:00,665
But the difference is that, whereas before we were just doing a standard c operation--

16
00:01:00,665 --> 00:01:05,963
g sub i equals g sub i plus 1, which has 1 read, a modify, and a write,

17
00:01:05,963 --> 00:01:09,213
here we're going to use the atomic add operation.

18
00:01:09,213 --> 00:01:14,195
This atomic ad function is a CUDA built in that's going to take a pointer--

19
00:01:14,195 --> 00:01:18,165
which should be to somewhere in device memory, shared or global memory--

20
00:01:18,165 --> 00:01:21,969
and an amount to add, a value to add, okay?

21
00:01:21,969 --> 00:01:24,222
So, this has the same effect.

22
00:01:24,222 --> 00:01:30,745
Every thread is going to go and add 1 to the value at g sub i, but the difference is

23
00:01:30,745 --> 00:01:35,227
that this is using special hardware that the GPU has built in to perform atomic operations.

24
00:01:35,227 --> 00:01:42,092
So now we're guaranteed that only one thread at a time can do this read modify write operation.

25
00:01:42,092 --> 00:01:48,613
So now we'll scroll down, change our code, call increment atomic--

26
00:01:48,613 --> 00:01:52,868
that's the only change we'll make--compile the code again, and run it.

27
00:01:52,868 --> 00:01:55,867
So this looks more like it.

28
00:01:55,867 --> 00:02:02,495
Now we're writing a million total threads and a thousand blocks writing into 10 array elements.

29
00:02:02,495 --> 00:02:08,132
And this time, as we expect, every array element contains the number 100,000 when all is done.

30
00:02:08,132 --> 00:02:11,650
So the atomic operation prevented these collisions where

31
00:02:11,650 --> 00:02:16,104
multiple threads were trying to read and write from the same memory location at the same time,

32
00:02:16,104 --> 00:02:19,248
and instead serialized them, making sure that only one thread

33
00:02:19,248 --> 00:02:22,876
at a time has access during the course of that read modify write--

34
00:02:22,876 --> 00:02:26,915
that add operation--and we get the right result.

35
00:02:26,915 --> 00:02:29,968
Notice that took longer. Before it took about 3 milliseconds.

36
00:02:29,968 --> 00:02:34,800
Now we're taking about 3.7 milliseconds. So atomic operations come with a cost,

37
00:02:34,800 --> 00:02:37,336
and that's something I'm going to go into.

38
00:02:37,336 --> 00:02:40,322
Let's run this a few more times to make sure that we get same result.

39
00:02:40,322 --> 00:02:46,672
That time it took about 4 milliseconds, closer to 4 again, down around 3, and so forth.
