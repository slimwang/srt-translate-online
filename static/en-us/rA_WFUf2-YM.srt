1
00:00:00,320 --> 00:00:02,910
Okay, Michael. So, I just gave you, you know, one

2
00:00:02,910 --> 00:00:04,680
particular way you might do filtering. But let's see if

3
00:00:04,680 --> 00:00:06,140
we can expand on that a little bit and come

4
00:00:06,140 --> 00:00:08,410
up with some similar ways we might do wrapping. Okay?

5
00:00:08,410 --> 00:00:09,310
>> Sure.

6
00:00:09,310 --> 00:00:12,410
>> Okay so, I just gave an example of

7
00:00:12,410 --> 00:00:15,470
a kind of filtering you might do through decision trees.

8
00:00:15,470 --> 00:00:18,470
But really what I'm doing there is I'm defining

9
00:00:18,470 --> 00:00:23,460
some criterion. And in this case it was information gain.

10
00:00:23,460 --> 00:00:23,940
>> Right.

11
00:00:23,940 --> 00:00:25,390
>> This is the way that I will, this is

12
00:00:25,390 --> 00:00:29,150
the function that I will use to evaluate the usefulness

13
00:00:29,150 --> 00:00:31,450
of a subset of features. How much information does that

14
00:00:31,450 --> 00:00:33,980
feature give me about a class label? Can you think

15
00:00:33,980 --> 00:00:36,590
of any other kind of filtering criteria we might use?

16
00:00:36,590 --> 00:00:41,950
>> So, I don't know, variance. Like so, I want features that have

17
00:00:41,950 --> 00:00:43,590
lots of different values. That'll do

18
00:00:43,590 --> 00:00:45,480
something similar to information gain, I suspect.

19
00:00:45,480 --> 00:00:50,080
>> M'kay. Yep. I like that one. So, we might call that variance.

20
00:00:51,630 --> 00:00:55,190
Or actually another version of that would be entropy. And, there's tons of

21
00:00:55,190 --> 00:00:58,640
ways to do this. Right, so, there's, there's something called the G index which

22
00:00:58,640 --> 00:01:02,120
is a kind of diversion of entropy. There's variance, as you telling me

23
00:01:02,120 --> 00:01:07,590
here that you're trying to pick features that show up a lot. Anything else?

24
00:01:07,590 --> 00:01:10,030
>> I could imagine running a neural net and

25
00:01:10,030 --> 00:01:12,710
then pruning away any input features that have low weight.

26
00:01:12,710 --> 00:01:16,910
>> Okay. How would we describe that? That's that's

27
00:01:16,910 --> 00:01:19,390
actually a kind of information gain too right?

28
00:01:19,390 --> 00:01:21,580
Because if the neural network doesn't need it, then

29
00:01:21,580 --> 00:01:24,630
somehow it doesn't seem to be terribly useful.

30
00:01:24,630 --> 00:01:25,870
In fact why don't we just write that down.

31
00:01:25,870 --> 00:01:26,610
>> Oh, I have another one.

32
00:01:26,610 --> 00:01:27,990
>> Mm-hm.

33
00:01:27,990 --> 00:01:31,490
>> So, if there's features that are linearly dependent

34
00:01:31,490 --> 00:01:34,410
on other features then maybe get rid of them.

35
00:01:34,410 --> 00:01:37,660
>> So you want independent features?

36
00:01:37,660 --> 00:01:38,090
>> Yeah.

37
00:01:38,090 --> 00:01:41,183
>> Or maybe we'll say non-redundant.

38
00:01:41,183 --> 00:01:41,820
>> Oh, but in this,

39
00:01:41,820 --> 00:01:43,660
in this kind of of linear algebraic sense.

40
00:01:43,660 --> 00:01:48,630
>> Right. So if I have feature. Let's say x2

41
00:01:48,630 --> 00:01:53,550
which turns out to be equal to, you know, x1 plus

42
00:01:53,550 --> 00:01:59,200
x3 then I should probably get rid of x2. Is that what you're suggesting?

43
00:01:59,200 --> 00:02:01,020
>> Sure, yeah, that's that is what I'm saying.

44
00:02:01,020 --> 00:02:03,830
>> Okay, and that makes sense because I don't need x2.

45
00:02:03,830 --> 00:02:06,880
>> Well, in principle it gives, yeah as you

46
00:02:06,880 --> 00:02:11,170
said redundant information. But it also is the case that, for some

47
00:02:11,170 --> 00:02:14,740
learners like neural nets, it shouldn't really,

48
00:02:14,740 --> 00:02:16,320
you'll feel the difference. For things

49
00:02:16,320 --> 00:02:19,740
like decision trees, it might matter though because having it, having those

50
00:02:19,740 --> 00:02:23,250
sums together in one place could be more useful to split on.

51
00:02:23,250 --> 00:02:25,190
>> That's true, but I do think you said

52
00:02:25,190 --> 00:02:27,070
something important there, which, you said it shouldn't feel

53
00:02:27,070 --> 00:02:29,790
the difference. You're defining whether this is good or

54
00:02:29,790 --> 00:02:32,380
bad in terms of whether it helps you to learn.

55
00:02:32,380 --> 00:02:36,430
But if we can get rid of a feature, then we save, as

56
00:02:36,430 --> 00:02:39,410
we get rid of features, we save exponentially on the need for data.

57
00:02:39,410 --> 00:02:40,840
>> Yes. So as long as it doesn't

58
00:02:40,840 --> 00:02:42,640
make the learning problem harder, that's a good thing.

59
00:02:42,640 --> 00:02:46,270
>> Right. So long as it makes the learning problem easier, in terms of the

60
00:02:46,270 --> 00:02:49,800
amount of data you need, it's a win, up until the point where it makes

61
00:02:49,800 --> 00:02:51,910
the learning problem harder because you don't

62
00:02:51,910 --> 00:02:54,200
have enough features to do the actual learning.

63
00:02:54,200 --> 00:02:55,620
And there's a bunch of these. And I

64
00:02:55,620 --> 00:02:57,860
think all of these are probably good answers.

65
00:02:57,860 --> 00:03:00,490
You could think of all kinds of statistical tests or

66
00:03:00,490 --> 00:03:03,130
relevance. I mean basically are built in here these are

67
00:03:03,130 --> 00:03:05,660
all different ways of doing the same thing which is

68
00:03:05,660 --> 00:03:08,370
figuring out which features you need in order to do learning

69
00:03:08,370 --> 00:03:11,910
by some generic criteria. I do want to point out

70
00:03:11,910 --> 00:03:15,560
that something like Entropy doesn't even depend on the labels.

71
00:03:15,560 --> 00:03:17,860
Where something like Information Gain which is a kind of

72
00:03:17,860 --> 00:03:22,800
conditional Entropy, doe's depend upon knowing the labels. What about wrapping?

73
00:03:22,800 --> 00:03:24,410
How would you go about doing the wrapping? So we

74
00:03:24,410 --> 00:03:27,260
got a bunch of different ways we might do filtering. What

75
00:03:27,260 --> 00:03:30,240
are sort of the way in which you could imagine doing

76
00:03:30,240 --> 00:03:32,650
a wrapping? You got some learning, don't know what it is.

77
00:03:32,650 --> 00:03:35,390
You got a bunch of features you want to search over those,

78
00:03:35,390 --> 00:03:37,540
pass those features on to the learner and see how well

79
00:03:37,540 --> 00:03:40,530
it does. But you don't want to pay an exponential cost,

80
00:03:40,530 --> 00:03:43,700
by looking at all possible subsets. What might you imagine doing?

81
00:03:43,700 --> 00:03:47,990
>> Well, so, I mean we've already talked about a bunch of algorithms that can

82
00:03:47,990 --> 00:03:53,370
search, that aren't necessarily exponential. So various kinds of local search

83
00:03:53,370 --> 00:03:56,390
and" hill climbing" could be a way of doing the search part.

84
00:03:56,390 --> 00:03:59,100
>> Hmm, I like that. Let's write that down, "Hill Climbing". Now when you

85
00:03:59,100 --> 00:04:00,280
say Hill Climbing, you really mean like

86
00:04:00,280 --> 00:04:02,630
a gradient, a regular deterministic gradient search?

87
00:04:02,630 --> 00:04:05,720
>> Hmmmmmmmmm, yeah. I mean, I guess I was thinking

88
00:04:05,720 --> 00:04:07,840
of something where what does the search algorithm does is

89
00:04:07,840 --> 00:04:10,420
it tells the learning algorithm here's the features to try

90
00:04:10,420 --> 00:04:13,110
to use. The learning algorithm runs and it comes back with

91
00:04:13,110 --> 00:04:16,160
some kind of error. Presumably, on how that validate

92
00:04:16,160 --> 00:04:19,310
data otherwise, we run the risk of over fitting.

93
00:04:19,310 --> 00:04:20,060
>> Fair.

94
00:04:20,060 --> 00:04:22,760
>> And it gives a number back and then the, the

95
00:04:22,760 --> 00:04:26,650
search algorithm uses that to decide what subset to try next.

96
00:04:26,650 --> 00:04:29,720
Okay so you imagine that it's going to jump in some gradient

97
00:04:29,720 --> 00:04:33,230
directly, or is it going to have to do something like randomize optimization.

98
00:04:33,230 --> 00:04:36,500
>> I think it would have to, yeah I mean like, randomized optimization.

99
00:04:36,500 --> 00:04:38,200
>> Okay so, in general we can

100
00:04:38,200 --> 00:04:41,430
take almost everything we just did,

101
00:04:41,430 --> 00:04:43,390
and the last lesson on randomized optimization

102
00:04:43,390 --> 00:04:47,020
and apply it here, so then we might just say. Just do random.

103
00:04:48,690 --> 00:04:52,080
>> Oh, I see. So could be, could be mimic.

104
00:04:52,080 --> 00:04:56,610
It could be, genetic algorithms. It could be simulated annealing length.

105
00:04:56,610 --> 00:04:59,370
>> Right. So you can think of, you can do randomized optimization.

106
00:04:59,370 --> 00:05:00,780
And that's going to be important precisely

107
00:05:00,780 --> 00:05:03,940
because we want to solve, this exponential problem.

108
00:05:03,940 --> 00:05:07,980
We don't want that to be an issue. Okay that makes sense. Now

109
00:05:07,980 --> 00:05:09,620
we've already thrown out exhaustive search

110
00:05:09,620 --> 00:05:11,100
where we look at all possible features

111
00:05:11,100 --> 00:05:14,400
because, again, that's exponential. Is there anything else you can think of? I'm

112
00:05:14,400 --> 00:05:16,040
thinking of one more, so let's see if you can read my mind.

113
00:05:16,040 --> 00:05:18,639
>> [LAUGH]

114
00:05:18,639 --> 00:05:20,710
>> And really, when I say I'm thinking of one more, I'm thinking of two more.

115
00:05:21,720 --> 00:05:25,130
>> Oh. That's, that are not randomized

116
00:05:25,130 --> 00:05:28,305
optimization algorithms, but they're not going to somehow search.

117
00:05:28,305 --> 00:05:29,600
>> Mm-hm.

118
00:05:30,895 --> 00:05:31,980
>> A star?

119
00:05:31,980 --> 00:05:33,020
>> Yeah, but that's not what I was thinking.

120
00:05:33,020 --> 00:05:34,820
>> Oh. I give up.

121
00:05:34,820 --> 00:05:39,079
>> I'll give you a hint. You could go forward or you could go backward.

122
00:05:40,210 --> 00:05:40,670
>> EM.

123
00:05:40,670 --> 00:05:45,810
>> [LAUGH] No. No. No. No. Although I like that.

124
00:05:45,810 --> 00:05:46,050
>> ME?

125
00:05:46,050 --> 00:05:49,730
>> [LAUGH] That wouldn't be forward or backward, would there? No,

126
00:05:49,730 --> 00:05:53,430
I'm thinking of exactly a forward sequential selection and backward elimination.

127
00:05:53,430 --> 00:05:55,700
>> I don't think I know what that means.

128
00:05:55,700 --> 00:05:56,540
>> Oh. So

129
00:05:56,540 --> 00:05:57,450
let's talk about that then.
