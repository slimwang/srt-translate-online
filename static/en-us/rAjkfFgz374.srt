1
00:00:00,360 --> 00:00:02,056
All right, that concludes the,

2
00:00:02,056 --> 00:00:06,220
the boosting discussion and
also the context of face detection.

3
00:00:06,220 --> 00:00:09,840
Like I said, it's rare that a single
paper can influence a field so much,

4
00:00:09,840 --> 00:00:10,830
like this one did.

5
00:00:10,830 --> 00:00:11,530
But but it did.

6
00:00:11,530 --> 00:00:12,840
And not only influenced the field,

7
00:00:12,840 --> 00:00:16,390
it influenced the people sort of
learning and thinking about the field.

8
00:00:16,390 --> 00:00:19,510
The representation,
the classification method and

9
00:00:19,510 --> 00:00:21,850
then the whole cascade, it just worked.

10
00:00:21,850 --> 00:00:23,130
It just worked really, really well.

11
00:00:24,590 --> 00:00:28,750
And, as I said, unlike SVM methods,
which we'll learn about lex, next time,

12
00:00:28,750 --> 00:00:31,700
you can actually just
write the code yourself,

13
00:00:31,700 --> 00:00:33,570
both boosting and random forest.

14
00:00:33,570 --> 00:00:38,210
By the way, random forests are sort
of related because it's this idea of

15
00:00:38,210 --> 00:00:41,430
adding these weak eh,
this consensus of weak learners.

16
00:00:41,430 --> 00:00:43,280
So, for those of you who want to
think about random forests,

17
00:00:43,280 --> 00:00:46,440
because you know about them, you can
think about them in that context.

18
00:00:46,440 --> 00:00:47,300
These boosting algorithms,

19
00:00:47,300 --> 00:00:51,130
these consensus algorithms, are just
much simpler to implement yourself.

20
00:00:51,130 --> 00:00:53,580
And therefore they sort of
become fun to play with.

21
00:00:53,580 --> 00:00:56,370
Next time we'll talk about support,
support vector machines,

22
00:00:56,370 --> 00:00:59,680
which are not fun to implement,
but just work really, really well.
