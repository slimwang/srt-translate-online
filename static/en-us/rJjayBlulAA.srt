1
00:00:00,200 --> 00:00:03,410
Okay, so in the general mathematical sense that's how we

2
00:00:03,410 --> 00:00:07,010
would optimize the parameters on our model theta by using

3
00:00:07,010 --> 00:00:09,850
batch gradient descent to minimize our cost function, j of

4
00:00:09,850 --> 00:00:13,830
theta. Below is an implementation of linear regression and gradient descent

5
00:00:13,830 --> 00:00:16,190
to try and determine how many lifetime home runs a

6
00:00:16,190 --> 00:00:19,520
player in a baseball data set will hit. Given their height

7
00:00:19,520 --> 00:00:22,410
and weight. I'd encourage you to really look through all

8
00:00:22,410 --> 00:00:25,280
of this code and make sure you understand how it works.

9
00:00:25,280 --> 00:00:26,860
But for now, I'd like you to write some code

10
00:00:26,860 --> 00:00:29,220
here that updates the values of theta a number of

11
00:00:29,220 --> 00:00:33,770
times equal to num_iterations. You should initialize an array called

12
00:00:33,770 --> 00:00:36,640
the cost_history, and every time you've computed the cost for a

13
00:00:36,640 --> 00:00:39,170
given set of thetas You should append it to cost

14
00:00:39,170 --> 00:00:42,980
history. The function should return both the final values of theta

15
00:00:42,980 --> 00:00:46,930
in your theta array, and cost history array. Your code

16
00:00:46,930 --> 00:00:50,290
should go here. Feel free to use other helper functions in

17
00:00:50,290 --> 00:00:52,130
this code, such as compute cost.
