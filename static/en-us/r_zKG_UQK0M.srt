1
00:00:00,190 --> 00:00:01,560
So let me show you an example.

2
00:00:01,560 --> 00:00:04,620
This example comes from Andrew Moore and
it's a really nice example.

3
00:00:04,620 --> 00:00:08,109
Suppose my vector x is just
two-dimensional to start with, and

4
00:00:08,109 --> 00:00:10,680
I, I need to be able to say
exactly how many there are, so

5
00:00:10,680 --> 00:00:13,060
I can show you the whole
thing written out.

6
00:00:13,060 --> 00:00:13,920
Okay?

7
00:00:13,920 --> 00:00:17,000
Let me define my kernel
function as this.

8
00:00:17,000 --> 00:00:20,300
Okay?
So my kernel function is just

9
00:00:20,300 --> 00:00:23,870
xi transpose xj, 1 plus that squared.

10
00:00:23,870 --> 00:00:24,610
Okay?

11
00:00:24,610 --> 00:00:29,263
This is referred to as a polynomial
kernel, because I take 1 plus the dot

12
00:00:29,263 --> 00:00:34,170
product of xi and xj and I raise it to
a power squared, so this is a quadratic.

13
00:00:35,405 --> 00:00:37,690
Let's write out what that actually is,
okay?

14
00:00:37,690 --> 00:00:43,480
So, we need to show that this function,
in order for it to be a kernel that we

15
00:00:43,480 --> 00:00:49,950
can use for the SVNs, it has to be a dot
product in some high dimensional space.

16
00:00:49,950 --> 00:00:52,670
I don't actually care
what that space is, but

17
00:00:52,670 --> 00:00:55,060
it must be a dot product in some space.

18
00:00:55,060 --> 00:01:00,290
So, just for this, th, demonstration
we'll actually show what that space is,

19
00:01:00,290 --> 00:01:03,660
but in general we, we don't, we tend
not to worry about that, all right.

20
00:01:03,660 --> 00:01:09,330
So, here I just wrote out K, so
it's one plus xi transpose xj squared.

21
00:01:09,330 --> 00:01:12,305
Multiply this whole thing out, okay, so

22
00:01:12,305 --> 00:01:17,320
just using the fact that x is only
two-dimensional and it's x1 x2.

23
00:01:17,320 --> 00:01:20,320
You get this, you know,
big, ugly expression.

24
00:01:20,320 --> 00:01:22,260
And that's only squared.

25
00:01:22,260 --> 00:01:25,470
Imagine if it's raised to
a higher power or imagine if x,

26
00:01:25,470 --> 00:01:30,130
instead of being just x1 x2 was,
you know, x1 through x5, right?

27
00:01:30,130 --> 00:01:32,730
You get all of these terms, all right?

28
00:01:32,730 --> 00:01:37,670
Here, we only have, one, two, three,
four, five, six, six terms, okay,

29
00:01:37,670 --> 00:01:38,880
because it's squared.

30
00:01:38,880 --> 00:01:43,450
But that expression can be
written as the following, okay?

31
00:01:43,450 --> 00:01:47,040
And you have to, sort of work
this out for yourself, right.

32
00:01:47,040 --> 00:01:52,790
So let's just take a look at this term,
okay, well, 1 times 1 is what?

33
00:01:52,790 --> 00:01:53,510
One, very good.

34
00:01:54,510 --> 00:01:58,010
So here I have x i squared, x j squared.

35
00:01:58,010 --> 00:02:01,650
Well, if I put xi squared and
xj squared here.

36
00:02:01,650 --> 00:02:09,350
xi squared times xi1 squared times xj1
squared is xi1 squared xj1 squared.

37
00:02:09,350 --> 00:02:11,900
This sounds like a Dr. Seuss thing okay.

38
00:02:11,900 --> 00:02:15,500
What's going on here is now let's
take a look at this term here.

39
00:02:15,500 --> 00:02:16,500
Right?

40
00:02:16,500 --> 00:02:20,460
These two, times each other,
is that, and so on.

41
00:02:21,670 --> 00:02:26,900
Basically, this big expression is just

42
00:02:26,900 --> 00:02:32,500
this term,
which is made out of the xi components,

43
00:02:32,500 --> 00:02:37,620
dotted with, transpose, the same
term made out of the xj components.

44
00:02:38,800 --> 00:02:42,000
Okay?
So the way of thinking about that is

45
00:02:42,000 --> 00:02:49,410
that's a dot product
between the phi of xi and

46
00:02:49,410 --> 00:02:54,780
phi of xj,
where phi of x is just defined by this.

47
00:02:54,780 --> 00:03:00,030
So here the idea is, we went from being
a two dimensional space to being a one,

48
00:03:00,030 --> 00:03:02,460
two, three, four, five,
six dimensional space.

49
00:03:03,620 --> 00:03:08,170
Now, we don't actually
need to know that space.

50
00:03:08,170 --> 00:03:12,610
We're just going to use the Kernel
function in our SVM and

51
00:03:12,610 --> 00:03:16,280
the way we're going to do that, is we're
going to use that, that kernel trick.

52
00:03:16,280 --> 00:03:21,480
As, so instead of computing explicitly,
that high dimensional space,

53
00:03:21,480 --> 00:03:23,960
right, we just define
some kernel function,

54
00:03:25,180 --> 00:03:29,330
which we know is a dot product
in a high dimensional space, but

55
00:03:29,330 --> 00:03:33,030
we don't actually even look at it,
we just know that it is.

56
00:03:33,030 --> 00:03:38,670
In which case, we convert our xi.x for

57
00:03:38,670 --> 00:03:43,580
testing our new vector x
to just the kernel of that.

58
00:03:43,580 --> 00:03:44,370
All right?

59
00:03:44,370 --> 00:03:48,290
And so we don't actually have to do
the thing in the high dimensional space,

60
00:03:48,290 --> 00:03:51,330
we just compute the kernel in
the lower dimensional space.
