1
00:00:00,000 --> 00:00:02,000
Now I want to do a little quiz,

2
00:00:02,000 --> 00:00:04,000
and ask you: True or False,

3
00:00:04,000 --> 00:00:07,000
which of the following are possible

4
00:00:07,000 --> 00:00:09,000
weaknesses in this TD learning

5
00:00:09,000 --> 00:00:12,000
with a passive approach to reinforcement learning?

6
00:00:12,000 --> 00:00:15,000
One: Is it possible that we would have

7
00:00:15,000 --> 00:00:17,000
a long convergence time--

8
00:00:17,000 --> 00:00:21,000
that it might take a long time to converge to the correct utility values?

9
00:00:21,000 --> 00:00:26,000
Secondly, are we limited by the policy that we choose?

10
00:00:26,000 --> 00:00:28,000
So remember: in passive reinforcement learning,

11
00:00:28,000 --> 00:00:30,000
we choose a fixed policy

12
00:00:30,000 --> 00:00:32,000
and execute that policy;

13
00:00:32,000 --> 00:00:36,000
and any deviance from the policy

14
00:00:36,000 --> 00:00:38,000
results from the stochasticity.

15
00:00:38,000 --> 00:00:40,000
We may visit different squares

16
00:00:40,000 --> 00:00:42,000
because the environment is stochastic,

17
00:00:42,000 --> 00:00:44,000
but not because we made different choices.

18
00:00:44,000 --> 00:00:47,000
So there's that elementation.

19
00:00:47,000 --> 00:00:50,000
Third, can there be a problem with missing states?

20
00:00:50,000 --> 00:00:53,000
That is, could there be some states that have

21
00:00:53,000 --> 00:00:56,000
a zero count--that we never visited,

22
00:00:56,000 --> 00:00:58,000
and never got a utility estimate?

23
00:00:58,000 --> 00:01:03,000
And fourth, could there be a problem with a poor estimate for certain states?

24
00:01:03,000 --> 00:01:08,000
So could it be that, even though a state didn't have a count of zero,

25
00:01:08,000 --> 99:59:59,999
it had a low count, and we weren't able to get a good utility estimate for that state?
