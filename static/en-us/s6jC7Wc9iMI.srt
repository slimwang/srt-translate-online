1
00:00:00,330 --> 00:00:02,490
You've already seen
some of those tricks.

2
00:00:02,490 --> 00:00:06,100
I asked you to make your inputs zero
mean and equal variance earlier.

3
00:00:06,100 --> 00:00:07,440
It's very important for SGD.

4
00:00:07,440 --> 00:00:10,860
I also told you to initialize
with random weights

5
00:00:10,860 --> 00:00:14,120
that have relatively small variance,
same thing.

6
00:00:14,120 --> 00:00:17,090
I'm going to talk about a few more
of those important tricks, and

7
00:00:17,090 --> 00:00:21,100
that should cover all you really need
to worry about to implement SGD.

8
00:00:21,100 --> 00:00:23,460
The first one is momentum.

9
00:00:23,460 --> 00:00:29,060
Remember that at each step, we're taking
a very small step in a random direction,

10
00:00:29,060 --> 00:00:33,120
but that on aggregate, those steps take
us toward the minimum of the loss.

11
00:00:33,120 --> 00:00:35,810
We can take advantage of
the knowledge that we've accumulated

12
00:00:35,810 --> 00:00:38,750
from previous steps about
where we should be headed.

13
00:00:39,840 --> 00:00:43,530
A cheap way to do that is to keep
a running average of the gradients, and

14
00:00:43,530 --> 00:00:46,770
to use that running average instead
of the direction of the current

15
00:00:46,770 --> 00:00:48,340
batch of the data.

16
00:00:48,340 --> 00:00:51,990
This momentum technique works very well
and often leads to better convergence.

17
00:00:53,170 --> 00:00:55,610
The second one is learning rate decay.

18
00:00:55,610 --> 00:01:00,110
Remember, when replacing gradient
descent with SGD, I said that we were

19
00:01:00,110 --> 00:01:04,420
going to take smaller,
noisier steps towards our objective.

20
00:01:04,420 --> 00:01:06,590
How small should that step be?

21
00:01:06,590 --> 00:01:08,150
That's a whole area of research as well.

22
00:01:09,260 --> 00:01:11,420
One thing that's always the case,
however,

23
00:01:11,420 --> 00:01:15,980
is that it's beneficial to make that
step smaller and smaller as you train.

24
00:01:15,980 --> 00:01:19,850
Some like to apply an exponential decay
to the learning rate some like to make

25
00:01:19,850 --> 00:01:23,890
it smaller every time the loss reaches
a plateau, there are lots of ways to go

26
00:01:23,890 --> 00:01:27,700
about it, but lowering it over
time is the key thing to remember.
