1
00:00:00,090 --> 00:00:01,859
Okay Michael so I think with that little

2
00:00:01,859 --> 00:00:05,450
bit of discussion, I feel like we're done.

3
00:00:05,450 --> 00:00:08,100
>> Cool! Alright. Well it's been nice talking to you. I hope

4
00:00:08,100 --> 00:00:11,120
the course went well and, oh you mean just for this lesson?

5
00:00:11,120 --> 00:00:12,330
>> Yeah just for this lesson.

6
00:00:12,330 --> 00:00:12,670
>> Alright.

7
00:00:12,670 --> 00:00:12,850
>> So let's.

8
00:00:12,850 --> 00:00:13,940
>> Well let's wrap up this lesson then.

9
00:00:13,940 --> 00:00:15,460
>> Yeah let's see what have we learned?

10
00:00:15,460 --> 00:00:17,640
So remind me Michael what have we learned?

11
00:00:17,640 --> 00:00:20,090
>> Well we were talking about instance based learning.

12
00:00:20,090 --> 00:00:23,330
>> That's true. That's the first thing we learned. You will notice

13
00:00:23,330 --> 00:00:25,480
by the way, I never actually told you why it was called instance

14
00:00:25,480 --> 00:00:25,905
based learning.

15
00:00:25,905 --> 00:00:28,520
>> Charles, why is it called instance based learning?

16
00:00:28,520 --> 00:00:30,690
>> I don't know but I am willing to guess that it has to do with a

17
00:00:30,690 --> 00:00:33,520
fact that we look at the exact instances that

18
00:00:33,520 --> 00:00:35,590
we have and we base our learning on that.

19
00:00:35,590 --> 00:00:40,030
>> Alright and we brought it up in

20
00:00:40,030 --> 00:00:43,160
by starting off thinking about eager and lazy learning.

21
00:00:43,160 --> 00:00:45,950
>> Right. What is the difference, Michael?

22
00:00:45,950 --> 00:00:49,770
>> I will tell you when I need to tell you.

23
00:00:49,770 --> 00:00:51,230
>> [Laugh]

24
00:00:51,230 --> 00:00:52,210
That's exactly right.

25
00:00:52,210 --> 00:00:55,610
>> So lazy learning is about putting off the work

26
00:00:55,610 --> 00:00:58,600
until it's actually needed. Eager is about, as soon as

27
00:00:58,600 --> 00:01:01,750
the problem is posed, solve it. And then, you know,

28
00:01:01,750 --> 00:01:05,790
if you're lucky, the answer will eventually come in handy.

29
00:01:05,790 --> 00:01:07,750
>> Exactly right. Okay what else?

30
00:01:07,750 --> 00:01:11,430
>> So as a concrete example of a lazy

31
00:01:11,430 --> 00:01:15,617
learner we talked about k-nearest neighbor or k-NN.

32
00:01:15,617 --> 00:01:17,930
>> kNN. And

33
00:01:17,930 --> 00:01:20,750
this whole notion of nearest neighbor. Is in

34
00:01:20,750 --> 00:01:26,390
fact one way of talking about similarity functions.

35
00:01:26,390 --> 00:01:31,240
>> Right and the similarity functions play a really central role in all this.

36
00:01:31,240 --> 00:01:33,450
>> Right. Similarity. We talk about it as

37
00:01:33,450 --> 00:01:35,850
if they were distance functions. But distance is

38
00:01:35,850 --> 00:01:39,870
just another way of talking about, a similarity.

39
00:01:39,870 --> 00:01:42,770
So, this is actually keeping. K-nn was a specific,

40
00:01:42,770 --> 00:01:45,290
algorithm we used, and we talked about various versions of it,

41
00:01:45,290 --> 00:01:48,050
and, the nearest neighbor part really got us to think a

42
00:01:48,050 --> 00:01:51,110
little bit about similarity and distance and, and what all that

43
00:01:51,110 --> 00:01:56,600
means. A really important thing here is, I think, that similarity is

44
00:01:56,600 --> 00:02:00,395
just another way of capturing domain knowledge. And K in

45
00:02:00,395 --> 00:02:02,780
k-NN is another way of capturing domain knowledge. And that

46
00:02:02,780 --> 00:02:05,540
if we saw through the quizes and some of our discussions,

47
00:02:05,540 --> 00:02:08,479
that this is actually very, very important. That domain knowledge matters.

48
00:02:10,090 --> 00:02:13,950
Now, we also talked about KNN in the context of both regression and

49
00:02:13,950 --> 00:02:19,880
classification. I see what you did there, 'Knnowledge', it's got KNN in it.

50
00:02:19,880 --> 00:02:24,270
>> >Okay, so yeah, classification and regression are different things. But,

51
00:02:24,270 --> 00:02:26,410
KNN can handle both of them. And, at the end of the

52
00:02:26,410 --> 00:02:30,710
day, that's all stuck in our notion of similarity, and our

53
00:02:30,710 --> 00:02:36,410
notion of averaging. Which we kind of took as an overall term,

54
00:02:36,410 --> 00:02:40,000
for a bunch of different things you might do, which some people find confusing.

55
00:02:40,000 --> 00:02:40,322
>> [LAUGH].

56
00:02:40,322 --> 00:02:41,950
>> But I think that other people

57
00:02:41,950 --> 00:02:43,390
would really kind of understand what we mean.

58
00:02:43,390 --> 00:02:45,240
>> I'm very tempted to take that out of

59
00:02:45,240 --> 00:02:47,770
Wikipedia, but that would, that would just be rude.

60
00:02:47,770 --> 00:02:51,960
>> It would be rude. Anything else? We learned one big thing.

61
00:02:51,960 --> 00:02:55,570
>> We looked at how to compose different learning algorithms

62
00:02:55,570 --> 00:02:59,120
together. For example in the context of locally weighted linear regression.

63
00:02:59,120 --> 00:02:59,268
>> Mm hm.

64
00:02:59,268 --> 00:03:01,680
>> We used this instance based idea

65
00:03:01,680 --> 00:03:05,010
along with linear regression to get something

66
00:03:05,010 --> 00:03:08,620
that was both locally smooth but globally bumpy.

67
00:03:08,620 --> 00:03:11,320
>> Right. So, I'm going to just say

68
00:03:11,320 --> 00:03:13,970
locally weighted regression. Where we can do any kind

69
00:03:13,970 --> 00:03:16,590
of regression we might want to. I was going to

70
00:03:16,590 --> 00:03:19,180
call that $X. So, stick in your favorite value.

71
00:03:20,510 --> 00:03:22,280
>> Let's see, what else? Oh, oh, a

72
00:03:22,280 --> 00:03:24,520
really big thing was Bellman's curse of dimensionality.

73
00:03:24,520 --> 00:03:25,480
>> Yes.

74
00:03:25,480 --> 00:03:27,120
>> And

75
00:03:27,120 --> 00:03:30,210
the idea there was that the more features that you

76
00:03:30,210 --> 00:03:34,030
include The more data you need to fill up that space.

77
00:03:34,030 --> 00:03:37,440
>> Yep, It's exponential.

78
00:03:37,440 --> 00:03:42,230
>> And in fact I even just I decided to go and play with this a little

79
00:03:42,230 --> 00:03:43,580
bit so that example that you were doing

80
00:03:43,580 --> 00:03:47,110
before where the y equals x1 squared plus x2

81
00:03:47,110 --> 00:03:48,450
>> Mm-hm.

82
00:03:48,450 --> 00:03:52,360
>> When I gave it well as we saw in the example we gave it like ten or 12

83
00:03:52,360 --> 00:03:56,700
examples and it did really badly. So it continued to do somewhat badly until

84
00:03:56,700 --> 00:03:59,800
I got to about 100,000 and then it was actually doing [LAUGH] really well.

85
00:03:59,800 --> 00:04:00,540
>> Hmm.

86
00:04:00,540 --> 00:04:02,140
>> But that seems like an awful lot of

87
00:04:02,140 --> 00:04:04,480
examples for what is otherwise a very simple problem.

88
00:04:04,480 --> 00:04:07,280
>> Right. Well if you think about it, the

89
00:04:07,280 --> 00:04:09,880
amount of data you have to see to determine

90
00:04:09,880 --> 00:04:13,680
the relative. Relevance of the two different dimensions is

91
00:04:13,680 --> 00:04:15,330
quite a bit in that particular kind of function.

92
00:04:15,330 --> 00:04:16,180
>> Hm.

93
00:04:16,180 --> 00:04:17,420
>> Yeah. That's a lot

94
00:04:17,420 --> 00:04:19,815
of space to cover. All possible real

95
00:04:19,815 --> 00:04:24,010
values [LAUGH] across a potentially infinite space.

96
00:04:24,010 --> 00:04:26,810
>> Yeah, I guess that's true.

97
00:04:26,810 --> 00:04:29,290
>> Yeah, so the cursor dimensionality is real and we

98
00:04:29,290 --> 00:04:32,670
just sort of can't get around it. Although As I mentioned

99
00:04:32,670 --> 00:04:36,000
earlier we will see in the second part of the

100
00:04:36,000 --> 00:04:39,250
course ways that people try to get around the curse of dimensionality.

101
00:04:39,250 --> 00:04:40,300
>> Ahah!

102
00:04:40,300 --> 00:04:42,120
>> Mm-hm. Okay.

103
00:04:42,120 --> 00:04:42,560
>> Or at least

104
00:04:42,560 --> 00:04:43,240
blunt it. Right?

105
00:04:43,240 --> 00:04:44,590
>> Yeah or at least blunt it because you can't

106
00:04:44,590 --> 00:04:47,820
actually get around the curse of dimensionality, you can only deal with it.

107
00:04:47,820 --> 00:04:50,870
>> There is no free lunch.

108
00:04:50,870 --> 00:04:52,570
>> There is no free lunch. In fact, that's a theorem.

109
00:04:52,570 --> 00:04:52,730
>> [LAUGH]

110
00:04:52,730 --> 00:04:53,850
>> Isn't that a theorem?

111
00:04:53,850 --> 00:04:54,480
>> Yeah, I think so.

112
00:04:54,480 --> 00:04:55,480
>> What's the theorem?

113
00:04:55,480 --> 00:05:01,650
>> No free lunch. That any learning algorithm that you create

114
00:05:01,650 --> 00:05:07,530
is going to have the property; that if you average over all possible instances,

115
00:05:07,530 --> 00:05:09,080
it's not doing any different than random.

116
00:05:09,080 --> 00:05:11,550
>> Right. And, and another way of thinking about

117
00:05:11,550 --> 00:05:13,480
that, a practical way of thinking about that is;

118
00:05:13,480 --> 00:05:15,070
if I don't know anything about the data that

119
00:05:15,070 --> 00:05:17,620
I'm going to have to learn over. Then, it doesn't

120
00:05:17,620 --> 00:05:20,050
really matter what I do because there's all possible

121
00:05:20,050 --> 00:05:22,320
kind of data sets. However, if I have domain

122
00:05:22,320 --> 00:05:26,030
knowledge, I can use that to choose the best

123
00:05:26,030 --> 00:05:28,690
learning algorithm for the problems that I'm going to encounter.

124
00:05:29,700 --> 00:05:32,820
>> So does that mean that, that all of machine learning really comes down to,

125
00:05:32,820 --> 00:05:34,790
you have to already know what you need to solve

126
00:05:34,790 --> 00:05:37,910
the problem to apply these. Techniques to solve the problem?

127
00:05:37,910 --> 00:05:39,930
>> No, but you have to know a little bit

128
00:05:39,930 --> 00:05:42,410
about your problem in order to decide what to do.

129
00:05:42,410 --> 00:05:44,020
And in fact, you could make the argument that this

130
00:05:44,020 --> 00:05:47,530
entire class is about exposing the students to a wide

131
00:05:47,530 --> 00:05:50,560
range of techniques and giving them enough practice so that

132
00:05:50,560 --> 00:05:52,740
they can do a pretty good job of telling, given

133
00:05:52,740 --> 00:05:54,890
a problem. Would it be better to use this kind

134
00:05:54,890 --> 00:05:57,900
of technique or this kind of technique? Is K-nn a sort of

135
00:05:57,900 --> 00:06:00,340
better way of approaching it? Decision tree a better way

136
00:06:00,340 --> 00:06:02,740
of approaching it? Um,It's a lot of what this class is

137
00:06:02,740 --> 00:06:05,840
about. Is helping them to get enough domain knowledge or enough

138
00:06:05,840 --> 00:06:08,510
knowledge anyways so that they can apply it to particular domains.

139
00:06:08,510 --> 00:06:11,730
>> Cool, so alright, that seems like a plenty useful lesson.

140
00:06:11,730 --> 00:06:14,770
>> Yes, it's a very hopeful note to end on, so let's end on that.

141
00:06:14,770 --> 00:06:16,650
>> Alright, see you next time.

142
00:06:16,650 --> 00:06:17,300
>> Alright, bye Michael.
