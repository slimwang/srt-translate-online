1
00:00:00,240 --> 00:00:02,290
Okay, Michael, so that gets us to the end of

2
00:00:02,290 --> 00:00:05,250
Support Vector Machines. So, let's recap. What have we learned?

3
00:00:05,250 --> 00:00:07,330
>> Well, we learned that support vector machine is

4
00:00:07,330 --> 00:00:11,160
not a command or a political statement. We talked about

5
00:00:11,160 --> 00:00:15,370
how a margin is a, is a useful concept in

6
00:00:15,370 --> 00:00:18,770
trying to understand how well a linear classifier might generalize.

7
00:00:18,770 --> 00:00:21,690
>> Okay, I like that. Lemme write that down. Margins are

8
00:00:21,690 --> 00:00:26,270
important. So we learned about margins In particular their relation to

9
00:00:26,270 --> 00:00:27,510
generalization and overfitting.

10
00:00:27,510 --> 00:00:28,220
>> Okay.

11
00:00:28,220 --> 00:00:32,689
>> In particular, we, we would like, given the choice,

12
00:00:32,689 --> 00:00:35,440
to find a linear separator that has the largest margin.

13
00:00:35,440 --> 00:00:36,210
>> Right, right.

14
00:00:36,210 --> 00:00:37,480
>> So maximizing margins.

15
00:00:37,480 --> 00:00:41,040
>> Right. At least when it comes to margins, bigger is better.

16
00:00:41,040 --> 00:00:46,850
>> Then we talked about how you could actually find. A linear separator

17
00:00:46,850 --> 00:00:51,170
that has maximum margin. I think we turned it into a quadratic program.

18
00:00:51,170 --> 00:00:53,220
>> Yes. We

19
00:00:53,220 --> 00:00:56,060
found an optimization problem for finding maximum margins

20
00:00:56,060 --> 00:00:58,400
and they turned out to be quadratic programming.

21
00:00:58,400 --> 00:01:00,900
>> And it was the dual of the quadratic

22
00:01:00,900 --> 00:01:05,190
program that showed us how, what the support vectors were.

23
00:01:05,190 --> 00:01:07,420
The support vectors were the points from the input

24
00:01:07,420 --> 00:01:12,130
data. That were necessary for defining that maximum margin separator.

25
00:01:12,130 --> 00:01:14,450
>> Right, right. So, we actually figured out what support vectors

26
00:01:14,450 --> 00:01:18,280
were. And then we tied all that in to instance based

27
00:01:18,280 --> 00:01:21,670
learning and other kind of ensemble methods. And so you could sort of think of

28
00:01:21,670 --> 00:01:24,160
support vector machines as being eager lazy

29
00:01:24,160 --> 00:01:26,860
learners. Or only as lazy as necessary to

30
00:01:26,860 --> 00:01:29,350
represent what you needed to represent. Because

31
00:01:29,350 --> 00:01:33,090
the, the classifier was based on just a

32
00:01:33,090 --> 00:01:35,210
subset of the data, or, or, or the raw

33
00:01:35,210 --> 00:01:37,240
data was being used for defining the classifier.

34
00:01:37,240 --> 00:01:39,050
>> Exactly right. Alright, so is there anything else?

35
00:01:39,050 --> 00:01:40,890
>> Oh. Oh. Right. Then there was the whole

36
00:01:40,890 --> 00:01:43,400
idea that, well, linear doesn't always seem like enough,

37
00:01:43,400 --> 00:01:47,900
but, we can project. Data into a higher dimension space and

38
00:01:47,900 --> 00:01:52,060
do the comparisons there. And that only made one little change

39
00:01:52,060 --> 00:01:54,790
to the algorithm. In particular, the dot product could be turned

40
00:01:54,790 --> 00:01:58,040
into some other similarity metric. And you called that the kernel trick.

41
00:01:58,040 --> 00:01:59,320
>> Right.

42
00:01:59,320 --> 00:02:02,190
>> love the kernel trick. And as you say,

43
00:02:02,190 --> 00:02:08,960
we took, basically, X transposed Y. And generalized it to

44
00:02:08,960 --> 00:02:14,270
a generic similarity function k of x comma y. And that actually

45
00:02:14,270 --> 00:02:17,190
ended up representing all of our domain knowledge, which will come up

46
00:02:17,190 --> 00:02:21,520
again and again throughout this course. What are the levers we have

47
00:02:21,520 --> 00:02:26,050
For expressing domain knowledge. Okay so,

48
00:02:26,050 --> 00:02:27,160
anything else for writing down Michiel?

49
00:02:27,160 --> 00:02:30,970
>> I think you said that kernels had to satisfy the merciful condition.

50
00:02:30,970 --> 00:02:33,400
>> No, no, no, mercer condition.

51
00:02:33,400 --> 00:02:33,625
>> Oh,

52
00:02:33,625 --> 00:02:35,230
okay, well that makes more sense.

53
00:02:35,230 --> 00:02:38,210
>> The mercer condition is interestingly quite merciful, because it's

54
00:02:38,210 --> 00:02:40,860
ok to use kernels that don't necessarily satisfy the mercer

55
00:02:40,860 --> 00:02:45,050
condition, often in practice it still works. Okay, well good,

56
00:02:45,050 --> 00:02:48,200
I think that is mostly it. So i guess we're done.

57
00:02:48,200 --> 00:02:50,210
>> Wait a second. Didn't you say that you were

58
00:02:50,210 --> 00:02:53,900
going to explain boosting to us? You kind of suggested maybe you'd

59
00:02:53,900 --> 00:02:56,290
do that during the boosting lecture. But then you said

60
00:02:56,290 --> 00:02:58,680
it would be during the SVM lecture. And now the SVM

61
00:02:58,680 --> 00:03:01,570
lecture's over and we still don't know why boosting doesn't over fit.

62
00:03:01,570 --> 00:03:03,850
>> Oh. That's a good point. That's a good point. I had forgotten about that.

63
00:03:03,850 --> 00:03:06,640
Thank you Micheal, this is why I keep you around, to remind me about stuff

64
00:03:06,640 --> 00:03:10,390
like this. Okay, let me take a moment then, just a moment. To see if

65
00:03:10,390 --> 00:03:13,410
we can tie together, over fitting and

66
00:03:13,410 --> 00:03:15,330
boosting about what we just learned about SVM's.
