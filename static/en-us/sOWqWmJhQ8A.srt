1
00:00:00,280 --> 00:00:02,090
>> Okay, so we agreed, Michael, that

2
00:00:02,090 --> 00:00:04,430
the problem's pretty hard. It's exponential in

3
00:00:04,430 --> 00:00:08,640
the number of features in this case N. And there are sort of two general

4
00:00:08,640 --> 00:00:11,280
approaches that people have used to try

5
00:00:11,280 --> 00:00:14,978
to attack this problem, broadly speaking. The first

6
00:00:14,978 --> 00:00:19,290
is called filtering, and the second is called wrapping, with a W, not an R.

7
00:00:19,290 --> 00:00:19,495
>> Hm.

8
00:00:19,495 --> 00:00:22,419
>> Well, it has an R too, but it starts with

9
00:00:22,419 --> 00:00:25,400
a W. And they actually look pretty similar. So I drew these

10
00:00:25,400 --> 00:00:27,690
little cartoons, which I hope would, would make it easier for

11
00:00:27,690 --> 00:00:30,720
you to see. So filtering basically works like this. You have a

12
00:00:30,720 --> 00:00:34,590
set of features as input. You run them through some algorithm

13
00:00:34,590 --> 00:00:38,670
that maximizes some kind of criteria. And then it outputs fewer features.

14
00:00:38,670 --> 00:00:39,450
>> Mm-hm.

15
00:00:39,450 --> 00:00:41,050
>> And passes those to some learning

16
00:00:41,050 --> 00:00:44,110
algorithm, that willl then use it for classification,

17
00:00:45,210 --> 00:00:48,480
say. It could be a regression, more generally,

18
00:00:48,480 --> 00:00:50,480
but you know, without loss of generality let's

19
00:00:50,480 --> 00:00:54,070
assume that we're worried about classification problems here. So

20
00:00:54,070 --> 00:00:56,260
you'll notice then that the criterion of how you know

21
00:00:56,260 --> 00:00:59,120
whether you're doing well or not is buried inside the

22
00:00:59,120 --> 00:01:02,360
search algorithm itself. By contrast, you could do what's called

23
00:01:02,360 --> 00:01:06,440
wrapping. And in the wrapping approach, you've taken your set

24
00:01:06,440 --> 00:01:08,830
of features, your N features, and you run them into

25
00:01:08,830 --> 00:01:11,700
this little box that basically searches over a subset of

26
00:01:11,700 --> 00:01:15,390
features, asks the learning algorithm to do something with them.

27
00:01:15,390 --> 00:01:17,340
The learning algorithm basically reports how well it

28
00:01:17,340 --> 00:01:20,240
does. And it uses that to update this new

29
00:01:20,240 --> 00:01:22,110
subset of features that it might look for, and

30
00:01:22,110 --> 00:01:24,620
passes to the algorithm. So, this is called filtering

31
00:01:24,620 --> 00:01:26,320
because this is a process that filters the

32
00:01:26,320 --> 00:01:28,210
features before handing it to a learning algorithm. And

33
00:01:28,210 --> 00:01:31,160
this is called wrapping because the search for the

34
00:01:31,160 --> 00:01:34,490
features is wrapped around whatever your learning algorithm is.

35
00:01:34,490 --> 00:01:35,395
>> Hm.

36
00:01:35,395 --> 00:01:35,830
>> You see that?

37
00:01:35,830 --> 00:01:37,980
>> I think so.

38
00:01:37,980 --> 00:01:40,810
>> Okay, so you might ask yourself,

39
00:01:40,810 --> 00:01:44,150
okay great, so you got two different approaches to this. Which one might

40
00:01:44,150 --> 00:01:48,180
you pick? And, unsurprisingly, there are tradeoffs. So, let's talk a little bit

41
00:01:48,180 --> 00:01:50,750
about what those tradeoffs are. Actually, before we do that, Michael do you

42
00:01:50,750 --> 00:01:53,370
have any suggestions for what would be

43
00:01:53,370 --> 00:01:55,660
good or bad about filtering versus wrapping?

44
00:01:55,660 --> 00:01:57,160
>> So filtering, it feels like is how we

45
00:01:57,160 --> 00:02:00,110
were talking about it before. It's this idea of whittling

46
00:02:00,110 --> 00:02:01,990
down to a set of fewer features and then

47
00:02:01,990 --> 00:02:05,720
handing it over, kind of one thing after the other.

48
00:02:05,720 --> 00:02:08,669
I, an advantage of that seems like it's sort of very flow

49
00:02:08,669 --> 00:02:11,750
forward, right? So it, it, the features come in, the search does its

50
00:02:11,750 --> 00:02:15,330
thing, it hands it over, everybody's got its job. The disadvantage of

51
00:02:15,330 --> 00:02:18,880
that, though, is that there isn't any feedback. So the learning algorithm can't

52
00:02:18,880 --> 00:02:21,810
inform the search algorithm as to hey, you know, you gave me

53
00:02:21,810 --> 00:02:24,830
this feature, but without that, you took away that feature, but without it

54
00:02:24,830 --> 00:02:27,030
I'm having a lot of trouble doing the learning. Maybe you should put

55
00:02:27,030 --> 00:02:30,960
that one back in. The second, the wrapping approach, I guess, would allow

56
00:02:30,960 --> 00:02:31,240
for that.

57
00:02:31,240 --> 00:02:32,990
>> In fact, that's exactly right. So as I

58
00:02:32,990 --> 00:02:36,550
said before, in the filtering case, the criterion or

59
00:02:36,550 --> 00:02:40,430
criteria is built inside the search algorithm itself without

60
00:02:40,430 --> 00:02:43,440
reference to the learner. By contrast, in wrapping the

61
00:02:43,440 --> 00:02:46,680
criteria, or criterion if it's a single one, is

62
00:02:46,680 --> 00:02:49,440
actually built inside what the learner wants. It's built

63
00:02:49,440 --> 00:02:53,130
inside something like classification error, or sum of squared

64
00:02:53,130 --> 00:02:56,050
errors, or whatever it is you measure the learner by.

65
00:02:56,050 --> 00:02:58,380
So we can write those down and then stare at them for a little while.
