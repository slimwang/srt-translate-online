1
00:00:00,180 --> 00:00:03,440
A couple more points about generative
models, they are pretty straightforward.

2
00:00:03,440 --> 00:00:06,510
First of all,
we've been doing the two class case.

3
00:00:06,510 --> 00:00:07,040
Right?

4
00:00:07,040 --> 00:00:09,050
You're either a skin or not skin.

5
00:00:09,050 --> 00:00:11,770
Well, suppose I've got a bunch
of classes, c sub i, and

6
00:00:11,770 --> 00:00:13,520
I've got a measurement, x.

7
00:00:13,520 --> 00:00:14,920
Which c should I choose?

8
00:00:14,920 --> 00:00:17,130
It's written here in green, c star.

9
00:00:17,130 --> 00:00:24,200
So, very simply, I'm going to take the
category that maximizes p of c given x.

10
00:00:24,200 --> 00:00:28,090
That is,
which category is most likely given x?

11
00:00:28,090 --> 00:00:31,740
Okay, that makes some sense, well,
but I don't have that directly, so

12
00:00:31,740 --> 00:00:35,670
in order to do that, I use
the likelihood times the, what's p of c?

13
00:00:35,670 --> 00:00:38,310
That's our prior.

14
00:00:38,310 --> 00:00:39,680
Okay, so I have to have my prior.

15
00:00:39,680 --> 00:00:44,840
So, basically in generative models,
as you add a new, category, you

16
00:00:44,840 --> 00:00:47,920
just need to build a likelihood model
and you need a prior model and that will

17
00:00:47,920 --> 00:00:52,980
allow you to compare, one category to
the next in, in deciding a new label.

18
00:00:52,980 --> 00:00:56,410
The other thing is, you remember before,
we were drawing these,

19
00:00:56,410 --> 00:00:59,800
histograms to represent those,
those probability distributions?

20
00:00:59,800 --> 00:01:04,450
Well, normally, what you want is a nice,
continuous generative model,

21
00:01:04,450 --> 00:01:08,250
so what you need is
a likelihood density.

22
00:01:08,250 --> 00:01:09,470
Okay?
So you don't just have a history,

23
00:01:09,470 --> 00:01:10,580
I mean you actually want a density.

24
00:01:10,580 --> 00:01:15,300
And remember, it's likelihood,
so it's p of x given c.

25
00:01:15,300 --> 00:01:16,530
Not p of c given x.

26
00:01:16,530 --> 00:01:18,664
That we get from Bayes' rule.

27
00:01:18,664 --> 00:01:19,440
All right?

28
00:01:19,440 --> 00:01:24,570
So typically we represent those
likelihoods using some parametric form.

29
00:01:24,570 --> 00:01:27,710
Here it's written as a,
a mixture of Gaussians, right?

30
00:01:27,710 --> 00:01:33,120
So here, this particular class 2,
and see it says p of x, given c 2?

31
00:01:33,120 --> 00:01:34,340
That's its likelihood.

32
00:01:34,340 --> 00:01:36,610
This is one little Gaussian lump.

33
00:01:36,610 --> 00:01:39,590
And we do that because we said oh,
that looks you know, may,

34
00:01:39,590 --> 00:01:43,360
maybe most of the distribution of those
pixel values is actually like that.

35
00:01:43,360 --> 00:01:49,145
But over here for class 1,
you see that we've got two humps.

36
00:01:49,145 --> 00:01:50,570
Okay?

37
00:01:50,570 --> 00:01:52,250
And that's just a mixture of Gaussians.

38
00:01:52,250 --> 00:01:55,390
And it's pretty,
it's not too difficult to estimate

39
00:01:55,390 --> 00:01:59,100
a mixture of Gaussians from
a modest amount of data.

40
00:01:59,100 --> 00:02:03,735
One, so one of the reasons to use these
kinds of parameterized distributions is

41
00:02:03,735 --> 00:02:07,700
it allows you to use much less
data in order to fit your density.

42
00:02:07,700 --> 00:02:09,650
So some of you might ask,
well wait a minute.

43
00:02:09,650 --> 00:02:12,000
Why am I doing this whole
continuous fitting,

44
00:02:12,000 --> 00:02:15,670
why not just use some like k-nearest
neighbor or Parzen window method or

45
00:02:15,670 --> 00:02:18,350
something else that you
learned in probability class?

46
00:02:18,350 --> 00:02:20,482
Megan is not going to remember that
question so I'm not going to ask her.

47
00:02:20,482 --> 00:02:21,770
Never mind.
So why won't you do that?

48
00:02:21,770 --> 00:02:23,640
Well, you actually, you might do that.

49
00:02:23,640 --> 00:02:27,900
If you've got a lot of data, you might
actually try to do some histogram

50
00:02:27,900 --> 00:02:30,190
KNN based method of
estimating your density.

51
00:02:30,190 --> 00:02:33,760
But, you would really need lots and
lots of data.

52
00:02:33,760 --> 00:02:36,820
Sort of everywhere that you're likely
to get a point, because you need

53
00:02:36,820 --> 00:02:40,940
the densities pretty much everywhere
you're likely to get a data point.

54
00:02:42,060 --> 00:02:47,010
And in some sense the whole
point of generative models is

55
00:02:47,010 --> 00:02:51,580
to be able to use a modest
amount of data so the reason to

56
00:02:51,580 --> 00:02:54,495
use the parameterized thing is
that you don't need a lot of data.

57
00:02:54,495 --> 00:02:55,120
Okay?

58
00:02:55,120 --> 00:02:58,330
It allows you to generate a model
that you can use for each category.
