1
00:00:00,510 --> 00:00:04,930
The next organization that we
will be looking at is similar to

2
00:00:04,930 --> 00:00:09,570
distributed shared memory, except that
we are no longer sharing memory, so

3
00:00:09,570 --> 00:00:11,860
it's called distributed memory.

4
00:00:11,860 --> 00:00:18,010
No sharing of memory means that only
one core can access a memory slice,

5
00:00:18,010 --> 00:00:19,720
and the others can not.

6
00:00:19,720 --> 00:00:23,910
So what we now have is cores
that have their own caches, and

7
00:00:23,910 --> 00:00:29,040
each of them has its own memory that
can only be locally accessed here.

8
00:00:29,040 --> 00:00:33,280
So really, each of the cores pretty
much has what looks like a complete

9
00:00:33,280 --> 00:00:35,800
single core computer system.

10
00:00:35,800 --> 00:00:40,770
And a network interface card
that connects it to a network.

11
00:00:40,770 --> 00:00:43,510
But now when a cork has a cache miss,

12
00:00:43,510 --> 00:00:46,600
that cash miss goes
directly to this memory.

13
00:00:46,600 --> 00:00:50,900
And if the core wants to
access something that is

14
00:00:50,900 --> 00:00:52,760
in another core's memory.

15
00:00:52,760 --> 00:00:57,100
It can not simply issue an access that
message in the cache and goes there.

16
00:00:57,100 --> 00:01:01,710
Now what it needs to do is actually
create a network message using some sort

17
00:01:01,710 --> 00:01:06,840
of a send primiative in the operating
system to actually send a request.

18
00:01:06,840 --> 00:01:09,509
The program here needs to
receive that,see what is

19
00:01:09,509 --> 00:01:11,890
being requested,respond to it and so on.

20
00:01:11,890 --> 00:01:16,950
So now communication is explicit
it's no longer sufficient to simply

21
00:01:16,950 --> 00:01:21,450
put data in memory and then another
core just reads it from there.

22
00:01:21,450 --> 00:01:25,490
You have to actually send the data
explicitly to another core

23
00:01:25,490 --> 00:01:27,350
if we want to communicate.

24
00:01:27,350 --> 00:01:28,195
So.

25
00:01:28,195 --> 00:01:31,725
That means that you write
programs differently from this.

26
00:01:31,725 --> 00:01:33,225
Symmetric shared memory and

27
00:01:33,225 --> 00:01:38,185
distributed share memory pass
data around using shared memory.

28
00:01:38,185 --> 00:01:41,985
Meaning reads and writes to
memory are used to exchange data.

29
00:01:41,985 --> 00:01:46,365
Now we are using what is called
message passing for communication.

30
00:01:46,365 --> 00:01:49,970
So now you are pretty
much writing a program

31
00:01:49,970 --> 00:01:55,030
as if these were independent machines
that communicate over a network.

32
00:01:55,030 --> 00:01:57,940
If you have a distributed
memory supercomputer, for

33
00:01:57,940 --> 00:02:01,040
example, it's just that this network and

34
00:02:01,040 --> 00:02:07,300
these network cards are a lot faster
than your normal internet connection.

35
00:02:07,300 --> 00:02:11,500
This type of a system is also called
a multicomputer because really each of

36
00:02:11,500 --> 00:02:14,380
these is like a complete computer.

37
00:02:14,380 --> 00:02:19,380
It has the processor, the cache,
the memory, thumb I/O, and

38
00:02:19,380 --> 00:02:22,210
the processor only directly
accesses the local memory.

39
00:02:22,210 --> 00:02:25,280
So it looks like complete
uniprocessor system.

40
00:02:25,280 --> 00:02:30,650
These are also called cluster computers,
because really, you put a bunch of

41
00:02:30,650 --> 00:02:34,980
normal computers together into
a tightly networked cluster, and

42
00:02:34,980 --> 00:02:38,720
you get something like
a distributed memory system.

43
00:02:38,720 --> 00:02:45,500
These types of computers tend to scale
to a large number of processors.

44
00:02:45,500 --> 00:02:49,490
The reason is not that they're
fundamentally better at

45
00:02:49,490 --> 00:02:54,920
communicating than
shared memory systems.

46
00:02:54,920 --> 00:02:59,450
The reason is mainly that
the programmer is forced to explicitly

47
00:02:59,450 --> 00:03:03,830
think about communication because you
use a different type of primitive to

48
00:03:03,830 --> 00:03:07,500
communicate than you use to
access your local memory.

49
00:03:07,500 --> 00:03:11,400
So the programmer is aware of
communication going on and

50
00:03:11,400 --> 00:03:14,320
then naturally they will tend
to minimize communication.

51
00:03:14,320 --> 00:03:16,970
And if at all possible,
access the local memory.

52
00:03:18,120 --> 00:03:23,110
So this approach works better, primarily
because it forces the programmer.

53
00:03:23,110 --> 00:03:27,340
To figure out things that
otherwise it might not notice.

54
00:03:27,340 --> 00:03:30,500
Because it forces
the programmer to figure out

55
00:03:30,500 --> 00:03:32,620
how to communicate efficiently.

56
00:03:32,620 --> 00:03:34,250
Whereas in shared memory,

57
00:03:34,250 --> 00:03:39,600
the programmer may not even realize
that some of the accesses are not local.

58
00:03:39,600 --> 00:03:42,940
And thus are slow and causing
a lot of communication to happen.
