1
00:00:00,000 --> 00:00:05,000
The next thing we did to increase the speed of the site was we added another database,

2
00:00:05,000 --> 00:00:11,000
and this was using a piece of software called Slony, I believe, to replicate our database.

3
00:00:11,000 --> 00:00:14,000
We still only had one app server at this time, and we made lots of database requests.

4
00:00:14,000 --> 00:00:16,000
Our caching was not nearly as advanced.

5
00:00:16,000 --> 00:00:19,000
We had some basic caching, it was just at the application level.

6
00:00:19,000 --> 00:00:21,000
This machine was hitting both of these Postgres machines.

7
00:00:21,000 --> 00:00:24,000
This is the first time we ran into the notion of replication lag,

8
00:00:24,000 --> 00:00:27,000
because we have these two separate machines, and Slony would normally keep them in sync,

9
00:00:27,000 --> 00:00:31,000
but sometimes, the slave machine could lag by about 5 seconds or so.

10
00:00:31,000 --> 00:00:34,000
If you had just submitted the link and then we redirected you to the permalink,

11
00:00:34,000 --> 00:00:38,000
and then we'd hit the slave machine that didn't have the permalink we would send the user a 404,

12
00:00:38,000 --> 00:00:40,000
which is really annoying right after you submitted a piece of content.

13
00:00:40,000 --> 00:00:42,000
That's when we started thinking about writing to our cache

14
00:00:42,000 --> 00:00:44,000
at the same time as writing to our database.

15
00:00:44,000 --> 00:00:47,000
We also had the issue with these two processes of keeping these caches in sync.

16
00:00:47,000 --> 00:00:50,000
We weren't using memcached yet--the way we kept our caches in sync

17
00:00:50,000 --> 00:00:55,000
was we used the library called Spread, and Spread is this network library that basically says

18
00:00:55,000 --> 00:00:58,000
if you send a message to one host, it will send it to all of these other hosts.

19
00:00:58,000 --> 00:01:01,000
Around this time was when we added our second app server

20
00:01:01,000 --> 00:01:05,000
that was running Python on as well, and we used Spread to keep the two hash tables

21
00:01:05,000 --> 00:01:07,000
that we were using as our cache kind of in sync.

22
00:01:07,000 --> 00:01:10,000
This worked sort of okay in terms of keeping these two caches in sync,

23
00:01:10,000 --> 00:01:13,000
but it wasn't going to scale very well, because eventually, we would add

24
00:01:13,000 --> 00:01:16,000
a couple of more app servers to deal with more load and Spread

25
00:01:16,000 --> 00:01:18,000
like every time one of these app servers would update its cache,

26
00:01:18,000 --> 00:01:21,000
you would have to tell all of the other app servers about it and so turned in to the huge mess,

27
00:01:21,000 --> 00:01:25,000
a lot of network traffic keeping all of these caches in sync.

28
00:01:25,000 --> 00:01:28,000
It was a total mess, and so eventually, we would switch to memcached,

29
00:01:28,000 --> 00:01:31,000
but that was not before we rewrote our database.

30
00:01:31,000 --> 00:01:35,000
Shortly after we switched a couple of machines and a couple of databases, we added comments,

31
00:01:35,000 --> 00:01:38,000
and the first version of comments--it was just another database table.

32
00:01:38,000 --> 00:01:42,000
There's nothing complicated about it--it had a link ID of what link it was on

33
00:01:42,000 --> 00:01:46,000
and had some contents, author ID--you can kind of guess the columns it would have,

34
00:01:46,000 --> 00:01:48,000
and we still did lots of joins--it was a relational database.

35
00:01:48,000 --> 00:01:51,000
We changed around this time to a more flexible database architecture,

36
00:01:51,000 --> 00:01:54,000
or at least a more flexible table design, because the challenge we had

37
00:01:54,000 --> 00:01:57,000
was every time we added a new feature, you might have to add a new table

38
00:01:57,000 --> 00:01:59,000
or you might add a new column to one of these tables.

39
00:01:59,000 --> 00:02:03,000
Adding a column would take time, and we might have to do a data migration

40
00:02:03,000 --> 00:02:06,000
or update an index and add all these load--it was just a total pain.

41
00:02:06,000 --> 00:02:09,000
We felt like the rate at which we could add features was limited

42
00:02:09,000 --> 00:02:13,000
by our ability to update our database and to do these big migrations.
