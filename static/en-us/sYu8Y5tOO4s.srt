1
00:00:00,220 --> 00:00:03,550
So, we've determined that we want to figure out which set of parameters provide

2
00:00:03,550 --> 00:00:06,680
the best predictions for our output variable, but how can we do that?

3
00:00:07,870 --> 00:00:10,520
We'll use an algorithm called gradient descent.

4
00:00:10,520 --> 00:00:14,930
First, we need to define some cost function, which we'll call J of big theta.

5
00:00:16,260 --> 00:00:20,320
I'm going to use big theta here, to represent our entire set of thetas, and

6
00:00:20,320 --> 00:00:22,820
I'll use this notation throughout the rest of this lesson.

7
00:00:22,820 --> 00:00:25,490
The cost function is meant to provide a measure of how well our

8
00:00:25,490 --> 00:00:28,850
current set of thetas does, at modeling the observed data.

9
00:00:28,850 --> 00:00:31,020
So, we want to minimize the cost function's value.

10
00:00:32,310 --> 00:00:35,470
As we discussed just a moment ago, when we're doing linear regression,

11
00:00:35,470 --> 00:00:39,350
our cost function J of theta, can simply measure the sum of the squares of

12
00:00:39,350 --> 00:00:42,290
the differences between our predicted and observed values.

13
00:00:42,290 --> 00:00:47,090
I am going to formalize this a little bit, and say this, J of big theta, is

14
00:00:47,090 --> 00:00:53,230
equal to one half, times the sum from i equals one to m, of y predicted of x i.

15
00:00:53,230 --> 00:00:55,840
Minus Y observed i squared.

16
00:00:55,840 --> 00:01:02,150
Where Y predicted x i, equals the sum of n from zero to big N, of theta n x n i.

17
00:01:03,400 --> 00:01:06,140
So, there's a lot going on here, and I have color coded this.

18
00:01:06,140 --> 00:01:08,010
So, why don't we walk through it.

19
00:01:08,010 --> 00:01:11,580
First, we're just saying that J of big theta is equal to one half,

20
00:01:11,580 --> 00:01:15,530
times the sum over all of our data points, of the predicted Y,

21
00:01:15,530 --> 00:01:20,230
given our Xs, our input variables, minus the observed Y squared.

22
00:01:20,230 --> 00:01:22,660
So, this is just our error squared term.

23
00:01:22,660 --> 00:01:24,650
Summed over all the data points.

24
00:01:24,650 --> 00:01:26,380
No different than the equation that we have up here.

25
00:01:28,200 --> 00:01:31,100
Down here, we're just defining the way that we calculate the predicted

26
00:01:31,100 --> 00:01:32,250
value of Y.

27
00:01:32,250 --> 00:01:33,960
Given our input variables.

28
00:01:33,960 --> 00:01:36,890
And the way that we do that, is that we say we sum from N equals zero,

29
00:01:36,890 --> 00:01:39,340
to big N, of theta N times X N.

30
00:01:39,340 --> 00:01:44,020
So, we're just saying that we sum up the X N each input variable,

31
00:01:44,020 --> 00:01:46,450
times its weight, theta.

32
00:01:46,450 --> 00:01:49,790
This is no different that the sum that we had, on the last slide.

33
00:01:49,790 --> 00:01:53,540
Note that we include an N equals zero term here, which corresponds to a constant

34
00:01:53,540 --> 00:01:57,220
term in our model, which doesn't correspond to any of the input variables.

35
00:01:57,220 --> 00:02:00,190
For those of you familiar with linear algebra, if we wanted to,

36
00:02:00,190 --> 00:02:02,700
we could also express this as theta transposed times x.

37
00:02:03,720 --> 00:02:05,990
If you don't know linear algebra, don't worry about this,

38
00:02:05,990 --> 00:02:08,669
this is just another way of expressing the same thing here,

39
00:02:08,669 --> 00:02:10,740
that might be easier for some students to comprehend.
