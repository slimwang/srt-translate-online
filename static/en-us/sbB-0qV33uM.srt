1
00:00:00,440 --> 00:00:04,080
Image classification is a pretty
common task in machine learning.

2
00:00:04,080 --> 00:00:08,280
But large scale image classification can
be computationally pretty intensive.

3
00:00:09,439 --> 00:00:13,929
You need to realize that although
images are like vectors with

4
00:00:13,929 --> 00:00:18,969
thousands of elements, the values of
these vectors can be highly correlated.

5
00:00:18,969 --> 00:00:22,719
They can have patterns that you can
exploit, and condense the data.

6
00:00:23,989 --> 00:00:28,856
Lorenzo came up with one scheme which
was taking the Fourier transform of

7
00:00:28,856 --> 00:00:33,587
the image, which would reduce
the dimensionality of the input data.

8
00:00:33,587 --> 00:00:39,359
Another method, probably a slightly
more applicable method in this case,

9
00:00:39,359 --> 00:00:42,785
would be to take
the principal components and

10
00:00:42,784 --> 00:00:46,582
the top few principal components for
each image.

11
00:00:46,582 --> 00:00:51,021
That would help to reduce
the dimensionality even further and

12
00:00:51,021 --> 00:00:53,929
retain the information from the image.

13
00:00:55,219 --> 00:01:00,460
Now, you need to realize that these
same transformations can be applied

14
00:01:00,460 --> 00:01:03,160
in slightly different domains as well.

15
00:01:03,159 --> 00:01:07,670
And this was something that was nicely
pointed out by Lorenzo, saying that

16
00:01:07,670 --> 00:01:12,840
Fourier transform can be applied, for
instance, in the audio domain as well.

17
00:01:12,840 --> 00:01:17,060
So when you're talking
about a particular data

18
00:01:17,060 --> 00:01:21,890
transformation algorithm or machine
learning algorithm, you should be able

19
00:01:21,890 --> 00:01:25,980
to answer questions about what domains
or fields they can be applied in.

