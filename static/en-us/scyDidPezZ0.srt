1
00:00:00,036 --> 00:00:02,935
Alright, let's get through these. So picking good algorithms,

2
00:00:02,935 --> 00:00:06,176
obviously this is the most important level of optimization,

3
00:00:06,176 --> 00:00:13,381
and when sorting a large random list, an order N log N algorithm like mergesort is just intrinsically going to be

4
00:00:13,381 --> 00:00:19,028
faster than an order N squared algorithm like insertion sort, and clearly this is a case of choosing the right algorithm.

5
00:00:19,028 --> 00:00:24,427
You know much of performance CPU programming relies on making efficient use of the cache.

6
00:00:24,427 --> 00:00:29,052
This is sort of the number one thing to keep in mind when you're trying to write efficient CPU code,

7
00:00:29,052 --> 00:00:32,713
is that there is this big cache hierarchy and you want to write code that's going to make good use of it.

8
00:00:32,713 --> 00:00:38,074
And so, I would consider this an example of optimization level 2,

9
00:00:38,074 --> 00:00:41,078
right, it's a basic principle of efficiency on a CPU.

10
00:00:41,078 --> 00:00:43,714
On the other hand, blocking for the L1 cache,

11
00:00:43,714 --> 00:00:48,383
which means carefully sizing your working set to fit exactly in the per-core cache on the CPU.

12
00:00:48,383 --> 00:00:53,389
This is a detailed optimization. It's going to depend on the exact CPU model you're using

13
00:00:53,389 --> 00:00:56,031
because every CPU core has different size L1 cache.

14
00:00:56,031 --> 00:01:01,563
I would also put the use of vector registers like SSE or AVX intrinsics into this category.

15
00:01:01,563 --> 00:01:09,738
Not every CPU has SSE; some have AVX. Some don't have either. So, the use of vector registers,

16
00:01:09,738 --> 00:01:09,738
the choice of blocking for the L1 cache, cache blocking,

17
00:01:12,807 --> 00:01:15,561
I would consider these architecture-specific detailed optimizations.

18
00:01:15,561 --> 00:01:22,502
So these kind of architectural-specific optimizations tend to be the domain of what I call ninja programmers.

19
00:01:22,502 --> 00:01:27,505
And we'll also touch on a few ninja-style GPU topics like shared-memory bank conflicts.

20
00:01:27,505 --> 00:01:31,679
I'll try to highlight these ninja topics with this little icon here.

21
00:01:31,679 --> 00:01:38,126
The idea is that these are really not needed or necessarily accessible to every programmer.

22
00:01:38,126 --> 00:01:42,794
They tend to be when you're sort of squabbling over the last few percent of optimizations,

23
00:01:42,794 --> 00:01:45,290
the last few percent of speedup.

24
00:01:45,290 --> 00:01:48,760
I think this is one of the big differences between CPU and GPU programming.

25
00:01:48,760 --> 00:01:54,074
On a CPU, these sort of ninja-level optimizations can make a really big difference.

26
00:01:54,074 --> 00:02:01,013
For example, if you ignore the existence of SSE registers or AVX registers on really modern processors,

27
00:02:01,013 --> 00:02:05,352
then you're only getting a fourth or an eighth of the power of each core on your CPU,

28
00:02:05,352 --> 00:02:07,890
so you're taking a huge hit in potential performance.

29
00:02:07,890 --> 00:02:14,628
On the GPU, for the most part as a rule of thumb, the speedups to be gained by these sort of ninja optimizations

30
00:02:14,628 --> 00:02:21,665
are usually comparatively minor. Okay, so we're talking more like a 30% or 80% speedup

31
00:02:21,665 --> 00:02:25,119
by using some of the techniques that we'll talk about with this little ninja icon

32
00:02:25,119 --> 00:02:29,151
versus the speedup that you might hope to get from just

33
00:02:29,151 --> 00:02:32,520
picking the right algorithm in the first place or just obeying the

34
00:02:32,520 --> 00:02:38,325
basic principles of efficiency on a GPU such as coalescing your global memory accesses.

35
00:02:38,325 --> 00:02:44,028
These can also often make a difference of 3 times, 10 times, sometimes more.

36
00:02:44,028 --> 00:02:53,008
So these, just doing a good job, the sort of higher level principles of optimization matter more on the GPU,

37
00:02:53,008 --> 00:02:57,541
and the ninja level optimizations definitely help, sometimes you can get a speedup here,

38
00:02:57,541 --> 00:03:02,784
but it's not a sort of vital step to extract maximum performance the way that it is on a CPU

39
00:03:02,784 --> 00:03:07,021
where if you don't start doing something to make sure you're using the vector registers,

40
00:03:07,021 --> 00:03:11,691
you're going to take a huge hit in terms of the efficiency that you get out of your modern CPU.

41
00:03:11,691 --> 00:03:15,497
Finally, this last one is, I really just threw that in there for fun.

42
00:03:15,497 --> 00:03:19,419
Clearly, shifting a number, casting it, shifting a floating point number,

43
00:03:19,419 --> 00:03:22,786
casting it to a float and subtracting it from this magic constant,

44
00:03:22,786 --> 00:03:27,634
is firmly in the category of micro optimization at this sort of bit-twiddling instruction level.

45
00:03:27,634 --> 00:03:32,866
This is an infamous hack that's been used in many places, most famously in the video game Quake 3,

46
00:03:32,882 --> 00:03:38,114
and if you're curious go to Wikipedia and look up fast inverse square root.

47
00:03:38,114 --> 00:03:43,754
Of course these are firmly in the ninja category, and we won't be talking about them at all today or in rest of the course.
