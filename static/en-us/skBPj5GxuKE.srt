1
00:00:00,210 --> 00:00:04,307
This transformation has actually reduced the amount of parallelism.

2
00:00:04,307 --> 00:00:09,114
Previously, we had P-squared threads per tile and P-squared way parallelism within a tile.

3
00:00:09,114 --> 00:00:13,425
Now we only have P threads per tile, so we have less parallelism than we did before.

4
00:00:13,425 --> 00:00:18,155
Well does that matter? Well, if we have a big, big N body problem to solve, it probably doesn't matter.

5
00:00:18,155 --> 00:00:21,363
For a big, big problem there's probably so much parallel work

6
00:00:21,363 --> 00:00:24,427
that will easily fill the machine, even with our new version,

7
00:00:24,427 --> 00:00:27,257
but if we have a more modest-sized problem,

8
00:00:27,257 --> 00:00:29,873
the lesser amount of parallelism might mean this method isn't the best,

9
00:00:29,873 --> 00:00:34,404
because we might reduce the amount of parallelism to the point where we can't keep the whole GPU busy.

10
00:00:34,404 --> 00:00:39,499
The big picture by what we've done here is that we've chosen an implementation that has fewer threads

11
00:00:39,499 --> 00:00:41,717
and does more work per thread.

12
00:00:41,717 --> 00:00:45,582
You'll often see this technique when you can convert communication between threads,

13
00:00:45,582 --> 00:00:49,453
which is correspondingly more expensive to communication within a thread,

14
00:00:49,453 --> 00:00:51,955
because communication between thread local storage

15
00:00:51,955 --> 00:00:59,991
is faster than communication between threads within shared memory.
