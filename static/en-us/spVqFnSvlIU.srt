1
00:00:00,130 --> 00:00:02,480
So here, are the two crucial ingredients.

2
00:00:02,480 --> 00:00:07,000
I have many features available to me, but I hypothesize that there's

3
00:00:07,000 --> 00:00:10,320
a smaller number of features that are actually driving the patterns in the data.

4
00:00:10,320 --> 00:00:13,810
And then, what I do is I take that knowledge and I try making a composite

5
00:00:13,810 --> 00:00:17,080
feature that more directly probes the underlying phenomenon.

6
00:00:17,080 --> 00:00:20,190
These composite features are what we'll be talking about in this lesson.

7
00:00:20,190 --> 00:00:22,050
They're called principle components.

8
00:00:22,050 --> 00:00:24,030
And it's an extremely powerful algorithm.

9
00:00:24,030 --> 00:00:24,610
In this lesson,

10
00:00:24,610 --> 00:00:27,970
we'll mostly talk about it in the context of dimensionality reduction,

11
00:00:27,970 --> 00:00:30,560
how you can use principle component analysis.

12
00:00:30,560 --> 00:00:33,710
Or PCA, to bring down the dimensionality of your features to

13
00:00:33,710 --> 00:00:35,930
turn a whole bunch of features into just a few.

14
00:00:35,930 --> 00:00:39,230
It's also a very powerful standalone method in its own right for

15
00:00:39,230 --> 00:00:41,930
unsupervised learning, and I'll give you some of my favorite examples of

16
00:00:41,930 --> 00:00:43,590
that in the latter half of this lesson.

17
00:00:43,590 --> 00:00:44,750
It's going to be really cool.

18
00:00:44,750 --> 00:00:46,860
So, here's an example of what PCA is,

19
00:00:46,860 --> 00:00:49,990
transforming from square footage plus number of rooms into a single

20
00:00:49,990 --> 00:00:53,140
variable that roughly tracks the size of the house.

21
00:00:53,140 --> 00:00:55,150
>> So, let's suppose this is my training data,

22
00:00:55,150 --> 00:00:59,840
I have the square footage on the x axis and the number of rooms on the y axis.

23
00:00:59,840 --> 00:01:03,650
Now, what I'm going to do is I'm going to draw in the principle component.

24
00:01:03,650 --> 00:01:05,230
It will look something like this.

25
00:01:05,230 --> 00:01:09,390
I know this looks like a regression at this point, but it's not, and here's why.

26
00:01:09,390 --> 00:01:11,700
With the regression, what you're trying to do is you're trying to

27
00:01:11,700 --> 00:01:15,990
predict an output variable with respect to the value of an input variable.

28
00:01:15,990 --> 00:01:18,100
Here, we're not really trying to predict anything.

29
00:01:18,100 --> 00:01:22,140
We're trying to come up with a direction in the data that we can project our

30
00:01:22,140 --> 00:01:25,780
data onto while losing a minimal amount of information.

31
00:01:25,780 --> 00:01:27,550
So, here's what that means.

32
00:01:27,550 --> 00:01:31,800
Once I've found my principal component, once I have the direction of this,

33
00:01:31,800 --> 00:01:35,450
of this vector, of this line, and just go with me for now, that it exists.

34
00:01:35,450 --> 00:01:37,480
We'll come back to how we find it in a little bit.

35
00:01:37,480 --> 00:01:40,710
Then, what I'm going to do is I'm going to take all of my data points.

36
00:01:40,710 --> 00:01:43,470
And, I'm going to go through a process that's called projection.

37
00:01:43,470 --> 00:01:46,810
And, what a projection means, it's a little bit of a physical term almost.

38
00:01:46,810 --> 00:01:49,600
Imagine that I have a light and the light is sort of

39
00:01:49,600 --> 00:01:53,550
shining down perpendicular to the surface of this principle component.

40
00:01:53,550 --> 00:01:55,640
Then, you can imagine that all of the data points are going to be

41
00:01:55,640 --> 00:01:59,320
sort of casting shadows down onto like a piece of paper if I put it there.

42
00:01:59,320 --> 00:02:01,530
Now, my principle component looks like this.

43
00:02:01,530 --> 00:02:03,598
So I start out with two dimensional data.

44
00:02:03,598 --> 00:02:06,690
But, once I've projected it down onto the principle component,

45
00:02:06,690 --> 00:02:09,560
it's in one dimension and it's going to look something like this.

46
00:02:09,560 --> 00:02:14,150
Now, instead of being the blue circles, my data is going to be the black xs.

47
00:02:14,150 --> 00:02:16,130
I've turned it into a one dimensional distribution.

48
00:02:16,130 --> 00:02:21,070
I've taken the feature that I've made in this diagram and sort of turned it so

49
00:02:21,070 --> 00:02:22,260
that it's lying flat.

50
00:02:22,260 --> 00:02:23,219
What's it going to look like?
