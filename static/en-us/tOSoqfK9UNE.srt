1
00:00:00,000 --> 00:00:02,000
What are the problems of kNN?

2
00:00:02,000 --> 00:00:04,000
Well, I would argue that there're two.

3
00:00:04,000 --> 00:00:07,000
One is very large data sets,

4
00:00:07,000 --> 00:00:10,000
and one is very large feature spaces.

5
00:00:10,000 --> 00:00:14,000
Now the first one results in lengthy searches

6
00:00:14,000 --> 00:00:17,000
when you try to find K's nearest neighbors.

7
00:00:17,000 --> 00:00:19,000
Now, fortunately there are

8
00:00:19,000 --> 00:00:22,000
methods to search efficiently.

9
00:00:22,000 --> 00:00:24,000
Often you represent your data

10
00:00:24,000 --> 00:00:27,000
not by a linear list, in which case the search

11
00:00:27,000 --> 00:00:29,000
would be linear in the number of data points,

12
00:00:29,000 --> 00:00:34,000
but by a tree, where the search becomes logarithmic.

13
00:00:34,000 --> 00:00:38,000
The method of choice is called kDD trees

14
00:00:38,000 --> 00:00:40,000
where there are many other ways

15
00:00:40,000 --> 00:00:43,000
to represent data points as trees.

16
00:00:43,000 --> 00:00:45,000
Now very large feature spaces

17
00:00:45,000 --> 00:00:48,000
cause more of a problem.

18
00:00:48,000 --> 00:00:51,000
It turns out computing nearest neighbors,

19
00:00:51,000 --> 00:00:54,000
as the feature space for the input vector increases,

20
00:00:54,000 --> 00:00:57,000
becomes increasingly difficult,

21
00:00:57,000 --> 00:01:00,000
and the tree methods become increasingly brittle.

22
00:01:00,000 --> 00:01:03,000
And the reason is shown in the following graph:

23
00:01:03,000 --> 00:01:06,000
If your graph input dimension to

24
00:01:06,000 --> 00:01:09,000
the average edge length of your neighborhood

25
00:01:09,000 --> 00:01:12,000
you'll find that for randomly chosen points

26
00:01:12,000 --> 00:01:16,000
very quickly all points are really far away.

27
00:01:16,000 --> 00:01:19,000
The edge length of one is obtained

28
00:01:19,000 --> 00:01:23,000
if your query point

29
00:01:23,000 --> 00:01:26,000
is unit one away from the nearest neighbor.

30
00:01:26,000 --> 00:01:28,000
If you have one hundred dimensions,

31
00:01:28,000 --> 00:01:29,000
that is almost certain.

32
00:01:29,000 --> 00:01:31,000
Why is that?

33
00:01:31,000 --> 00:01:33,000
Well, in one hundred dimensions,

34
00:01:33,000 --> 00:01:35,000
they are to be one where just by chance

35
00:01:35,000 --> 00:01:37,000
your're far away.

36
00:01:37,000 --> 00:01:39,000
The number of points you need

37
00:01:39,000 --> 00:01:40,000
to get something close

38
00:01:40,000 --> 00:01:45,000
grows exponentially with the number of dimensions.

39
00:01:45,000 --> 00:01:47,000
So, for any fixed data set size

40
00:01:47,000 --> 00:01:49,000
you will find yourself in a situation

41
00:01:49,000 --> 00:01:52,000
where all your neighbors are far away.

42
00:01:52,000 --> 00:01:54,000
Nearest neighbor works really well

43
00:01:54,000 --> 00:01:58,000
for small input spaces like three or four dimensions.

44
00:01:58,000 --> 00:01:59,000
It works very poorly

45
00:01:59,000 --> 00:02:01,000
if your input space is twenty, twenty-five,

46
00:02:01,000 --> 00:02:03,000
or maybe one hundred dimensions.

47
00:02:03,000 --> 00:02:06,000
So don't trust nearest neighbor to do a good job

48
00:02:06,000 --> 99:59:59,999
if your input and measure spaces are high.
