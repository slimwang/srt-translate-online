1
00:00:00,420 --> 00:00:01,640
How do we learn?

2
00:00:01,640 --> 00:00:04,950
Although times may change,
some concepts stay the same.

3
00:00:04,950 --> 00:00:07,780
Unchanging, information
outlasts the body.

4
00:00:07,780 --> 00:00:09,000
It's stored in our brain,

5
00:00:09,000 --> 00:00:12,060
but can be passed down from
generation to generation.

6
00:00:12,060 --> 00:00:14,870
Our brain is capable of
synthesizing the diverse set of

7
00:00:14,870 --> 00:00:19,660
inputs we call our five senses, and from
them, creating a hierarchy of concepts.

8
00:00:19,660 --> 00:00:20,240
If we're lucky,

9
00:00:20,240 --> 00:00:23,730
we can learn a task while being
supervised by a teacher directly.

10
00:00:23,730 --> 00:00:27,040
While interacting with our environment,
we can feel our surroundings,

11
00:00:27,040 --> 00:00:30,660
see our obstacles and
try to predict the next steps.

12
00:00:30,660 --> 00:00:33,170
If we try at first and
fail, that's okay.

13
00:00:33,170 --> 00:00:36,920
Through the process of trial and
error, we can learn anything.

14
00:00:36,920 --> 00:00:39,550
But what is it that gives our
brain this special capability

15
00:00:39,550 --> 00:00:41,290
unlike anything else in nature?

16
00:00:41,290 --> 00:00:44,500
Everything we've ever experienced or
felt, all our thoughts and memories,

17
00:00:44,500 --> 00:00:47,570
our very sense of self,
is produced by the brain.

18
00:00:47,570 --> 00:00:48,750
At the molecular level,

19
00:00:48,750 --> 00:00:53,100
our brain consists of an estimated 100
billion nerve cells called neurons.

20
00:00:53,100 --> 00:00:54,960
Each neuron has three jobs,

21
00:00:54,960 --> 00:00:57,800
receive a set of signals from
what are called its dendrites.

22
00:00:57,800 --> 00:01:00,683
Integrate those signals together
to determine whether or

23
00:01:00,683 --> 00:01:03,986
not the information should be passed
on in the cell body, or soma.

24
00:01:03,986 --> 00:01:07,144
And then if the sum of the signals
passes a certain threshold,

25
00:01:07,144 --> 00:01:10,243
send this resulting signal,
called the action potential,

26
00:01:10,243 --> 00:01:12,690
onwards via its axon to
the next set of neurons.

27
00:01:12,690 --> 00:01:13,340
Hello world.

28
00:01:13,340 --> 00:01:16,960
It's Siraj, and we're going to build
our own neural network in Python.

29
00:01:16,960 --> 00:01:20,070
The rules that govern the brain
give rise to intelligence.

30
00:01:20,070 --> 00:01:24,070
It's the same algorithm that invented
modern language, space flight,

31
00:01:24,070 --> 00:01:25,280
Shia LaBeouf.

32
00:01:25,280 --> 00:01:26,810
It's what makes us, us.

33
00:01:26,810 --> 00:01:30,860
It's what allowed us to survive and
thrive on planet Earth.

34
00:01:30,860 --> 00:01:32,880
But as far as we've come as a species,

35
00:01:32,880 --> 00:01:36,990
we still face a host of existential
threats to our existence.

36
00:01:36,990 --> 00:01:39,230
There's the impending
threat of climate change,

37
00:01:39,230 --> 00:01:43,210
the possibility of biochemical warfare,
an asteroid impact.

38
00:01:43,210 --> 00:01:47,250
These are nontrivial problems that could
take our biological neural networks

39
00:01:47,250 --> 00:01:49,010
many generations to solve.

40
00:01:49,010 --> 00:01:52,590
But what if we could harness this power,
what if we could create an artificial

41
00:01:52,590 --> 00:01:57,160
neural network, and have it run on
a non-biological substrate like silicon?

42
00:01:57,160 --> 00:02:01,019
We could give it more computing power
and data than any one human would be

43
00:02:01,019 --> 00:02:04,491
capable of handling, and
have it solve problems a thousand, or

44
00:02:04,491 --> 00:02:07,153
even a million times faster
than we could alone.

45
00:02:07,153 --> 00:02:11,014
In 1943, two early computer
scientists named Warren McCulloch and

46
00:02:11,014 --> 00:02:15,160
Walter Pitts invented the first
computational model of a neuron.

47
00:02:15,160 --> 00:02:19,600
Their model demonstrated a neuron that
received binary inputs, summed them, and

48
00:02:19,600 --> 00:02:23,000
if the sum exceeded a certain
threshold value, output a 1.

49
00:02:23,000 --> 00:02:24,599
If not, output a 0.

50
00:02:24,599 --> 00:02:25,640
It was a simple model.

51
00:02:25,640 --> 00:02:28,670
But in the early days of AI,
this was a big deal, and

52
00:02:28,670 --> 00:02:31,770
got computer scientists talking
about the possibilities.

53
00:02:31,770 --> 00:02:35,712
A few years later, a psychologist named
Frank Rosenblatt was frustrated that

54
00:02:35,712 --> 00:02:39,200
the McCulloch-Pitts model still
lacked the mechanism for learning.

55
00:02:39,200 --> 00:02:42,480
So he conceived a neural model
that built on their idea

56
00:02:42,480 --> 00:02:45,730
which he called the Perceptron,
which is another word for

57
00:02:45,730 --> 00:02:48,810
a single layer feedforward
neural network.

58
00:02:48,810 --> 00:02:53,240
We call it feedforward because the data
just flows in one direction, forward.

59
00:02:53,240 --> 00:02:56,930
The Perceptron incorporated
the idea of weights on the inputs.

60
00:02:56,930 --> 00:02:59,780
So, given some training set
of input output examples,

61
00:02:59,780 --> 00:03:04,210
it should learn a function from it by
increasing or decreasing the weights

62
00:03:04,210 --> 00:03:07,890
continuously for each example,
depending on what its output was.

63
00:03:07,890 --> 00:03:10,820
These weight values are mathematically
applied to the input,

64
00:03:10,820 --> 00:03:14,540
such that after each iteration, the
output prediction gets more accurate.

65
00:03:14,540 --> 00:03:18,770
To best understand this process we call
training, letâ€™s build our own single

66
00:03:18,770 --> 00:03:23,010
layer neural network in Python
using only Numpy as our dependency.

67
00:03:23,010 --> 00:03:25,610
In our main function, we'll first
initialize our neural network,

68
00:03:25,610 --> 00:03:28,115
which w'ell later define
as its own class.

69
00:03:28,115 --> 00:03:31,355
Then print out its starting weights for
a reference when we demo it.

70
00:03:31,355 --> 00:03:32,955
We can now define our data set.

71
00:03:32,955 --> 00:03:34,595
We've got four examples.

72
00:03:34,595 --> 00:03:38,184
Each example has three input values and
one output value.

73
00:03:38,184 --> 00:03:39,835
They're all ones and zeros.

74
00:03:39,835 --> 00:03:43,230
The T function transposes the matrix
from horizontal to vertical.

75
00:03:43,230 --> 00:03:45,810
So the computer is storing
the numbers like this.

76
00:03:45,810 --> 00:03:47,880
We'll train our neural
network on these values so

77
00:03:47,880 --> 00:03:50,840
that given a new list of ones and zeros,
it'll be able to predict whether or

78
00:03:50,840 --> 00:03:52,700
not the output should be a one or zero.

79
00:03:52,700 --> 00:03:55,390
Since we are identifying
which category it belongs to,

80
00:03:55,390 --> 00:03:58,590
this is considered a classification
task in machine learning.

81
00:03:58,590 --> 00:04:02,530
We'll train our network on this data
by using them as arguments to our train

82
00:04:02,530 --> 00:04:04,200
function, as well as a number, 10,000,

83
00:04:04,200 --> 00:04:08,200
which is the amount of times we'd
like to iterate during training.

84
00:04:08,200 --> 00:04:10,830
After it's done training,
we'll print out the updated weights so

85
00:04:10,830 --> 00:04:15,010
we can compare them, and finally, we'll
predict the output given a new input.

86
00:04:15,010 --> 00:04:16,700
We've got our main function ready, so

87
00:04:16,700 --> 00:04:19,190
let's now define our
NeuralNetwork class.

88
00:04:19,190 --> 00:04:23,380
When we initialize the class, the first
thing we want to do is seed it.

89
00:04:23,380 --> 00:04:26,810
We'll initialize our weight values
randomly in a second, and seeding them

90
00:04:26,810 --> 00:04:30,970
makes sure that it generates the same
numbers every time the program runs.

91
00:04:30,970 --> 00:04:33,100
This is useful for debugging later on.

92
00:04:33,100 --> 00:04:37,570
We'll assign random weights to a three
by one matrix with values in the range

93
00:04:37,570 --> 00:04:39,820
of -1 to 1 with a mean of 0.

94
00:04:39,820 --> 00:04:44,150
Since our single neuron has three input
connections and one output connection.

95
00:04:44,150 --> 00:04:47,220
Next we'll write out our activation
function, which, in our case,

96
00:04:47,220 --> 00:04:48,240
will be a sigmoid.

97
00:04:48,240 --> 00:04:50,180
It describes an s shaped curve.

98
00:04:50,180 --> 00:04:52,580
We pass the weighted sum
of the inputs through it,

99
00:04:52,580 --> 00:04:56,500
and it will convert them to
a probability between 0 and 1.

100
00:04:56,500 --> 00:04:59,030
This probability will
help make our prediction.

101
00:04:59,030 --> 00:05:02,090
We'll use our sigmoid function
directly in our predict function,

102
00:05:02,090 --> 00:05:05,970
which takes inputs as parameters and
passes them through our neuron.

103
00:05:05,970 --> 00:05:07,796
To get the weighted sum of our inputs,

104
00:05:07,796 --> 00:05:10,775
we'll compute the dot product
of our inputs and our weights.

105
00:05:10,775 --> 00:05:14,025
This is how our weights govern
the attention of how data flows in our

106
00:05:14,025 --> 00:05:17,025
neural net, and this function
will return our prediction.

107
00:05:17,025 --> 00:05:19,923
Now we can write out our train function,
which is the real meat of our code.

108
00:05:19,923 --> 00:05:23,812
We'll write a for loop to iterate
10,000 times, as we specified,

109
00:05:23,812 --> 00:05:27,888
then use our predict function to pass
the training set through the network and

110
00:05:27,888 --> 00:05:30,482
get the output value,
which is our prediction.

111
00:05:30,482 --> 00:05:31,925
We'll next calculate the error,

112
00:05:31,925 --> 00:05:35,530
which is the difference between the
desired output and our predicted output.

113
00:05:35,530 --> 00:05:37,790
We want to minimize
this error as we train,

114
00:05:37,790 --> 00:05:40,340
and we'll do this by iteratively
updating our weights.

115
00:05:40,340 --> 00:05:43,950
We'll calculate the necessary adjustment
by computing the dot product of our

116
00:05:43,950 --> 00:05:45,410
input's transpose and

117
00:05:45,410 --> 00:05:48,610
the error, multiplied by
the gradient of the sigmoid curve.

118
00:05:48,610 --> 00:05:51,000
So less confident weights
are adjusted more, and

119
00:05:51,000 --> 00:05:54,180
inputs that are zero don't
cause changes to the weights.

120
00:05:54,180 --> 00:05:56,800
This process is called gradient descent.

121
00:05:56,800 --> 00:05:58,770
&gt;&gt; Yeah, I'm descending that gradient!

122
00:05:58,770 --> 00:06:01,610
&gt;&gt; We'll also write out the function
that calculates the derivative

123
00:06:01,610 --> 00:06:04,850
of our sigmoid,
which gives us its gradient, or slope.

124
00:06:04,850 --> 00:06:08,180
This measures how confident we are of
the existing weight value, and

125
00:06:08,180 --> 00:06:11,140
helps us update our prediction
in the right direction.

126
00:06:11,140 --> 00:06:13,150
Finally, once we have our adjustment,

127
00:06:13,150 --> 00:06:15,290
we'll update our weights
with that value.

128
00:06:15,290 --> 00:06:18,940
This process of propagating our
error value back into our network,

129
00:06:18,940 --> 00:06:22,020
to adjust our weights,
is called back propagation.

130
00:06:22,020 --> 00:06:23,720
Let's demo this baby in Terminal.

131
00:06:23,720 --> 00:06:27,350
Because the training set is so small,
it took milliseconds to train it.

132
00:06:27,350 --> 00:06:30,370
We can see that our weight
values updated themselves after

133
00:06:30,370 --> 00:06:31,350
all those iterations.

134
00:06:31,350 --> 00:06:33,180
And when we fed it a novel input,

135
00:06:33,180 --> 00:06:36,190
it predicted that the output
was very likely a one.

136
00:06:36,190 --> 00:06:38,950
We just made our first neural network,
from scratch!

137
00:06:38,950 --> 00:06:41,702
Anyways, about backpropagation, I-

138
00:06:41,702 --> 00:06:51,702
[MUSIC]

139
00:07:16,897 --> 00:07:19,947
So as dope as Rosenblatt's idea was,
in the decades following it,

140
00:07:19,947 --> 00:07:23,490
neural networks didn't really give
us any kind of noteworthy results.

141
00:07:23,490 --> 00:07:25,300
They could only
accomplish simple things.

142
00:07:25,300 --> 00:07:27,867
But as the World Wide Web
grew from a CERN project

143
00:07:27,867 --> 00:07:31,140
to the massive nervous system for
humanity that it is today,

144
00:07:31,140 --> 00:07:34,475
we've seen an explosion in data and
computing power.

145
00:07:34,475 --> 00:07:37,425
And a small group of researchers
funded by the Canadian government

146
00:07:37,425 --> 00:07:40,185
held fast to their belief in
the power of neural networks

147
00:07:40,185 --> 00:07:42,735
to help us find
solutions from this data.

148
00:07:42,735 --> 00:07:47,420
When they took a neural net and made
it not one or two but many layers deep.

149
00:07:47,420 --> 00:07:51,170
Gave it a huge data set and lots of
computing power, they discovered it

150
00:07:51,170 --> 00:07:55,160
could outperform humans in tasks
that we thought only we could do.

151
00:07:55,160 --> 00:07:56,610
This is profound.

152
00:07:56,610 --> 00:08:00,450
Our biological neural network is
carbon-based, sending electrochemicals,

153
00:08:00,450 --> 00:08:04,480
like acetylcholine, glutamate,
and serotonin, as signals.

154
00:08:04,480 --> 00:08:08,460
An artificial neural network doesn't
even exist in physical space.

155
00:08:08,460 --> 00:08:11,530
It's an abstract concept we
programmatically created, and

156
00:08:11,530 --> 00:08:14,040
it's represented on silicon transistors.

157
00:08:14,040 --> 00:08:17,080
Yet despite the complete difference
in mediums, they both developed

158
00:08:17,080 --> 00:08:21,470
a very similar mechanism for processing
information, and the results show that.

159
00:08:21,470 --> 00:08:25,260
Perhaps there's a law of intelligence
encoded into our universe, and

160
00:08:25,260 --> 00:08:27,650
we're coming ever closer to finding it.

161
00:08:27,650 --> 00:08:31,890
So to break it down, a neural network is
a biologically inspired algorithm that

162
00:08:31,890 --> 00:08:34,365
learns to identify patterns in data.

163
00:08:34,365 --> 00:08:36,820
Backpropagation is
a popular technique to

164
00:08:36,820 --> 00:08:41,039
train a neural network by continually
updating weights via gradient descent.

165
00:08:41,039 --> 00:08:44,820
And when we train a many layer deep
neural network on lots of data,

166
00:08:44,820 --> 00:08:48,640
using lots of computing power,
we call this process deep learning.

167
00:08:48,640 --> 00:08:51,440
The coding challenge winner for
last week is Ludo Bouan.

168
00:08:51,440 --> 00:08:55,530
Ludo made a really slick iPython
notebook to demo not just 2D repression,

169
00:08:55,530 --> 00:08:58,580
but 3D regression as well on
a climate change data set.

170
00:08:58,580 --> 00:08:59,910
Wizard of the week.

171
00:08:59,910 --> 00:09:02,020
And the runner up is Amanullah Tariq.

172
00:09:02,020 --> 00:09:04,050
He completed the bonus
with great results.

173
00:09:04,050 --> 00:09:07,672
The challenge for this video is
to create a not one, not two,

174
00:09:07,672 --> 00:09:11,451
but three layer feedforward
neural network using just numpy.

175
00:09:11,451 --> 00:09:13,015
Post your GitHub link
in the comments and

176
00:09:13,015 --> 00:09:14,980
I'll announce the winner in one week.

177
00:09:14,980 --> 00:09:18,870
Please subscribe, and for now, I've got
to update my weights, so thanks for

178
00:09:18,870 --> 00:09:19,159
watching.