1
00:00:01,100 --> 00:00:05,740
Let's switch gears and test our model
out against the validation sample

2
00:00:05,740 --> 00:00:09,790
to see how well it holds up
against its independent data.

3
00:00:09,790 --> 00:00:12,530
So what we're going to do is
have the same model that we

4
00:00:12,530 --> 00:00:16,129
already built up before in
that decision tree model, but

5
00:00:16,129 --> 00:00:18,590
now we're going to plug in
the model comparison tool.

6
00:00:20,180 --> 00:00:23,410
We're going to configure this model
comparison tool the same way that we

7
00:00:23,410 --> 00:00:24,490
did before.

8
00:00:24,490 --> 00:00:27,610
So the model object goes
into the M side and

9
00:00:27,610 --> 00:00:30,440
the validation sample
goes into the D side.

10
00:00:31,950 --> 00:00:35,062
The positive class, yet
again, is going to be yes,

11
00:00:35,062 --> 00:00:40,120
then I'm going to bring in a browse
tool, after this R, a report output.

12
00:00:40,120 --> 00:00:44,510
What we're able to see in
this model comparison report

13
00:00:44,510 --> 00:00:49,620
is that the overall accuracy is
that 89% which is quite strong,

14
00:00:49,620 --> 00:00:53,550
but it is slightly less than
the logistic regression model.

15
00:00:54,720 --> 00:00:57,370
What I'm able to see
the confusion matrix yet again.

16
00:00:57,370 --> 00:00:59,795
How many yeses were predicted yeses,

17
00:00:59,795 --> 00:01:03,250
and how many nos were
actually predicted nos.

18
00:01:03,250 --> 00:01:07,910
So yet again it seems like the yeses
were a little more difficult to predict

19
00:01:07,910 --> 00:01:09,140
than the nos.

20
00:01:09,140 --> 00:01:14,420
This is probably because we have many
more no values than yes values, and

21
00:01:14,420 --> 00:01:17,920
yet again, we're going again to these
diagnostic plots a little bit later.

22
00:01:19,090 --> 00:01:19,710
But overall,

23
00:01:19,710 --> 00:01:24,250
this seems to be a very good model at
predicting this validation data set.

24
00:01:24,250 --> 00:01:29,250
Now the question is, which model
should we use in production to predict

25
00:01:29,250 --> 00:01:32,502
who is actually likely to respond
to the marketing campaign?
