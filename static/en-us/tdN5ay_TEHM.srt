1
00:00:00,170 --> 00:00:03,469
Now we will look at the Vapnik Chervonenkis dimension.

2
00:00:04,860 --> 00:00:08,690
This gives us a way to measure the complexity of the model.

3
00:00:08,690 --> 00:00:12,910
Remember the complexity of the model determines the performance of

4
00:00:12,910 --> 00:00:16,950
the cost on both the training and the test sets.

5
00:00:16,950 --> 00:00:21,870
There are various techniques to determine bounds on the generalization error,

6
00:00:21,870 --> 00:00:22,800
on the test set.

7
00:00:22,800 --> 00:00:27,520
A good treatment of this is given in the reference in the instructor's notes.

8
00:00:27,520 --> 00:00:30,140
Let us now look at the following expression.

9
00:00:30,140 --> 00:00:35,640
This shows that the error calculated on the test set has an upper bound.

10
00:00:35,640 --> 00:00:41,280
We can quantify a degree of belief in this upper bound using a parameter eta.

11
00:00:41,280 --> 00:00:47,945
Such that given eta for example 0.01 we have a probability of

12
00:00:47,945 --> 00:00:54,040
91% that the test error is bounded above by the expression inside the radical.

13
00:00:54,040 --> 00:00:59,250
Also notice the expression inside the radical depends on the quantity h.

14
00:01:00,350 --> 00:01:04,489
This quantity h quantifies the complexity of the model.

15
00:01:04,489 --> 00:01:06,800
Let's look at the following diagram.

16
00:01:06,800 --> 00:01:07,850
In this diagram,

17
00:01:07,850 --> 00:01:13,060
we're increasing the complexity of the model as we go along the horizontal axis.

18
00:01:13,060 --> 00:01:17,520
The cost calculated on each of the data set is on the vertical axis.

19
00:01:17,520 --> 00:01:19,230
Now if you look at the training term,

20
00:01:19,230 --> 00:01:24,050
which is the training error here, you will have curve like this.

21
00:01:24,050 --> 00:01:26,480
Looking at the term under the radical sign,

22
00:01:26,480 --> 00:01:29,580
here, is the complexity term that increases like that.

23
00:01:30,770 --> 00:01:34,980
So you see, the test error is upper bounded by this red curve.

24
00:01:36,230 --> 00:01:39,750
At some point, we can select the minimum of the test error,

25
00:01:39,750 --> 00:01:41,370
which is something over here.

26
00:01:41,370 --> 00:01:46,860
If we fix the sample size N, and the training set error is given,

27
00:01:46,860 --> 00:01:49,360
then we see that the expression here,

28
00:01:49,360 --> 00:01:55,180
depends on the quantity h which we have already introduced as the VC dimension.

29
00:01:55,180 --> 00:01:58,840
There are various ways one can calculate the VC dimension and

30
00:01:58,840 --> 00:02:02,960
the topic is covered in detail in the references and the instructor's notes.

31
00:02:04,510 --> 00:02:09,160
For general models, calculating the VC dimension is a challenging problem.

32
00:02:09,160 --> 00:02:12,920
However, for linear classifier models with m dimensions or

33
00:02:12,920 --> 00:02:17,590
variables, or features, we have a very simple expressions.

34
00:02:17,590 --> 00:02:23,204
The VC dimension is given as h which is simply m plus 1.
