1
00:00:00,370 --> 00:00:01,810
Alright, the next topic we're going to get into

2
00:00:01,810 --> 00:00:04,990
is to nail down a concept called PAC learning.

3
00:00:04,990 --> 00:00:08,220
So to do that, we're going to delve into

4
00:00:08,220 --> 00:00:10,620
what the error of a hypothesis is. And there's

5
00:00:10,620 --> 00:00:13,410
two kinds of errors. There's the training error and

6
00:00:13,410 --> 00:00:16,590
the true error. So the training error is on

7
00:00:16,590 --> 00:00:19,430
the training set. What is the fraction of those

8
00:00:19,430 --> 00:00:22,830
examples that are classifieds by some given hypotheses H.

9
00:00:22,830 --> 00:00:23,160
>> Okay.

10
00:00:23,160 --> 00:00:25,410
>> Now H could be different

11
00:00:25,410 --> 00:00:29,100
from the target concept. The target concept ought to have a training error

12
00:00:29,100 --> 00:00:34,000
of zero. But some other hypothesis h might have some error. Okay?

13
00:00:34,000 --> 00:00:34,940
>> Sure, that makes sense.

14
00:00:34,940 --> 00:00:37,420
>> Okay, so the true error is actually the fractions

15
00:00:37,420 --> 00:00:40,870
of examples that would be misclassified on a sample drawn from

16
00:00:40,870 --> 00:00:46,400
D in essentially the infinite limit. So what is the, essentially the probability

17
00:00:46,400 --> 00:00:51,120
that a sample drawn from D Would be mis-classified

18
00:00:51,120 --> 00:00:52,680
by some hypothesis, h.

19
00:00:52,680 --> 00:00:53,220
>> Okay.

20
00:00:53,220 --> 00:00:57,410
>> And we can actually write that mathematically. Error with respect to some

21
00:00:57,410 --> 00:01:01,780
distribution D of some hypothesis h, is the probability that if we draw

22
00:01:01,780 --> 00:01:06,980
the input X from that distribution that you're going to get a mismatch between

23
00:01:06,980 --> 00:01:09,400
the true label, according to the concept,

24
00:01:09,400 --> 00:01:11,240
and the hypothesis that we're currently evaluating.

25
00:01:11,240 --> 00:01:13,190
>> Right, so this sort of captures the notion

26
00:01:13,190 --> 00:01:16,650
that it's okay for you to misclassify examples

27
00:01:16,650 --> 00:01:18,090
you will never ever ever see.

28
00:01:18,090 --> 00:01:22,670
>> Yes, that's right. So we're only being penalized proportional to the

29
00:01:22,670 --> 00:01:25,590
probability of getting that thing wrong. So, for examples that we never

30
00:01:25,590 --> 00:01:28,960
see, that's fine. For examples that we see really really rarely. We

31
00:01:28,960 --> 00:01:31,420
get only a tiny little bit of contribution to the overall error.

32
00:01:31,420 --> 00:01:33,480
>> Okay, I can follow that.
