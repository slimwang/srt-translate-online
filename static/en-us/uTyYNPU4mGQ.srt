1
00:00:00,012 --> 00:00:04,582
So in short, CUDA makes few guarantees about when and where thread blocks will run,

2
00:00:04,582 --> 00:00:06,788
and this is actually a huge advantage for CUDA.

3
00:00:06,788 --> 00:00:10,368
This is one of the big reasons why GPU programs can go so fast.

4
00:00:10,368 --> 00:00:11,789
Why is that?

5
00:00:11,789 --> 00:00:14,132
So among the advantages that the GPU gains from this programing model

6
00:00:14,132 --> 00:00:17,938
is that the hardware can run things really efficiently, because it has so much flexibility.

7
00:00:17,938 --> 00:00:22,774
For example, if 1 thread block completes quickly, the SM can immediately schedule another thread block

8
00:00:22,774 --> 00:00:25,477
without waiting for any others to complete.

9
00:00:25,477 --> 00:00:27,919
But the biggest advantage is scalability.

10
00:00:27,919 --> 00:00:30,750
Because you've made no guarantees about

11
00:00:30,750 --> 00:00:32,747
where the thread blocks will run

12
00:00:32,747 --> 00:00:35,219
or how many thread blocks might be running at a time,

13
00:00:35,219 --> 00:00:39,424
that means that you can scale all the way down to a GPU that would be running with a single SM,

14
00:00:39,424 --> 00:00:41,792
something that you might find in a tablet or a cell phone,

15
00:00:41,792 --> 00:00:45,409
all the way up to the massive GPUs that are used in supercomputers.

16
00:00:45,409 --> 00:00:48,667
Importantly, the scalability also applies to future GPUs,

17
00:00:48,667 --> 00:00:52,137
so you can be sure that GPUs will get more and more SMs

18
00:00:52,137 --> 00:00:55,317
as Moore's Law gives us more and more transistors on a chip,

19
00:00:55,317 --> 00:01:01,385
and by writing the code in such a way that it can run on an arbitrary number of SMs

20
00:01:01,385 --> 00:01:03,415
and doesn't depend on a certain number of SM's,

21
00:01:03,415 --> 00:01:05,817
a certain number of thread blocks being resident at a time,

22
00:01:05,817 --> 00:01:10,305
you can be sure that your CUDA code will scale forward to larger and larger GPUs.

23
00:01:10,305 --> 00:01:14,273
So this scalability applies from cell phones to supercomputers,

24
00:01:14,273 --> 00:01:18,231
from current to future GPUs, and that's a really huge advantage.

25
00:01:18,231 --> 00:01:20,631
And there are also consequences to this programming model,

26
00:01:20,631 --> 00:01:22,840
which are the things we've been talking about.

27
00:01:22,840 --> 00:01:26,305
You can make no assumptions about what blocks will run on what SM,

28
00:01:26,305 --> 00:01:29,341
and you can't have any explicit communication between blocks.

29
00:01:29,341 --> 00:01:34,861
For example, if block x is waiting for block y to give it some result before it can proceed,

30
00:01:34,861 --> 00:01:38,584
but block y has already run to completion and exited,

31
00:01:38,584 --> 00:01:40,783
then you're going to be in a bad place.

32
00:01:40,783 --> 00:01:46,005
That, by the way, is an example of something called dead lock in parallel computing.

33
00:01:46,005 --> 00:01:48,827
This really means that threads and blocks must complete, okay?

34
00:01:48,827 --> 00:01:51,099
You can't simply have a thread that hangs around forever,

35
00:01:51,099 --> 00:01:54,098
processing input or doing something,

36
00:01:54,098 --> 00:01:56,301
because that thread must complete

37
00:01:56,301 --> 00:01:59,956
in order for the block that it's in to complete so that other threads and blocks

38
00:01:59,956 --> 00:02:02,988
can get scheduled onto that SM.
