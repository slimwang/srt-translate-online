1
00:00:00,270 --> 00:00:03,040
So what would be a piece of
information that if we knew it,

2
00:00:03,040 --> 00:00:06,490
would make it relatively straightforward
to figure out what new state we were in?

3
00:00:06,490 --> 00:00:11,600
>> Well if we knew what state we were
in before, since we have a transition

4
00:00:11,600 --> 00:00:16,180
model we know which states
we are likely to be in next.

5
00:00:16,180 --> 00:00:17,790
>> Yeah okay, let's just write that.

6
00:00:17,790 --> 00:00:18,840
That's a that's a really good start.

7
00:00:18,840 --> 00:00:19,920
So what we're saying is,

8
00:00:19,920 --> 00:00:23,680
let's break things down by
the possible state that we started in.

9
00:00:23,680 --> 00:00:26,880
And what we want to know and
we're going to weight things by,

10
00:00:26,880 --> 00:00:29,830
what's the probability that
that's the state we started in?

11
00:00:29,830 --> 00:00:31,340
Given B A Z.

12
00:00:31,340 --> 00:00:36,630
Then given that, what's the probability
of the next state given that state?

13
00:00:37,920 --> 00:00:41,320
So this is going to help us
break things down, successfully.

14
00:00:41,320 --> 00:00:42,340
So let's see if we can do that.

15
00:00:42,340 --> 00:00:46,130
So what do we know about
the probability of being in a state,

16
00:00:46,130 --> 00:00:50,950
given a belief state that we took
an action and we made some observation?

17
00:00:50,950 --> 00:00:55,400
So I think the thing to point out here
is that the observation that we saw

18
00:00:56,840 --> 00:00:58,650
is generated by the state
that we ended up in.

19
00:00:58,650 --> 00:01:02,250
And so we're actually trying to figure
out what state we are in, basing

20
00:01:02,250 --> 00:01:05,080
observation we got, when, in fact,
the generative model goes the other way.

21
00:01:05,080 --> 00:01:08,772
So this is a prime situation
to try to use Bayes' Rule.

22
00:01:08,772 --> 00:01:11,100
>> Prime.
>> [LAUGH] It's an s prime situation for

23
00:01:11,100 --> 00:01:11,830
using Bayes' Rule.

24
00:01:11,830 --> 00:01:15,180
So that gives us the probability of
making an observation, given the state

25
00:01:15,180 --> 00:01:20,920
that we're in, or that we arrived in s
prime and carrying over the a and the s.

26
00:01:20,920 --> 00:01:24,320
And the probability that that's
the state we ended in, given a and

27
00:01:24,320 --> 00:01:26,270
s, divided by the normalization factor.

28
00:01:26,270 --> 00:01:27,910
The probability of the observation,
given a and s.

29
00:01:27,910 --> 00:01:28,880
>> Right.
>> All right.

30
00:01:28,880 --> 00:01:32,040
So now we're getting really close
to quantities that we recognize.

31
00:01:32,040 --> 00:01:36,410
In particular, the probability of an
observation given the state that we just

32
00:01:36,410 --> 00:01:41,660
landed in is independent of the action
and the state that we just talked.

33
00:01:41,660 --> 00:01:43,900
So this is going to be
the observation function.

34
00:01:43,900 --> 00:01:46,590
This right here is the transition
function the probability of landing in

35
00:01:46,590 --> 00:01:50,510
some state as prime given that we
were in state S and took action A.

36
00:01:50,510 --> 00:01:52,980
And then this is just going to be
a normalization factor at the bottom.

37
00:01:52,980 --> 00:01:55,360
>> Right.
>> So substituting those quantities in

38
00:01:55,360 --> 00:01:59,330
and rewriting out this
normalization factors is basically

39
00:01:59,330 --> 00:02:03,360
the numerator divided by the sum of
all the possible next eight s prime.

40
00:02:03,360 --> 00:02:07,800
We get one component of the new police
state be prime if we apply the same idea

41
00:02:07,800 --> 00:02:09,639
over all possible states as prime.

42
00:02:09,639 --> 00:02:13,750
Then we get a probability distribution
over all the states that represents

43
00:02:13,750 --> 00:02:17,590
the likelihood of being in those states
given that we In belief state b,

44
00:02:17,590 --> 00:02:20,760
took action a, and made observation z.

45
00:02:20,760 --> 00:02:23,570
>> So that's intuitively obvious,
even to the most casual observer.

46
00:02:23,570 --> 00:02:28,045
But more importantly, it just uses
all the quantities we already had.

47
00:02:28,045 --> 00:02:31,940
>> [LAUGH] It does, it seems like
sort of when a TV series is over and

48
00:02:31,940 --> 00:02:35,040
they bring all the characters back for
one last show.

49
00:02:35,040 --> 00:02:38,960
It's like we've got to B back,
and the O back, and the Z back.

50
00:02:38,960 --> 00:02:40,550
Everybody is friends again.

51
00:02:40,550 --> 00:02:41,235
>> And it spells bOT.

52
00:02:41,235 --> 00:02:43,470
>> [LAUGH] And it spells bOT.

53
00:02:43,470 --> 00:02:47,560
So my point is that these quantities
can be easily calculated from.

54
00:02:47,560 --> 00:02:51,560
Or sorry, this quantity, this b prime
s prime can be easily calculated from

55
00:02:51,560 --> 00:02:56,010
quantities that we have lying around
because we have the model, the POMDP.

56
00:02:56,010 --> 00:02:57,800
And we know what the previous
belief state is and

57
00:02:57,800 --> 00:03:00,820
we know what action we just took and
what observation we made.

58
00:03:00,820 --> 00:03:02,560
And so all this can be updated.

59
00:03:02,560 --> 00:03:08,220
And so we can keep track of
this notion of where we are.

60
00:03:08,220 --> 00:03:11,410
This belief state notion
of the belief MDP.

61
00:03:11,410 --> 00:03:12,700
>> I like it.

62
00:03:12,700 --> 00:03:13,610
>> Cool.
All right so

63
00:03:13,610 --> 00:03:17,620
this doesn't actually tell us how to
do decision making because all we

64
00:03:17,620 --> 00:03:22,980
done actually it's we
turned a POMDP into an MDP.

65
00:03:22,980 --> 00:03:25,590
That by the way has
an infinite number of states.

66
00:03:25,590 --> 00:03:28,600
So we can you know we have a number of
algorithms that we can run like linear

67
00:03:28,600 --> 00:03:32,082
programming or policy iteration or
value iteration.

68
00:03:32,082 --> 00:03:36,530
And [LAUGH] they all at best grow
polynomial in the number of states.

69
00:03:36,530 --> 00:03:40,100
So it's polynomial in infinity,
so infinity.

70
00:03:40,100 --> 00:03:41,010
>> So it's a constant.

71
00:03:41,010 --> 00:03:41,865
>> Yeah, no.

72
00:03:41,865 --> 00:03:44,110
[LAUGH] It's bigger than any constant.

73
00:03:44,110 --> 00:03:45,350
So this is problematic.

74
00:03:45,350 --> 00:03:49,430
We can't just Take this MDP now that
we've defined it and just solve it using

75
00:03:49,430 --> 00:03:52,420
our existing algorithms, we're going to
have to be a little bit more careful.

76
00:03:52,420 --> 00:03:54,430
And so I want to step through
the algorithmic process for

77
00:03:54,430 --> 00:03:56,220
how you can actually do that.

78
00:03:56,220 --> 00:03:58,510
It's kind of nice I think.

79
00:03:58,510 --> 00:04:01,130
>> Okay, I mean, I can't wait to see
how you make infinity finity, so

80
00:04:01,130 --> 00:04:01,950
let's do it.
