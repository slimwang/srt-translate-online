1
00:00:00,000 --> 00:00:10,000
[BLANK_AUDIO]

2
00:00:14,263 --> 00:00:17,330
Let's see, let's see everybody.

3
00:00:17,330 --> 00:00:18,630
I'll minimize myself.

4
00:00:18,630 --> 00:00:19,970
I don't need to see myself.

5
00:00:19,970 --> 00:00:21,260
I need to see you guys.

6
00:00:21,260 --> 00:00:25,861
I need to see you guys.

7
00:00:25,861 --> 00:00:27,336
The world, this is Rash!

8
00:00:27,336 --> 00:00:30,170
I am hyped for this live session.

9
00:00:31,250 --> 00:00:34,700
Yo, class is in session everybody.

10
00:00:34,700 --> 00:00:38,260
We are about to do some math this
live session, so I'm super excited.

11
00:00:38,260 --> 00:00:41,570
But first of all,
let me take some roll call, all right?

12
00:00:41,570 --> 00:00:47,368
So, let's see, Collin,
Brandon, Nil, David, Dakosh,

13
00:00:47,368 --> 00:00:54,140
Sebastian, Raj, Spencer,
Naresh, Niko, Clement, hi, guys!

14
00:00:54,140 --> 00:00:57,060
Michael, Benjamin, all right.

15
00:00:57,060 --> 00:00:57,940
So, that was roll call.

16
00:00:59,690 --> 00:01:06,090
Welcome to this live session for
the deporting, inter-deporting course.

17
00:01:06,090 --> 00:01:06,380
Okay.

18
00:01:06,380 --> 00:01:08,880
This is going to be so awesome.

19
00:01:08,880 --> 00:01:11,390
Because, I have been
waiting to do some math.

20
00:01:11,390 --> 00:01:12,040
Guess what guys.

21
00:01:12,040 --> 00:01:12,620
Guess what.

22
00:01:12,620 --> 00:01:17,350
I bought this pad to write some math on.

23
00:01:17,350 --> 00:01:17,570
Okay.

24
00:01:17,570 --> 00:01:21,020
I've never used this before so,
I'm super excited for this.

25
00:01:21,020 --> 00:01:22,690
I'm going to show you guys the math.

26
00:01:22,690 --> 00:01:24,630
Behind linear regression.

27
00:01:24,630 --> 00:01:27,840
By the end of this video, you guys

28
00:01:28,920 --> 00:01:33,820
are going to know like the back of your
hand, how to do linear regression.

29
00:01:33,820 --> 00:01:36,040
That includes gradient descent.

30
00:01:36,040 --> 00:01:39,870
And guess what?, we use gradient descent
all over the place in machine learning.

31
00:01:39,870 --> 00:01:42,420
Don't worry if you don't know what
that is, I'm going to show it to you.

32
00:01:42,420 --> 00:01:43,096
Okay?

33
00:01:43,096 --> 00:01:45,100
So, we're going to deep dive into this.

34
00:01:45,100 --> 00:01:48,560
So, we're going to start
off with a five minute Q&amp;A,

35
00:01:48,560 --> 00:01:53,820
like always, and I think we've got some
Udacity peeps in the house as well.

36
00:01:53,820 --> 00:01:54,399
Drew.

37
00:01:54,399 --> 00:01:59,439
Nico, and Max, who's the other

38
00:01:59,439 --> 00:02:03,761
instructor for the course.

39
00:02:03,761 --> 00:02:07,025
So, I think you're here shout out,
say something so people

40
00:02:07,025 --> 00:02:11,121
know who you are and, so I'm going to
do my five minute Q&amp;A, like always, and

41
00:02:11,121 --> 00:02:15,153
I'm going to answer all the questions
related to me, and my everything, but

42
00:02:15,153 --> 00:02:19,330
if you have any Udacity specific
questions, they will answer those, okay?

43
00:02:19,330 --> 00:02:21,793
So, let's start up with
a five minute Q&amp;A, and

44
00:02:21,793 --> 00:02:24,751
then we're going to get right
into the code and that, okay?

45
00:02:27,540 --> 00:02:29,450
Do I have to know about
partial derivatives?

46
00:02:29,450 --> 00:02:33,520
We are going to do a partial derivative,
but I'll show you how that works.

47
00:02:33,520 --> 00:02:33,797
[BLANK_AUDIO]

48
00:02:33,797 --> 00:02:38,920
I had to cut off cutie
pie to catch this.

49
00:02:38,920 --> 00:02:41,640
Wow I'm honored, I'm honored.

50
00:02:41,640 --> 00:02:43,636
Hey baby girl,
let me see that regression.

51
00:02:43,636 --> 00:02:46,950
All right, so that's not a question.

52
00:02:46,950 --> 00:02:52,760
Let's get some real questions in there,
some quality questions.

53
00:02:52,760 --> 00:02:54,310
All right.

54
00:02:54,310 --> 00:02:58,670
[BLANK_AUDIO]

55
00:02:58,670 --> 00:03:01,460
Would you want to check out
my Vive AI Assistant demo?

56
00:03:01,460 --> 00:03:05,026
Sure, yes, post a GitHub link in
the comments of one of my videos,

57
00:03:05,026 --> 00:03:07,716
I read all my comments,
I answer all my comments.

58
00:03:07,716 --> 00:03:09,218
See, I'm not fake,
you know what I'm saying?

59
00:03:09,218 --> 00:03:10,760
I answer all my comments.

60
00:03:10,760 --> 00:03:15,160
I'm here for you guys, have I
enrolled in- is calculus required for

61
00:03:15,160 --> 00:03:15,990
linear regression?

62
00:03:15,990 --> 00:03:16,860
Yes.

63
00:03:16,860 --> 00:03:19,380
A little bit of calculus, but
I'm going to go through that.

64
00:03:19,380 --> 00:03:20,700
Don't be afraid by the word calculus.

65
00:03:20,700 --> 00:03:22,300
This is actually very intuitive.

66
00:03:22,300 --> 00:03:22,688
[BLANK_AUDIO]

67
00:03:22,688 --> 00:03:27,908
Can you mention some details
about the upcoming as well,

68
00:03:27,908 --> 00:03:31,350
looking to predict the genre from-

69
00:03:31,350 --> 00:03:40,846
[BLANK_AUDIO]

70
00:03:40,846 --> 00:03:41,934
All right.

71
00:03:41,934 --> 00:03:45,996
[BLANK_AUDIO]

72
00:03:45,996 --> 00:03:47,440
What basic maths will be needed?

73
00:03:47,440 --> 00:03:49,610
You'll need to know basic algebra, okay?

74
00:03:49,610 --> 00:03:52,870
And then we're going to learn
the calculus necessary to do this

75
00:03:52,870 --> 00:03:53,910
in this video, okay?

76
00:03:55,420 --> 00:03:56,830
Are against the future, yes.

77
00:03:56,830 --> 00:04:02,762
I mean the idea, between generate
model in general are really exciting,

78
00:04:02,762 --> 00:04:06,605
because you can generate
except don't exist.

79
00:04:06,605 --> 00:04:07,175
This.

80
00:04:07,175 --> 00:04:12,225
And that has a lot of potential for
art and culture.

81
00:04:12,225 --> 00:04:13,377
GANs can change culture, right?

82
00:04:13,377 --> 00:04:15,455
We can generate music.

83
00:04:15,455 --> 00:04:16,370
We can generate art.

84
00:04:16,370 --> 00:04:18,635
We can generate paintings in
ways that humans couldn't.

85
00:04:19,825 --> 00:04:21,625
Best book to understand math behind ML.

86
00:04:21,625 --> 00:04:23,325
Machine Learning and
Probabilistic Approach.

87
00:04:23,325 --> 00:04:25,035
That's a pretty good one.

88
00:04:25,035 --> 00:04:26,195
Just mastering or coding too.

89
00:04:26,195 --> 00:04:26,525
Both.

90
00:04:26,525 --> 00:04:27,425
Mostly coding.

91
00:04:27,425 --> 00:04:30,901
Linear regression versus
other classifiers like SGDC.

92
00:04:30,901 --> 00:04:33,190
Linear regression is definitely easier.

93
00:04:33,190 --> 00:04:34,130
What is no free lunch?

94
00:04:34,130 --> 00:04:38,179
It's a theorem, the no free lunch
theorem at a very high level it's like,

95
00:04:38,179 --> 00:04:40,120
well you can't make assumptions.

96
00:04:40,120 --> 00:04:45,045
You can't make assumptions whenever
you are doing anything related to

97
00:04:45,045 --> 00:04:46,530
proving something.

98
00:04:48,270 --> 00:04:50,580
Just, when will you do NLP?

99
00:04:50,580 --> 00:04:53,440
Yo, I'm going to do so
much NLP in this course.

100
00:04:53,440 --> 00:04:54,640
I can't wait for NLP, it's coming up.

101
00:04:54,640 --> 00:04:58,180
Will you cover GANs?

102
00:04:58,180 --> 00:05:00,730
I kind of want to just do GANs
right now, you know what I mean?

103
00:05:00,730 --> 00:05:02,970
I'm super excited for
GANs, I will do GANs.

104
00:05:02,970 --> 00:05:08,676
[BLANK_AUDIO]

105
00:05:08,676 --> 00:05:11,585
I will give an intuition why to
do graded descent over, yes,

106
00:05:11,585 --> 00:05:12,620
I will explain that.

107
00:05:13,890 --> 00:05:15,290
Linear algebra is the way to go?, yes.

108
00:05:15,290 --> 00:05:17,520
What's the difference between
cycle learn and TF learn?

109
00:05:17,520 --> 00:05:19,250
Cycle learn and TF learn,
great question.

110
00:05:19,250 --> 00:05:22,510
So, TF learn is a high level
wrapper on top of transfer flow.

111
00:05:22,510 --> 00:05:25,740
It's very similar looking
to cycle learn, but

112
00:05:25,740 --> 00:05:31,680
cycle learn specifically is, it does.

113
00:05:31,680 --> 00:05:35,042
So, TF1 only focused on
deep neural networks.

114
00:05:35,042 --> 00:05:39,280
Cycle learn uses support vectrum shades,
and all sorts of other machinery models.

115
00:05:39,280 --> 00:05:41,180
Whereas TF1 is the same kind of.

116
00:05:41,180 --> 00:05:46,210
It has the same brevity, but
it focuses only on deep neural networks.

117
00:05:46,210 --> 00:05:47,410
Do you prefer WICA?

118
00:05:47,410 --> 00:05:48,510
No.

119
00:05:48,510 --> 00:05:53,110
No, when will you start
working on Anaconda?

120
00:05:53,110 --> 00:05:59,500
I mean, I'll most likely start using
docker to contain those things.

121
00:05:59,500 --> 00:06:00,235
All right, rap for 50 case off,
let me rap for 50 case off.

122
00:06:00,235 --> 00:06:02,025
In this time,
I'm going to play an instrumental.

123
00:06:03,055 --> 00:06:05,799
I'm not going to just rap with that kind
of instrumental, you know what I mean?

124
00:06:05,799 --> 00:06:09,006
Don't be discouraged, rap,
hip hop instrumental on YouTube,

125
00:06:09,006 --> 00:06:10,475
whatever it starts playing.

126
00:06:10,475 --> 00:06:13,195
Someone say a keyword and
then we're going to get started.

127
00:06:13,195 --> 00:06:16,907
Triumph hip hop instrumental,
what is this about.

128
00:06:16,907 --> 00:06:17,154
Let's go, play it.

129
00:06:17,154 --> 00:06:20,983
All right, let me just unplug my mic, so

130
00:06:20,983 --> 00:06:25,170
you guys can see this,
where's the music.

131
00:06:25,170 --> 00:06:31,290
I'm going to say something,
you know what I'm saying?

132
00:06:31,290 --> 00:06:31,524
[MUSIC]

133
00:06:31,524 --> 00:06:32,115
50k subs.

134
00:06:32,115 --> 00:06:34,330
I got 50k subs, man,
my mind is so fresh.

135
00:06:34,330 --> 00:06:36,870
I'm looking at this coffee mess,
looks like the best.

136
00:06:36,870 --> 00:06:40,331
I got caffeine on my mind,
it takes me so high through the sky.

137
00:06:40,331 --> 00:06:42,683
I got a USB 4, my my.

138
00:06:42,683 --> 00:06:45,675
I'm going to be writing math today,
like it's all mine.

139
00:06:45,675 --> 00:06:48,589
Online, I see you man, it's all fine.

140
00:06:48,589 --> 00:06:51,163
It's all writing piece
of equations online.

141
00:06:51,163 --> 00:06:54,123
I see you coming back like threw
me your progression, wait.

142
00:06:54,123 --> 00:06:59,227
So that was it for the rap.

143
00:06:59,227 --> 00:07:01,319
Okay, so, that's it for the rap.

144
00:07:01,319 --> 00:07:01,970
So, now we're going to get
started with the code, okay?

145
00:07:01,970 --> 00:07:03,220
So, let's go ahead and do this.

146
00:07:04,900 --> 00:07:07,840
I'm going to start screen sharing,
and then we're going to get started.

147
00:07:07,840 --> 00:07:08,717
All right, here we go.

148
00:07:08,717 --> 00:07:13,340
[SOUND] Here we go, Google Hangouts.

149
00:07:13,340 --> 00:07:16,200
All right, and
what does Hangouts want to do?

150
00:07:16,200 --> 00:07:18,540
Hangouts wants to screen share.

151
00:07:19,740 --> 00:07:21,970
Hangouts wants to screen share.

152
00:07:21,970 --> 00:07:23,750
Your entire screen.

153
00:07:23,750 --> 00:07:24,702
Chair.

154
00:07:24,702 --> 00:07:30,980
All right, so I'll minimize this,
and minimize,

155
00:07:30,980 --> 00:07:36,950
and then I'll move this out of the way,
so I can see what you guys are doing.

156
00:07:36,950 --> 00:07:39,260
Okay, and we're going to code this baby.

157
00:07:39,260 --> 00:07:42,690
Okay, I am in the corner here, let
make sure that you guys are seeing is,

158
00:07:42,690 --> 00:07:43,770
what I want you to see.

159
00:07:43,770 --> 00:07:46,543
[BLANK_AUDIO]

160
00:07:46,543 --> 00:07:50,185
Yes, what you guys are seeing is exactly
what I want you to see, perfect.

161
00:07:50,185 --> 00:07:51,995
All right.

162
00:07:51,995 --> 00:07:55,977
[BLANK_AUDIO]

163
00:07:55,977 --> 00:07:58,786
Okay, so here we go.

164
00:07:58,786 --> 00:07:59,544
[BLANK_AUDIO]

165
00:07:59,544 --> 00:08:01,900
Here's what we're going to do guys,
let me make this statement [SOUND].

166
00:08:01,900 --> 00:08:02,840
This is big enough right?

167
00:08:04,140 --> 00:08:06,800
So in this lesson,
we're going to do linear regression.

168
00:08:06,800 --> 00:08:09,730
And what is linear regression, right?

169
00:08:09,730 --> 00:08:14,260
So linear regression, in this case, and
let me make sure everything's working.

170
00:08:16,140 --> 00:08:19,050
Everybody's here, live chat's working,
live video is not working.

171
00:08:19,050 --> 00:08:20,830
Okay, so here's how it goes.

172
00:08:20,830 --> 00:08:21,950
So we're going to do this, okay?

173
00:08:21,950 --> 00:08:25,420
So this is going to be
called linear regression.

174
00:08:25,420 --> 00:08:29,190
This is linear regression and
let me just show you guys.

175
00:08:29,190 --> 00:08:31,840
The best way to explain it is
to show it through visuals, so

176
00:08:31,840 --> 00:08:35,010
I'll show it through visuals,
what exactly we're going to be doing.

177
00:08:35,010 --> 00:08:38,340
And to show you visually,
I will give you a link to this, and

178
00:08:38,340 --> 00:08:39,530
I will just show it right here.

179
00:08:39,530 --> 00:08:41,150
This is what's happening.

180
00:08:41,150 --> 00:08:46,880
So we have a set of points, and these
points are the test scores of students,

181
00:08:46,880 --> 00:08:48,390
and the amount of hours studied, okay?

182
00:08:48,390 --> 00:08:49,230
So this is what it looks like.

183
00:08:49,230 --> 00:08:55,320
So this, right on the right, this graph
here, these set of points are the set.

184
00:08:55,320 --> 00:08:58,250
The x values are the amount
of hours they studied and

185
00:08:58,250 --> 00:09:00,360
the y values are the test
scores they got.

186
00:09:00,360 --> 00:09:00,870
Okay.

187
00:09:00,870 --> 00:09:02,070
And intuitively, to us,

188
00:09:02,070 --> 00:09:04,970
there must be some kind of
correlation between these two values.

189
00:09:04,970 --> 00:09:08,580
But we want to prove this
programmatically, we want to prove this,

190
00:09:08,580 --> 00:09:11,480
I'm sorry, mathematically, we want to
prove that there is a relationship.

191
00:09:11,480 --> 00:09:14,200
And how do we prove that
there is a relationship?

192
00:09:14,200 --> 00:09:15,680
We draw a line of best fit.

193
00:09:15,680 --> 00:09:19,790
So how do we know what that line of best
fit is, or that linear regression is?

194
00:09:19,790 --> 00:09:23,050
Well we don't know, we don't know.

195
00:09:23,050 --> 00:09:24,560
We have to find that, and

196
00:09:24,560 --> 00:09:28,110
the way we're going to find the line
of best fit is using gradient descent.

197
00:09:28,110 --> 00:09:31,380
And that process,
that training process looks like this.

198
00:09:31,380 --> 00:09:34,560
We're going to draw a random line,
compute the error for that line.

199
00:09:34,560 --> 00:09:36,930
And I'll talk about how we're
going to compute that error.

200
00:09:36,930 --> 00:09:41,840
And that error value is going to say
how well-fit is this line to the data?

201
00:09:41,840 --> 00:09:43,830
And then based on that error,
it's going to act as a compass.

202
00:09:43,830 --> 00:09:45,670
It's going to tell us, well,

203
00:09:45,670 --> 00:09:50,360
how best should you re draw the lines
to be closer to the line invested.

204
00:09:50,360 --> 00:09:51,940
And we'll keep doing that.

205
00:09:51,940 --> 00:09:54,910
So, it'll be like draw a line, compute
error, draw a line, compute error, until

206
00:09:54,910 --> 00:09:59,460
eventually the line that we draw is the
optimal line that we should draw, okay?

207
00:09:59,460 --> 00:10:00,560
So, that's at a very high level.

208
00:10:00,560 --> 00:10:02,380
But now I'm going to
go into the code and

209
00:10:02,380 --> 00:10:03,740
we're going to talk
about this in detail.

210
00:10:03,740 --> 00:10:05,405
All right, so
lets go ahead and start it.

211
00:10:05,405 --> 00:10:07,769
[BLANK_AUDIO]

212
00:10:07,769 --> 00:10:13,275
So to start off, to start off I'm
going to write my main function, okay?

213
00:10:13,275 --> 00:10:17,250
So let me move all this stuff out of the
way, so I'll get right into the code,

214
00:10:17,250 --> 00:10:17,892
all right?

215
00:10:17,892 --> 00:10:20,730
I'll get right into the code.

216
00:10:22,100 --> 00:10:27,000
And guys, if people have questions and
I'm not able to answer them because

217
00:10:27,000 --> 00:10:29,860
I'm busy doing something,
please help me answer questions.

218
00:10:29,860 --> 00:10:32,120
I very much appreciate it.

219
00:10:32,120 --> 00:10:33,460
I very much appreciate it.

220
00:10:33,460 --> 00:10:33,580
Okay?

221
00:10:33,580 --> 00:10:36,280
So let me just start off by
writing the main function.

222
00:10:36,280 --> 00:10:38,400
What does the main function do?

223
00:10:38,400 --> 00:10:40,340
That's where the meat of the code goes.

224
00:10:40,340 --> 00:10:44,216
Right, okay so in the main function,
we'll write a run function,

225
00:10:44,216 --> 00:10:47,170
which is where we're going to
store all of our logic.

226
00:10:47,170 --> 00:10:49,070
Okay, so let's write up a run function.

227
00:10:49,070 --> 00:10:53,160
So the run function is a chance for us
to show what we're doing at high levels,

228
00:10:53,160 --> 00:10:53,890
at a high level.

229
00:10:53,890 --> 00:10:56,240
So step one, is collect our data, right?

230
00:10:56,240 --> 00:10:59,450
Always in machine learning,
we want to collect our data.

231
00:10:59,450 --> 00:11:00,870
So we'll get our data points.

232
00:11:00,870 --> 00:11:05,050
And what we're going to do, how are we
going to collect our data, right?

233
00:11:05,050 --> 00:11:07,600
Well, to collect our data, we have to
import the one library that we're using.

234
00:11:07,600 --> 00:11:10,180
I know guys,
we're using a single library.

235
00:11:10,180 --> 00:11:13,240
And that library is NumPy, all right?

236
00:11:13,240 --> 00:11:16,070
And we're going to use this little
symbol that means we don't have to

237
00:11:16,070 --> 00:11:19,860
continually say NumPy whenever we
call its method or its functions.

238
00:11:19,860 --> 00:11:22,446
Okay, so what is the function
we're going to use for NumPy?

239
00:11:22,446 --> 00:11:25,956
So the function we're going to use for
NumPy,

240
00:11:25,956 --> 00:11:30,485
I'm sorry, right, main,
thank you, main, good call.

241
00:11:30,485 --> 00:11:33,636
So, the function we're going to use for
NumPy is genfromtxt().

242
00:11:33,636 --> 00:11:34,956
And what this is going to do,

243
00:11:34,956 --> 00:11:37,600
is it's going to get the data
point from our data file.

244
00:11:37,600 --> 00:11:40,380
And let me show you guys
the data file as well.

245
00:11:40,380 --> 00:11:43,810
But basically we're going to
separate it by the compass.

246
00:11:43,810 --> 00:11:46,290
Okay, and
we're going to get those points.

247
00:11:46,290 --> 00:11:48,150
So what does this,
what does this data look like?

248
00:11:48,150 --> 00:11:49,810
Well, let me pull up terminal, and

249
00:11:49,810 --> 00:11:53,090
show you guys exactly what
this data looks like.

250
00:11:53,090 --> 00:11:57,000
So it looks like beta.

251
00:11:57,000 --> 00:11:57,430
Okay?

252
00:11:57,430 --> 00:11:58,900
So let me zoom in on this.

253
00:12:01,650 --> 00:12:02,720
Zoom, way more.

254
00:12:02,720 --> 00:12:03,380
200 zoom.

255
00:12:03,380 --> 00:12:07,230
So these are just the hours studied,
on the left side, and then the test

256
00:12:07,230 --> 00:12:10,590
scores for a bunch of students, for
an intro to computer science class.

257
00:12:10,590 --> 00:12:10,910
Okay?

258
00:12:10,910 --> 00:12:15,490
The hours studied and
the test scores they got.

259
00:12:15,490 --> 00:12:16,560
Okay, so
that's what we're going to pull.

260
00:12:16,560 --> 00:12:17,140
That's our data set.

261
00:12:17,140 --> 00:12:21,400
That's what we're going to
pull into our points variable.

262
00:12:21,400 --> 00:12:25,020
So, points is going to contain
a bunch of xy value pairs.

263
00:12:25,020 --> 00:12:29,180
Where x is the amount of hours
studied and y is the test score.

264
00:12:29,180 --> 00:12:29,430
Okay?

265
00:12:29,430 --> 00:12:33,270
And it's separated by the comma.

266
00:12:33,270 --> 00:12:34,890
Okay, so that's step one.

267
00:12:34,890 --> 00:12:38,091
We've done that, and genfromtext is
essentially running two main loops.

268
00:12:38,091 --> 00:12:40,690
The first loop converts each line
of the to a sequence of strings.

269
00:12:40,690 --> 00:12:44,620
And the second one is converting each
string to the appropriate data type.

270
00:12:44,620 --> 00:12:45,680
Okay, so that's step one.

271
00:12:45,680 --> 00:12:49,410
Now, step two is to define
our hyperparameters.

272
00:12:49,410 --> 00:12:53,790
Okay, in machine learning, we have
what are called hyper-parameters.

273
00:12:53,790 --> 00:12:55,840
These are tuning nuts for our model.

274
00:12:55,840 --> 00:13:01,140
They are basically the parameters
that define how our model is

275
00:13:01,140 --> 00:13:02,250
analyzing certain data.

276
00:13:02,250 --> 00:13:04,680
How fast it's spinning through the data.

277
00:13:04,680 --> 00:13:07,350
What operations performing on the data.

278
00:13:07,350 --> 00:13:09,680
There's a whole bunch
of hyper-parameters.

279
00:13:09,680 --> 00:13:13,010
Thank you for the feedback.

280
00:13:13,010 --> 00:13:14,390
There's a whole bunch
of hyper parameters and

281
00:13:15,490 --> 00:13:18,670
what we're going to use
is the learning rates.

282
00:13:18,670 --> 00:13:24,592
Now the learning rate is used
a lot in machine learning,

283
00:13:24,592 --> 00:13:31,151
and it basically defines how
fast should our model converge?

284
00:13:31,151 --> 00:13:35,438
Convergence means when you
get the optimal result,

285
00:13:35,438 --> 00:13:39,970
the optimal model,
the line of best fit, in our case.

286
00:13:39,970 --> 00:13:41,500
That is convergence.

287
00:13:41,500 --> 00:13:42,850
So how fast should we converge?

288
00:13:42,850 --> 00:13:45,609
You might be thinking, well, shouldn't
the learning rate just be a million,

289
00:13:45,609 --> 00:13:46,855
if you want to converge super fast?

290
00:13:46,855 --> 00:13:47,640
Well, no.

291
00:13:47,640 --> 00:13:50,330
Like all hyper-parameters,
it's a balance, okay?

292
00:13:50,330 --> 00:13:53,750
So if the learning rate is too small,
we're going to get slow convergence.

293
00:13:53,750 --> 00:13:58,010
But if it's too big, then our error
function might not decrease, okay?

294
00:13:58,010 --> 00:14:00,350
So it might not converge.

295
00:14:00,350 --> 00:14:02,578
So, that's our first hyper-parameter.

296
00:14:02,578 --> 00:14:06,060
Our next hyper-parameter is going
to be the initial value for

297
00:14:06,060 --> 00:14:08,570
b, and the initial value for m.

298
00:14:08,570 --> 00:14:10,210
And what is b and m?

299
00:14:10,210 --> 00:14:13,860
Well what we're going to do, is we're
going to calculate the slope, right?

300
00:14:13,860 --> 00:14:16,490
So this looks like a y equals mx plus b,
and so

301
00:14:16,490 --> 00:14:19,980
this is why I said we only
need to know basic algebra.

302
00:14:19,980 --> 00:14:24,220
This is the formula,
this is the slope formula, okay?

303
00:14:24,220 --> 00:14:27,170
All lines follow this formula, where y.

304
00:14:27,170 --> 00:14:31,110
So, m is the slope, b is the y
intercept, x and y are the points.

305
00:14:31,110 --> 00:14:32,660
Okay, so that's the line, okay?

306
00:14:32,660 --> 00:14:36,540
So, this our initial b value,
our initial slope, and

307
00:14:36,540 --> 00:14:38,000
our initial y intercept.

308
00:14:38,000 --> 00:14:40,860
They're going to start off as 0, okay?

309
00:14:40,860 --> 00:14:45,930
So, and then the last type of parameter
is going to be the number of iterations.

310
00:14:45,930 --> 00:14:49,600
How much do we want to train this model?

311
00:14:49,600 --> 00:14:52,240
Well, we have a very,
very small data set.

312
00:14:52,240 --> 00:14:55,280
There's only a 100 points, okay.

313
00:14:55,280 --> 00:14:57,760
And for that,

314
00:14:57,760 --> 00:15:00,240
we're not going to need to iterate
a million times or 100,000 times.

315
00:15:00,240 --> 00:15:02,790
We're just going to iterate 1,000 times.

316
00:15:02,790 --> 00:15:03,010
Okay?

317
00:15:03,010 --> 00:15:06,755
So that's our hyper-parameters, and

318
00:15:06,755 --> 00:15:12,040
now step three is going to be to fit,
train our models.

319
00:15:12,040 --> 00:15:13,660
It's train our model.

320
00:15:13,660 --> 00:15:14,760
Train our model.

321
00:15:14,760 --> 00:15:15,270
Okay, so

322
00:15:15,270 --> 00:15:20,590
the first step is going to be to show
the starting gradient descent, okay?

323
00:15:20,590 --> 00:15:24,130
At b equals,
what is the starting gradient descent?

324
00:15:24,130 --> 00:15:25,570
It's going to be zero, right?

325
00:15:25,570 --> 00:15:31,680
And then m is going to be the starting
point, for that we'll say one.

326
00:15:31,680 --> 00:15:34,450
And this is just for
us to see the difference here, okay?

327
00:15:34,450 --> 00:15:40,117
[BLANK_AUDIO]

328
00:15:40,117 --> 00:15:48,007
All right, .format(initial_b,
initial_m).

329
00:15:48,007 --> 00:15:51,656
And so, what's happening here?

330
00:15:51,656 --> 00:15:56,484
[BLANK_AUDIO]

331
00:15:56,484 --> 00:16:03,213
Compute error, for_line_given_points.

332
00:16:03,213 --> 00:16:06,704
So, all right, let me just write
this out and I'll explain.

333
00:16:06,704 --> 00:16:12,260
initial_b, initial_m,
and then the points.

334
00:16:12,260 --> 00:16:14,120
Okay, so what's happening here?

335
00:16:14,120 --> 00:16:15,635
Let's go over what I just wrote here.

336
00:16:15,635 --> 00:16:19,060
So, in this line, we're going to show
the starting b value, the starting m

337
00:16:19,060 --> 00:16:22,770
value, so what is our starting
y-intercept, what is our starting slope.

338
00:16:22,770 --> 00:16:24,200
And what is our starting error?

339
00:16:24,200 --> 00:16:26,679
And I'm going to show you how we're
going to calculate that error.

340
00:16:26,679 --> 00:16:30,050
And to get that error,
given our b and m values,

341
00:16:30,050 --> 00:16:35,969
we have this function here called
compute_error_for_line_given_points.

342
00:16:35,969 --> 00:16:37,793
It's going to take the b,
m and the points, and

343
00:16:37,793 --> 00:16:40,542
it's going to compute the error for
that and it's going to out put that.

344
00:16:40,542 --> 00:16:42,870
So, that's going to be
our starting point, okay?

345
00:16:42,870 --> 00:16:48,840
And then, now, we're going to actually
perform our gradient descent,

346
00:16:48,840 --> 00:16:54,027
and it's going to give us
the optimal b and the optimal slope.

347
00:16:54,027 --> 00:16:57,068
I'm sorry, it's going to go to the
optimal slope and the optimal y descent.

348
00:16:57,068 --> 00:17:00,358
So, for gradient descent,
we're going to call this method

349
00:17:00,358 --> 00:17:03,760
the gradient_descent_runner,
so a given point.

350
00:17:03,760 --> 00:17:09,304
Given an initial b value, I'm sorry
initial m value given our learning rate,

351
00:17:09,304 --> 00:17:13,839
so this is where we're going to
use all that kind of parameters,

352
00:17:13,839 --> 00:17:18,136
right?, because this is where
we're training our model.

353
00:17:18,136 --> 00:17:19,770
So, number of iterations.

354
00:17:19,770 --> 00:17:22,010
Those are all the things we need for
this, okay?, and

355
00:17:22,010 --> 00:17:23,740
we're going to define this
function in a second.

356
00:17:23,740 --> 00:17:27,579
We're going to go deep dive and
define these functions.

357
00:17:27,579 --> 00:17:31,340
Okay, so then after we print our model,
well now we can just print it out,

358
00:17:31,340 --> 00:17:31,530
right?

359
00:17:31,530 --> 00:17:33,045
So, let me just copy and paste this.

360
00:17:33,045 --> 00:17:37,989
So, now, this is not our starting point,
this is now our ending gradient,

361
00:17:37,989 --> 00:17:39,004
ending point.

362
00:17:39,004 --> 00:17:46,120
So, face our ending point where b is
two, m is two and then error is three.

363
00:17:46,120 --> 00:17:47,980
And this number just define.

364
00:17:50,780 --> 00:17:54,024
What we're going to see at the end.

365
00:17:54,024 --> 00:17:58,394
For the number of iterations for b.

366
00:17:58,394 --> 00:17:59,053
And then-

367
00:17:59,053 --> 00:18:02,780
[BLANK_AUDIO]

368
00:18:02,780 --> 00:18:07,376
For m and then for computing the error
for line at given points given that

369
00:18:07,376 --> 00:18:11,700
the final b, the final m value,
and then our points.

370
00:18:11,700 --> 00:18:12,232
Okay, so.

371
00:18:12,232 --> 00:18:15,286
[BLANK_AUDIO]

372
00:18:15,286 --> 00:18:19,739
Okay, so that is high level,
what's happening here?

373
00:18:19,739 --> 00:18:22,276
So, all I did was I just
printed out the initial b and

374
00:18:22,276 --> 00:18:24,872
m value, which is nothing,
and then the error, and

375
00:18:24,872 --> 00:18:28,610
then I computed the rate of descent,
and then I print out the final values.

376
00:18:28,610 --> 00:18:30,828
So, I'm about to do this now.

377
00:18:30,828 --> 00:18:33,445
Okay, so we haven't actually done this,
now we're going to do it.

378
00:18:33,445 --> 00:18:35,665
So, the first thing I'm
going to talk about is,

379
00:18:35,665 --> 00:18:37,155
how we going to compute that error.

380
00:18:37,155 --> 00:18:40,583
Let's write at that first function.

381
00:18:40,583 --> 00:18:42,105
What was that first function called?

382
00:18:42,105 --> 00:18:46,255
It was called
compute_error_for_line_given_points.

383
00:18:49,690 --> 00:18:53,060
Okay, so and the data set I'm
going to provide that as well,

384
00:18:53,060 --> 00:18:55,960
but let's go ahead and
run up this method okay?

385
00:18:55,960 --> 00:18:57,244
So, this is the first step.

386
00:18:57,244 --> 00:18:58,670
We're going to write up this method.

387
00:18:58,670 --> 00:19:00,300
Compute error for line at given points.

388
00:19:00,300 --> 00:19:02,429
Okay, I'm so
excited to show you guys this,

389
00:19:02,429 --> 00:19:04,511
because I get to use my math pad for
a second.

390
00:19:04,511 --> 00:19:07,628
Okay, so let me write this out,
okay?, hold on.

391
00:19:07,628 --> 00:19:09,714
Okay, here we go.

392
00:19:09,714 --> 00:19:14,542
So, let me write this out.

393
00:19:14,542 --> 00:19:20,458
Okay, so we've got a line here.

394
00:19:20,458 --> 00:19:24,588
Man, what a great line that is.

395
00:19:24,588 --> 00:19:26,621
Okay, so this is our plot, okay?

396
00:19:26,621 --> 00:19:28,527
And, so
we've got a bunch of data points here.

397
00:19:28,527 --> 00:19:30,380
We've got a bunch of data points.

398
00:19:30,380 --> 00:19:32,864
Write this all over the place and

399
00:19:32,864 --> 00:19:37,742
what we are going to do is to draw
a random line through the data.

400
00:19:37,742 --> 00:19:39,282
We don't know the line invested, so

401
00:19:39,282 --> 00:19:41,429
we are going to draw a random
line through the data.

402
00:19:41,429 --> 00:19:45,161
And then, we are going to compute
the error of that line, so

403
00:19:45,161 --> 00:19:47,936
that error will tell us
how good our line is.

404
00:19:47,936 --> 00:19:50,610
Okay, so
how do we know how good our line is?

405
00:19:50,610 --> 00:19:54,840
But what we're going to do is, we're
going to go for every single y value,

406
00:19:54,840 --> 00:20:00,050
on that line we're going to calculate
the distance from each point

407
00:20:00,050 --> 00:20:01,930
from our data to the line.

408
00:20:01,930 --> 00:20:06,660
Okay, so all of these distances,
all of these distances, distance one,

409
00:20:06,660 --> 00:20:10,340
distance two, distance three, distance
four, distance five, distance six and

410
00:20:10,340 --> 00:20:13,020
then you probably have more data
points down here, these distances,

411
00:20:13,020 --> 00:20:14,630
the distance to this line.

412
00:20:14,630 --> 00:20:18,560
And, so we're going to take all those
distances and we want to sum them.

413
00:20:18,560 --> 00:20:21,850
And, so let me show you the equation for
that, okay?

414
00:20:21,850 --> 00:20:26,174
So, rather than actually
writing out this equation,

415
00:20:26,174 --> 00:20:31,726
like really sloppily, I'm going to
show it to you using this, okay?

416
00:20:31,726 --> 00:20:32,610
So, okay.

417
00:20:32,610 --> 00:20:33,920
So, this is the equation.

418
00:20:33,920 --> 00:20:36,144
So, let me explain what this is.

419
00:20:36,144 --> 00:20:39,281
So, we got all those distances,
right?, we got all those distances.

420
00:20:39,281 --> 00:20:41,976
We're going to sum those
distances together and

421
00:20:41,976 --> 00:20:44,020
that, and I get the average of that.

422
00:20:44,020 --> 00:20:46,943
But guess what, we're not just
going to sum those values alone,

423
00:20:46,943 --> 00:20:48,723
we're going to square those values.

424
00:20:48,723 --> 00:20:50,247
And why are we squaring those values?

425
00:20:50,247 --> 00:20:54,797
Because, we're squaring those values,
because we want it first of all to be

426
00:20:54,797 --> 00:20:58,790
positive, and it doesn't really
matter what the actual value is.

427
00:20:58,790 --> 00:21:01,980
It's more about the magnitude
of those values, right?

428
00:21:01,980 --> 00:21:03,720
And we want to minimize
that magnitude over time.

429
00:21:03,720 --> 00:21:05,020
So, this is the equation for that.

430
00:21:05,020 --> 00:21:08,088
Okay, so let me explain what
the hell this is, okay?

431
00:21:08,088 --> 00:21:10,220
So, we're computing the error.

432
00:21:10,220 --> 00:21:13,586
We are computing the error
of our line given m and b.

433
00:21:13,586 --> 00:21:16,950
So, given m and b we are going to
compute the error of our line.

434
00:21:16,950 --> 00:21:19,220
M is our slope and b is our y intercept.

435
00:21:19,220 --> 00:21:23,011
So, this E, looking thing,
is called sigma notation.

436
00:21:23,011 --> 00:21:26,115
It's a little weird,
giving you guys a little refresher here.

437
00:21:26,115 --> 00:21:26,525
This E thing,

438
00:21:26,525 --> 00:21:31,235
we're going to see it a lot in machine
learning, it's called sigma notation.

439
00:21:31,235 --> 00:21:33,735
And basically it's a way of describing,

440
00:21:33,735 --> 00:21:38,505
calculating the sum of a set of values,
all right?

441
00:21:38,505 --> 00:21:41,470
So, the sum of a set of values,
which is what we're doing.

442
00:21:41,470 --> 00:21:45,703
We're calculating the sum of a set
of points, so if the starting point

443
00:21:45,703 --> 00:21:50,210
is where i equals 1 and the ending
point, and N is for every point.

444
00:21:50,210 --> 00:21:54,074
Okay, so for every point, you want to
calculate the difference in y values.

445
00:21:54,074 --> 00:21:57,392
So, it's y-(mx+b).

446
00:21:57,392 --> 00:21:58,989
And why do we say (mx+b)?

447
00:21:58,989 --> 00:22:03,064
Because in the sub equation,
N y equals (mx+b) right?

448
00:22:03,064 --> 00:22:08,876
So, it's y-(mx+b),
which essentially boils down to just y.

449
00:22:08,876 --> 00:22:11,541
So, it's y minus y squared.

450
00:22:11,541 --> 00:22:14,990
And then we're doing that for
every single point.

451
00:22:14,990 --> 00:22:17,750
And, so we're going to add
all of those points together.

452
00:22:17,750 --> 00:22:20,020
Okay, and then get the average.

453
00:22:20,020 --> 00:22:21,448
And, so that why 1/N.

454
00:22:21,448 --> 00:22:23,390
Because we're going to
get the average of that.

455
00:22:23,390 --> 00:22:24,757
And that's value.

456
00:22:24,757 --> 00:22:26,780
That value is the error.

457
00:22:26,780 --> 00:22:29,287
Okay?, so at high level,
that is what that is.

458
00:22:29,287 --> 00:22:31,879
So, now let's programmatically
write this out, okay?

459
00:22:31,879 --> 00:22:38,230
So, we're going to start by initializing
the error, initialize it at zero.

460
00:22:38,230 --> 00:22:40,929
Okay?, so our total error at
the start is just going to be zero.

461
00:22:40,929 --> 00:22:42,318
There's not anything that's-

462
00:22:42,318 --> 00:22:46,164
[BLANK_AUDIO]

463
00:22:46,164 --> 00:22:47,561
We don't have an error yet, okay?

464
00:22:47,561 --> 00:22:52,674
So, then for every point, so for
i in range of starting at zero,

465
00:22:52,674 --> 00:22:56,862
and then going for
the length of the points, right?

466
00:22:56,862 --> 00:23:00,234
So all of our data points, so for
every data point that we have.

467
00:23:00,234 --> 00:23:05,467
We're going to say,
let's get the x value,

468
00:23:05,467 --> 00:23:08,509
so x = points [i, 0].

469
00:23:08,509 --> 00:23:10,580
And then we're going to
get that y value, right?

470
00:23:10,580 --> 00:23:13,540
So, get the y value, right?

471
00:23:13,540 --> 00:23:17,300
So, I'm just basically
programmatically showing what I just

472
00:23:19,190 --> 00:23:21,000
talked about mathematically.

473
00:23:21,000 --> 00:23:21,190
Right?

474
00:23:21,190 --> 00:23:24,380
So, we've got the x value,
we've got the y value.

475
00:23:24,380 --> 00:23:26,842
And we want to compute that distance,
right?

476
00:23:26,842 --> 00:23:28,563
We're going to do this
every single time.

477
00:23:28,563 --> 00:23:32,933
[BLANK_AUDIO]

478
00:23:32,933 --> 00:23:34,359
Then get the difference.

479
00:23:34,359 --> 00:23:36,771
[BLANK_AUDIO]

480
00:23:36,771 --> 00:23:40,086
Square it, and

481
00:23:40,086 --> 00:23:45,450
then add it to the total.

482
00:23:45,450 --> 00:23:48,523
Okay, so
here's the actual equation, right?

483
00:23:48,523 --> 00:23:52,463
So, we're going to do plus equal,
because it's a summation, and we're

484
00:23:52,463 --> 00:23:56,671
going to programmatically show what I
just talked about right here, right?

485
00:23:56,671 --> 00:23:59,728
y-(mx+b) squared.

486
00:23:59,728 --> 00:24:00,643
Okay?

487
00:24:00,643 --> 00:24:02,978
And we're going to get the sum of that.

488
00:24:02,978 --> 00:24:11,449
So, y-(m * x + b) squared, okay?

489
00:24:11,449 --> 00:24:15,439
And we're going to do that for
every point, so this whole iteration

490
00:24:15,439 --> 00:24:19,640
loop right here, is that equation,
okay?, minus the average part.

491
00:24:19,640 --> 00:24:21,660
So, that's going to give
us the total value.

492
00:24:21,660 --> 00:24:23,033
The last part is to average it.

493
00:24:23,033 --> 00:24:27,872
So, we'll take totalError
/ float [len[points]).

494
00:24:27,872 --> 00:24:28,779
So, we want it to be a float value.

495
00:24:28,779 --> 00:24:31,233
[BLANK_AUDIO]

496
00:24:31,233 --> 00:24:33,620
And that is the equation.

497
00:24:33,620 --> 00:24:35,100
That is the equation right there.

498
00:24:35,100 --> 00:24:36,626
Okay, so and then get the average.

499
00:24:36,626 --> 00:24:37,684
Get the average.

500
00:24:37,684 --> 00:24:41,254
[BLANK_AUDIO]

501
00:24:41,254 --> 00:24:44,187
So, this ten line
function just described,

502
00:24:44,187 --> 00:24:48,130
what I talked about right here
in this math equation, okay?

503
00:24:49,152 --> 00:24:53,350
We sum all the distances between all
those points, as I showed right here.

504
00:24:53,350 --> 00:24:58,750
We summed them all up, we squared
them and then we got the average.

505
00:24:58,750 --> 00:24:59,900
And that is our error.

506
00:24:59,900 --> 00:25:00,220
Okay?

507
00:25:01,755 --> 00:25:06,795
And we're calculating that,
because we want a way for

508
00:25:06,795 --> 00:25:09,705
us, a measure of us,
something to minimize over time.

509
00:25:09,705 --> 00:25:10,185
Right?

510
00:25:10,185 --> 00:25:12,035
Something to minimize every
time we redraw our line,

511
00:25:12,035 --> 00:25:13,645
we want to minimize this error.

512
00:25:13,645 --> 00:25:17,255
Because this error basically is
a signal, it's a compass for us.

513
00:25:17,255 --> 00:25:19,894
It's telling us,
this is how bad your line is.

514
00:25:19,894 --> 00:25:20,480
It needs to get better.

515
00:25:20,480 --> 00:25:22,185
You need to make me smaller.

516
00:25:22,185 --> 00:25:24,045
I'm really big right now,
make me smaller.

517
00:25:24,045 --> 00:25:26,140
And that's what gradient descent does.

518
00:25:26,140 --> 00:25:27,350
That's what gradient descent does.

519
00:25:27,350 --> 00:25:30,963
And I'm going to explain how
gradient descent works in a second.

520
00:25:30,963 --> 00:25:32,840
But that's that curves function, right?

521
00:25:32,840 --> 00:25:35,130
Okay?, what was the second
function we wrote?

522
00:25:35,130 --> 00:25:37,373
It was called gradient descent runner.

523
00:25:37,373 --> 00:25:41,090
So, this is our actual
brain descent function.

524
00:25:41,090 --> 00:25:42,783
So, now let's write this out.

525
00:25:42,783 --> 00:25:46,781
Okay?, this is our second of
three methods, before we're done.

526
00:25:46,781 --> 00:25:51,600
So, gradient_descent_runner.

527
00:25:51,600 --> 00:25:54,380
So, given a set of points,
given a starting value for

528
00:25:54,380 --> 00:25:58,030
b, given a starting value for m,

529
00:26:00,990 --> 00:26:04,200
given our learning rates and
given our number of iterations.

530
00:26:04,200 --> 00:26:09,078
We're going to use all of these things
to calculate gradient descents.

531
00:26:09,078 --> 00:26:10,604
We're going to use every single thing.

532
00:26:10,604 --> 00:26:11,236
Okay?

533
00:26:11,236 --> 00:26:12,931
[BLANK_AUDIO]

534
00:26:12,931 --> 00:26:13,391
Okay.

535
00:26:13,391 --> 00:26:16,606
So, let's get that starting b and
m value, okay?

536
00:26:16,606 --> 00:26:19,890
So, the starting value for
b, we're going to say to b.

537
00:26:19,890 --> 00:26:23,345
And the starting value for
m, we're going to say to m.

538
00:26:23,345 --> 00:26:24,066
Okay?

539
00:26:24,066 --> 00:26:24,761
Simple enough.

540
00:26:24,761 --> 00:26:28,083
And now,
we're going to perform gradient descent.

541
00:26:28,083 --> 00:26:29,472
What is gradient descent?

542
00:26:29,472 --> 00:26:31,251
I cannot wait to explain
gradient descent, guys.

543
00:26:31,251 --> 00:26:33,834
I found the perfect analogy for gradient
descent, and I'm really excited.

544
00:26:33,834 --> 00:26:41,290
Okay, before I explain that.

545
00:26:41,290 --> 00:26:45,045
Let's just perform that you can erase
this, because the actual math is going

546
00:26:45,045 --> 00:26:47,718
to start in the last function
that I'm about to write.

547
00:26:47,718 --> 00:26:50,167
So, for
every single iteration that we define,

548
00:26:50,167 --> 00:26:53,164
we're going to perform what's
called gradient descent.

549
00:26:53,164 --> 00:26:57,851
So, we're going to update b and
m with the new more accurate b and

550
00:26:57,851 --> 00:27:01,440
m by performing a gradient descent.

551
00:27:01,440 --> 00:27:04,892
By performing this gradient step, okay?

552
00:27:04,892 --> 00:27:12,067
So, b and m, we're going to returned b
and m by performing this gradient step.

553
00:27:12,067 --> 00:27:15,450
We can already explain,
this is where the math is happening.

554
00:27:15,450 --> 00:27:16,542
Given out current b,

555
00:27:16,542 --> 00:27:19,404
our current m, given r the array
of points that we have.

556
00:27:19,404 --> 00:27:21,160
And then finally given
the learning rate.

557
00:27:22,490 --> 00:27:25,040
We're going to calculate
that final value of b and m.

558
00:27:25,040 --> 00:27:26,270
And guess what?

559
00:27:26,270 --> 00:27:27,980
Once this gradient descent is done.

560
00:27:27,980 --> 00:27:31,360
We're going to return that optimal e and
f, right?

561
00:27:31,360 --> 00:27:34,120
And, so that's what we talked
about at the starting part, right?

562
00:27:34,120 --> 00:27:35,516
We returned that optimal b and
m and value.

563
00:27:35,516 --> 00:27:39,463
And before the gradient descent,
and then we then printed it out,

564
00:27:39,463 --> 00:27:42,998
because that optimal b and
m value gave us a line of best fit.

565
00:27:42,998 --> 00:27:46,500
We plug them into the y =
(mx+b) equate the formula.

566
00:27:46,500 --> 00:27:48,050
It gave us the line of best fit.

567
00:27:48,050 --> 00:27:51,160
So, now we're going to write
out the gradient step.

568
00:27:51,160 --> 00:27:55,430
And this is gradient
mother f-ing descent.

569
00:27:55,430 --> 00:27:58,627
Okay, so
this is how it's going to go down, okay?

570
00:27:58,627 --> 00:28:01,640
Here's how it's going to go down,
step_gradient.

571
00:28:01,640 --> 00:28:05,650
So, I'm just going to say, it's time for
the magic, the magic, the greatest,

572
00:28:05,650 --> 00:28:08,040
the greatest, okay?

573
00:28:08,040 --> 00:28:11,714
So, that's how excited am I,
just wrote the greatest twice.

574
00:28:11,714 --> 00:28:12,821
Okay, [LAUGH].

575
00:28:12,821 --> 00:28:18,960
So, given our current b and
m values points and the learningRates.

576
00:28:18,960 --> 00:28:25,516
And this actually isn't going to
help with that, so I'll delete that.

577
00:28:25,516 --> 00:28:27,022
So, here are learningRates, okay?

578
00:28:27,022 --> 00:28:27,956
Let's perform gradient_descent.

579
00:28:27,956 --> 00:28:29,982
So, okay, what is gradient_descent?

580
00:28:29,982 --> 00:28:32,343
Okay, so let me show you guys this.

581
00:28:32,343 --> 00:28:37,149
[SOUND]
How best do I describe this?

582
00:28:37,149 --> 00:28:38,108
So, we have.

583
00:28:38,108 --> 00:28:40,841
[BLANK_AUDIO]

584
00:28:40,841 --> 00:28:43,423
Let me just show you this image.

585
00:28:43,423 --> 00:28:44,320
This is going to help a lot.

586
00:28:44,320 --> 00:28:47,578
[BLANK_AUDIO]

587
00:28:47,578 --> 00:28:49,810
Okay, so this is a graph.

588
00:28:49,810 --> 00:28:55,003
So, let's just look at the graph,
I mean it's the same graph.

589
00:28:55,003 --> 00:28:56,910
It's looking at it from
two different angles.

590
00:28:56,910 --> 00:28:58,254
It's the same graph, okay?

591
00:28:58,254 --> 00:29:01,044
So, let's look at the one on the left,
just to pick one.

592
00:29:01,044 --> 00:29:03,650
It's the same graph though.

593
00:29:03,650 --> 00:29:07,160
We have a bunch of y values,
sorry a bunch of b values,

594
00:29:07,160 --> 00:29:08,930
and a bunch of m values.

595
00:29:08,930 --> 00:29:10,440
And then we have that error, right?

596
00:29:10,440 --> 00:29:13,128
That error that I just talked about,
right?

597
00:29:13,128 --> 00:29:16,731
So, given the 2D graph of b given
are every single y intercept,

598
00:29:16,731 --> 00:29:21,130
we could have given every single m
value we could have, what is the error?

599
00:29:21,130 --> 00:29:24,790
Okay, so for every y intercept and
slope curve what is the error?

600
00:29:24,790 --> 00:29:28,050
And, so we will find this is
a three dimensional graph.

601
00:29:28,050 --> 00:29:29,880
This is a three dimensional graph.

602
00:29:29,880 --> 00:29:35,230
Because the error value it's kind
of like, it's start up high,

603
00:29:35,230 --> 00:29:39,160
and then I do approach what's called
the local minimal in our case.

604
00:29:39,160 --> 00:29:42,550
A local minimal, which is the small
that point at the very bottom,

605
00:29:42,550 --> 00:29:45,230
that is our that is where
we're trying to get to.

606
00:29:45,230 --> 00:29:45,590
Okay so.

607
00:29:47,110 --> 00:29:50,810
Given a set of y-intercepts,
and given a set of slopes.

608
00:29:50,810 --> 00:29:51,730
Possible y-intercepts and

609
00:29:51,730 --> 00:29:55,520
possible slopes, we want to compute
the error for those three things.

610
00:29:55,520 --> 00:29:59,896
And if we were to graph the relationship
between these three things,

611
00:29:59,896 --> 00:30:01,485
it would look like this.

612
00:30:01,485 --> 00:30:04,460
Now, it tends to always
look very similar to this.

613
00:30:04,460 --> 00:30:07,769
In more complex cases we'd have many
minimal, we'd have many little values.

614
00:30:07,769 --> 00:30:12,450
But what we're trying to do is get that
point, where the error is smallest.

615
00:30:12,450 --> 00:30:14,910
And, so how do we get that point
where the error is smallest?

616
00:30:14,910 --> 00:30:18,090
Well, we're going to perform
what's called gradient descent

617
00:30:18,090 --> 00:30:19,770
to get that smallest point.

618
00:30:19,770 --> 00:30:21,523
That value, smallest point.

619
00:30:21,523 --> 00:30:25,124
And a great analogy for this is a bowl.

620
00:30:25,124 --> 00:30:28,210
So, let me just search bowl, okay?

621
00:30:30,250 --> 00:30:31,450
It's kind of like a bowl.

622
00:30:31,450 --> 00:30:37,498
It's like we drop a ball into a bowl,
and we want to find that point,

623
00:30:37,498 --> 00:30:42,806
where the ball stops,
that endpoint, the lowest point.

624
00:30:42,806 --> 00:30:47,620
That b, m value is our optimal
line of vested fit value.

625
00:30:47,620 --> 00:30:51,540
Okay?, and the way we're going to
get that is gradient descent.

626
00:30:51,540 --> 00:30:54,910
We're going to descend, right?,
we're descending down the bowl using

627
00:30:54,910 --> 00:30:57,519
the gradient, and
gradient is another word for slope.

628
00:30:57,519 --> 00:31:01,629
We're going to descend down that bowl
until we get, through iteration,

629
00:31:01,629 --> 00:31:03,090
that lowest point.

630
00:31:03,090 --> 00:31:04,825
And gradient descent is used.

631
00:31:04,825 --> 00:31:06,122
Everywhere in machine learning.

632
00:31:06,122 --> 00:31:06,474
Okay?

633
00:31:06,474 --> 00:31:09,635
It is like the optimization method for
deep neural networks.

634
00:31:09,635 --> 00:31:11,017
It's not that apparent right now.

635
00:31:11,017 --> 00:31:12,475
But know this.

636
00:31:12,475 --> 00:31:15,585
Know and understand gradient descent
like the back of your hands,

637
00:31:15,585 --> 00:31:19,125
because it is going to be very
useful in the future, okay?

638
00:31:19,125 --> 00:31:20,480
So.

639
00:31:20,480 --> 00:31:21,570
I don't know why I'm
doing that equation.

640
00:31:21,570 --> 00:31:22,502
That was unneccessary.

641
00:31:22,502 --> 00:31:23,910
That was the equation for

642
00:31:23,910 --> 00:31:28,640
the sum of squared errors that we just
talked about, sum of squared distances.

643
00:31:28,640 --> 00:31:31,536
So, how are we going to
calculate that gradient descent.

644
00:31:31,536 --> 00:31:32,664
Well, now let's actually do it.

645
00:31:32,664 --> 00:31:33,742
So,

646
00:31:33,742 --> 00:31:36,240
[BLANK_AUDIO]

647
00:31:36,240 --> 00:31:38,416
For our step gradient function,

648
00:31:38,416 --> 00:31:42,073
we'll start off with an initial
gradient value for a b.

649
00:31:42,073 --> 00:31:48,240
So, b is going to be zero and x gradient
is going to be zero as well, okay?

650
00:31:48,240 --> 00:31:52,040
These are the starting points for
our gradients and gradient means slope.

651
00:31:52,040 --> 00:31:54,866
And, so the gradient is going
to act like a compass, and

652
00:31:54,866 --> 00:31:58,115
it's going to always point down hill,
so this is what I mean by,

653
00:31:58,115 --> 00:32:01,631
once we calculate that error,
it's going to act as a compass for us.

654
00:32:01,631 --> 00:32:02,590
It's going to tell us.

655
00:32:03,670 --> 00:32:04,770
Where we should be going?

656
00:32:04,770 --> 00:32:06,568
What direction we should be going?

657
00:32:06,568 --> 00:32:08,578
How we should next redraw our lines.

658
00:32:08,578 --> 00:32:09,637
So for-

659
00:32:09,637 --> 00:32:11,600
[BLANK_AUDIO]

660
00:32:11,600 --> 00:32:14,610
Okay, someone asked why is
the lowest point the best?

661
00:32:14,610 --> 00:32:19,460
The lowest point is the best, because
it is where our error is the smallest.

662
00:32:19,460 --> 00:32:21,209
And when our error is the smallest,

663
00:32:22,300 --> 00:32:24,230
that's when we have
the line of best fit.

664
00:32:24,230 --> 00:32:27,590
When the error is smallest,
that b and m value, those two,

665
00:32:27,590 --> 00:32:31,839
what we plug into our slope equation, is
going to give us the line of best fit.

666
00:32:31,839 --> 00:32:34,820
So, that's why we're
calculating the error, okay?

667
00:32:34,820 --> 00:32:35,285
So.

668
00:32:35,285 --> 00:32:36,961
[BLANK_AUDIO]

669
00:32:36,961 --> 00:32:40,385
So, for i in range[0, len[points]).

670
00:32:40,385 --> 00:32:42,293
[BLANK_AUDIO]

671
00:32:42,293 --> 00:32:46,875
Okay, so what we're going to do is we're
going to iterate through every single

672
00:32:46,875 --> 00:32:48,510
point on our scatter plot.

673
00:32:48,510 --> 00:32:51,620
Okay, so every single data point that
we have, we're going to collect it.

674
00:32:51,620 --> 00:32:55,180
Okay, so we're going to say, okay,
what is, so for google our first point,

675
00:32:55,180 --> 00:32:55,583
right?

676
00:32:55,583 --> 00:32:58,300
First point,
which gives us an x value and a y value.

677
00:32:59,940 --> 00:33:01,800
X value and y value.

678
00:33:03,510 --> 00:33:05,830
So, let me also write out
a little comment for this.

679
00:33:05,830 --> 00:33:09,173
Starting points for our gradients, okay?

680
00:33:09,173 --> 00:33:11,826
[BLANK_AUDIO]

681
00:33:11,826 --> 00:33:16,811
Now, we're going to get the direction
with respect to b and m.

682
00:33:16,811 --> 00:33:23,880
Now, this is the last part, but
it's a very, very important part.

683
00:33:25,310 --> 00:33:29,289
And this is where calculus
comes into play, okay?

684
00:33:29,289 --> 00:33:32,694
So, I'm going to talk about
how we're doing this.

685
00:33:32,694 --> 00:33:36,698
Okay, so let me talk about
what we're about to do.

686
00:33:36,698 --> 00:33:40,629
So, what we're going to do, is so,
given for every single point, for

687
00:33:40,629 --> 00:33:42,464
every single point that we have,

688
00:33:42,464 --> 00:33:46,605
we're going to calculate what's
called the partial derivative, okay?

689
00:33:46,605 --> 00:33:51,045
It's called the partial
derivative with respect to b and

690
00:33:51,045 --> 00:33:53,365
with respect to m, okay?

691
00:33:53,365 --> 00:33:56,741
And what that's going to do, is it's
going to give us a direction to go for

692
00:33:56,741 --> 00:33:58,735
both the b value and the m value, right?

693
00:33:58,735 --> 00:34:02,990
So, remember, in this graph,
we want a direction, right?

694
00:34:02,990 --> 00:34:09,578
We want to be going down the gradient.

695
00:34:09,578 --> 00:34:13,040
And, so on this left hand side
you see this gradient search.

696
00:34:13,040 --> 00:34:16,752
The m values and the b values are
increasing in the direction that they

697
00:34:16,752 --> 00:34:21,290
should be, because gradient intersect
is essentially a search policy.

698
00:34:21,290 --> 00:34:22,310
It's a search policy.

699
00:34:22,310 --> 00:34:24,835
We're trying to find
that minimum error value.

700
00:34:24,835 --> 00:34:25,600
Okay?

701
00:34:25,600 --> 00:34:27,761
And what we're going to do to get that,

702
00:34:27,761 --> 00:34:32,313
is we're going to compute the partial
derivative with respect to b, n, and f.

703
00:34:32,313 --> 00:34:35,812
Okay, let me show you the equation for
the partial derivative, okay?

704
00:34:35,812 --> 00:34:40,533
The partial derivative is
going to be right here.

705
00:34:40,533 --> 00:34:44,498
[BLANK_AUDIO]

706
00:34:44,498 --> 00:34:46,460
So, this is what the partial
derivative does.

707
00:34:46,460 --> 00:34:52,870
The partial derivative, we

708
00:34:52,870 --> 00:34:58,380
call it partial, because it's not
telling us the whole story, right?

709
00:34:58,380 --> 00:35:03,358
We say, it's partial, because we're
calculating it for both b and m.

710
00:35:03,358 --> 00:35:04,690
There are two different dates.

711
00:35:04,690 --> 00:35:07,505
And, so
it's going to give us the tangent line.

712
00:35:07,505 --> 00:35:10,220
So, it's going to give us this
line as you see right here, right?

713
00:35:10,220 --> 00:35:13,180
See this line,
that line is our direction.

714
00:35:13,180 --> 00:35:16,460
And we're going to use it to
update our g and m values.

715
00:35:16,460 --> 00:35:16,780
Okay?

716
00:35:20,210 --> 00:35:21,270
So, that's what that is.

717
00:35:21,270 --> 00:35:25,560
And let me also show you the equation
for the partial derivative,

718
00:35:25,560 --> 00:35:27,950
because we're about to write it out.

719
00:35:27,950 --> 00:35:32,310
So, here's what the equation for the
partial derivative with respect to m and

720
00:35:32,310 --> 00:35:33,430
b looks like.

721
00:35:33,430 --> 00:35:34,260
Okay?

722
00:35:34,260 --> 00:35:36,960
They're two different equations, right?

723
00:35:36,960 --> 00:35:38,650
So, let's talk about the one on top.

724
00:35:38,650 --> 00:35:41,821
So, this little curvy thing
that you see up here,

725
00:35:41,821 --> 00:35:45,381
that just signifies that this
is a partial derivative.

726
00:35:45,381 --> 00:35:48,000
That's that signifier that
this is a partial derivative.

727
00:35:48,000 --> 00:35:49,986
Now, we talked about sigma notation,
right?,

728
00:35:49,986 --> 00:35:51,841
because it's a summation of values,
right?

729
00:35:51,841 --> 00:35:53,249
And that's what we're doing.

730
00:35:53,249 --> 00:35:59,033
We're summing the partial derivative for
all of our points, okay?

731
00:35:59,033 --> 00:36:01,990
For all of them to compute
that gradient value, okay?

732
00:36:01,990 --> 00:36:08,201
And the partial variable with respect
to m and b is going to look like this.

733
00:36:08,201 --> 00:36:11,762
So, let's write this out, okay?

734
00:36:11,762 --> 00:36:14,100
So, the b gradient, so
it's going to give us two values.

735
00:36:14,100 --> 00:36:16,960
So, the b gradient is
going to be plus equals.

736
00:36:16,960 --> 00:36:17,900
And then what was it?

737
00:36:17,900 --> 00:36:20,580
Let me look at the equation again.

738
00:36:20,580 --> 00:36:24,684
2 over N, so
negative 2 over N, all right?

739
00:36:24,684 --> 00:36:30,112
[BLANK_AUDIO]

740
00:36:30,112 --> 00:36:33,757
Thanks good vibes.

741
00:36:33,757 --> 00:36:36,800
And then it was y minus, right?

742
00:36:36,800 --> 00:36:38,810
And these are the equations,
they are laws.

743
00:36:38,810 --> 00:36:41,496
They are beautiful laws,
that always stay the same.

744
00:36:41,496 --> 00:36:46,618
And they give us a way of understanding

745
00:36:46,618 --> 00:36:51,750
the direction that we want to move in.

746
00:36:51,750 --> 00:36:54,538
Okay, so, b_current.

747
00:36:54,538 --> 00:36:56,090
Okay so.

748
00:36:56,090 --> 00:37:01,820
All right, so then we'll do the same
thing, and what was the second equation.

749
00:37:01,820 --> 00:37:05,831
It looked pretty much the same,
minus it doesn't have this x, right?

750
00:37:05,831 --> 00:37:07,850
The second one doesn't have this x,
right?

751
00:37:07,850 --> 00:37:12,883
So, we'll say, but it does have this 2N.

752
00:37:12,883 --> 00:37:15,464
It does have this 2N,
and then it does have

753
00:37:15,464 --> 00:37:20,388
[BLANK_AUDIO]

754
00:37:20,388 --> 00:37:20,702
Let's see.

755
00:37:20,702 --> 00:37:21,952
Let's have this x.

756
00:37:21,952 --> 00:37:25,099
It does have

757
00:37:25,099 --> 00:37:33,283
(y-([m_current * x).

758
00:37:33,283 --> 00:37:35,872
[BLANK_AUDIO]

759
00:37:35,872 --> 00:37:42,410
+ b_current, okay?

760
00:37:42,410 --> 00:37:45,070
Okay, so now, we've computed
our partial derivatives, right?

761
00:37:45,070 --> 00:37:47,540
So, let me one more time show you guys.

762
00:37:47,540 --> 00:37:51,040
It's giving us directions to go for
both b and m.

763
00:37:52,570 --> 00:37:54,770
And remember, they're partial.

764
00:37:54,770 --> 00:37:57,410
It's not telling us the whole story,
it's telling us what direction should we

765
00:37:57,410 --> 00:37:59,730
go for b, and
what direction should we go for m?

766
00:37:59,730 --> 00:38:03,366
And it's going to tell us the direction,
remember a bowl to get to that bottom

767
00:38:03,366 --> 00:38:06,325
point, where that error is
the smallest right here, okay?

768
00:38:06,325 --> 00:38:10,082
So, right here where my mouse is,
that point is what we want to get to,

769
00:38:10,082 --> 00:38:13,484
and that's what the partial
derivative is going to help us with.

770
00:38:13,484 --> 00:38:17,939
So, once we've computed
the partial derivatives,

771
00:38:17,939 --> 00:38:21,107
the sum of them with respect to b and m,

772
00:38:21,107 --> 00:38:25,562
now we're going to update our b and
m values, right?

773
00:38:25,562 --> 00:38:27,573
So, we're going to use that
to update our b and m values.

774
00:38:27,573 --> 00:38:28,098
And guess what?

775
00:38:28,098 --> 00:38:29,960
This is our last step.

776
00:38:29,960 --> 00:38:32,354
This is our last step using
this partial derivative.

777
00:38:32,354 --> 00:38:37,650
[BLANK_AUDIO]

778
00:38:37,650 --> 00:38:40,370
Using our partial derivatives,
right plural?

779
00:38:40,370 --> 00:38:41,454
There's two of them.

780
00:38:41,454 --> 00:38:44,347
So, and that's going to give us a new
value for b and m, our updated b and

781
00:38:44,347 --> 00:38:44,742
m value.

782
00:38:44,742 --> 00:38:47,384
So, we have our current value for
b whatever it is,

783
00:38:47,384 --> 00:38:51,610
that we fed into the separated function
that keeps updating every time.

784
00:38:51,610 --> 00:38:54,680
And this is where our learning_rate
comes into play, okay?

785
00:38:54,680 --> 00:38:59,220
This is why our learning rate is so
important, because it defines the rate

786
00:38:59,220 --> 00:39:04,040
at which we're updating our b and
n values, right?

787
00:39:04,040 --> 00:39:06,946
So, remember that 0.0001, right?

788
00:39:06,946 --> 00:39:10,281
And then also our n_current.

789
00:39:10,281 --> 00:39:12,687
[BLANK_AUDIO]

790
00:39:12,687 --> 00:39:15,348
That is learning_rate,

791
00:39:15,348 --> 00:39:17,590
[BLANK_AUDIO]

792
00:39:17,590 --> 00:39:20,476
Times the m gradient.

793
00:39:20,476 --> 00:39:24,340
Okay, and
then it'll return those values.

794
00:39:24,340 --> 00:39:26,020
And we're doing this every time, right?

795
00:39:26,020 --> 00:39:29,771
This is new b, and new m,
they our final b and m.

796
00:39:29,771 --> 00:39:33,630
It's a step function, where we're
doing this every iteration, right?

797
00:39:33,630 --> 00:39:38,390
We're doing this for
the number of iterations we had 1000.

798
00:39:38,390 --> 00:39:41,015
But it's going to return a new b and
m value every time.

799
00:39:41,015 --> 00:39:42,120
And guess what guys?

800
00:39:42,120 --> 00:39:43,808
That's it for our code.

801
00:39:43,808 --> 00:39:48,576
That was it, so
let's go over what we've done.

802
00:39:48,576 --> 00:39:51,529
Okay, but actually let me check for
errors, right?

803
00:39:51,529 --> 00:39:53,235
[BLANK_AUDIO]

804
00:39:53,235 --> 00:39:56,199
Let me check for errors, and
then I'm going to answer more questions,

805
00:39:56,199 --> 00:39:59,280
because I really want to make sure you
guys understand how this works, okay?

806
00:39:59,280 --> 00:40:01,677
So, let me demo this.

807
00:40:01,677 --> 00:40:06,140
So, python demo.py Only and
is not defined.

808
00:40:06,140 --> 00:40:07,740
Okay, right, guess what.

809
00:40:07,740 --> 00:40:10,430
I didn't define N.

810
00:40:10,430 --> 00:40:13,240
N is the number of points.

811
00:40:13,240 --> 00:40:17,150
Length of points.

812
00:40:17,150 --> 00:40:17,270
Okay?

813
00:40:17,270 --> 00:40:17,870
So, let's go.

814
00:40:19,480 --> 00:40:20,310
Learning rate is not defined.

815
00:40:20,310 --> 00:40:20,700
Where?

816
00:40:20,700 --> 00:40:22,040
Where is learning rate not defined?

817
00:40:23,650 --> 00:40:25,310
Learning rate is not defined.

818
00:40:27,430 --> 00:40:30,600
Wait a second.

819
00:40:30,600 --> 00:40:31,418
Yeah, right.

820
00:40:31,418 --> 00:40:36,510
Learning rate, right.

821
00:40:36,510 --> 00:40:37,790
Okay, what else is bad?

822
00:40:39,520 --> 00:40:42,377
I've got an overflow for double scalars.

823
00:40:42,377 --> 00:40:45,598
[BLANK_AUDIO]

824
00:40:45,598 --> 00:40:52,796
14 y minus

825
00:40:52,796 --> 00:40:54,622
[BLANK_AUDIO]

826
00:40:54,622 --> 00:40:55,975
Uh-huh, uh-huh, uh-huh.

827
00:40:55,975 --> 00:40:59,964
[BLANK_AUDIO]

828
00:40:59,964 --> 00:41:03,477
[INAUDIBLE] Okay, so.

829
00:41:05,620 --> 00:41:07,530
What's going on here?

830
00:41:07,530 --> 00:41:08,160
Okay, let's save this.

831
00:41:08,160 --> 00:41:14,589
So yeah, it printed out the final, okay
so it got our final value right here.

832
00:41:14,589 --> 00:41:18,510
And if we wanted to,
let's see, hold on a second.

833
00:41:18,510 --> 00:41:23,830
If we wanted to,
we got our backup here just in case.

834
00:41:23,830 --> 00:41:25,250
So right?

835
00:41:25,250 --> 00:41:27,310
So let me blow this up.

836
00:41:27,310 --> 00:41:28,450
Like way, way up.

837
00:41:29,530 --> 00:41:31,000
Let me just separate it.

838
00:41:31,000 --> 00:41:33,290
So this is what our outputs
going to look like.

839
00:41:33,290 --> 00:41:33,480
Right.

840
00:41:33,480 --> 00:41:33,880
So boom!

841
00:41:33,880 --> 00:41:34,370
Just like that.

842
00:41:34,370 --> 00:41:36,310
That's how fast it trade,
in milliseconds.

843
00:41:36,310 --> 00:41:36,500
Why?

844
00:41:36,500 --> 00:41:37,920
Because our data set is so small.

845
00:41:40,180 --> 00:41:41,740
Okay, it's data set was so small.

846
00:41:43,070 --> 00:41:43,650
Alright, so.

847
00:41:45,560 --> 00:41:48,728
That what's happened and
after a thousand iterations,

848
00:41:48,728 --> 00:41:50,580
we got the optimal b and m values.

849
00:41:50,580 --> 00:41:54,060
So, right as we start up with b and
m at o at we calculate the error for

850
00:41:54,060 --> 00:41:56,370
our random line that we drew and
it was huge.

851
00:41:56,370 --> 00:42:00,010
But, eventually, after running
gradient descend we got the optimal b,

852
00:42:00,010 --> 00:42:02,020
the optimal m and
the lowest error point,

853
00:42:02,020 --> 00:42:05,160
which is at the smallest
point in the bowl.

854
00:42:05,160 --> 00:42:10,060
And we to do that we use gradient
decent with respect to b and m.

855
00:42:10,060 --> 00:42:13,730
Okay so let me go over one last time
every single thing that we just done.

856
00:42:13,730 --> 00:42:17,090
Is to really go over it and then will
do my last five minute Q &amp; A okay.

857
00:42:17,090 --> 00:42:19,430
So we start out by collecting
our data set, right.

858
00:42:19,430 --> 00:42:23,480
Our data set was a collection
of test scores and

859
00:42:23,480 --> 00:42:25,030
the amount of hours studied, right.

860
00:42:25,030 --> 00:42:27,010
The x y value the test scores and

861
00:42:27,010 --> 00:42:29,800
the amount of hours studied
a two variable data set.

862
00:42:29,800 --> 00:42:33,120
Then we define our type of parameters
for our linear regression.

863
00:42:33,120 --> 00:42:36,430
Our learning weight, which talks
about how fast we should learn,

864
00:42:36,430 --> 00:42:39,840
our initial BNM values for
the slope equation: y=mx+b.

865
00:42:39,840 --> 00:42:43,490
The number of iterations, 1,000,
because our data set is pretty small.

866
00:42:43,490 --> 00:42:44,980
And then we ran gradient descent.

867
00:42:44,980 --> 00:42:47,010
So, what did gradient descent look like?

868
00:42:47,010 --> 00:42:50,615
Well for every iteration, for

869
00:42:50,615 --> 00:42:55,150
a thousand iterations, we computed the
gradients with respect to both b and m.

870
00:42:55,150 --> 00:43:00,030
And we did that constantly,
until we got that optimal b and m value.

871
00:43:00,030 --> 00:43:02,530
That gives us that line of best fit.

872
00:43:02,530 --> 00:43:04,280
Now, how did we compute the gradients?

873
00:43:04,280 --> 00:43:05,470
To do that, we said,

874
00:43:05,470 --> 00:43:08,850
okay, we'll have a starting point
of 0 for both of those gradients.

875
00:43:08,850 --> 00:43:14,510
Remember, gradient is just
another word for slope.

876
00:43:14,510 --> 00:43:16,494
And then we said, okay so for

877
00:43:16,494 --> 00:43:20,299
every single point in our scatter plot,
for our data,

878
00:43:20,299 --> 00:43:25,825
we'll compute the partial derivative
with the respect to of both b and m.

879
00:43:25,825 --> 00:43:28,565
And those two values are going
to give us a direction,

880
00:43:28,565 --> 00:43:31,215
a sense of direction of
where we want to go.

881
00:43:31,215 --> 00:43:33,375
How do we get to that lowest
point in that goal, right?

882
00:43:33,375 --> 00:43:36,370
That three dimensional graphic,
that lowest point and

883
00:43:36,370 --> 00:43:41,290
we use the learning rate to determine
how fast we want to update our

884
00:43:41,290 --> 00:43:44,340
DMN values, we got the difference
between the current value, and

885
00:43:44,340 --> 00:43:46,830
what we had before, and we return that.

886
00:43:46,830 --> 00:43:50,260
So for every point, we did that for
a thousand iterations, okay?

887
00:43:50,260 --> 00:43:52,470
And that's what gave us the output and

888
00:43:52,470 --> 00:43:55,066
it looks like, visually,
it looks like this.

889
00:43:55,066 --> 00:43:57,054
[SOUND] Right?

890
00:43:57,054 --> 00:44:00,560
It's like up, up, up, up, up,
up, up, up, up, up, up, up.

891
00:44:00,560 --> 00:44:01,840
It's kind of like Wheel of Fortune,
right?

892
00:44:01,840 --> 00:44:05,250
It starts off fast, and it gets slower
and slower as it approaches convergence,

893
00:44:05,250 --> 00:44:09,880
the word we use when we have the optimal
line of best fit, convergence.

894
00:44:09,880 --> 00:44:11,910
See, let me do it one more time.

895
00:44:11,910 --> 00:44:14,460
Up, just like that, okay?

896
00:44:14,460 --> 00:44:18,470
So that was that, and

897
00:44:18,470 --> 00:44:23,840
now I'm going to screen share and
do a last five minute Q and A.

898
00:44:23,840 --> 00:44:26,120
Alright, stop screen share.

899
00:44:26,120 --> 00:44:30,640
Hi everybody, okay,
let me bring you guys back on screen,

900
00:44:30,640 --> 00:44:34,400
do my last five minute Q and
A, ask me anything and yeah.

901
00:44:39,670 --> 00:44:41,427
How's it going everybody?

902
00:44:41,427 --> 00:44:43,463
[BLANK_AUDIO]

903
00:44:43,463 --> 00:44:44,390
Any questions?

904
00:44:44,390 --> 00:44:45,977
I'm open to questions.

905
00:44:45,977 --> 00:44:49,062
[BLANK_AUDIO]

906
00:44:49,062 --> 00:44:50,040
Where did I use NumPy?

907
00:44:50,040 --> 00:44:50,877
It's at the very top.

908
00:44:50,877 --> 00:44:54,590
So, right, what's the practical
use of linear regression?

909
00:44:54,590 --> 00:44:55,490
Great question.

910
00:44:55,490 --> 00:45:00,341
Any time we want to find
the relationship between two different

911
00:45:00,341 --> 00:45:02,820
variables.

912
00:45:02,820 --> 00:45:05,720
And then in more complex
cases there could be more.

913
00:45:05,720 --> 00:45:07,380
But, we want to prove mathematically.

914
00:45:07,380 --> 00:45:07,870
Right?

915
00:45:07,870 --> 00:45:11,690
Math is all about proving things
in a way that is unfalsifiable,

916
00:45:11,690 --> 00:45:14,580
that no one can say,
hey, that's not true.

917
00:45:14,580 --> 00:45:16,900
Well I can prove it mathematically.

918
00:45:16,900 --> 00:45:21,620
So it's a way to show the relationship
between two value pairs.

919
00:45:21,620 --> 00:45:25,520
So maybe housing prices,
and the time of year right?

920
00:45:25,520 --> 00:45:29,580
What is the real estate
market going to look like?

921
00:45:29,580 --> 00:45:32,880
Any time intuitively you think there
was a relationship you can prove

922
00:45:32,880 --> 00:45:37,460
it with linear regression, but
really I did this to show Grady the set.

923
00:45:37,460 --> 00:45:41,150
That optimization process is very
popular in Deep Learning and

924
00:45:41,150 --> 00:45:45,234
we're going to use that in our Deep,
Run networks on

925
00:45:45,234 --> 00:45:49,135
the rest of the course, okay?

926
00:45:49,135 --> 00:45:53,520
And, why a device for this google?

927
00:45:53,520 --> 00:46:02,220
Because it is the deepest learning
library that is out there right now.

928
00:46:02,220 --> 00:46:02,950
That's why.

929
00:46:02,950 --> 00:46:05,890
And, of course it would be,
because Google knows what they're doing.

930
00:46:05,890 --> 00:46:09,010
They handle billions and
billions of queries every day.

931
00:46:09,010 --> 00:46:12,140
They have to be able to do
machine learning at scale.

932
00:46:12,140 --> 00:46:16,010
And, problems, they solve problems that
no one else has even thought of solving.

933
00:46:16,010 --> 00:46:18,558
And all of those solutions
are found in TensorFlow.

934
00:46:18,558 --> 00:46:21,510
For machine learning or
please think of the eye doctor.

935
00:46:21,510 --> 00:46:26,380
You can create a classifier to
classify between different types of

936
00:46:27,390 --> 00:46:29,110
disorder that you see in an x-ray.

937
00:46:29,110 --> 00:46:33,220
That's going to augment doctors at
first, but eventually replace them.

938
00:46:33,220 --> 00:46:36,203
How about fitting a quadratic
curve inside of a linear line?

939
00:46:36,203 --> 00:46:37,895
We could do that as well.

940
00:46:37,895 --> 00:46:41,684
[BLANK_AUDIO]

941
00:46:41,684 --> 00:46:43,420
I'm going to provide the data set and
the code.

942
00:46:44,620 --> 00:46:45,730
I can talk slower, sure.

943
00:46:47,350 --> 00:46:49,360
How to find the optimal morning rate?

944
00:46:49,360 --> 00:46:50,530
That's a great question.

945
00:46:50,530 --> 00:46:53,450
There's several methods of doing that,
but that's great intuition.

946
00:46:53,450 --> 00:46:57,810
Sometimes we can use machine learning to
find the optimal hyper-parameters, so

947
00:46:57,810 --> 00:47:00,590
it's kind of like machine learning for
machine learning, but

948
00:47:00,590 --> 00:47:04,550
we'll talk about that later.

949
00:47:04,550 --> 00:47:10,220
This is the first course,
he just calculates,I'll

950
00:47:10,220 --> 00:47:15,660
do more of that in the future,
I'm going to keep doing calculus, okay?

951
00:47:15,660 --> 00:47:18,650
Two more questions then
we're good to go, two more.

952
00:47:20,540 --> 00:47:22,468
How would you recommend me
to start machine learning?

953
00:47:22,468 --> 00:47:25,520
Watch this series.

954
00:47:25,520 --> 00:47:28,865
And watch my Learn Python for
Data Science series, watch my Intro to

955
00:47:28,865 --> 00:47:32,830
Tension Flow series, watch my
Machine Learning for Hackers series.

956
00:47:34,100 --> 00:47:34,740
Watch my videos.

957
00:47:35,920 --> 00:47:37,380
Why is your Udacity too extensive?

958
00:47:37,380 --> 00:47:39,020
I didn't decide the price guys.

959
00:47:39,020 --> 00:47:40,470
I try to get it low.

960
00:47:40,470 --> 00:47:41,310
It's whatever.

961
00:47:41,310 --> 00:47:43,720
You get paid graders for that okay.

962
00:47:43,720 --> 00:47:46,330
And grading is not cheap,
okay human graders.

963
00:47:46,330 --> 00:47:49,970
But look all the videos are going to be
released here on my channel all right.

964
00:47:49,970 --> 00:47:53,200
So I'm here for you guys, okay?

965
00:47:54,690 --> 00:47:55,510
I'm trying to grow my brand.

966
00:47:55,510 --> 00:47:58,617
I'm trying to grow myself,
Sharad Ravel, okay?

967
00:47:58,617 --> 00:48:03,296
[BLANK_AUDIO]

968
00:48:03,296 --> 00:48:04,530
This is the end, okay?

969
00:48:04,530 --> 00:48:05,730
So that's it for the questions.

970
00:48:07,480 --> 00:48:11,056
And all right, so for now, I've gotta,

971
00:48:11,056 --> 00:48:13,071
[BLANK_AUDIO]

972
00:48:13,071 --> 00:48:15,250
Shoot a findings scene.

973
00:48:15,250 --> 00:48:16,150
For my next video.

974
00:48:16,150 --> 00:48:16,284
What?

975
00:48:16,284 --> 00:48:19,105
Yeah, so, thanks for watching.

976
00:48:19,105 --> 00:48:22,165
[SOUND] Love you guys.

977
00:48:22,165 --> 00:48:27,480
I'll post the link in the comments
right when I'm done alright?

978
00:48:27,480 --> 00:48:28,220
The video description.

979
00:48:28,220 --> 00:48:30,890
I'll post the GitHub link, and
then the data set, everything.

980
00:48:30,890 --> 00:48:34,290
So don't go to the descriptions
within the hour, okay?

981
00:48:34,290 --> 00:48:36,576
Bye!

982
00:48:36,576 --> 00:48:37,280
Okay.

983
00:48:37,280 --> 00:48:40,900
[BLANK_AUDIO]