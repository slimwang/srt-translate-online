1
00:00:00,240 --> 00:00:02,009
Now that we have the characterizations and

2
00:00:02,009 --> 00:00:05,040
the knowledge representations the four concept's worked out,

3
00:00:05,040 --> 00:00:09,710
let us see how the AI agent might actually use them. So let's look at the bowl.

4
00:00:09,710 --> 00:00:13,040
Here was the knowledge representation of the characterization of the bowl.

5
00:00:13,040 --> 00:00:18,040
The AI agent will abstract some knowledge from this particular example.

6
00:00:18,040 --> 00:00:21,220
Here is its abstraction. Two things have happened here.

7
00:00:21,220 --> 00:00:27,050
First, it is abstracting only those things, that are in fact causally related.

8
00:00:27,050 --> 00:00:31,180
Simple features that have no causal relationship with other things, are not

9
00:00:31,180 --> 00:00:35,240
important and they can be dropped. So we can add one other element of a notion

10
00:00:35,240 --> 00:00:39,290
of an explanation. The explanation is a causal explanation. The AI agent is

11
00:00:39,290 --> 00:00:44,190
trying to build a causal explanation that will connect the instance, the object,

12
00:00:44,190 --> 00:00:49,780
into the cup. Second, the AI agent creates an abstraction of this

13
00:00:49,780 --> 00:00:55,430
characterization of the bowl. And so in the bowl, it replaces it with an object.

14
00:00:55,430 --> 00:01:00,260
So here the bowl carries liquids, because it is concave, and it is abstracted to

15
00:01:00,260 --> 00:01:04,860
the object carries liquid because it is concave. This is the abstraction that is

16
00:01:04,860 --> 00:01:07,970
going to play an important role in constructing the causal explanation.
