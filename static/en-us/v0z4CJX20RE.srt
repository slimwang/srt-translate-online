1
00:00:00,240 --> 00:00:03,510
Here's another algebraic way
of thinking about this, okay?

2
00:00:04,600 --> 00:00:08,610
Using our old definition
of b transpose b, okay,

3
00:00:08,610 --> 00:00:14,365
you'll notice that what it is
is actually this matrix okay?

4
00:00:14,365 --> 00:00:19,565
Sum of x squared, sum of xys,
sum of xys, sum of y squared.

5
00:00:19,565 --> 00:00:23,115
Well that's just the covariance matrix

6
00:00:23,115 --> 00:00:26,405
of the points assuming that
there are about the origin.

7
00:00:26,405 --> 00:00:29,365
So I don't have to track that the mean,
that would be the covariance.

8
00:00:29,365 --> 00:00:33,910
Speaking of which, yet one more
way of thinking about this is that

9
00:00:33,910 --> 00:00:38,100
B transpose B here is
the sum of this xx transpose.

10
00:00:38,100 --> 00:00:40,720
That's what's referred
to as the outer product.

11
00:00:40,720 --> 00:00:44,150
And if it's about the origin, I don't
have to subtract off the mean, but

12
00:00:44,150 --> 00:00:47,770
the, here it is written just like that.

13
00:00:47,770 --> 00:00:52,220
That again is the definition
of the covariance matrix.

14
00:00:52,220 --> 00:00:55,670
This makes some sense, for those of you
thinking about it, the covariance matrix

15
00:00:55,670 --> 00:00:58,400
is a relationship between
the expected value of the squares,

16
00:00:58,400 --> 00:01:03,120
the sum of the squares of points
distributed about the mean.

17
00:01:03,120 --> 00:01:04,750
So that's actually,

18
00:01:04,750 --> 00:01:08,060
if you think about it,
the saying what we wanted to, to know.

19
00:01:08,060 --> 00:01:10,130
We wanted to know the greatest variance.

20
00:01:10,130 --> 00:01:12,900
For those of you that care, or
are thinking about, if you think about

21
00:01:12,900 --> 00:01:17,220
a covariance matrix as an ellipsoid in
space your eigenvectors are going to be,

22
00:01:17,220 --> 00:01:19,920
end up being,
the major axis of these ellipsoids.
