1
00:00:00,000 --> 00:00:01,649
let me explain why measuring performance

2
00:00:01,649 --> 00:00:03,000
is subtle

3
00:00:03,000 --> 00:00:04,919
let's go back to our classification task

4
00:00:04,919 --> 00:00:06,929
you've got a whole lot of images with

5
00:00:06,929 --> 00:00:09,808
labels you could say okay I'm going to

6
00:00:09,808 --> 00:00:12,119
run my classifier and those images and

7
00:00:12,119 --> 00:00:14,189
see how many i got right that's mayor

8
00:00:14,189 --> 00:00:16,410
measure and then you go out and use your

9
00:00:16,410 --> 00:00:18,868
classifier on new images images that

10
00:00:18,868 --> 00:00:20,789
you've never seen in the past and you

11
00:00:20,789 --> 00:00:22,920
measure how many you get right and your

12
00:00:22,920 --> 00:00:24,960
performance gets worse the classifier

13
00:00:24,960 --> 00:00:27,989
doesn't do as well so what happened well

14
00:00:27,989 --> 00:00:29,969
imagine i construct a classifier that

15
00:00:29,969 --> 00:00:33,149
simply compares the new image to any of

16
00:00:33,149 --> 00:00:35,039
the other images that I've already seen

17
00:00:35,039 --> 00:00:37,170
in my training set and just returns the

18
00:00:37,170 --> 00:00:39,210
label by the measure we defined earlier

19
00:00:39,210 --> 00:00:41,549
it's a great classifier it would get a

20
00:00:41,549 --> 00:00:43,170
hundred person accuracy on the training

21
00:00:43,170 --> 00:00:46,259
sets but as soon as it sees a new image

22
00:00:46,259 --> 00:00:49,049
it's lost it has no idea what to do

23
00:00:49,049 --> 00:00:51,628
it's not a great classifier the problem

24
00:00:51,628 --> 00:00:53,609
is that your classifier has memorized

25
00:00:53,609 --> 00:00:55,530
the training set and it fails to

26
00:00:55,530 --> 00:00:57,570
generalize to new examples

27
00:00:57,570 --> 00:00:59,009
it's not just a theoretical problem

28
00:00:59,009 --> 00:01:01,409
every classifier that you will build

29
00:01:01,409 --> 00:01:03,539
will tend to try and memorize the

30
00:01:03,539 --> 00:01:05,640
training sets and it will usually do

31
00:01:05,640 --> 00:01:08,459
that very very well your job though is

32
00:01:08,459 --> 00:01:10,530
to help it generalize to new data

33
00:01:10,530 --> 00:01:13,140
instead so how do we measure the

34
00:01:13,140 --> 00:01:15,209
generalization instead of measuring how

35
00:01:15,209 --> 00:01:17,099
well the classifier memorize the data

36
00:01:17,099 --> 00:01:20,040
the simplest way is to take a small

37
00:01:20,040 --> 00:01:22,890
subset of the training set not use it in

38
00:01:22,890 --> 00:01:25,019
training and measure the error on that

39
00:01:25,019 --> 00:01:27,629
test data problem solved

40
00:01:27,629 --> 00:01:29,609
now your classifier cannot cheat because

41
00:01:29,609 --> 00:01:31,920
it never sees the test data so we can't

42
00:01:31,920 --> 00:01:33,810
memorize it but there is still a problem

43
00:01:33,810 --> 00:01:36,569
because training a classifier is usually

44
00:01:36,569 --> 00:01:38,849
a process of trial and error you try a

45
00:01:38,849 --> 00:01:41,159
classifier you measure its performance

46
00:01:41,159 --> 00:01:43,259
and then you try another one and you

47
00:01:43,259 --> 00:01:45,390
measure again and another and another

48
00:01:45,390 --> 00:01:48,180
you tweak the model you explore the

49
00:01:48,180 --> 00:01:51,209
parameters you measure and finally you

50
00:01:51,209 --> 00:01:52,859
have what you think is the perfect

51
00:01:52,859 --> 00:01:55,438
classifier and then after all this care

52
00:01:55,438 --> 00:01:57,868
you've taken to separate your test data

53
00:01:57,868 --> 00:01:59,489
from your training data and only

54
00:01:59,489 --> 00:02:00,840
measuring your performance on the test

55
00:02:00,840 --> 00:02:03,420
data now you deploy your system in a

56
00:02:03,420 --> 00:02:05,399
real production environment and you get

57
00:02:05,399 --> 00:02:07,590
more data and you score your performance

58
00:02:07,590 --> 00:02:10,020
on that new data and it's doesn't do

59
00:02:10,020 --> 00:02:12,628
nearly as well what can possibly have

60
00:02:12,628 --> 00:02:13,129
happened

61
00:02:13,129 --> 00:02:15,259
what happened is that your classifier

62
00:02:15,259 --> 00:02:17,629
has seen your test data indirectly

63
00:02:17,629 --> 00:02:19,969
through your own eyes every time you

64
00:02:19,969 --> 00:02:21,740
made a decision about which classifier

65
00:02:21,740 --> 00:02:24,500
to use which parameter to tune you

66
00:02:24,500 --> 00:02:25,939
actually gave information to your

67
00:02:25,939 --> 00:02:28,939
classifier about the test sets just a

68
00:02:28,939 --> 00:02:31,129
tiny bit but it adds up

69
00:02:31,129 --> 00:02:33,289
so over time as you're in many and many

70
00:02:33,289 --> 00:02:35,810
experiments your test data bleeds into

71
00:02:35,810 --> 00:02:38,719
your training data so what can you do

72
00:02:38,719 --> 00:02:40,009
there are many ways to deal with this

73
00:02:40,009 --> 00:02:42,319
i'll give you the simplest one take

74
00:02:42,319 --> 00:02:44,449
another chunk of your training sets and

75
00:02:44,449 --> 00:02:46,520
hide it under a rock never look at it

76
00:02:46,520 --> 00:02:47,990
until you've made your final decision

77
00:02:47,990 --> 00:02:50,900
you can use your validation sets to

78
00:02:50,900 --> 00:02:53,449
measure your actual error and maybe the

79
00:02:53,449 --> 00:02:55,340
validation set will bleed into their

80
00:02:55,340 --> 00:02:57,110
training sets but that's okay because

81
00:02:57,110 --> 00:02:59,599
you always have this test sets that you

82
00:02:59,599 --> 00:03:01,370
can rely on to actually measure your

83
00:03:01,370 --> 00:03:07,030
real performance

