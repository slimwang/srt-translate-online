1
00:00:00,240 --> 00:00:04,610
So I think it will help if we think
through what the basic update rule is

2
00:00:04,610 --> 00:00:06,980
when you're doing reinforcement
learning,or cue learning.

3
00:00:06,980 --> 00:00:10,870
And then to try to adapt to this
notion that are we're representing our

4
00:00:10,870 --> 00:00:13,490
value function using some kind
of function approximater.

5
00:00:13,490 --> 00:00:15,350
>> All right.
>> So here's a way to think about this,

6
00:00:15,350 --> 00:00:19,340
we've got Q function that we're
going to try to represent as

7
00:00:19,340 --> 00:00:23,100
some function of a set of parameters,
or weights.

8
00:00:23,100 --> 00:00:25,920
Maybe let's say one set of
parameters for each action.

9
00:00:25,920 --> 00:00:27,010
>> Sure.

10
00:00:27,010 --> 00:00:29,420
>> And
what F is doing is taking a state and

11
00:00:29,420 --> 00:00:31,080
translating it into a set of features.

12
00:00:31,080 --> 00:00:32,400
We're going to feed those features and

13
00:00:32,400 --> 00:00:34,600
the parameters of the function
into something called F.

14
00:00:34,600 --> 00:00:37,280
And what will come out of
that will be the Q value.

15
00:00:37,280 --> 00:00:39,720
So this is intended
to be fairly general.

16
00:00:39,720 --> 00:00:42,880
And we'll talk about some specific
examples of this in a moment.

17
00:00:42,880 --> 00:00:46,210
But let's just imagine that when Q
is being represented by some kind of

18
00:00:46,210 --> 00:00:48,998
function approximater, and
here's how Q learning works.

19
00:00:48,998 --> 00:00:53,220
We have an experienced couple that
comes in, state, action, reward next,

20
00:00:53,220 --> 00:00:55,710
state and then we do an update.

21
00:00:55,710 --> 00:00:57,690
Now what's the update usually?

22
00:00:57,690 --> 00:01:02,230
We try to move the the value of
that state action pair a little bit

23
00:01:02,230 --> 00:01:06,890
towards the reward plus the discounted
value of the next state S prime.

24
00:01:06,890 --> 00:01:10,310
So, that's the bellman equation
kind of hidden in here and

25
00:01:10,310 --> 00:01:14,190
the difference between that and
the current Q value is the TD error.

26
00:01:14,190 --> 00:01:15,830
And what do we want to
do with the TD error?

27
00:01:15,830 --> 00:01:19,380
The TD error tells us is our
current prediction too high,

28
00:01:19,380 --> 00:01:20,680
too low, just right.

29
00:01:20,680 --> 00:01:22,450
You know does it like porridge?

30
00:01:22,450 --> 00:01:25,175
And what we want to do is change.

31
00:01:25,175 --> 00:01:27,670
[LAUGH]
>> That's the three bears I guess.

32
00:01:27,670 --> 00:01:31,320
But the fact that matter is we actually
need to know the kind of the direction

33
00:01:31,320 --> 00:01:32,260
that we should be going.

34
00:01:32,260 --> 00:01:36,770
Are we too high or too low and we want
to move the parameters that represent

35
00:01:36,770 --> 00:01:41,840
the Q function, you know so
a given weight say W super a sub i.

36
00:01:41,840 --> 00:01:43,310
We want to move it a little bit,

37
00:01:43,310 --> 00:01:47,540
this is a learning rate alpha,
in the direction of that TD Error.

38
00:01:47,540 --> 00:01:49,060
But how much do we move the weight,

39
00:01:49,060 --> 00:01:53,410
depends on how that weight actually
influences the Q function, right?

40
00:01:53,410 --> 00:01:57,210
So what's the partial derivative of
the Q value for that state action pair

41
00:01:57,210 --> 00:02:00,040
as a function of that particular
weight that we want to change?

42
00:02:00,040 --> 00:02:03,550
And we're going to change that weight
proportion, we're going to change things

43
00:02:03,550 --> 00:02:06,050
proportionately to that,
to that gradient.

44
00:02:06,050 --> 00:02:07,370
So this should look kind of familiar.

45
00:02:07,370 --> 00:02:08,229
Does this look familiar?

46
00:02:09,259 --> 00:02:13,030
>> Yeah, it looks exactly like the,
you know, we drew up the update rule.

47
00:02:13,030 --> 00:02:15,580
The things that we did with perceptrons.

48
00:02:15,580 --> 00:02:18,780
Yeah, it's a normal kind
of gradient update rule.

49
00:02:19,810 --> 00:02:20,540
>> Yeah, right.

50
00:02:20,540 --> 00:02:21,460
Exactly so.

51
00:02:21,460 --> 00:02:22,020
Right and so

52
00:02:22,020 --> 00:02:25,110
when you're representing a function in
terms of a set of parameters we want to

53
00:02:25,110 --> 00:02:28,840
know how did the parameters impact the
value that we're predicting and let's

54
00:02:28,840 --> 00:02:32,590
move those parameters in a direction
that makes the prediction more accurate.

55
00:02:32,590 --> 00:02:38,030
So, this is a way that we can actually
view Q learning as an update rule for

56
00:02:38,030 --> 00:02:39,699
underlying parameters of a function or
proxy.

57
00:02:41,900 --> 00:02:47,910
And I guess the only difference is that,
unlike the sort of y star minus

58
00:02:47,910 --> 00:02:53,320
y thing that we used to do, this is
what we got on this particular step.

59
00:02:53,320 --> 00:02:56,790
So that's what makes a TD Error
as opposed to actual error,

60
00:02:56,790 --> 00:02:57,610
if that makes any sense.

61
00:02:57,610 --> 00:02:58,410
Does that make sense?

62
00:02:58,410 --> 00:02:59,660
>> Yeah, I think that's exactly right.

63
00:02:59,660 --> 00:03:03,530
So when we were thinking about
supervised learning, what was happening

64
00:03:03,530 --> 00:03:06,600
is we have some kind of output for
our current input and we'll call that y.

65
00:03:06,600 --> 00:03:09,290
And then we have a target output, y*.

66
00:03:09,290 --> 00:03:11,860
And we want to change
the parameters of the function so

67
00:03:11,860 --> 00:03:15,190
that instead of getting something like
y we get something much more like y*.

68
00:03:15,190 --> 00:03:20,775
So I think think what your point is is
that this Bellman equation kind of value

69
00:03:20,775 --> 00:03:25,065
and this kind of current prediction
do play the rules of y* and y.

70
00:03:25,065 --> 00:03:26,775
So this I think is
a really good analogy.

71
00:03:26,775 --> 00:03:29,705
But one way that it's different is that
why star when we're doing supervised

72
00:03:29,705 --> 00:03:32,535
learning is actually the right value,
right?

73
00:03:32,535 --> 00:03:36,530
It's coming from some kind of
trusted source, and it is the label.

74
00:03:36,530 --> 00:03:41,210
Here, we're bootstrapping,
which I think is a kind of wrapping.

75
00:03:41,210 --> 00:03:46,610
And what we're doing with bootstrapping
is we're using our current predictions

76
00:03:46,610 --> 00:03:48,770
as a kind of way of making a target for

77
00:03:48,770 --> 00:03:51,730
what our other prediction
should be moving toward.

78
00:03:51,730 --> 00:03:55,090
So, this this should look
a little bit unnerving.

79
00:03:55,090 --> 00:03:59,260
But, it's not a bad idea but
it's also not clearly of great idea

80
00:03:59,260 --> 00:04:03,630
because instead of using real label data
we're actually making up our own labels

81
00:04:03,630 --> 00:04:06,580
using the current prediction
from the Q function.

82
00:04:06,580 --> 00:04:07,440
>> Right.
>> And we'll see

83
00:04:07,440 --> 00:04:09,520
sometimes this can really
run us in a trouble and

84
00:04:09,520 --> 00:04:11,880
we have to be a little bit careful
to make sure that it doesn't.

85
00:04:11,880 --> 00:04:12,660
Okay.
>> Right?

86
00:04:12,660 --> 00:04:14,630
So this is in a very general form.

87
00:04:14,630 --> 00:04:17,450
I think it would be helpful though to
make it make more sense if we dive in

88
00:04:17,450 --> 00:04:21,269
and think about some particular
representations of the Q function.

89
00:04:21,269 --> 00:04:21,769
>> Okay.
