1
00:00:00,410 --> 00:00:03,870
The secret sauce in Tornado for achieving the

2
00:00:03,870 --> 00:00:09,120
scalability is the concept called clustered object. The idea

3
00:00:09,120 --> 00:00:11,710
is that from the point of view of

4
00:00:11,710 --> 00:00:14,350
all the pieces of the operating system, executing on

5
00:00:14,350 --> 00:00:17,370
the different nodes, there's a single object reference.

6
00:00:17,370 --> 00:00:20,120
The object reference is the same. But the object

7
00:00:20,120 --> 00:00:23,760
reference under the covers may have multiple representations.

8
00:00:23,760 --> 00:00:25,420
So for instance, n0 may have a represenatation that

9
00:00:25,420 --> 00:00:28,990
it is lookating at, different from n1, different

10
00:00:28,990 --> 00:00:32,780
from n2 but the object reference is the same.

11
00:00:32,780 --> 00:00:37,380
So there is an illusion of a single object. So, that's what I meant when I said

12
00:00:37,380 --> 00:00:40,890
logically the operating system designer. I think of

13
00:00:40,890 --> 00:00:43,520
a shared data structure's logically the same thing. But

14
00:00:43,520 --> 00:00:47,020
physically, it may be replicated under the covers. Course,

15
00:00:47,020 --> 00:00:50,460
who decides to replicate it, that's the decision of

16
00:00:50,460 --> 00:00:54,090
the operating system as well. We'll see that in a minute. This is where the idea

17
00:00:54,090 --> 00:00:56,500
of clustering comes about. The na, the name

18
00:00:56,500 --> 00:00:59,820
clustered object. The degree of clustering, that is, the

19
00:00:59,820 --> 00:01:04,890
replication of a particular object, it's an implementation choice of

20
00:01:04,890 --> 00:01:09,760
the service, so as a designer of the service you make a decision whether

21
00:01:09,760 --> 00:01:15,600
a particular object is going to have a singleton representation,

22
00:01:15,600 --> 00:01:22,100
or is going to be one per core in the machine or one per cpu meaning

23
00:01:22,100 --> 00:01:28,100
it is shared by all the cores that may be there on a single cpu. Or maybe one

24
00:01:28,100 --> 00:01:30,920
representation of an object for a group of

25
00:01:30,920 --> 00:01:34,300
processes. So these are all design decisions that is

26
00:01:34,300 --> 00:01:36,910
left up to the implementor of the service. But

27
00:01:36,910 --> 00:01:41,330
when designing the service, you can think abstractly about

28
00:01:41,330 --> 00:01:44,710
the components of the service containing objects and

29
00:01:44,710 --> 00:01:48,100
each object is giving you the illusion that it

30
00:01:48,100 --> 00:01:51,060
is single object reference. But under the covers

31
00:01:51,060 --> 00:01:54,040
you might choose to implement the object with different

32
00:01:54,040 --> 00:01:56,870
level up replication, and of course if we

33
00:01:56,870 --> 00:01:59,700
are talking about replicated objects you have to worry

34
00:01:59,700 --> 00:02:03,150
about the consistency of the replicated objects, and

35
00:02:03,150 --> 00:02:07,460
this is were the, suggestion in the tornado system

36
00:02:07,460 --> 00:02:10,340
is to maintain the consistency of the objects.

37
00:02:10,340 --> 00:02:13,330
Through protective procedure call, that is implemented under

38
00:02:13,330 --> 00:02:15,260
the colors in the operating system. So in

39
00:02:15,260 --> 00:02:18,870
other words, as a designer of the service, you

40
00:02:18,870 --> 00:02:22,360
are going to orchestrate the sharing of the

41
00:02:22,360 --> 00:02:26,930
data structures that are replicated and you orchestrate

42
00:02:26,930 --> 00:02:29,380
maintenance of the consistency of the share data

43
00:02:29,380 --> 00:02:32,850
structures. Through protective procedure call that you execute across

44
00:02:32,850 --> 00:02:35,680
these replicas, and don't use the hardware coherence

45
00:02:35,680 --> 00:02:39,100
mechanism in order to maintain the consistency, and

46
00:02:39,100 --> 00:02:41,440
the reason for that is the hardware cache

47
00:02:41,440 --> 00:02:45,050
coherence it can be indiscriminate about how it does

48
00:02:45,050 --> 00:02:46,990
the hardware cache coherence whenever you touch a

49
00:02:46,990 --> 00:02:51,210
shared memory location. If it is present elsewhere, it

50
00:02:51,210 --> 00:02:53,520
is going to update that. And that's the

51
00:02:53,520 --> 00:02:58,080
reason we don't want to incur the overhead of

52
00:02:58,080 --> 00:03:00,560
the hardware cache coherence and replicate it. But if

53
00:03:00,560 --> 00:03:03,670
you replicate it then the hardware is normal. And,

54
00:03:03,670 --> 00:03:07,260
therefore, you have to worry about keeping these copies

55
00:03:07,260 --> 00:03:09,760
consistent with one another. But, of course, when in doubt,

56
00:03:09,760 --> 00:03:13,100
use a single representation. And that way, you have

57
00:03:13,100 --> 00:03:16,510
the hardware cache coherence as a security blanket when you're

58
00:03:16,510 --> 00:03:19,750
not sure yet about the level of clustering that

59
00:03:19,750 --> 00:03:23,180
you want in order to reduce the amount of contention

60
00:03:23,180 --> 00:03:25,750
for shared data structures. All of these may seem

61
00:03:25,750 --> 00:03:27,830
a little bit abstract at this point of time,

62
00:03:27,830 --> 00:03:30,040
but I'll make it very concrete, when we talk

63
00:03:30,040 --> 00:03:33,570
about a simple example, namely the memory management subsystem.
