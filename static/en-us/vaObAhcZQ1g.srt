1
00:00:00,130 --> 00:00:03,980
Okay, let's look at multiplying
by a positive scalar.

2
00:00:03,980 --> 00:00:08,600
So I started out by writing
out this equation Q,

3
00:00:08,600 --> 00:00:11,170
which you recognize as
>> The usual thing.

4
00:00:11,170 --> 00:00:13,000
>> The usual thing,
the Bellman equation, uh-huh.

5
00:00:14,270 --> 00:00:18,570
So let's imagine we've got some reward
function R and it therefore has

6
00:00:18,570 --> 00:00:24,390
a corresponding Q function or
solution to the Bellman equation.

7
00:00:24,390 --> 00:00:26,610
And this is going to
tell us how to behave.

8
00:00:26,610 --> 00:00:31,390
If you know the Q function for a given
reward function, then for each state

9
00:00:31,390 --> 00:00:34,200
you're going to take the action that
maximizes the Q value in that state.

10
00:00:34,200 --> 00:00:35,560
So this is kind of capturing

11
00:00:36,680 --> 00:00:39,940
everything we need to know if
we knew the solution here.

12
00:00:39,940 --> 00:00:42,570
But now what we're going to do is we're
going to generate a different reward

13
00:00:42,570 --> 00:00:43,840
function, R prime.

14
00:00:43,840 --> 00:00:51,430
So R prime for a given state action pair
is going to be defined as the old R,

15
00:00:51,430 --> 00:00:54,950
the old reward function for that state
action pair, multiplied by a constant.

16
00:00:56,100 --> 00:00:58,630
And that constant should be positive.

17
00:00:58,630 --> 00:01:01,330
Why should it be not 0?

18
00:01:01,330 --> 00:01:03,500
>> Well,
if it's 0 then everything's the same.

19
00:01:03,500 --> 00:01:06,450
>> Yes, if it's 0 then we shouldn't
expect any of this to work.

20
00:01:06,450 --> 00:01:08,650
If you multiply all the rewards by 0.

21
00:01:08,650 --> 00:01:09,790
>> You lose all information.

22
00:01:09,790 --> 00:01:10,580
>> Yeah, very good.

23
00:01:10,580 --> 00:01:12,560
And negative, what do you suppose
would happen with a negative?

24
00:01:12,560 --> 00:01:14,270
>> I'm going to say
the opposite of what you want,

25
00:01:14,270 --> 00:01:16,344
except I'm not sure what
opposite means in this case.

26
00:01:16,344 --> 00:01:19,680
>> That's right, it kind of flips
reward and punishment so then you end

27
00:01:19,680 --> 00:01:23,247
up trying to find the reward function
that maximizes pain, which could be

28
00:01:23,247 --> 00:01:27,450
an interesting exercise, but I think not
what we're trying to do in this case.

29
00:01:27,450 --> 00:01:28,865
>> Yeah, it's just like grad school.

30
00:01:28,865 --> 00:01:31,220
>> [LAUGH] I don't think the sign
is flipped for grad school.

31
00:01:31,220 --> 00:01:32,456
I think it's just-
>> Oh,

32
00:01:32,456 --> 00:01:35,132
I'm pretty sure the sign is flipped for
grad school.

33
00:01:35,132 --> 00:01:38,545
>> [SOUND] All right, so
here's what I would like you to do.

34
00:01:38,545 --> 00:01:42,698
I would like you to make a guess
as to if Q is the solution to this

35
00:01:42,698 --> 00:01:44,980
original Bellman equation, and

36
00:01:44,980 --> 00:01:50,930
R prime is our new reward function as
a function of the old reward function.

37
00:01:50,930 --> 00:01:55,946
How can we write the new Q function,
corresponding to this R prime,

38
00:01:55,946 --> 00:02:01,052
as a function of the solution to
the old Bellman equation with this Q?

39
00:02:01,052 --> 00:02:05,417
So I want Q'(s,a) in terms of Q(s,a).

40
00:02:05,417 --> 00:02:09,689
And that's going to give us some insight
as to whether or not we have a monotonic

41
00:02:09,689 --> 00:02:13,520
transform, that the action that
was best before is still best now.
