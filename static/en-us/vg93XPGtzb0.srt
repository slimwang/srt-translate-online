1
00:00:00,000 --> 00:00:03,555
So let's wrap up and, and summarize some of the things that we've learned.

2
00:00:03,555 --> 00:00:07,390
We've learned about parallel communication patterns going beyond the simple map

3
00:00:07,390 --> 00:00:11,387
operation that you saw in Unit 1 to encompass other important communication patterns,

4
00:00:11,387 --> 00:00:14,572
like gather, scatter, stencil, and transpose.

5
00:00:14,572 --> 00:00:18,116
We've learned more about the GPU programming model and the underlying hardware

6
00:00:18,116 --> 00:00:21,724
such as how thread blocks run on streaming multi-processors, or SMs,

7
00:00:21,724 --> 00:00:23,934
and what assumptions you can make about ordering,

8
00:00:23,934 --> 00:00:27,827
and how threads and thread blocks can synchronize to safely share data and memory.

9
00:00:27,827 --> 00:00:33,014
We've learned about that GPU memory model, topics like local, and global, and shared memory,

10
00:00:33,014 --> 00:00:37,970
and how atomic operations can simplify memory writes by concurrent threads.

11
00:00:37,970 --> 00:00:42,479
Finally we got a quick preview of strategies for efficient GPU programming.

12
00:00:42,479 --> 00:00:46,788
The first principle is to minimize the time spent on memory accesses

13
00:00:46,788 --> 00:00:49,737
by doing things like coalescing global memory access.

14
00:00:49,737 --> 00:00:53,470
We saw that the extremely fast global memory on the GPU operates best

15
00:00:53,470 --> 00:00:57,183
when adjacent threads access contiguous chunks of global memory,

16
00:00:57,183 --> 00:00:59,627
and this is called a coalesced memory access.

17
00:00:59,627 --> 00:01:03,225
We also learned to move frequently accessed data to faster memory.

18
00:01:03,225 --> 00:01:05,725
So, for example, promoting data to shared memory.

19
00:01:05,725 --> 00:01:09,941
And we learned that atomic memory operations have a cost, but they're great and they're

20
00:01:09,941 --> 00:01:12,754
useful and you shouldn't necessarily freak out about the cost.

21
00:01:12,754 --> 00:01:15,543
And often the cost is negligible, but it's something to be aware of.

22
00:01:15,543 --> 00:01:18,609
Along the same lines, we learned about avoiding thread divergence

23
00:01:18,609 --> 00:01:23,161
that comes with branches and loops and, once again, thread divergence comes at a cost.

24
00:01:23,161 --> 00:01:27,000
You should be aware of that, but it isn't necessarily something that you should be freaked out about.

25
00:01:27,000 --> 00:01:32,557
We're going to revisit these topics and talk much more about optimizing GPU programs in Unit 5. So that's it.
