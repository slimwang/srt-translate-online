1
00:00:00,280 --> 00:00:02,370
A third alternative is what is called a big

2
00:00:02,370 --> 00:00:05,364
flip. In the big flip, what we are doing

3
00:00:05,364 --> 00:00:07,740
is we are bringing down half the nodes at

4
00:00:07,740 --> 00:00:11,470
once. So in other words, the service is available

5
00:00:11,470 --> 00:00:17,140
at 50% of the full capacity. So you can see that with a big flip, half the nodes

6
00:00:17,140 --> 00:00:20,840
are down. So we have reduced the server capacity,

7
00:00:20,840 --> 00:00:25,400
the total DQ available by 50% for u units

8
00:00:25,400 --> 00:00:29,414
of time. And the total upgrade is going to

9
00:00:29,414 --> 00:00:32,680
last 2 times u because we have partitioned the

10
00:00:32,680 --> 00:00:35,900
entire server capacity in two halves, and upgrading one

11
00:00:35,900 --> 00:00:39,040
half, upgrading the second half for 2 times u

12
00:00:39,040 --> 00:00:42,851
time units we're not going to have full capacity. And

13
00:00:42,851 --> 00:00:45,898
during this 2 times U time unit we get

14
00:00:45,898 --> 00:00:49,180
the service at 50% capacity, and at the end

15
00:00:49,180 --> 00:00:50,860
of this it is back up to full capacity.

16
00:00:50,860 --> 00:00:53,180
So these are the three different choices that

17
00:00:53,180 --> 00:00:56,170
you've got, the fast reboot where the upgrade

18
00:00:56,170 --> 00:01:00,280
time for the entire service is just equal

19
00:01:00,280 --> 00:01:03,430
to the upgrade time for a single node. But

20
00:01:03,430 --> 00:01:07,150
here, we don't have the service available for

21
00:01:07,150 --> 00:01:09,260
any of the users for that period of

22
00:01:09,260 --> 00:01:11,350
time, and this might be a, a good

23
00:01:11,350 --> 00:01:16,400
strategy for services that can exploit the diurnal property

24
00:01:16,400 --> 00:01:20,180
of user community. Rolling upgrade is the more common

25
00:01:20,180 --> 00:01:24,570
one, where what we are doing is we are upgrading

26
00:01:24,570 --> 00:01:27,194
the servers one at a time but it's going

27
00:01:27,194 --> 00:01:29,490
to take a long time especially when you're talking about

28
00:01:29,490 --> 00:01:32,830
data centers having thousands and thousands of processors. Rolling

29
00:01:32,830 --> 00:01:35,470
upgrade is going to take a long time and it

30
00:01:35,470 --> 00:01:38,170
might be done in batches for instance instead of being

31
00:01:38,170 --> 00:01:42,110
exactly one at a time. And the extreme of that

32
00:01:42,110 --> 00:01:44,735
rolling upgrade is this big flip where we are

33
00:01:44,735 --> 00:01:47,690
saying we'll bring down 50% of the nodes, upgrade

34
00:01:47,690 --> 00:01:50,650
them, and then turn them back on, and then

35
00:01:50,650 --> 00:01:53,570
do the upgrade for the remaining 50%. So in the

36
00:01:53,570 --> 00:01:56,750
third case, in the big flip, the service is

37
00:01:56,750 --> 00:02:01,280
always available, but at 50% capacity for a certain duration

38
00:02:01,280 --> 00:02:03,140
of time. But there is no way to hide

39
00:02:03,140 --> 00:02:06,970
the DQ loss. And you'll see that the DQ loss,

40
00:02:06,970 --> 00:02:09,830
which is shown by the area of this

41
00:02:09,830 --> 00:02:12,990
shaded rectangle in each one of these cases, the

42
00:02:12,990 --> 00:02:15,860
area in the shaded rectangle is exactly the

43
00:02:15,860 --> 00:02:19,220
same. In other words, the DQ loss is the

44
00:02:19,220 --> 00:02:22,070
same for all three strategies, and it is

45
00:02:22,070 --> 00:02:25,240
the DQ loss of an individual node, the upgrade

46
00:02:25,240 --> 00:02:28,050
time for an individual node, and the number of

47
00:02:28,050 --> 00:02:32,300
servers, n, that you have in your server farm.

48
00:02:32,300 --> 00:02:36,450
That's the total DQ loss for online evolution, and

49
00:02:36,450 --> 00:02:40,240
all that these different strategies are saying is, as a

50
00:02:40,240 --> 00:02:43,072
system administrator, you have a choice on how you want

51
00:02:43,072 --> 00:02:46,740
to dish out the DQ loss. And make it apparent

52
00:02:46,740 --> 00:02:48,950
and not apparent to the user community. In this

53
00:02:48,950 --> 00:02:51,420
case, it's going to be apparent to the entire user

54
00:02:51,420 --> 00:02:54,980
community that there is upgrade going on. In this case,

55
00:02:54,980 --> 00:02:57,910
different segments of the user community are going to notice

56
00:02:57,910 --> 00:03:00,200
that the service has become unavailable for a

57
00:03:00,200 --> 00:03:02,980
short duration of time. And here it's sort of

58
00:03:02,980 --> 00:03:05,900
in between these two extremes and that half

59
00:03:05,900 --> 00:03:08,800
the user community may see a service being unavailable

60
00:03:08,800 --> 00:03:10,340
for some amount of time. So, in other

61
00:03:10,340 --> 00:03:15,650
words, using the DQ principle, maintenance and upgrades are

62
00:03:15,650 --> 00:03:20,780
controlled failures that can be handled by the administrator

63
00:03:20,780 --> 00:03:23,670
of a system service. So what we are seeing

64
00:03:23,670 --> 00:03:28,640
through this concept of DQ is a way by which a system

65
00:03:28,640 --> 00:03:33,480
developer and a system administrator can work together in figuring out how

66
00:03:33,480 --> 00:03:38,430
to architect the system in terms of how the data should be

67
00:03:38,430 --> 00:03:43,295
partitioned or replicated and how the system should

68
00:03:43,295 --> 00:03:48,770
fine-tune the service by looking at the instantaneous offered

69
00:03:48,770 --> 00:03:55,230
load and tuning whether to keep the yield the same or the harvest the same,

70
00:03:55,230 --> 00:03:57,500
knowing that DQ is a constant and the

71
00:03:57,500 --> 00:04:01,540
server is getting saturated. And finally, when service

72
00:04:01,540 --> 00:04:04,630
has to evolve once again, the system administrator

73
00:04:04,630 --> 00:04:07,260
can make an informed decision on how to

74
00:04:07,260 --> 00:04:11,080
do this online evolution by controlling the DQ

75
00:04:11,080 --> 00:04:14,450
loss that is experienced by the user community

76
00:04:14,450 --> 00:04:15,350
at any point of time.
