1
00:00:00,470 --> 00:00:04,530
Alright so we're now in a great position to talk about what the network part of

2
00:00:04,530 --> 00:00:06,530
the neural network is about. So now the

3
00:00:06,530 --> 00:00:09,720
idea is that we can construct using exactly these

4
00:00:09,720 --> 00:00:14,520
kind of sigmoid units, a chain of relationships between the input

5
00:00:14,520 --> 00:00:18,940
layer, which are the different components of x, with the output.

6
00:00:20,870 --> 00:00:25,320
Y, and the way this is going to happen is, there's

7
00:00:25,320 --> 00:00:27,800
other layers of, of units in between. That each

8
00:00:27,800 --> 00:00:31,720
one is computing the weighted sum, sigmoided, of the

9
00:00:31,720 --> 00:00:35,800
layer before it. These other layers of units are

10
00:00:35,800 --> 00:00:38,060
often referred to as hidden layers, because you can kind

11
00:00:38,060 --> 00:00:39,850
of see the inputs, you can see the outputs.

12
00:00:39,850 --> 00:00:42,850
This, this other stuff is, is less constrained. Or indirectly

13
00:00:42,850 --> 00:00:45,750
constrained. And what's happening is that each of these

14
00:00:45,750 --> 00:00:50,180
units, it's, it's running exactly that kind of, you know,

15
00:00:50,180 --> 00:00:53,020
take the weights, multiply by the things coming into it,

16
00:00:53,020 --> 00:00:55,090
put it through the sigmoid and that's your activation, that's your

17
00:00:55,090 --> 00:00:58,880
output. So, so what's cool about this is, in the case

18
00:00:58,880 --> 00:01:02,220
where all these are sigmoid units this mapping from input to

19
00:01:02,220 --> 00:01:07,250
output. Is differentiable in terms of the weights, and by saying

20
00:01:07,250 --> 00:01:10,530
the whole thing is differentiable, what I'm saying is that we

21
00:01:10,530 --> 00:01:13,320
can figure out for any given weight in the network how

22
00:01:13,320 --> 00:01:15,270
moving it up or down a little bit is going to

23
00:01:15,270 --> 00:01:17,190
change the mapping from inputs to outputs. So we can

24
00:01:17,190 --> 00:01:19,600
move all those weights in the direction of producing something more

25
00:01:19,600 --> 00:01:21,710
like the output that we want. Even though that there's

26
00:01:21,710 --> 00:01:26,740
all these sort of crazy non linearities in between. And so,

27
00:01:26,740 --> 00:01:30,440
this leads to an idea called back propagation, which is

28
00:01:30,440 --> 00:01:34,380
really just at its heart, a computationally beneficial organization of the

29
00:01:34,380 --> 00:01:37,640
chain rule. We're just computing the derivatives with respect to

30
00:01:37,640 --> 00:01:40,690
all the different weights in the network, all in one convenient

31
00:01:40,690 --> 00:01:45,500
way, that has, this, this lovely interpretation of having information flowing

32
00:01:45,500 --> 00:01:48,900
from the inputs to the outputs. And then error information flowing

33
00:01:48,900 --> 00:01:52,040
back from the outputs towards the inputs, and that tells you

34
00:01:52,040 --> 00:01:54,740
how to compute all the derivatives. And then, therefore how to make

35
00:01:54,740 --> 00:01:58,050
all the weight updates to make, the network produce something more

36
00:01:58,050 --> 00:02:00,020
like what you wanted it to produce. So this is where

37
00:02:00,020 --> 00:02:03,340
learning is actually taking place, and it's really neat! You know,

38
00:02:03,340 --> 00:02:05,770
this back propagation is referring to the fact that the errors are

39
00:02:05,770 --> 00:02:08,910
flowing backwards. Sometimes it is even called error back propagation.

40
00:02:08,910 --> 00:02:16,480
>> Nice, so here's a question for you Michael. What happens if I replace

41
00:02:16,480 --> 00:02:20,880
the sigmoid units with some other function and, and let's say that function is

42
00:02:20,880 --> 00:02:23,970
also differentiable. Well, if it's differentiable, then

43
00:02:23,970 --> 00:02:25,490
we can still do this, this basic

44
00:02:25,490 --> 00:02:28,130
kind of trick that says we can

45
00:02:28,130 --> 00:02:30,900
compute derivatives, and therefore we can move weights

46
00:02:30,900 --> 00:02:34,220
around to try to get the network to produce what we want it to produce.

47
00:02:34,220 --> 00:02:37,270
>> Hmm. That's a big win. Does it still act like a preceptron?

48
00:02:37,270 --> 00:02:39,250
>> Well, even this doesn't act exactly

49
00:02:39,250 --> 00:02:41,340
like a preceptron, right? So it's really just

50
00:02:41,340 --> 00:02:43,370
analogous to a preceptron, because we're not really

51
00:02:43,370 --> 00:02:45,900
doing the hard thresholding, we don't have guarantees

52
00:02:45,900 --> 00:02:48,370
of, of convergence in finite time. In

53
00:02:48,370 --> 00:02:51,030
fact, the error function can have many local

54
00:02:51,030 --> 00:02:56,030
optima, and what, what we mean by that is this idea that we're trying to get

55
00:02:56,030 --> 00:02:57,750
the, we're trying to set the weight so that the error

56
00:02:57,750 --> 00:03:01,010
is low, but you can get to these situations where none of

57
00:03:01,010 --> 00:03:04,060
the weights can really change without making the error worse. And you'd

58
00:03:04,060 --> 00:03:06,350
like to think, well good, then we're done, we've made the error

59
00:03:06,350 --> 00:03:08,090
as low as we can make it, but in fact it

60
00:03:08,090 --> 00:03:10,920
could actually just be stuck in a local optima, that there's a

61
00:03:10,920 --> 00:03:13,320
much better way of setting the weights It's just we have to

62
00:03:13,320 --> 00:03:16,220
change more than just one weight at a time to get there.

63
00:03:16,220 --> 00:03:17,960
>> Oh so that makes sense, so if we think about

64
00:03:17,960 --> 00:03:21,450
sigmoid the sigmoid and the error function that we picked, right.

65
00:03:21,450 --> 00:03:24,800
The error function was sum of squared errors,

66
00:03:24,800 --> 00:03:26,660
so that looks like a porabola in some

67
00:03:26,660 --> 00:03:29,310
high dimensional space, but once we start combining

68
00:03:29,310 --> 00:03:31,430
them with others like this over, over, and over

69
00:03:31,430 --> 00:03:35,860
again Then we have an error space where there may be lots of places that look

70
00:03:35,860 --> 00:03:37,610
low but only look low if you're standing

71
00:03:37,610 --> 00:03:40,870
there but globally would not be the lowest point.

72
00:03:40,870 --> 00:03:43,040
>> Right, exactly right and so you can get these

73
00:03:43,040 --> 00:03:46,580
situations in just the one unit version where the error

74
00:03:46,580 --> 00:03:48,920
function as you said is this nice little parabola and you can

75
00:03:48,920 --> 00:03:51,010
move down the gradient and when you get down to the bottom you're

76
00:03:51,010 --> 00:03:54,200
done. But now when we start throwing these networks of units together we

77
00:03:54,200 --> 00:03:57,170
can get an error surface that looks just in its cartoon form looks

78
00:03:57,170 --> 00:04:00,550
crazy like this, that there's, it's smooth but there's these Place where

79
00:04:00,550 --> 00:04:03,430
it goes down, comes up again and goes down maybe further, comes up

80
00:04:03,430 --> 00:04:07,160
again and doesn't come down as far and you could easily get yourself

81
00:04:07,160 --> 00:04:12,340
stuck at a point like this where you're not at the global minimum.

82
00:04:12,340 --> 00:04:13,630
Your at some local optimum.
