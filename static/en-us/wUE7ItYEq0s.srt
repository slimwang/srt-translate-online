1
00:00:00,310 --> 00:00:03,550
We didn't go into enough detail
about what it is we're trying to

2
00:00:03,550 --> 00:00:04,689
optimize here.

3
00:00:04,689 --> 00:00:09,390
I just said something vague like we
want to maximize the sum of our reward.

4
00:00:09,390 --> 00:00:14,340
Well, it's not so simple, in fact,
here's a great story to illustrate that.

5
00:00:14,340 --> 00:00:18,560
There's a great Russian comedian,
Yakov Smirnoff,

6
00:00:18,560 --> 00:00:22,250
you may remember him or not, but he
told this joke once that I really loved.

7
00:00:22,250 --> 00:00:26,420
He said,
have you heard about the Soviet lottery,

8
00:00:26,420 --> 00:00:31,070
it's a million rubles if you win,
one ruble a year for a million years.

9
00:00:32,500 --> 00:00:38,260
So the point is, and if you recall
from one of our earlier lessons,

10
00:00:38,260 --> 00:00:45,770
that one dollar or one ruble delivered
to us a million years in the future

11
00:00:45,770 --> 00:00:51,190
is really not as valuable as a dollar or
ruble that we get now.

12
00:00:51,190 --> 00:00:55,230
And so, for instance, if we think
about a robot living forever,

13
00:00:55,230 --> 00:00:59,370
it might do something just mundane
to gather a dollar a year.

14
00:00:59,370 --> 00:01:02,340
That's an infinite amount of money, but

15
00:01:02,340 --> 00:01:05,090
in practice it doesn't
really work that well.

16
00:01:05,090 --> 00:01:07,800
So to consider that,
and to illustrate that,

17
00:01:07,800 --> 00:01:10,460
I'm going to show you a little maze
problem here, and we'll think about

18
00:01:10,460 --> 00:01:14,920
what the robot ought to do that
would be optimal in this maze.

19
00:01:14,920 --> 00:01:18,850
So here's our robot, and
here's the challenge for our robot.

20
00:01:18,850 --> 00:01:23,830
We have a reward here of $1 and
a reward over here of $1 million.

21
00:01:24,850 --> 00:01:30,230
So if the robot comes over here and
gets this $1, it's special in that each

22
00:01:30,230 --> 00:01:36,140
time he touches it, he gets $1 and
it goes away but then it comes back.

23
00:01:36,140 --> 00:01:39,320
So the robot could come here go back and

24
00:01:39,320 --> 00:01:42,650
forth and
get a dollar each time it moves here.

25
00:01:42,650 --> 00:01:46,970
This one, once the robot tags it,
it's gone.

26
00:01:46,970 --> 00:01:50,260
But clearly it's worthwhile to
come over here and grab it.

27
00:01:50,260 --> 00:01:53,420
Now this red area is obstacle,
it can't go there.

28
00:01:53,420 --> 00:01:57,700
And here I wrote some
rewards that the robot,

29
00:01:57,700 --> 00:02:00,580
in fact negative one is a penalty.

30
00:02:00,580 --> 00:02:05,780
But the penalties the robot would
get as it went this way, and

31
00:02:05,780 --> 00:02:06,846
zero penalty that way.

32
00:02:06,846 --> 00:02:11,710
Now, if we say that what we want to
optimize is the sum of all future

33
00:02:11,710 --> 00:02:16,540
rewards, then it doesn't matter
whether we go this way and

34
00:02:16,540 --> 00:02:18,990
just get that dollar over and
over and over again.

35
00:02:18,990 --> 00:02:21,800
Or if we go this way,
get the million dollars,

36
00:02:21,800 --> 00:02:24,870
come back and get that $1 over and
over and over again.

37
00:02:24,870 --> 00:02:30,060
Now there's no difference because
they both sum to infinity over time.

38
00:02:30,060 --> 00:02:37,430
Now what if we say, okay, I want to
optimize my reward over three moves.

39
00:02:37,430 --> 00:02:40,690
So I've got a finite horizon.

40
00:02:40,690 --> 00:02:44,740
Let's consider the rewards we get
with a finite horizon of three

41
00:02:44,740 --> 00:02:46,680
if we go this way versus this way.

42
00:02:46,680 --> 00:02:52,706
So if we go this way,
we're going to get rewards of -1,

43
00:02:52,706 --> 00:02:57,815
-1, -1, and
if we go this way we get zero,

44
00:02:57,815 --> 00:03:04,900
$1, and then we have to move down here,
and get another zero.

45
00:03:06,620 --> 00:03:08,230
So clearly, starting here,

46
00:03:08,230 --> 00:03:12,700
with a finite horizon of three,
the best thing to do is go up there.

47
00:03:12,700 --> 00:03:17,450
Now, if we extend the horizon a little
bit further, say out to eight,

48
00:03:18,790 --> 00:03:22,150
we would find that this
is the best thing to do.

49
00:03:22,150 --> 00:03:23,540
So if we go this way,

50
00:03:23,540 --> 00:03:26,850
we get -1, -1, -1, until we hit
the jackpot here and get $1M.

51
00:03:26,850 --> 00:03:31,110
Clearly if you sum this up,
it's a pretty good prize.

52
00:03:31,110 --> 00:03:36,520
If we go this way and touch that $1
over and over again, we get this.

53
00:03:36,520 --> 00:03:43,830
So clearly as we expand our finite
horizon trivially up to say eight steps,

54
00:03:43,830 --> 00:03:46,430
going this way and tagging at one
million is the best thing to do.

55
00:03:46,430 --> 00:03:50,510
If we carried it even further, we'd
discover that then we should come back

56
00:03:50,510 --> 00:03:55,140
this way and go to that dollar and
tag it over and over and over again.

57
00:03:55,140 --> 00:03:57,060
Let me formalize these a little bit.

58
00:03:57,060 --> 00:04:02,820
With the infinite horizon what
we're trying to maximize is the sum

59
00:04:02,820 --> 00:04:05,810
of all rewards over all of the future.

60
00:04:05,810 --> 00:04:11,050
So it's the sum of each of these
rewards for i equals one to infinity.

61
00:04:11,050 --> 00:04:15,130
The finite horizon is very similar,
it's just we don't go to infinity.

62
00:04:15,130 --> 00:04:20,959
So for optimizing over horizon
of four steps, n would be four.

63
00:04:20,959 --> 00:04:25,770
We're just trying to maximize the sum
of the reward for the next four steps.

64
00:04:25,770 --> 00:04:31,380
Now, there is yet
another formulation that if you think

65
00:04:31,380 --> 00:04:36,520
back to that lecture a while back about
what's the value of a future dollar.

66
00:04:37,780 --> 00:04:39,220
We can dig that up and

67
00:04:39,220 --> 00:04:42,270
it makes a lot of sense in terms
of reinforcement learning.

68
00:04:42,270 --> 00:04:47,910
So remember that if it takes us say,
four years to get a dollar,

69
00:04:47,910 --> 00:04:51,080
that dollar is less valuable
than say if it takes one year.

70
00:04:51,080 --> 00:04:56,130
And the same way, if it takes,
say, eight steps to make a dollar,

71
00:04:56,130 --> 00:05:00,830
that dollar is less valuable than
a dollar I can get just in one step.

72
00:05:00,830 --> 00:05:04,810
And the way we represent
that is very simple.

73
00:05:04,810 --> 00:05:08,640
Just like we represented
the sum of future dividends and

74
00:05:08,640 --> 00:05:12,920
it looks like this,
it's called discounted reward.

75
00:05:12,920 --> 00:05:18,470
So instead of just summing up the r
sub i is we multiply it by this factor

76
00:05:18,470 --> 00:05:25,130
gamma to the i minus 1, such that our
immediate reward, the very next one

77
00:05:25,130 --> 00:05:30,160
we get, whatever gamma is when it gets
raised to the zero power is just one.

78
00:05:30,160 --> 00:05:34,560
So that means for
the very next step we get r.

79
00:05:34,560 --> 00:05:38,460
But for the step after it,
it's gamma to the one.

80
00:05:39,680 --> 00:05:43,910
So it devalues that reward a little bit.

81
00:05:43,910 --> 00:05:47,140
Gamma is a value between zero and one,

82
00:05:48,170 --> 00:05:53,730
the closer it is to one,
the more we value rewards in the future.

83
00:05:53,730 --> 00:05:57,700
The closer it is to zero,
the less we value rewards in the future.

84
00:05:57,700 --> 00:06:00,180
In fact, if gamma is set equal to one,

85
00:06:00,180 --> 00:06:03,390
this is exactly the same
as the infinite horizon.

86
00:06:03,390 --> 00:06:07,660
But gamma relates very strongly
to interest rates if you recall.

87
00:06:07,660 --> 00:06:14,310
So, if say, gamma were 0.95 it
means each step in the future is

88
00:06:14,310 --> 00:06:20,210
worth about 5% less than the immediate
reward if we got it right away.

89
00:06:20,210 --> 00:06:24,760
This is the method that
we use in q learning.

90
00:06:24,760 --> 00:06:29,020
One reason is that the math
turns out to be very handy, and

91
00:06:29,020 --> 00:06:31,280
it provides nice conversion properties.
