1
00:00:00,200 --> 00:00:04,120
So just to drive the point home
a little bit I just want to write down,

2
00:00:04,120 --> 00:00:07,090
sort of summarize what it is that
we've talked about so far and

3
00:00:07,090 --> 00:00:08,450
how it all kind of comes together.

4
00:00:08,450 --> 00:00:12,140
And I think it's actually going to
push it back to the very first slide.

5
00:00:12,140 --> 00:00:16,020
So, we've done MDPs
we talk about SMDP's.

6
00:00:16,020 --> 00:00:18,190
We talked a little about what
options actually do for you,

7
00:00:18,190 --> 00:00:22,250
And we talked a little bit
about state abstraction before.

8
00:00:22,250 --> 00:00:26,840
This all brings us to
a bunch of neat little facts.

9
00:00:26,840 --> 00:00:31,633
The first one is that because we
are doing this SNDP type thing,

10
00:00:31,633 --> 00:00:34,000
because we are doing options.

11
00:00:34,000 --> 00:00:39,150
We get to inherit all of the optimality
guarantees up to a point

12
00:00:39,150 --> 00:00:43,220
of what we had before in regular
MDPs with regular actions.

13
00:00:43,220 --> 00:00:46,030
So in particular we get all
the convergence proofs,

14
00:00:46,030 --> 00:00:47,850
we get all the stability proofs.

15
00:00:47,850 --> 00:00:49,830
In other words,
everything is all nice, and well, and

16
00:00:49,830 --> 00:00:52,260
good in SMDP land as it was in MDP land.

17
00:00:53,320 --> 00:00:55,700
There's a slight caveat to that
which is worth mentioning, and

18
00:00:55,700 --> 00:00:58,420
it's really what you
brought up over the quiz.

19
00:00:58,420 --> 00:01:02,470
Which is well if we don't pay attention
all these things, and we live around in

20
00:01:02,470 --> 00:01:07,820
the options space are we actually
going to be able to really be optimal?

21
00:01:07,820 --> 00:01:09,250
We really going to build
to solve the problem?

22
00:01:09,250 --> 00:01:11,010
Do I need to know all of these things?

23
00:01:11,010 --> 00:01:14,080
And the answer is, well if you've got
the right sort of options in the right

24
00:01:14,080 --> 00:01:17,660
side of world, right kind of world,
then in fact, everything's good.

25
00:01:18,820 --> 00:01:22,880
But you cannot guarantee that if
all you do is reason over options,

26
00:01:22,880 --> 00:01:25,330
that you will in fact end
up with an optimal policy.

27
00:01:25,330 --> 00:01:28,920
We have another notion called
hierarchical optimality.

28
00:01:28,920 --> 00:01:32,920
And that's the notion that given
the set of options that you have,

29
00:01:32,920 --> 00:01:38,059
you can in fact converge to an optimal
policy with respect to those options.

30
00:01:39,560 --> 00:01:41,360
And I think that makes sense.

31
00:01:41,360 --> 00:01:44,780
And of course whether you are allow
those options to pay attention

32
00:01:44,780 --> 00:01:46,220
to all of the state space,

33
00:01:46,220 --> 00:01:50,900
also affects whether you're going to get
the true optimality that you would have.

34
00:01:50,900 --> 00:01:55,330
Or only a sort of optimality with
respect to the particular set of

35
00:01:55,330 --> 00:01:58,230
options, and the states,
the state features that you happen to be

36
00:01:58,230 --> 00:02:01,270
paying attention But I actually don't
think this is a big deal right because I

37
00:02:01,270 --> 00:02:03,320
think this is true for
the problem in general.

38
00:02:03,320 --> 00:02:06,750
So I've got an MDP and
I have a bunch of states.

39
00:02:06,750 --> 00:02:09,360
I made up the states anyway
as we've talked about

40
00:02:09,360 --> 00:02:12,180
before we talk about the palm and
MDPs and PSRs.

41
00:02:12,180 --> 00:02:16,060
It's not clear these dates actually
exist in some very real sense.

42
00:02:16,060 --> 00:02:18,740
I came up with the state features
I described the way the world was

43
00:02:18,740 --> 00:02:19,295
supposed to be.

44
00:02:19,295 --> 00:02:23,130
There're clearly some ways of describing
the state of Pac-Man that make learning

45
00:02:23,130 --> 00:02:27,240
easier and some that make it harder,
some that allow us to be truly optimal,

46
00:02:27,240 --> 00:02:28,360
and some that don't.

47
00:02:28,360 --> 00:02:31,560
Like if I had came up with
state features that only track

48
00:02:31,560 --> 00:02:34,500
the closest dots to me
I may not be optimal.

49
00:02:34,500 --> 00:02:36,550
But on the other hand it
might be easier to learn and

50
00:02:36,550 --> 00:02:38,500
these are tradeoffs that
we make all the time.

51
00:02:38,500 --> 00:02:42,700
So, in some sense we're always dealing
with a limited notion of optimality.

52
00:02:42,700 --> 00:02:45,490
Which we define with respect to
the set of actions we decide

53
00:02:45,490 --> 00:02:48,090
that we're going to consider
in the set of state features

54
00:02:48,090 --> 00:02:50,280
that we're going to pay any
attention to at any given time.
