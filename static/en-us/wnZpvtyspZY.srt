1
00:00:00,000 --> 00:00:06,000
In linear regression, we are given data and data has still has more than one dimension.

2
00:00:06,000 --> 00:00:09,000
We have just studied for two dimensions throughout this class.

3
00:00:09,000 --> 00:00:13,000
So this might be the data and we're looking for the best line that fits the data.

4
00:00:13,000 --> 00:00:18,000
We'll put differently the parameters a and b that we discussed before that defines the line.

5
00:00:18,000 --> 00:00:20,000
The word best is interesting.

6
00:00:20,000 --> 00:00:24,000
Obviously, it's impossible to put a line through the data points over here.

7
00:00:24,000 --> 00:00:27,000
These are what's called nonlinear data that go up and down.

8
00:00:27,000 --> 00:00:32,000
Data often looks like this even if the relationship between x and y is linear--

9
00:00:32,000 --> 00:00:37,000
that's usually what's called noise in the data, some amount of randomness you can't explain.

10
00:00:37,000 --> 00:00:45,000
So in finding the best fit, we're trying to find a line that minimizes the difference between the data

11
00:00:45,000 --> 00:00:49,000
and the line in the y direction and that's somewhat counter intuitive.

12
00:00:49,000 --> 00:00:53,000
You'd normally think the distance is given by these lines over here in red

13
00:00:53,000 --> 00:00:56,000
and that is the shortest distance irrespective of x and y.

14
00:00:56,000 --> 00:01:03,000
But in linear regression, it turns out that we're minimizing the distance just in the y direction.

15
00:01:03,000 --> 00:01:06,000
And there's a lot of theory why this is a good idea.

16
00:01:06,000 --> 00:01:12,000
In summary, we assume that our data is the result of this blank some unknown linear function,

17
00:01:12,000 --> 00:01:19,000
bx+a+noise and if this noise is assumed to be Gaussian, they're minimizing the quadratic deviation

18
00:01:19,000 --> 00:01:23,000
between the data points and the line happens to be the correct mathematical answer.

19
00:01:23,000 --> 00:01:28,000
But leaving this aside, what we're doing is we are adding over all data points

20
00:01:28,000 --> 00:01:34,000
the difference between our function and the y value of the data point with the square

21
00:01:34,000 --> 00:01:37,000
and that distance is the distance we're minimizing.

22
00:01:37,000 --> 00:01:40,000
So let's look into this with examples--for the following four data points,

23
00:01:40,000 --> 00:01:43,000
which of these lines has the smallest quadratic error

24
00:01:43,000 --> 99:59:59,999
and would be the best regression line--pick one of the three.
