1
00:00:00,380 --> 00:00:03,440
So that's at least in a nut shell the reinforcement learning story.

2
00:00:03,440 --> 00:00:06,050
There's there's a lot of other topics and I think we are

3
00:00:06,050 --> 00:00:08,450
planning to get to some of them in a topics lesson a

4
00:00:08,450 --> 00:00:11,640
little bit later, but there's you know, courses and books worth of stuff

5
00:00:11,640 --> 00:00:15,040
to study in this area. Just like supervised learning as a whole.

6
00:00:15,040 --> 00:00:17,800
But, we're going to just kind of end it there with the idea

7
00:00:17,800 --> 00:00:21,190
that we now have a handle on how we can use learning

8
00:00:21,190 --> 00:00:25,460
to do decisions with delayed rewards. So, can you help us summarize what

9
00:00:25,460 --> 00:00:27,000
we what we learned in this lesson?

10
00:00:27,000 --> 00:00:29,370
>> Okay, sure. I think what did I learn here today,

11
00:00:29,370 --> 00:00:31,180
I think I learned a lot of things, some of which had

12
00:00:31,180 --> 00:00:35,530
to do with reinforcement learning Mainly that you can actually learn how

13
00:00:35,530 --> 00:00:38,670
to solve an MDP. I think that's actually a pretty big deal.

14
00:00:38,670 --> 00:00:41,300
>> Right. Meaning that we don't know T and R.

15
00:00:41,300 --> 00:00:42,460
>> Mm-hm.

16
00:00:42,460 --> 00:00:44,940
>> But we just have access to. The

17
00:00:44,940 --> 00:00:52,182
ability to interact with the environment and recieve transitions.

18
00:00:52,182 --> 00:00:53,820
>> And, that's actually that's actually pretty

19
00:00:53,820 --> 00:00:56,000
impressive. And, a very powerful thing. Because,

20
00:00:56,000 --> 00:00:59,670
we're often not, if we assume the world is an MDP, but we don't know

21
00:00:59,670 --> 00:01:03,670
TNR. If we don't have some way of learning in that we don't really have

22
00:01:03,670 --> 00:01:06,450
much we can do. And, you've showed me that their is something we can do.

23
00:01:07,610 --> 00:01:07,850
>> Cool.

24
00:01:07,850 --> 00:01:09,850
>> I think that's the biggest thing. We learned

25
00:01:09,850 --> 00:01:12,333
some specific things. In particular, we learned about Q-learning.

26
00:01:12,333 --> 00:01:15,100
>> Several kinds of Q-learning, but one is actually a real word.

27
00:01:15,100 --> 00:01:18,140
>> Yes [LAUGH]

28
00:01:18,140 --> 00:01:23,050
indeed they're all real, Michael. We learned about Q-learning. And I think the

29
00:01:23,050 --> 00:01:24,460
other most important thing that we learned

30
00:01:24,460 --> 00:01:27,360
about is the exploration versus exploitation trail.

31
00:01:27,360 --> 00:01:30,910
>> And with Q-learning, we learned a little bit about when it converges.

32
00:01:30,910 --> 00:01:31,355
>> Mm-hm.

33
00:01:31,355 --> 00:01:33,340
>> And that it's actually a family of

34
00:01:33,340 --> 00:01:36,940
algorithms. And different members of that family have different

35
00:01:36,940 --> 00:01:38,570
behaviors associated with them. Oh, there's one other

36
00:01:38,570 --> 00:01:40,430
thing I wanted to say on that topic, actually.

37
00:01:40,430 --> 00:01:43,260
>> Okay. Which is, that one way to

38
00:01:43,260 --> 00:01:47,330
achieve this exploration-exploitation balance, was to

39
00:01:47,330 --> 00:01:49,170
randomly choose actions. So to change

40
00:01:49,170 --> 00:01:52,590
the we're doing action selection. But there's another one too, which is that

41
00:01:52,590 --> 00:01:58,540
we can actually by manipulating the initialization of the Q function. We can

42
00:01:58,540 --> 00:02:02,600
actually get another kind of exploration, can you see how that might work?

43
00:02:02,600 --> 00:02:03,920
>> Oh, I know what you do. If you could, if

44
00:02:03,920 --> 00:02:08,340
you set, say the Q values to all be the highest

45
00:02:08,340 --> 00:02:09,650
possible value they could be.

46
00:02:09,650 --> 00:02:15,640
>> Great, so if we initialized the Q hat to awesome values, then what the

47
00:02:15,640 --> 00:02:17,170
Q lettering algorithm would do, even with

48
00:02:17,170 --> 00:02:19,380
greedy exploration, what it will do is it

49
00:02:19,380 --> 00:02:22,730
will try things that it hasn't tried very much, and it still thinks are awesome.

50
00:02:22,730 --> 00:02:23,890
And little by little, it gets a more

51
00:02:23,890 --> 00:02:26,850
realistic sense of how the environment works. And.

52
00:02:26,850 --> 00:02:27,810
>> So it's very optimistic?

53
00:02:27,810 --> 00:02:33,410
>> That's right, exactly and it's referred to often as optimism in

54
00:02:33,410 --> 00:02:37,200
the face of uncertainty and it's a similar kind of idea that's

55
00:02:37,200 --> 00:02:41,330
used in algorithms like, A*, if you're familiar with search algorithms in AI.

56
00:02:41,330 --> 00:02:42,940
>> Oh yes, I remember those.

57
00:02:42,940 --> 00:02:45,200
>> But this is, this is a really powerful idea and it's

58
00:02:45,200 --> 00:02:46,820
used in, in reinforcement learning and

59
00:02:46,820 --> 00:02:50,400
banded algorithms and planning and And search.

60
00:02:50,400 --> 00:02:52,000
>> Okay, and that makes sense because if

61
00:02:52,000 --> 00:02:54,630
everything is awesome. Then your true key value

62
00:02:54,630 --> 00:02:56,410
can only go down if awesome is bigger

63
00:02:56,410 --> 00:02:58,510
than the biggest Q value you could ever have.

64
00:02:58,510 --> 00:03:01,970
And so that means you're going to look at every single action. And as you learn

65
00:03:01,970 --> 00:03:02,920
more about them, then you will just

66
00:03:02,920 --> 00:03:04,960
get more depressed about them. And that's good.

67
00:03:04,960 --> 00:03:08,620
>> [LAUGH] Yes the world slowly beats you

68
00:03:08,620 --> 00:03:12,470
down. [LAUGH] So is that it? Is that all

69
00:03:12,470 --> 00:03:14,430
we really talked about? I guess that's about

70
00:03:14,430 --> 00:03:16,380
right. We talked about what a que function was.

71
00:03:16,380 --> 00:03:17,830
>> Right.

72
00:03:17,830 --> 00:03:20,580
>> And how that kind of binds everything together

73
00:03:20,580 --> 00:03:23,940
and we talked about different approaches including policy search and

74
00:03:23,940 --> 00:03:25,090
model base reinforcement learning.

75
00:03:25,090 --> 00:03:28,190
>> Yeah that was very nice. We tied it all back into planning.

76
00:03:28,190 --> 00:03:30,660
>> So one, one thing we didn't talk about

77
00:03:30,660 --> 00:03:34,550
is connecting to function approximation, and the issues in

78
00:03:34,550 --> 00:03:36,290
machine learning that are really important things things like

79
00:03:36,290 --> 00:03:39,450
over fitting. They come up in the reinforcement learning setting,

80
00:03:39,450 --> 00:03:41,330
but not in this simplified setting that were looking

81
00:03:41,330 --> 00:03:43,480
at here where we learn a separate value. For

82
00:03:43,480 --> 00:03:45,570
each state action pair, we're going to have to

83
00:03:45,570 --> 00:03:49,190
start generalizing to see the importance of that. And that's,

84
00:03:49,190 --> 00:03:50,360
we're going to do in a later lesson.

85
00:03:50,360 --> 00:03:52,360
>> Okay. I like it. And we also learned

86
00:03:52,360 --> 00:03:55,550
a bunch of things about letters. Like exploration versus exploitation.

87
00:03:55,550 --> 00:03:59,060
>> In fact, we know enough that we can now get an A in letters.

88
00:03:59,060 --> 00:04:01,610
>> [LAUGH]

89
00:04:01,610 --> 00:04:05,450
I like it. Okay. Well, I think we learned a lot, Michael.

90
00:04:05,450 --> 00:04:09,740
>> [LAUGH] Okay. Well, good. Well, thanks. It's very nice

91
00:04:09,740 --> 00:04:11,350
to get a chance to talk to you about this stuff.

92
00:04:11,350 --> 00:04:14,390
>> Cool. And so I guess. What are we going to talk about next.

93
00:04:14,390 --> 00:04:17,579
>> Well Whatever it says on the syllabus.

94
00:04:17,579 --> 00:04:18,829
>> I think it's game theory.

95
00:04:18,829 --> 00:04:19,870
>> That's pretty cool.

96
00:04:19,870 --> 00:04:22,370
>> Oh. I see why we're going to do that.

97
00:04:22,370 --> 00:04:25,400
Because all we've been talking about the world as if

98
00:04:25,400 --> 00:04:28,340
there were just one agent and nobody else. And now

99
00:04:28,340 --> 00:04:31,090
we're going to see what happens. When there are other people.

100
00:04:31,090 --> 00:04:34,070
>> Right. Other people show up at the party, next time.

101
00:04:34,070 --> 00:04:35,660
>> On, Machine Learning.
