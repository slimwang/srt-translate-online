1
00:00:00,000 --> 00:00:03,000
Let's build a probabilistic word model of segmentation.

2
00:00:03,000 --> 00:00:08,000
By definition, the best segmentation, which we'll call S*,

3
00:00:08,000 --> 00:00:15,000
is equal to the one which maximizes the joint probability of the segmentation.

4
00:00:15,000 --> 00:00:18,000
So we're going to segment the text into a sequence of words--

5
00:00:18,000 --> 00:00:20,000
word 1 through word n--

6
00:00:20,000 --> 00:00:26,000
and find that segmentation into words that maximize the joint probability.

7
00:00:26,000 --> 00:00:33,000
By the definition of joint probability, that's the same as maximizing the product over the words

8
00:00:33,000 --> 00:00:41,000
of the probability of each word given all the previous words.

9
00:00:41,000 --> 00:00:46,000
Now this is going to be a little unwieldy to deal with, so we can make an approximation.

10
00:00:46,000 --> 00:00:52,000
We can say that the best segmentation is approximately equal to the one that maximizes,

11
00:00:52,000 --> 00:00:56,000
and what we could do here is we could make the Markov assumption

12
00:00:56,000 --> 00:01:00,000
and say we're only going to be considering the few previous words.

13
00:01:00,000 --> 00:01:04,000
But I'm going to go all the way and make the naive Bayes assumption

14
00:01:04,000 --> 00:01:08,000
and say we're going to treat each word independently.

15
00:01:08,000 --> 00:01:12,000
We just want to maximize the probability of each individual word

16
00:01:12,000 --> 00:01:15,000
regardless of the word that comes before or after it.

17
00:01:15,000 --> 00:01:19,000
Now, I know that that assumption is wrong and that the words do depend

18
00:01:19,000 --> 00:01:21,000
on the words to the right or the left of them,

19
00:01:21,000 --> 00:01:27,000
but I'm going to hope that this simplification is going to make the process of learning easier

20
00:01:27,000 --> 99:59:59,999
and will turn out to be good enough.
