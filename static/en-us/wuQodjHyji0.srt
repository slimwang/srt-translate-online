1
00:00:00,000 --> 00:00:03,000
There are a couple of things to observe here.

2
00:00:03,000 --> 00:00:10,000
One is in general the fake data pulls everything towards 0.5.

3
00:00:10,000 --> 00:00:15,000
Where you go to extremes over here, we are less extreme in this case.

4
00:00:15,000 --> 00:00:20,000
0.33 is further away from 0.5 than 0.4.

5
00:00:20,000 --> 00:00:24,000
So, all these numbers get moved towards 0.5.

6
00:00:24,000 --> 00:00:27,000
This is somewhat smoother.

7
00:00:27,000 --> 00:00:32,000
We also see that these two outcomes--the first and the last--

8
00:00:32,000 --> 00:00:36,000
on the division model gives us the same extreme estimate,

9
00:00:36,000 --> 00:00:43,000
but the more data we get in our new estimator, the more we are willing to move away from 0.5.

10
00:00:43,000 --> 00:00:49,000
One observation of heads gave us 0.667, two of them 0. 75.

11
00:00:49,000 --> 00:00:54,000
I can promise you in the limit, as you only see heads for infinitely many,

12
00:00:54,000 --> 00:00:57,000
we will finally approach 1. Now, this is really cool.

13
00:00:57,000 --> 00:01:05,000
We added fake data, and I will tell you that I generally think these are better estimates in practice.

14
00:01:05,000 --> 00:01:13,000
The reason why is it's really reckless after a single coin flip to assume that all coins come up positive.

15
00:01:13,000 --> 00:01:17,000
I think it's much more moderate to say, well, we already have some evidence

16
00:01:17,000 --> 00:01:21,000
that heads might be more likely, but we're not quite convinced yet.

17
00:01:21,000 --> 00:01:26,000
The not quite convinced is the same as having a prior.

18
00:01:26,000 --> 00:01:29,000
There's an entire literature that talks about these priors.

19
00:01:29,000 --> 00:01:31,000
They have a very cryptic name.

20
00:01:31,000 --> 00:01:33,000
They're called Dirichlet priors.

21
00:01:33,000 --> 00:01:40,000
But, more importantly, the method of adding fake data is called a Laplacian estimator.

22
00:01:40,000 --> 00:01:44,000
When there is plenty data, Laplacian estimator gives about the same results

23
00:01:44,000 --> 00:01:46,000
as the maximum likelihood estimator.

24
00:01:46,000 --> 00:01:52,000
But when data is scarce, this works usually much, much, much better

25
00:01:52,000 --> 00:01:54,000
than the maximum likelihood estimator.

26
00:01:54,000 --> 99:59:59,999
It's a really important lesson in statistics.
