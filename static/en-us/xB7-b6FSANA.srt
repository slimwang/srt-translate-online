1
00:00:00,390 --> 00:00:02,910
All right.
Now, follow me carefully, all right.

2
00:00:03,950 --> 00:00:05,090
u is some vector.

3
00:00:05,090 --> 00:00:06,090
Which vector?

4
00:00:06,090 --> 00:00:08,039
u, not me, u.

5
00:00:08,039 --> 00:00:13,610
A matrix times a vector, since
everything is linear, if I describe u

6
00:00:13,610 --> 00:00:18,410
as being made up of a bunch of
components that add up to u,

7
00:00:18,410 --> 00:00:21,940
it's the same thing as sigma times
those bunch of those components.

8
00:00:21,940 --> 00:00:29,370
So in particular, the components could
be the eigenvectors of sigma, so I could

9
00:00:29,370 --> 00:00:32,980
express u to be eigenvector of sigma
point to whatever direction they point.

10
00:00:32,980 --> 00:00:37,200
I can think of u as being made up
of the some of those, all right?

11
00:00:37,200 --> 00:00:40,890
So u is a unit vector.

12
00:00:40,890 --> 00:00:43,050
Okay, it can be expressed in term and

13
00:00:43,050 --> 00:00:46,900
it's what it says the linear sum
of those eigenvectors of sigma.

14
00:00:48,390 --> 00:00:52,170
The question is which of those
directions would return me the largest

15
00:00:52,170 --> 00:00:54,400
value, okay?

16
00:00:54,400 --> 00:00:57,560
Well, we know that

17
00:00:57,560 --> 00:01:02,130
sigma times an eigenvector is
equal to lambda eigenvector.

18
00:01:02,130 --> 00:01:05,459
If I'm going to pick a single
direction that will return the largest

19
00:01:05,459 --> 00:01:08,740
vector back, and then I'm going to
take the, the dot product itself to

20
00:01:08,740 --> 00:01:12,895
get the magnitude, I would need the
eigenvector with the largest eigenvalue.

21
00:01:14,020 --> 00:01:15,100
And at the heart of it,

22
00:01:15,100 --> 00:01:19,860
that That's actually why the eigenvector
with the largest eigenvalue

23
00:01:19,860 --> 00:01:24,090
is the direction along which
you get the greatest variance.

24
00:01:24,090 --> 00:01:26,800
So, you can either spend a little
time thinking about that and

25
00:01:26,800 --> 00:01:29,910
understand why that's true or
you can believe me.

26
00:01:29,910 --> 00:01:32,340
I guess I could have written it
out more, but I think most of you,

27
00:01:32,340 --> 00:01:35,890
at this point know that I take the, the
eigenvector, the largest eigenvalue of

28
00:01:35,890 --> 00:01:39,870
the covariance matrix, that's
the direction of the greatest variation.

29
00:01:39,870 --> 00:01:43,250
So that's what this says here, that
the direction that captures the maximum

30
00:01:43,250 --> 00:01:46,720
covariance of the data
is the eigenvector

31
00:01:46,720 --> 00:01:51,515
corresponding to the largest eigenvalue
of the data covariance matrix.

32
00:01:51,515 --> 00:01:53,700
Ta-da, that's what we're doing.

33
00:01:54,780 --> 00:01:59,450
So, furthermore, that's the longest one,
I can take the top, I don't know,

34
00:01:59,450 --> 00:02:00,640
k of them.

35
00:02:00,640 --> 00:02:03,660
Orthogonal direct, and
they're all orthogonal to each other.

36
00:02:03,660 --> 00:02:08,830
And that will capture the most variation
that I can, of k orthogonal vectors.

37
00:02:08,830 --> 00:02:11,474
So I can say well I've got
this 10,000 dimensional thing.

38
00:02:11,474 --> 00:02:14,190
Maybe I'll take the top 10, 20, 30, 40.

39
00:02:14,190 --> 00:02:19,093
That captures the, as much
variation as I can with 10, 20, 30,

40
00:02:19,093 --> 00:02:20,989
40 orthogonal vectors.

41
00:02:20,989 --> 00:02:24,690
So that's what I, I want to use.

42
00:02:24,690 --> 00:02:25,966
But in order to do that,

43
00:02:25,966 --> 00:02:29,680
in order to do PCA in images,
on images I have to teach you a trick.

44
00:02:29,680 --> 00:02:33,490
And this is the PCA for d much,
much, much bigger than m or n trick.

45
00:02:33,490 --> 00:02:34,440
All right.

46
00:02:35,690 --> 00:02:37,380
And so let's do the trick.
