1
00:00:00,480 --> 00:00:04,290
In order to motivate, we're going to go let's do the math behind some

2
00:00:04,290 --> 00:00:07,200
simple fitting that you, that you already know.

3
00:00:07,200 --> 00:00:09,990
I'm talking about typical least squares line fitting.

4
00:00:09,990 --> 00:00:15,530
All right, so in least squares line fitting we're going to assume that you know,

5
00:00:15,530 --> 00:00:16,820
we've got some data points.

6
00:00:16,820 --> 00:00:18,840
X1, y1 up through x n, y n.

7
00:00:18,840 --> 00:00:20,490
All right.

8
00:00:20,490 --> 00:00:23,580
And, we're going to use that equation y equals m x plus b

9
00:00:23,580 --> 00:00:26,240
because it's just easier to show what we're doing here.

10
00:00:26,240 --> 00:00:28,590
We had done that whole polar coordinate thing for

11
00:00:28,590 --> 00:00:31,340
Hough transform, we don't need to, to do that here.

12
00:00:31,340 --> 00:00:36,170
So, least squares line fitting would say find the m and

13
00:00:36,170 --> 00:00:41,040
b that minimizes this error function, okay.

14
00:00:41,040 --> 00:00:44,770
And this error function, which says I've got some y and

15
00:00:44,770 --> 00:00:50,050
I want you to find an m and b that minimizes the difference the y that

16
00:00:50,050 --> 00:00:55,000
was actually measured and the y that would be predicted based upon the x value.

17
00:00:55,000 --> 00:00:57,160
Okay. And that looks like this in the picture.

18
00:00:57,160 --> 00:00:57,840
All right?

19
00:00:57,840 --> 00:01:01,520
So the idea is that the distance that we're minimizing,

20
00:01:01,520 --> 00:01:05,730
is the this vertical distance to the line.

21
00:01:05,730 --> 00:01:07,710
How do we do that mathematically?

22
00:01:07,710 --> 00:01:08,880
well, it's pretty straightforward.

23
00:01:08,880 --> 00:01:11,390
We take our original equation, and

24
00:01:11,390 --> 00:01:15,030
all we're going to do is we're going to essentially vectorize and matricize it.

25
00:01:15,030 --> 00:01:16,115
I know vectorize is a word.

26
00:01:16,115 --> 00:01:17,200
Matricize should be a word.

27
00:01:17,200 --> 00:01:19,220
It's just like a really great word.

28
00:01:19,220 --> 00:01:21,830
So, in today's day and age, I'm going to trademark that, so

29
00:01:21,830 --> 00:01:24,740
if any of you uses it on Twitter, you have to pay me money.

30
00:01:24,740 --> 00:01:26,570
All right, here's what we're going to do.

31
00:01:26,570 --> 00:01:28,350
It's pretty simple.

32
00:01:28,350 --> 00:01:31,300
So, this was our original equation.

33
00:01:31,300 --> 00:01:32,500
Okay, where what I've done is,

34
00:01:32,500 --> 00:01:36,440
I've turned this component into vector multiplication.

35
00:01:36,440 --> 00:01:37,680
All right?

36
00:01:37,680 --> 00:01:39,545
And then, because I'm doing this sum,

37
00:01:39,545 --> 00:01:44,120
okay, over this squares, what I'm going to do is, I say, okay.

38
00:01:44,120 --> 00:01:50,220
I take this vector minus these squares.

39
00:01:50,220 --> 00:01:51,690
Okay, that gives me what?

40
00:01:51,690 --> 00:01:56,810
That's another vector that is just the deltas between the y and the prediction.

41
00:01:56,810 --> 00:01:58,730
And I say I want the magnitude of the,

42
00:01:58,730 --> 00:02:01,830
of the square magnitude, which would just be the sum of the squares.

43
00:02:01,830 --> 00:02:05,140
So that just gives me this very simple expression.

44
00:02:05,140 --> 00:02:10,410
So I go through all this mess to get this very simple expression, y minus Xh.

45
00:02:11,880 --> 00:02:15,850
Where the y here is this y vector.

46
00:02:15,850 --> 00:02:20,740
The x here is this matrix of the x and 1 for

47
00:02:20,740 --> 00:02:22,810
all the rows for each of the points.

48
00:02:22,810 --> 00:02:27,990
And then this h, I just use h to be the parameter for m and b.

49
00:02:29,310 --> 00:02:33,440
So what we have is this y minus Xh squared.

50
00:02:33,440 --> 00:02:34,460
All right? That's our error.

51
00:02:34,460 --> 00:02:37,650
And what we want to do, of course, is minimize that.

52
00:02:37,650 --> 00:02:38,250
All right?

53
00:02:38,250 --> 00:02:39,580
So I just expand that out.

54
00:02:39,580 --> 00:02:40,990
That's what I have here on the bottom.

55
00:02:42,060 --> 00:02:43,630
All right. Group the whole thing.

56
00:02:43,630 --> 00:02:45,530
I've now got this equation there.

57
00:02:45,530 --> 00:02:47,030
So E is equal to that.

58
00:02:47,030 --> 00:02:50,200
Just copy that error function over again.

59
00:02:50,200 --> 00:02:53,450
And we're going to take the derivative of that error, with respect to h,

60
00:02:53,450 --> 00:02:56,410
and remember h was just the little vector and b.

61
00:02:56,410 --> 00:02:59,740
If you don't remember how to take the derivative of a quantity with respect to

62
00:02:59,740 --> 00:03:03,600
a vector, go remember and look it up, all right?

63
00:03:03,600 --> 00:03:07,910
Transform that, get to this very simple picture, okay?

64
00:03:07,910 --> 00:03:12,640
And what you have is h is equal to this quantity times y and

65
00:03:12,640 --> 00:03:14,750
that's the pseudoinverse.

66
00:03:14,750 --> 00:03:17,220
Right? So I hope you remember that from your linear algebra.

67
00:03:17,220 --> 00:03:22,760
Basically this is just the standard over-constrained least squares solutions.

68
00:03:22,760 --> 00:03:27,140
So the model fitting has something to do with minimizing squared error and

69
00:03:27,140 --> 00:03:31,830
when you work that all out you get to the pseudoinverse least squares solution.
