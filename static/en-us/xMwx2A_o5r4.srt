1
00:00:00,440 --> 00:00:02,890
Let's look at one way to
learn those embeddings.

2
00:00:02,890 --> 00:00:04,750
It's called Word2Vec.

3
00:00:04,750 --> 00:00:09,150
Word2Vec is a surprisingly simple
model that works very well.

4
00:00:09,150 --> 00:00:12,230
Imagine you had a corpus of
text with a sentence say,

5
00:00:12,230 --> 00:00:15,320
the quick brown fox
jumps over the lazy dog.

6
00:00:15,320 --> 00:00:19,870
For each word in this sentence,
we're going to map it to an embedding.

7
00:00:19,870 --> 00:00:21,415
Initially a random one.

8
00:00:21,415 --> 00:00:24,187
And then we're going to use
that embedding to try and

9
00:00:24,187 --> 00:00:25,954
predict the context of the word.

10
00:00:25,954 --> 00:00:30,090
In this model, the context is
simply the words that are nearby.

11
00:00:30,090 --> 00:00:33,590
Pick a random word in a window
around the original word,

12
00:00:33,590 --> 00:00:35,930
and that's your target.

13
00:00:35,930 --> 00:00:39,690
Then train your model exactly as
if it were a supervised problem.

14
00:00:39,690 --> 00:00:42,930
The model you're going to use
to predict this nearby word

15
00:00:42,930 --> 00:00:44,760
is a simple logistic regression.

16
00:00:44,760 --> 00:00:47,260
Nothing deep about it,
just a simple linear model.
