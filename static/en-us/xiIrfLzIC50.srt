1
00:00:00,430 --> 00:00:04,434
Let's shift gears now and talk about ways to avoid programming in the first place.

2
00:00:04,434 --> 00:00:07,294
There are many libraries available for CUDA, more come out every day.

3
00:00:07,294 --> 00:00:10,146
There are some libraries that are developed by NVIDIA, some by third parties,

4
00:00:10,146 --> 00:00:12,849
there's open source libraries, and there's commercial products,

5
00:00:12,849 --> 00:00:15,970
and of course there's no way I can cover all of the available libraries out there,

6
00:00:15,970 --> 00:00:18,450
but I do want to hit a few highlights here

7
00:00:18,450 --> 00:00:20,858
and talk about some of the more popular and useful libraries out there.

8
00:00:20,858 --> 00:00:23,775
The libraries I'm going to talk about are, for the most part, fairly mature.

9
00:00:23,775 --> 00:00:26,496
They're designed and optimized for performance by experts,

10
00:00:26,496 --> 00:00:30,284
and they get tuned up and re-released every time a new GPU architecture comes out.

11
00:00:30,284 --> 00:00:32,445
So if you can use these libraries, you should.

12
00:00:32,445 --> 00:00:35,720
So the first one I want to highlight is called cuBLAS,

13
00:00:35,720 --> 00:00:40,401
and cuBLAS is an implementation of the BLAS, or Basic Linear Algebra Subroutines.

14
00:00:40,401 --> 00:00:43,364
This is a venerable library that's been around for a long time.

15
00:00:43,364 --> 00:00:45,777
It's used in Fortran and C,

16
00:00:45,777 --> 00:00:51,660
and scientists and engineers everywhere use this as one of their go-to workhorses for dealing with dense linear algebra.

17
00:00:51,660 --> 00:00:53,923
Next is cuFFT.

18
00:00:53,923 --> 00:00:59,062
FFT stands for Fast Fourier Transform, and so not surprisingly, this is the CUDA Fast Fourier Transform.

19
00:00:59,062 --> 00:01:01,722
This includes various batched transforms,

20
00:01:01,722 --> 00:01:07,110
as well as support for various real to complex, complex to complex FFTs and so forth.

21
00:01:07,110 --> 00:01:10,952
It has an interface similar in ways to the FFTW,

22
00:01:10,952 --> 00:01:14,141
the popular Fastest Fourier Transform in the West routine,

23
00:01:14,141 --> 00:01:21,693
so it's a familiar interface to anybody who uses FFTs as part of their bread and butter toolbox.

24
00:01:21,693 --> 00:01:26,496
cuSPARSE is BLAS-like routines for doing linear algebra on sparse matrix formats.

25
00:01:26,496 --> 00:01:31,061
Sparse matrices, as you know, are matrices that are mostly 0 and therefore stored in some sort of compressed format,

26
00:01:31,061 --> 00:01:33,908
and cuSPARSE supports a variety of formats

27
00:01:33,908 --> 00:01:39,262
and includes higher level routines like incomplete LU factorization.

28
00:01:39,262 --> 00:01:44,240
cuRAND is a bunch of pseudo and quasi random number generation routines

29
00:01:44,240 --> 00:01:47,944
for making random numbers, and this includes device side functions as well as host interfaces

30
00:01:47,944 --> 00:01:51,738
for quickly filling arrays with numbers drawn from particular distributions,

31
00:01:51,738 --> 00:01:55,390
various high quality random number generations basically.

32
00:01:55,390 --> 00:01:58,576
NPP stands for NVIDIA Performance Primitives,

33
00:01:58,576 --> 00:02:02,466
and this is basically, for the most part, low-level image processing primitives,

34
00:02:02,466 --> 00:02:06,084
so highly optimized low-level primitives for image processing.

35
00:02:06,084 --> 00:02:09,202
Magma is developed by the same group that wrote the original LAPACK library,

36
00:02:09,202 --> 00:02:14,772
and LAPACK is another one of these tools in the toolbox of many scientists and engineers

37
00:02:14,772 --> 00:02:17,874
who simply use these tools for linear algebra all the time.

38
00:02:17,874 --> 00:02:23,719
So Magma provides GPU and multicore CPU implementations of many LAPACK routines,

39
00:02:23,719 --> 00:02:31,221
so it's sort of a modern parallel rethinking of our implementation of LAPACK style linear algebra.

40
00:02:31,221 --> 00:02:33,289
Another linear algebra tool is CULA,

41
00:02:33,289 --> 00:02:37,972
which is implementations of Eigensolvers and matrix factorizations and matrix solvers,

42
00:02:37,972 --> 00:02:45,735
and for dense matrices similar to the LAPACK API, and there's also a sparse CULA package as well.

43
00:02:45,735 --> 00:02:50,442
Array Fire is a framework for data parallel manipulation of different types of array data,

44
00:02:50,442 --> 00:02:56,980
including a wide variety of built-in operations from numerical linear algebra to signal processing to financial,

45
00:02:56,980 --> 00:03:02,413
so this is somewhere in between a domain-specific library like these others

46
00:03:02,413 --> 00:03:06,338
and programming power tool, which is sort of the next category we'll talk about.
