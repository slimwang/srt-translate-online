1
00:00:00,210 --> 00:00:02,740
Okay. I hope you enjoyed the course to this point.

2
00:00:02,740 --> 00:00:05,390
In this lesson, we're going to work with a collection of

3
00:00:05,390 --> 00:00:08,150
tweets. Now, I need to make clear that since this is

4
00:00:08,150 --> 00:00:11,070
a collection that was gathered some time ago, it does not

5
00:00:11,070 --> 00:00:13,070
reflect the state of Twitter feeds as they look right

6
00:00:13,070 --> 00:00:17,300
now. It's a small snapshot in time. So our tweets had

7
00:00:17,300 --> 00:00:20,822
the following form. So you can see that there is a,

8
00:00:20,822 --> 00:00:25,340
unique identifier. There will be text for the tweet itself and

9
00:00:25,340 --> 00:00:28,040
then an entities field. Now the entities field is broken

10
00:00:28,040 --> 00:00:30,290
down into user mentions, urls and hashtags and we actually

11
00:00:30,290 --> 00:00:33,120
took a look at one tweet, in the last lesson,

12
00:00:33,120 --> 00:00:35,700
so this should be at least partially familiar to you.

13
00:00:35,700 --> 00:00:39,920
User mentions, urls and hashtags represent that type of data,

14
00:00:39,920 --> 00:00:43,430
and where it's found in the text of a tweet.

15
00:00:43,430 --> 00:00:46,000
It's been extracted for us and stored in these individual

16
00:00:46,000 --> 00:00:50,349
fields. Okay and then with each tweet, there's information about the

17
00:00:50,349 --> 00:00:52,278
user at the time the tweet was made. As

18
00:00:52,278 --> 00:00:55,450
you will see, our tweet documents actually contain many more

19
00:00:55,450 --> 00:00:58,299
fields. We're representing those by the ellipses you see

20
00:00:58,299 --> 00:01:01,130
in this example. As with the other data sets we've

21
00:01:01,130 --> 00:01:04,599
considered, this type of data, is representative of what

22
00:01:04,599 --> 00:01:08,070
you might work with as a data scientist. Many data

23
00:01:08,070 --> 00:01:10,900
scientists are employed in spaces that work heavily with social

24
00:01:10,900 --> 00:01:15,420
media. Google, Facebook, and Twitter are some of the most

25
00:01:15,420 --> 00:01:18,890
prominent of thousands of firms, actually, that employ people to analyze this

26
00:01:18,890 --> 00:01:21,840
type of data. Now, just for a moment imagine the types of

27
00:01:21,840 --> 00:01:24,710
analysis you might want to do on tweets. Common for this type

28
00:01:24,710 --> 00:01:28,870
of data is to understand the behavior of users, and the networks involved.

29
00:01:28,870 --> 00:01:31,270
There are lots of ways we can do that. Now, one of

30
00:01:31,270 --> 00:01:34,630
the most powerful things about putting our data in a database is

31
00:01:34,630 --> 00:01:38,610
that most databases provide some analytics tools built in, that enable us

32
00:01:38,610 --> 00:01:40,670
to explore our data a bit and get a sense for the story

33
00:01:40,670 --> 00:01:44,910
it tells. In MongoDB, the built-in analytics tools take

34
00:01:44,910 --> 00:01:47,180
the form of what we call the aggregation framework.

35
00:01:48,180 --> 00:01:50,310
While not a replacement for MapReduce in a

36
00:01:50,310 --> 00:01:53,490
lot of situations, it does provide a powerful tool for

37
00:01:53,490 --> 00:01:57,000
exploring our data, and exploring it whether we're auditing

38
00:01:57,000 --> 00:01:59,950
the quality of our data or doing some analysis.

39
00:01:59,950 --> 00:02:03,120
And with each major release of MongoDB, this becomes

40
00:02:03,120 --> 00:02:05,824
a more powerful tool. There are several really valuable feature

41
00:02:05,824 --> 00:02:07,600
enhancements actually in the 2.6 release.
