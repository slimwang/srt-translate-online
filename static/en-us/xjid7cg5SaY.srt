1
00:00:00,240 --> 00:00:02,370
Let's now look at
the experimental results.

2
00:00:02,370 --> 00:00:05,000
We will start with
the best case numbers.

3
00:00:05,000 --> 00:00:09,800
To gather the best case numbers,
they used a synthetic load in which

4
00:00:09,800 --> 00:00:14,980
they varied the number of requests that
are issued against the web server,

5
00:00:14,980 --> 00:00:18,100
and every single one of the requests
is for the exact same file.

6
00:00:18,100 --> 00:00:19,300
Like for instance,

7
00:00:19,300 --> 00:00:24,140
every single one of the requests
is trying to get index.html.

8
00:00:24,140 --> 00:00:27,510
This is the best case
because really in reality

9
00:00:27,510 --> 00:00:31,290
clients will likely be asking for
different files, and

10
00:00:31,290 --> 00:00:35,230
in this pathological best case it's
likely basically the file will be in

11
00:00:35,230 --> 00:00:40,745
cash so every one of these requests
will be serviced as fast as possible.

12
00:00:40,745 --> 00:00:44,590
There definitely won't be any need for
any kind of disk IO.

13
00:00:44,590 --> 00:00:47,740
So for the best case experiments,
they measure bandwidth and

14
00:00:47,740 --> 00:00:52,800
they do that, they vary the file
size of zero to 200 kilobytes and

15
00:00:52,800 --> 00:00:57,310
they measure bandwidth as the n, the
number of requests, times the file size

16
00:00:57,310 --> 00:01:02,650
over the time that it takes to process
the n number of requests for this file.

17
00:01:03,670 --> 00:01:05,310
By varying the file size,

18
00:01:05,310 --> 00:01:10,040
they varied the work that both the web
server performs on each request but

19
00:01:10,040 --> 00:01:13,610
also the amount of bytes that
are generated on a request.

20
00:01:13,610 --> 00:01:17,620
You sort of assume that as we increase
the file size that the bandwidth

21
00:01:17,620 --> 00:01:19,060
will start increasing.

22
00:01:20,190 --> 00:01:22,210
So let's look at the results now.

23
00:01:22,210 --> 00:01:27,350
The results show the curves for every
one of the cases that they compare.

24
00:01:27,350 --> 00:01:31,980
The flash results are the green bar,
SPED is the single process

25
00:01:31,980 --> 00:01:36,930
event driven model, MT,
multi-threaded, MP, multi-process,

26
00:01:36,930 --> 00:01:41,580
Apache, this bottom curve, corresponds
to the Apache implementation And Zeus,

27
00:01:41,580 --> 00:01:43,950
that corresponds to the darker blue.

28
00:01:43,950 --> 00:01:48,380
This is the SPED module that
has two instances of SPED so

29
00:01:48,380 --> 00:01:51,350
the dual process event driven model.

30
00:01:51,350 --> 00:01:53,300
We can make the following observations.

31
00:01:53,300 --> 00:01:58,420
First, for all of the curves,
initially when the file size is small,

32
00:01:58,420 --> 00:02:03,390
bandwidth is slow, and as the file size
increases, the bandwidth increases.

33
00:02:03,390 --> 00:02:06,370
We see that all of the implementations
have very similar results.

34
00:02:07,660 --> 00:02:08,660
SPED is really the best.

35
00:02:08,660 --> 00:02:12,680
That's the single process event driven,
and that's expected because it doesn't

36
00:02:12,680 --> 00:02:16,960
have any threads or processes among
which it needs to context switch.

37
00:02:16,960 --> 00:02:21,940
Flash is similar but it performs that
extra check for the memory presence.

38
00:02:21,940 --> 00:02:26,140
In this case,
because this is the single file tree.

39
00:02:26,140 --> 00:02:30,780
So every single one of the requests is
for the single file, there's no need for

40
00:02:30,780 --> 00:02:31,990
blocking I/O.

41
00:02:31,990 --> 00:02:35,460
So none of the helper processes
will be invoked, but nonetheless,

42
00:02:35,460 --> 00:02:36,540
this check is performed.

43
00:02:36,540 --> 00:02:40,130
So that's why we see a little
bit lower performance for flash.

44
00:02:40,130 --> 00:02:41,330
Zeus has an anomaly.

45
00:02:41,330 --> 00:02:43,510
Its performance drops here a little bit,
and

46
00:02:43,510 --> 00:02:47,710
that has to do with some misalignment
for some of the DMA operations.

47
00:02:47,710 --> 00:02:52,795
So not all of the optimizations are
bug-proof in the Zeus implementation.

48
00:02:52,795 --> 00:02:57,720
For the multi-thread and
multi-process models, the performance

49
00:02:57,720 --> 00:03:02,040
is slower because of the context
switching and extra synchronization.

50
00:03:02,040 --> 00:03:04,140
And the performance of
Apache is the worst,

51
00:03:04,140 --> 00:03:08,790
because it doesn't have any
optimizations that the others implement.

52
00:03:08,790 --> 00:03:12,660
Now, since real clients don't
behave like the synthetic workload,

53
00:03:12,660 --> 00:03:16,300
we need to look at what happens
with some of the realistic traces,

54
00:03:16,300 --> 00:03:18,370
the Owlnet and the CS trace.

55
00:03:18,370 --> 00:03:20,880
Let's take a look at
the Owlnet trace first.

56
00:03:20,880 --> 00:03:23,270
First we see that for the Owlnet trace,

57
00:03:23,270 --> 00:03:27,730
the performance is very similar to
the best case with SPED and Flash

58
00:03:27,730 --> 00:03:33,130
being the best and then Multi-thread and
Multi-process and Apache dropping down.

59
00:03:33,130 --> 00:03:36,270
Note that we're not including
the Zeus performance.

60
00:03:36,270 --> 00:03:40,060
The reason for this trend is because
the Owlnet trace is the small trace,

61
00:03:40,060 --> 00:03:44,460
so most of it will fit in the cache and
we'll have a similar behavior like what

62
00:03:44,460 --> 00:03:49,200
we had in the best case, where all the
requests are serviced from the cache.

63
00:03:49,200 --> 00:03:51,940
Sometimes, however,
blocking I/O is required.

64
00:03:51,940 --> 00:03:54,340
It mostly fits in the cache.

65
00:03:54,340 --> 00:03:57,260
Given this,
given the blocking I/O possibility,

66
00:03:57,260 --> 00:03:59,580
SPED will occasionally block.

67
00:03:59,580 --> 00:04:03,450
Where as in Flash their helper processes
will help resolve the problem.

68
00:04:03,450 --> 00:04:06,930
And that's why we see here that the
performance of Flash is slightly higher

69
00:04:06,930 --> 00:04:09,320
than the performance of the SPED.

70
00:04:09,320 --> 00:04:12,860
Now if we take a look at what's
happening with the CS trace, this,

71
00:04:12,860 --> 00:04:15,070
remember, is a larger trace.

72
00:04:15,070 --> 00:04:16,839
So it will mostly require I/O.

73
00:04:16,839 --> 00:04:21,170
It's not going to fed in the cache,
in memory in the system.

74
00:04:21,170 --> 00:04:24,570
Since the system does not support
asynchronous I/O operations,

75
00:04:24,570 --> 00:04:27,770
the performance of SPED
will drop significantly.

76
00:04:27,770 --> 00:04:29,630
So relative to where it was,

77
00:04:29,630 --> 00:04:33,590
close to Flash, now it's significantly
below Flash and, in fact,

78
00:04:33,590 --> 00:04:37,254
it's below the multi-process and
the multi-threaded implementations.

79
00:04:37,254 --> 00:04:40,073
Considering the multi-thread and
the multi-process,

80
00:04:40,073 --> 00:04:43,581
we see that the multi-threaded is
better than the multi-process, and

81
00:04:43,581 --> 00:04:47,089
the main reason for that is that
the multi-threaded implementation has

82
00:04:47,089 --> 00:04:48,980
a smaller memory footprint.

83
00:04:48,980 --> 00:04:53,600
The smaller memory footprint means that
there will be more memory available to

84
00:04:53,600 --> 00:04:57,570
cache files,
in turn that will lead to less I/O, so

85
00:04:57,570 --> 00:04:59,740
this is a better implementation.

86
00:04:59,740 --> 00:05:02,490
In addition, the synchronization and
coordination and

87
00:05:02,490 --> 00:05:06,900
contact switching between threads in a
multi-thread implementation is cheaper,

88
00:05:06,900 --> 00:05:12,420
it happens faster than long processes
in a multi-process implementation.

89
00:05:12,420 --> 00:05:15,320
In all cases, Flash performs best.

90
00:05:15,320 --> 00:05:19,237
Again, it has the smaller memory
footprint compared to multi-threaded and

91
00:05:19,237 --> 00:05:22,920
the multi-process, and that results
in more memory available for

92
00:05:22,920 --> 00:05:25,720
caching files or caching headers.

93
00:05:25,720 --> 00:05:29,140
As a result of that,
fewer requests will lead to a blocking

94
00:05:29,140 --> 00:05:32,180
I Operation which further
speeds things up.

95
00:05:32,180 --> 00:05:36,180
And finally, given that everything
happens in the same address space,

96
00:05:36,180 --> 00:05:37,320
there isn't a need for

97
00:05:37,320 --> 00:05:42,260
explicit synchronization like with the
multi-threaded or multi-process model.

98
00:05:42,260 --> 00:05:45,860
And this is what makes Flash
perform best, in this case.

99
00:05:45,860 --> 00:05:51,370
In both of those cases, Apache performed
worse, so let's try to understand

100
00:05:51,370 --> 00:05:55,660
if there's really an impact of
the optimizations performed in Flash.

101
00:05:55,660 --> 00:05:58,890
And here the results represent
the different optimizations.

102
00:05:58,890 --> 00:06:01,790
The performance that's scattered with

103
00:06:01,790 --> 00:06:05,320
Flash without any optimizations
that's the bottom line.

104
00:06:05,320 --> 00:06:10,270
Then Flash with the path only
optimizations, so the path only, that's

105
00:06:10,270 --> 00:06:15,700
the directory lookup caching, so that's
like the computation caching part.

106
00:06:15,700 --> 00:06:18,520
Then the red line here,
the path and maps, so

107
00:06:18,520 --> 00:06:23,330
this includes caching of the directory
lookup plus caching of the file.

108
00:06:23,330 --> 00:06:26,800
And then the final bar, so
the final line, the black line,

109
00:06:26,800 --> 00:06:28,420
that includes all of the optimization.

110
00:06:28,420 --> 00:06:30,330
So this is the directory lookup,

111
00:06:30,330 --> 00:06:34,890
the file caching as well as
the header computations of the file.

112
00:06:34,890 --> 00:06:37,920
And we see that as we add
some of the optimizations,

113
00:06:37,920 --> 00:06:43,340
this impacts the connection rates of
the performance that can be achieved

114
00:06:43,340 --> 00:06:46,530
by the web server
significantly improves.

115
00:06:46,530 --> 00:06:49,580
We're able to sustain
a higher connection rate

116
00:06:49,580 --> 00:06:52,190
as we add these optimizations.

117
00:06:52,190 --> 00:06:53,400
This tells us two things.

118
00:06:53,400 --> 00:06:57,420
First, that these optimizations
are indeed very important.

119
00:06:57,420 --> 00:07:00,840
And second, they tell us that the
performance of Apache would have been

120
00:07:00,840 --> 00:07:04,390
also impacted,
if it had integrated some of

121
00:07:04,390 --> 00:07:06,550
these same optimizations as
the other implementations.
