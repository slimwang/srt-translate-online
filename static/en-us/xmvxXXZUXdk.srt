1
00:00:00,350 --> 00:00:02,600
To conclude our discussion of information theory, we will

2
00:00:02,600 --> 00:00:07,330
also discuss something called Kullback-Leibler divergence. It is also famously

3
00:00:07,330 --> 00:00:10,270
called the KL divergence. And you must have heard this

4
00:00:10,270 --> 00:00:12,860
term in our previous lectures. So it is useful to

5
00:00:12,860 --> 00:00:16,070
realize that mutual information is also a particular case

6
00:00:16,070 --> 00:00:19,300
of KL divergence. So KL divergence actually measures the difference

7
00:00:19,300 --> 00:00:22,560
between any two distributions. It is used as a distance

8
00:00:22,560 --> 00:00:25,980
measure. For this particular lesson, it is sufficient to understand

9
00:00:25,980 --> 00:00:29,810
how KL divergence is used to measure the distance between

10
00:00:29,810 --> 00:00:33,270
two distributions. The KL divergence is given by this particular

11
00:00:33,270 --> 00:00:37,070
formula. And it is always non-negative and zero only when

12
00:00:37,070 --> 00:00:39,790
P is equal to Q. When P is equal to Q,

13
00:00:39,790 --> 00:00:41,940
the log of 1 is zero, and that's why the

14
00:00:41,940 --> 00:00:45,850
distance is zero. Otherwise, it is always some non-negative quantity. So

15
00:00:45,850 --> 00:00:48,650
it serves as a distance measure. But it is not

16
00:00:48,650 --> 00:00:51,890
completely a distance measure because it doesn't follow the triangle law.

17
00:00:51,890 --> 00:00:53,650
But then you should ask yourself why you need to

18
00:00:53,650 --> 00:00:58,100
know KL divergence, or where it is used. Usually, and usually

19
00:00:58,100 --> 00:01:01,060
in supervised learning you are always trying to model our

20
00:01:01,060 --> 00:01:05,489
data to a particular distribution. So in that case our distrib,

21
00:01:05,489 --> 00:01:09,010
one of our distributions can be of unknown distribution. And

22
00:01:09,010 --> 00:01:11,460
we can denote that as P of X. And then can

23
00:01:11,460 --> 00:01:14,310
sample our data set to find out Q of X.

24
00:01:14,310 --> 00:01:17,510
While doing that, we can use KL divergence as a substitute

25
00:01:17,510 --> 00:01:20,498
to the least square formula that we used for

26
00:01:20,498 --> 00:01:23,820
fitting. So it's just a different way of trying to

27
00:01:23,820 --> 00:01:26,810
fit your data to your existing model. And we'll come

28
00:01:26,810 --> 00:01:29,290
back to KL divergence in some of our problem sets.
