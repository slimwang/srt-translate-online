1
00:00:00,310 --> 00:00:02,570
I mentioned log cleaning that has to be

2
00:00:02,570 --> 00:00:06,350
done. Now as client activities go on, log

3
00:00:06,350 --> 00:00:10,290
segments evolve on the disk. So for instance,

4
00:00:10,290 --> 00:00:13,910
if on a particular client node some blocks

5
00:00:13,910 --> 00:00:19,220
were written to the log segment, a log segment may fill up like this. And now

6
00:00:19,220 --> 00:00:22,070
it is sitting on the disk. So the

7
00:00:22,070 --> 00:00:25,600
blocks that are containing this log segment corresponds

8
00:00:25,600 --> 00:00:28,150
to write to file blocks one, two, and

9
00:00:28,150 --> 00:00:32,520
five. And these file blocks may belong to different

10
00:00:32,520 --> 00:00:34,450
files, but it is okay. So, so far

11
00:00:34,450 --> 00:00:38,140
as the file system is concerned, segment one is

12
00:00:38,140 --> 00:00:43,900
a contiguous file. It is a log segment, but it is a file. And that's the one

13
00:00:43,900 --> 00:00:46,230
that is residing on the disk. On the

14
00:00:46,230 --> 00:00:51,350
client node, this particular block one may get overwritten

15
00:00:51,350 --> 00:00:58,150
due to activity on the client. So now we have a new content for that same file

16
00:00:58,150 --> 00:01:01,340
block one, one double prime. And once this new

17
00:01:01,340 --> 00:01:04,400
content has been created, this is no longer valid,

18
00:01:04,400 --> 00:01:07,840
and so block one is overwritten, which means we

19
00:01:07,840 --> 00:01:10,580
have to kill the old block that was in

20
00:01:10,580 --> 00:01:13,720
this segment. So there's a new segment in which

21
00:01:13,720 --> 00:01:16,708
we wrote the contents of the new file block,

22
00:01:16,708 --> 00:01:19,430
one double prime, and we have to go

23
00:01:19,430 --> 00:01:23,590
back to this old segment and kill this particular

24
00:01:23,590 --> 00:01:28,290
copy of that block, which is a stale copy of that same block. So we don't need

25
00:01:28,290 --> 00:01:30,706
that anymore. So you can that see as

26
00:01:30,706 --> 00:01:34,372
client activities progress we are going to create holes

27
00:01:34,372 --> 00:01:38,340
in the segments. Remember that these segments are persistent

28
00:01:38,340 --> 00:01:42,140
data structures on the disk. This segment was valid

29
00:01:42,140 --> 00:01:44,620
at some point of time. But once that

30
00:01:44,620 --> 00:01:48,300
particular block one was overwritten on the client

31
00:01:48,300 --> 00:01:51,310
node, this segment contains the latest and the

32
00:01:51,310 --> 00:01:54,870
current copy of that same file block. And therefore

33
00:01:54,870 --> 00:01:58,520
we nuke this particular file block and so

34
00:01:58,520 --> 00:02:00,809
we create hole in this log segment. And

35
00:02:00,809 --> 00:02:04,730
subsequently, let's say that the client writes to

36
00:02:04,730 --> 00:02:07,262
other file blocks, three and four. So the log

37
00:02:07,262 --> 00:02:10,436
segment two contains one double prime, three prime,

38
00:02:10,436 --> 00:02:12,506
and four prime are the blocks that it

39
00:02:12,506 --> 00:02:15,682
contains. And segment one contains two prime and

40
00:02:15,682 --> 00:02:19,130
five prime. One prime is not relevant anymore because

41
00:02:19,130 --> 00:02:21,560
it has been overwritten by one double prime.

42
00:02:21,560 --> 00:02:25,320
Activities continue on the client box and a third

43
00:02:25,320 --> 00:02:28,854
segment is created. And that third segment contains

44
00:02:28,854 --> 00:02:32,920
two double prime. That is, this block number two

45
00:02:32,920 --> 00:02:36,490
is overwritten by new contents, two double prime.

46
00:02:36,490 --> 00:02:39,590
And when this happens the file system has

47
00:02:39,590 --> 00:02:42,620
to create another hole by nuking this two

48
00:02:42,620 --> 00:02:45,530
prime to indicate that this block is not relevant

49
00:02:45,530 --> 00:02:48,090
anymore, because there is a more recent version

50
00:02:48,090 --> 00:02:50,530
of the block in segment three. So the

51
00:02:50,530 --> 00:02:53,600
block two is overwritten, killing the old block.

52
00:02:53,600 --> 00:02:57,960
Now you see that as client activities progress in

53
00:02:57,960 --> 00:03:01,000
the entire distributed system, we are creating a

54
00:03:01,000 --> 00:03:05,390
number of these segments, log segments on the disk,

55
00:03:05,390 --> 00:03:08,860
and these log segments may progressively have holes

56
00:03:08,860 --> 00:03:12,190
in them because they have been overwritten by new

57
00:03:12,190 --> 00:03:15,880
contents, by activities on the client machine. And

58
00:03:15,880 --> 00:03:18,230
this is what log cleaning is all about. It

59
00:03:18,230 --> 00:03:23,470
has to do with cleaning up the disk and getting rid of all of this junk so

60
00:03:23,470 --> 00:03:26,180
that we don't fill up the disk with unnecessary

61
00:03:26,180 --> 00:03:30,010
junk. So for example, what we want to do is

62
00:03:30,010 --> 00:03:34,460
recognize that we have three segments here, and the segments

63
00:03:34,460 --> 00:03:36,800
have holes in them. And what we are going to

64
00:03:36,800 --> 00:03:39,880
do is, we're going to aggregate all the live blocks

65
00:03:39,880 --> 00:03:43,040
from all of these segments in to a new segment.

66
00:03:43,040 --> 00:03:45,664
So we've got five from this segment that is still

67
00:03:45,664 --> 00:03:48,864
alive, and from this segment we've got one double prime,

68
00:03:48,864 --> 00:03:53,140
three prime, and four prime that are still alive. And from this segment we've

69
00:03:53,140 --> 00:03:59,520
got two double prime that is still alive. So we have coalesced all of the live

70
00:03:59,520 --> 00:04:03,020
blocks from the existing segments into one

71
00:04:03,020 --> 00:04:07,500
new segment. Now once we have aggregated this

72
00:04:07,500 --> 00:04:13,880
into one new segment, all of the old log segments can be garbage collected and

73
00:04:13,880 --> 00:04:16,680
that's what log cleaning is all about. And

74
00:04:16,680 --> 00:04:19,310
this is very similar if you think about it

75
00:04:19,310 --> 00:04:22,605
to the way we described cleaning up the

76
00:04:22,605 --> 00:04:26,845
diff files that are created in the DSM system,

77
00:04:26,845 --> 00:04:30,280
Treadmarks. And the same thing is happening except

78
00:04:30,280 --> 00:04:33,460
that these data structures are on the disk we

79
00:04:33,460 --> 00:04:36,400
are conserving the space on the disk by getting

80
00:04:36,400 --> 00:04:39,120
rid of all the old log segments and garbage

81
00:04:39,120 --> 00:04:43,010
collecting them and saving this space once we've

82
00:04:43,010 --> 00:04:47,250
aggregated all the live blocks in these segments into

83
00:04:47,250 --> 00:04:49,470
a new segment. So this is what log

84
00:04:49,470 --> 00:04:52,080
cleaning is all about and in a distributed file

85
00:04:52,080 --> 00:04:55,410
system, there is a lot of garbage that

86
00:04:55,410 --> 00:04:58,870
is being created all across the storage service. And

87
00:04:58,870 --> 00:05:00,880
we don't want this to be done by a

88
00:05:00,880 --> 00:05:04,230
single manager. We would ideally like it to be

89
00:05:04,230 --> 00:05:07,190
done in a distributed manner. This is

90
00:05:07,190 --> 00:05:10,950
another step towards a true distributed file

91
00:05:10,950 --> 00:05:14,180
system by making this log cleaning activity

92
00:05:14,180 --> 00:05:18,010
also a distributed activity. So log cleaner's

93
00:05:18,010 --> 00:05:23,660
responsibilities include the following. It has to find the utilization status of

94
00:05:23,660 --> 00:05:30,160
the old log segments. Then it has to pick some set of log segments to clean, and

95
00:05:30,160 --> 00:05:32,480
once it picks a certain number of log segments

96
00:05:32,480 --> 00:05:34,940
to clean, it has to read all the live blocks

97
00:05:34,940 --> 00:05:37,920
that it finds in these log segments that it

98
00:05:37,920 --> 00:05:42,080
has chosen for cleaning, write it into a new log

99
00:05:42,080 --> 00:05:45,420
segment. And once it has done that, it can

100
00:05:45,420 --> 00:05:48,190
garbage collect all of those log segments. So this is

101
00:05:48,190 --> 00:05:51,140
the cleaning activity that an LFS cleaner has to

102
00:05:51,140 --> 00:05:55,570
do and in XFS they distribute this log cleaning activity

103
00:05:55,570 --> 00:06:02,080
as well. Now remember that this log cleaning activity is happening concurrently

104
00:06:02,080 --> 00:06:08,720
with writing to files on the nodes in the distributed system. So there are lots

105
00:06:08,720 --> 00:06:15,320
of subtle issues involved in managing this log cleaning in parallel with new

106
00:06:15,320 --> 00:06:18,340
activity that may be creating new log

107
00:06:18,340 --> 00:06:21,590
segments, or writing to existing log segments.

108
00:06:21,590 --> 00:06:25,470
I encourage you to read the paper that I've assigned to you, the XSF paper, to

109
00:06:25,470 --> 00:06:28,180
get a good feel for all the subtleties

110
00:06:28,180 --> 00:06:33,730
that are involved in managing log cleaning concurrently with

111
00:06:33,730 --> 00:06:35,861
writing to the files. So in XFS they

112
00:06:35,861 --> 00:06:39,775
may make the clients also responsible for log

113
00:06:39,775 --> 00:06:43,510
cleaning. There is no separation between client and

114
00:06:43,510 --> 00:06:46,610
server. Any node can be a client or a

115
00:06:46,610 --> 00:06:52,130
server depending on what it is doing and each client, meaning a node that is

116
00:06:52,130 --> 00:06:55,990
generating a log segments, it is responsible

117
00:06:55,990 --> 00:07:00,690
for the segment utilization information for the files

118
00:07:00,690 --> 00:07:05,110
that they are writing. After all the activity is happening at the client end in

119
00:07:05,110 --> 00:07:07,910
terms of creating new files, and new

120
00:07:07,910 --> 00:07:12,550
file writes are manifesting as creating blocks and

121
00:07:12,550 --> 00:07:17,390
log segments in XFS. And so the clients are responsible

122
00:07:17,390 --> 00:07:22,820
for knowing the utilization of the segments that

123
00:07:22,820 --> 00:07:28,010
are resident at that client node. And since we have divvied up the entire

124
00:07:28,010 --> 00:07:32,440
space of servers into stripe groups, each stripe

125
00:07:32,440 --> 00:07:38,000
group is responsible for cleaning activity that is

126
00:07:38,000 --> 00:07:42,010
in that set of servers. And every stripe

127
00:07:42,010 --> 00:07:44,360
group has a leader, and the leader in that

128
00:07:44,360 --> 00:07:48,870
stripe group is responsible for assigning cleaning services to

129
00:07:48,870 --> 00:07:51,380
the members of that stripe group. Recall that the

130
00:07:51,380 --> 00:07:55,000
manager is responsible for the integrity of the files

131
00:07:55,000 --> 00:07:58,540
because it is doing metadata management. And, requests for

132
00:07:58,540 --> 00:08:00,250
reading and writing files are going to come to

133
00:08:00,250 --> 00:08:03,120
the manager. On the other hand, the log cleaning

134
00:08:03,120 --> 00:08:05,710
responsibility is going to the leader of a

135
00:08:05,710 --> 00:08:08,390
stripe group, and the manager is the one that

136
00:08:08,390 --> 00:08:11,770
is responsible for resolving conflicts that may arise

137
00:08:11,770 --> 00:08:14,940
between client updates that want to change some log

138
00:08:14,940 --> 00:08:18,160
segments and cleaner functions that want to garbage

139
00:08:18,160 --> 00:08:22,560
collect some log segments. Those conflicts are resolved by

140
00:08:22,560 --> 00:08:25,260
the manager. These again are subtle details which

141
00:08:25,260 --> 00:08:27,530
I want you to read carefully in the paper.
