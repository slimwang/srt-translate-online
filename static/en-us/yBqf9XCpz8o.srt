1
00:00:00,180 --> 00:00:03,610
Usually when researchers
are evaluating a learning algorithm,

2
00:00:03,610 --> 00:00:05,560
they split their data into two chucks.

3
00:00:05,560 --> 00:00:08,700
The training chunk, and a testing chunk.

4
00:00:08,700 --> 00:00:13,409
Training usually is about 60% of
the data, and testing is about 40%.

5
00:00:13,409 --> 00:00:16,600
Now if you train and
then test on that data,

6
00:00:16,600 --> 00:00:21,690
that's one trial and in many
cases that's enough, you measured

7
00:00:21,690 --> 00:00:25,330
your root means square error and
that's an assessment of your algorithm.

8
00:00:25,330 --> 00:00:27,410
You might compare it
against another algorithm.

9
00:00:27,410 --> 00:00:30,490
But one problem researchers
sometimes encounter

10
00:00:30,490 --> 00:00:34,380
is they don't have enough data to
effectively analyze their algorithm.

11
00:00:34,380 --> 00:00:38,890
One thing they can do is effectively
create more data by slicing it up and

12
00:00:38,890 --> 00:00:40,170
running more trials.

13
00:00:40,170 --> 00:00:41,700
Here's how that works.

14
00:00:41,700 --> 00:00:47,170
So what we can do is we can slice our
data into say five different chunks,

15
00:00:47,170 --> 00:00:52,319
and then we can train here on 80%
of the data, and test on 20%.

16
00:00:52,319 --> 00:00:54,230
That's one trial.

17
00:00:54,230 --> 00:00:58,510
Then we can switch things up and
train on this 80% of the data.

18
00:00:58,510 --> 00:01:01,840
And test on that,
that's another trial, and so on.

19
00:01:01,840 --> 00:01:04,300
I'm sure you see how this is going.

20
00:01:04,300 --> 00:01:08,110
We can effectively get five different
trials out of this one set of data.
