1
00:00:00,230 --> 00:00:04,620
Just like precision, we have another quantity that is called recall.

2
00:00:04,620 --> 00:00:07,520
The recall is the number of true positives,

3
00:00:07,520 --> 00:00:12,760
divided by the number of true positives plus false negatives.

4
00:00:12,760 --> 00:00:16,370
Let's quickly look at our confusion matrix again.

5
00:00:16,370 --> 00:00:20,070
In this case we are taking the true positives and

6
00:00:20,070 --> 00:00:24,560
dividing by the sum of true positives and false negatives.

7
00:00:24,560 --> 00:00:27,490
If you remember the definition of sensitivity,

8
00:00:27,490 --> 00:00:32,720
you will see that recall is the same quantity as sensitivity.

9
00:00:32,720 --> 00:00:35,360
The reason we have two different names for

10
00:00:35,360 --> 00:00:41,120
the same quantity is a consequence of how the entire field of model building.

11
00:00:41,120 --> 00:00:43,700
Was developed through many dif, different fields and

12
00:00:43,700 --> 00:00:48,940
disciplines and only recently we are beginning to consolidate the terminology.

13
00:00:48,940 --> 00:00:51,890
So usually if you look at statistics books,

14
00:00:51,890 --> 00:00:55,330
you will see the terminology of sensitivity.

15
00:00:55,330 --> 00:00:57,440
While if you look at a machine learning book,

16
00:00:57,440 --> 00:01:00,150
you will probably find the word recall.

17
00:01:00,150 --> 00:01:03,050
Also, when we are comparing with specificity.

18
00:01:03,050 --> 00:01:05,310
We tend to use the word sensitivity.

19
00:01:05,310 --> 00:01:10,190
When we are comparing with precision, we try to use the word recall.

20
00:01:10,190 --> 00:01:13,410
Mathematically, they are the same quantity.

21
00:01:13,410 --> 00:01:16,960
Also remember as we varied our threshold,

22
00:01:16,960 --> 00:01:21,460
we had different values of sensitivity and specificity.

23
00:01:21,460 --> 00:01:25,540
In the same way we can get different values of precision and

24
00:01:25,540 --> 00:01:29,490
recall by changing our threshold or cut.

25
00:01:30,630 --> 00:01:35,760
The resulting curve is very similar to our ROC curves.

26
00:01:35,760 --> 00:01:40,570
In this case these curves are called precision recall curves.

27
00:01:40,570 --> 00:01:43,340
Let's go back to our iPython Notebook.

28
00:01:43,340 --> 00:01:46,210
We had already built our models and

29
00:01:46,210 --> 00:01:51,170
we had received some accuracy numbers for the given classifiers.

30
00:01:51,170 --> 00:01:53,310
Let's look at this piece of code.

31
00:01:53,310 --> 00:01:57,030
When you run this piece of code you will get the following table.

32
00:01:57,030 --> 00:02:02,560
Now we have the same kind of confusion matrix that we had before.

33
00:02:02,560 --> 00:02:04,950
Let's look at the next piece of code.

34
00:02:04,950 --> 00:02:08,840
Here we are using this method of plot precision recall curve

35
00:02:08,840 --> 00:02:11,170
to plot a precision recall curve.

36
00:02:11,170 --> 00:02:15,800
We're using the scikit learns method to actually calculate the values of

37
00:02:15,800 --> 00:02:18,450
precision recall for every cut.

38
00:02:18,450 --> 00:02:20,590
Let's run this piece of code now.

39
00:02:21,610 --> 00:02:24,800
The next line should actually draw the curves for you.

40
00:02:26,100 --> 00:02:30,820
If everything runs properly you should get a curve that looks like this.

41
00:02:30,820 --> 00:02:34,410
In this case the recall has been plotted on the x axis.

42
00:02:34,410 --> 00:02:36,960
And the precision on the y axis.

43
00:02:36,960 --> 00:02:41,140
Remember the values of both range from zero to one.

44
00:02:41,140 --> 00:02:45,020
In this case the Logistic Regression, the Radom Forest Classifier, and

45
00:02:45,020 --> 00:02:51,040
the Support Vector classifiers are shown in blue, green, and red respectively.

46
00:02:51,040 --> 00:02:54,290
We have also calculated the area under the curves for

47
00:02:54,290 --> 00:02:57,390
each of these precision recall curves.

48
00:02:57,390 --> 00:03:02,890
This gives us an average measure of performance for these classifiers.

49
00:03:02,890 --> 00:03:06,790
It is often useful to use the PR plot if we

50
00:03:06,790 --> 00:03:11,770
are interested in the proportion of positive outcomes over the negative ones.
