1
00:00:00,000 --> 00:00:05,875
The most general kernel launch we can do looks like thi:, square of 3 parameters.

2
00:00:05,875 --> 00:00:08,347
The first is the dimensionality of the grid of blocks

3
00:00:08,347 --> 00:00:12,288
that has bx X by X bz blocks.

4
00:00:12,288 --> 00:00:20,855
Each one of those blocks is specified by this parameter: the block of threads that has tx X ty X tz threads in it,

5
00:00:20,855 --> 00:00:23,393
and recall that this has a maximum size.

6
00:00:23,393 --> 00:00:26,846
Finally, there's a third argument that defaults to zero if you don't use it,

7
00:00:26,846 --> 00:00:29,100
and we're not going to cover it specifically today.

8
00:00:29,100 --> 00:00:33,487
It's the amount of shared memory in bytes allocated per thread block.

9
00:00:33,487 --> 00:00:39,167
With this one kernel call, you can launch an enormous number of threads.

10
00:00:39,167 --> 00:00:44,275
And let's all remember, with great power comes great responsibility, so launch your kernels wisely.

11
00:00:44,275 --> 00:00:46,342
One more important thing about blocks and threads.

12
00:00:46,342 --> 00:00:51,220
Recall from our square kernel, that each thread knows its thread ID within a block.

13
00:00:51,220 --> 00:00:52,845
It actually knows many things.

14
00:00:52,845 --> 00:00:57,588
First is threaded x, as we've seen, which thread it is within the block.

15
00:00:57,588 --> 00:00:58,757
Here we have a block.

16
00:00:58,757 --> 00:01:05,235
Each thread, say this thread here, knows its index in each of the x, y, and z dimensions,

17
00:01:05,235 --> 00:01:10,940
and we can access those as thread idx.x, thread idx.y, and dot z.

18
00:01:10,940 --> 00:01:14,801
We also know block Dim, the size of a block.

19
00:01:14,801 --> 00:01:17,234
How many threads are there in this block

20
00:01:17,234 --> 00:01:21,080
along the x dimension, the y dimension, and potentially the z dimension?

21
00:01:21,080 --> 00:01:23,284
So we know those two things for a block.

22
00:01:23,284 --> 00:01:25,757
We know the analogous things for a grid.

23
00:01:25,757 --> 00:01:31,899
Block index for instance is which block am I in within the grid. Again dot x, dot y, and dot z.

24
00:01:31,899 --> 00:01:35,896
And grid Dim will tell us the size of the grid, how many blocks there are

25
00:01:35,896 --> 00:01:39,131
in the x dimension, the y dimension, and the z dimension.

26
00:01:39,131 --> 00:01:42,700
What I want you to take home from this little discussion is only the following.

27
00:01:42,700 --> 00:01:48,199
It's convenient to have multi-dimensional grids and blocks when your problem has multiple dimensions.

28
00:01:48,199 --> 00:01:50,937
CUDA implements this natively and efficiently.

29
00:01:50,940 --> 00:01:56,971
When you call thread at idx.x, or block dim.y, that's a very efficient thing within CUDA.

30
00:01:56,971 --> 00:01:59,375
Since we're doing image processing in this course,

31
00:01:59,375 --> 00:02:03,513
you should be counting on finding a lot of two dimensional grids and blocks.

32
00:02:03,513 --> 00:02:05,713
So, let's wrap up with a little quiz.

33
00:02:05,713 --> 00:02:07,581
Let's say I launch the following kernel.

34
00:02:07,581 --> 00:02:14,428
Kernel with 2 parameters dim 3 (8, 4, 2, 2) and dim 3 (16, 16).

35
00:02:14,428 --> 00:02:17,011
How many blocks will this call launch,

36
00:02:17,011 --> 00:02:20,508
how many threads per block, and how many total threads?
