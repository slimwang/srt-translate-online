1
00:00:00,000 --> 00:00:08,000
Now, there are many different ways to apply linear functions in machine learning.

2
00:00:08,000 --> 00:00:12,000
We so far have studied linear functions for regression,

3
00:00:12,000 --> 00:00:16,000
but linear functions are also used for classification,

4
00:00:16,000 --> 00:00:21,000
and specifically for an algorithm called the perceptron algorithm.

5
00:00:21,000 --> 00:00:27,000
This algorithm happens to be a very early model of a neuron,

6
00:00:27,000 --> 00:00:30,000
as in the neurons we have in our brains,

7
00:00:30,000 --> 00:00:33,000
and was invented in the 1940s.

8
00:00:33,000 --> 00:00:41,000
Suppose we give a data set of positive samples and negative samples.

9
00:00:41,000 --> 00:00:49,000
A linear separator is a linear equation that separates positive from negative examples.

10
00:00:49,000 --> 00:00:55,000
Obviously, not all sets possess a linear separator, but some do.

11
00:00:55,000 --> 00:01:02,000
For those we can define the algorithm of the perceptron and it actually converges.

12
00:01:02,000 --> 00:01:07,000
To define a linear separator, let's start with our linear equation as before--

13
00:01:07,000 --> 00:01:18,000
w1x + w0 in cases where x is higher dimensional this might actually be a vector--never mind.

14
00:01:18,000 --> 00:01:26,000
If this is larger or equal to zero, then we call our classification 1.

15
00:01:26,000 --> 00:01:30,000
Otherwise, we call it zero.

16
00:01:30,000 --> 00:01:35,000
Here's our linear separation classification function

17
00:01:35,000 --> 00:01:39,000
where this is our common linear function.

18
00:01:39,000 --> 00:01:45,000
Now, as I said, perceptron only converges if the data is linearly separable,

19
00:01:45,000 --> 00:01:49,000
and then it converges to a linear separation of the data,

20
00:01:49,000 --> 00:01:52,000
which is quite amazing.

21
00:01:52,000 --> 00:01:56,000
Perceptron is an iterative algorithm that is not dissimilar from grade descent.

22
00:01:56,000 --> 00:02:03,000
In fact, the update rule echoes that of grade descent, and here's how it goes.

23
00:02:03,000 --> 00:02:09,000
We start with a random guess for w1 and w0,

24
00:02:09,000 --> 00:02:13,000
which may correspond to a random separation line,

25
00:02:13,000 --> 00:02:17,000
but usually is inaccurate.

26
00:02:17,000 --> 00:02:29,000
Then the mth weight-i is obtained by using the old weight plus some learning rate alpha

27
00:02:29,000 --> 00:02:33,000
times the difference between the desired target label

28
00:02:33,000 --> 00:02:39,000
and the target label produced by our function at the point m-1.

29
00:02:39,000 --> 00:02:45,000
Now, this is an online learning rule, which is we don't process all the data in batch.

30
00:02:45,000 --> 00:02:50,000
We process one data at a time, and we might go through the data many, many times--

31
00:02:50,000 --> 00:02:52,000
hence the j over here--

32
00:02:52,000 --> 00:02:55,000
but every time we do this, we apply this rule over here.

33
00:02:55,000 --> 00:03:03,000
What this rule gives us is a method to adapt our weights in proportion to the error.

34
00:03:03,000 --> 00:03:07,000
If the prediction of our function f equals our target label,

35
00:03:07,000 --> 00:03:11,000
and the error is zero, then no update occurs.

36
00:03:11,000 --> 00:03:18,000
If there is a difference, however, we update in a way so as to minimize the error.

37
00:03:18,000 --> 00:03:22,000
Alpha is a small learning weight.

38
00:03:22,000 --> 00:03:28,000
Once again, perceptron converges to a correct linear separator

39
00:03:28,000 --> 00:03:31,000
if such linear separator exists.

40
00:03:31,000 --> 00:03:36,000
Now, the case of linear separation has recently received a lot of attention in machine learning.

41
00:03:36,000 --> 00:03:42,000
If you look at the picture over here, you'll find there are many different linear separators.

42
00:03:42,000 --> 00:03:47,000
There is one over here. There is one over here. There is one over here.

43
00:03:47,000 --> 00:03:53,000
One of the questions that has recently been researched extensively is which one to prefer.

44
00:03:53,000 --> 00:03:57,000
Is it a, b, or c?

45
00:03:57,000 --> 00:04:01,000
Even though you probably have never seen this literature,

46
00:04:01,000 --> 00:04:05,000
I will just ask your intuition in this following quiz.

47
00:04:05,000 --> 00:04:10,000
Which linear separator would you prefer if you look at these three different linear separators--

48
00:04:10,000 --> 99:59:59,999
a, b, c, or none of them?
