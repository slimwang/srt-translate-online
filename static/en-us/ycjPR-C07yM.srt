1
00:00:00,070 --> 00:00:02,740
>> Okay Michael, so let's see if we can actually use this as a way

2
00:00:02,740 --> 00:00:04,830
of deriving something maybe that we already

3
00:00:04,830 --> 00:00:06,310
knew. So I'm going to go through a couple

4
00:00:06,310 --> 00:00:07,770
of these because I I actually think,

5
00:00:07,770 --> 00:00:09,690
well, frankly I just think it's kind of cool.

6
00:00:09,690 --> 00:00:12,820
But, I I'm hoping I can convince you it's sort of cool too and that

7
00:00:12,820 --> 00:00:15,180
we get something out of it. Okay, so let me set up the word, I'm

8
00:00:15,180 --> 00:00:18,960
going to set up a a problem, and it's going to be a kind of generic problem,

9
00:00:18,960 --> 00:00:21,440
and I'm going to see what we can get out of it, okay? So this is

10
00:00:21,440 --> 00:00:22,890
machine learning, so we're going to be given a

11
00:00:22,890 --> 00:00:25,210
bunch of data, so there are three assumptions

12
00:00:25,210 --> 00:00:26,540
that I'm going to make here. The first is

13
00:00:26,540 --> 00:00:27,810
that we're going to be given a bunch of labeled

14
00:00:27,810 --> 00:00:32,390
training data, which I'm writing here as x sub i and d sub i, so x sub i

15
00:00:32,390 --> 00:00:37,890
is whatever the input space is, and d sub i are these labels. And let's say, it

16
00:00:37,890 --> 00:00:40,290
doesn't actually even matter what the labels are, but

17
00:00:40,290 --> 00:00:43,400
let's say that the labels are classification labels. Okay?

18
00:00:43,400 --> 00:00:43,860
>> Hm.

19
00:00:43,860 --> 00:00:46,820
>> All right. And furthermore, not only we're given

20
00:00:46,820 --> 00:00:50,430
this data as examples drawn from some underlying concept c,

21
00:00:50,430 --> 00:00:52,490
but they're, in fact, noise-free. Okay? So they're

22
00:00:52,490 --> 00:00:56,060
true examples that tell you what c is. Okay?

23
00:00:56,060 --> 00:00:57,471
>> Mm-hm.

24
00:00:57,471 --> 00:00:59,210
>> So I'm going to say, in fact, let me write

25
00:00:59,210 --> 00:01:03,320
that down because I think it's important. They're noise-free examples. Okay.

26
00:01:03,320 --> 00:01:05,640
>> Like di equals c of xi.

27
00:01:05,640 --> 00:01:09,960
>> That's right, for all xi. So, the second assumption, is that the true

28
00:01:09,960 --> 00:01:12,320
concept c, is actually in our hypothesis

29
00:01:12,320 --> 00:01:15,980
space, whatever that hypothesis space is. And finally,

30
00:01:15,980 --> 00:01:17,750
we have no reason to believe that

31
00:01:17,750 --> 00:01:20,400
any particular hypothesis in our hypothesis space is

32
00:01:20,400 --> 00:01:24,920
more likely than any other. And so, we have a uniform prior over our hypotheses.

33
00:01:24,920 --> 00:01:27,700
>> So it's like the one thing we know is that we don't know anything.

34
00:01:27,700 --> 00:01:31,620
>> That's right. So, sometimes people called this an uninformative prior

35
00:01:31,620 --> 00:01:34,600
because you don't know anything. Except of course I've always thought that's

36
00:01:34,600 --> 00:01:37,880
a terrible name because its a completely informative prior. In fact

37
00:01:37,880 --> 00:01:41,010
its equally as informative as every other prior in that it tells

38
00:01:41,010 --> 00:01:44,810
you something that all hypothesis are equally likely. But that's

39
00:01:44,810 --> 00:01:47,010
>> I thought it was called an uninformed prior.

40
00:01:47,010 --> 00:01:50,470
>> Is it? So its just an ignorant prior is what you're telling me. Yeah.

41
00:01:50,470 --> 00:01:52,340
>> Okay. Well, then maybe that's the problem.

42
00:01:52,340 --> 00:01:53,230
I just always had a problem with it

43
00:01:53,230 --> 00:01:54,870
because people keep calling it uninformative and the

44
00:01:54,870 --> 00:01:58,020
really mean uninformed. Okay. In any case, so these

45
00:01:58,020 --> 00:01:59,420
are our, these are our assumptions. We've got

46
00:01:59,420 --> 00:02:01,960
a bunch of data, it's noise free, the concept

47
00:02:01,960 --> 00:02:03,650
is actually in the hypothesis base we care

48
00:02:03,650 --> 00:02:06,360
about and we have a uniform prior. So we

49
00:02:06,360 --> 00:02:08,940
need to compute the best hypothesis. So given

50
00:02:08,940 --> 00:02:12,400
that we want to somehow compute the probability of

51
00:02:12,400 --> 00:02:15,200
some hypothesis given the data, right? That's just

52
00:02:15,200 --> 00:02:18,790
Bay's Rule. So, Michael, you've got the problem right?

53
00:02:18,790 --> 00:02:19,482
>> Yes.

54
00:02:19,482 --> 00:02:21,550
>> [LAUGH]

55
00:02:21,550 --> 00:02:24,610
okay. So in order to compute the probability of a hypothesis

56
00:02:24,610 --> 00:02:27,550
given the data, we just need to figure out all of these

57
00:02:27,550 --> 00:02:29,620
other terms. So let me just write down some of the terms

58
00:02:29,620 --> 00:02:32,030
and you can tell me what a you think the answer. Okay.

59
00:02:32,030 --> 00:02:33,870
>> Well, what was the question?

60
00:02:33,870 --> 00:02:36,410
>> The question is, while we want to compute some

61
00:02:36,410 --> 00:02:41,180
kind of expression for the probability of a hypothesis given

62
00:02:41,180 --> 00:02:44,320
the data. So given some particular hypothesis, I want to know

63
00:02:44,320 --> 00:02:47,090
what's the probability of that hypothesis given the data, okay?

64
00:02:47,090 --> 00:02:47,550
>> Yeah.

65
00:02:47,550 --> 00:02:49,670
>> Okay, you got the setup. So, we're

66
00:02:49,670 --> 00:02:51,370
going to compute that by figuring out these three

67
00:02:51,370 --> 00:02:54,720
terms over here. So, let's just pick, one

68
00:02:54,720 --> 00:02:58,410
of them to do. Let's try the prior probability.

69
00:02:58,410 --> 00:03:01,250
So Michael, what's the prior probability on H?

70
00:03:01,250 --> 00:03:04,000
>> Did we say that it was a finite hypothesis class?

71
00:03:04,000 --> 00:03:05,570
>> It is a finite hypothesis class.

72
00:03:05,570 --> 00:03:08,800
>> Then it's like, one over the

73
00:03:08,800 --> 00:03:11,370
size of that hypothesis class because it's uniform.

74
00:03:11,370 --> 00:03:15,970
>> Exactly right, uniform means Exactly that. Okay so we've got one of our

75
00:03:15,970 --> 00:03:23,570
terms, good job. Lets pick another term. How about the probability of

76
00:03:23,570 --> 00:03:26,600
data given the hypothesis. What's that?

77
00:03:26,600 --> 00:03:29,860
>> The probability, so I guess noise free, and we know that it's

78
00:03:29,860 --> 00:03:32,310
noise free so it's always, so they're always going to be zeros and ones.

79
00:03:32,310 --> 00:03:33,853
>> Mm-hm.

80
00:03:33,853 --> 00:03:36,240
>> So, and it's going to be a question of whether or not

81
00:03:36,240 --> 00:03:41,070
the data is consistent with that hypothesis. Right, if the labels all match.

82
00:03:41,070 --> 00:03:41,750
>> Right.

83
00:03:41,750 --> 00:03:45,140
>> What we expect them to be if that really were the hypothesis, then

84
00:03:45,140 --> 00:03:48,590
we get a one, otherwise we get a zero. That's exactly right. So let me

85
00:03:48,590 --> 00:03:52,160
see if I can write down what I think you just said. The probability of the data,

86
00:03:52,160 --> 00:04:00,300
given the hypothesis, is, therefore one if it's the case, that the labels

87
00:04:00,300 --> 00:04:05,378
And the hypothesis agree for every single one of the training exercises. Right?

88
00:04:05,378 --> 00:04:05,713
>> Yep

89
00:04:05,713 --> 00:04:09,120
>> Is that what you said? Good. And if any of

90
00:04:09,120 --> 00:04:13,770
them disagree, then the probability is zero. So that's actually very important.

91
00:04:13,770 --> 00:04:16,870
It's important to, to understand exactly what it means for,

92
00:04:16,870 --> 00:04:18,839
to have the probability to get a hypothesis, as we

93
00:04:18,839 --> 00:04:22,510
mentioned before. That the English version of this is, what's

94
00:04:22,510 --> 00:04:26,320
the probability that I would see data with these labels in

95
00:04:26,320 --> 00:04:29,370
a universe where H is actually true. Which is different

96
00:04:29,370 --> 00:04:31,930
from saying that H is trure or H is false. It's

97
00:04:31,930 --> 00:04:33,940
really a common about the labels that you see on

98
00:04:33,940 --> 00:04:36,480
a data. In a universe, where H happens to be true.

99
00:04:36,480 --> 00:04:38,770
>> Okay, but you know, it's occurring to me now

100
00:04:38,770 --> 00:04:42,340
that you wrote that down, that we've talked about this idea before.

101
00:04:42,340 --> 00:04:42,840
>> When?

102
00:04:42,840 --> 00:04:46,460
>> Well, so, like there's a shorter way of writing that. Which is

103
00:04:46,460 --> 00:04:51,460
D of H equals one if H is in the version space of D.

104
00:04:51,460 --> 00:04:53,400
>> Huh, that's exactly right, that's exactly right. So, in

105
00:04:53,400 --> 00:04:56,880
fact, that will help us to compute the final term

106
00:04:56,880 --> 00:05:00,150
that we need, which is the probability of seeing the

107
00:05:00,150 --> 00:05:03,830
data labels. So, how do we go about computing that? Well,

108
00:05:03,830 --> 00:05:05,910
it's exactly going to boil down to the version space

109
00:05:05,910 --> 00:05:07,980
as you say, let me just write out a couple

110
00:05:07,980 --> 00:05:11,060
of steps so that it's pretty Kind of easy to

111
00:05:11,060 --> 00:05:15,320
see. It's sometimes easier in these situations to kind of break

112
00:05:15,320 --> 00:05:18,690
things up. So, the probability of the data sort of

113
00:05:18,690 --> 00:05:22,700
formally, is equal to just this. So we can write the

114
00:05:22,700 --> 00:05:26,500
probability of the data as being, basically, a marginalized version

115
00:05:26,500 --> 00:05:29,410
of the probability of the data given each of the hypotheses

116
00:05:29,410 --> 00:05:32,040
times the probability of the hypotheses. Now, this is only

117
00:05:32,040 --> 00:05:35,580
true in a world where our hypotheses are mutually exclusive.

118
00:05:35,580 --> 00:05:38,910
Okay so let's assume we are in that world because

119
00:05:38,910 --> 00:05:41,750
frankly that's what we always assume and this little trick

120
00:05:41,750 --> 00:05:44,140
is going to workout for us because we are going to

121
00:05:44,140 --> 00:05:46,670
get to take advantage of two terms that we already

122
00:05:46,670 --> 00:05:49,990
computed naming the probability that the data given the hypothesis

123
00:05:49,990 --> 00:05:54,460
and the prior probability of a particular hypothesis so we know that

124
00:05:54,460 --> 00:05:57,590
prior probability of a hypothesis is right, its

125
00:05:57,590 --> 00:06:01,040
just one over the size of the hypothesis space

126
00:06:01,040 --> 00:06:03,400
and how am I going to substitute in this equation

127
00:06:03,400 --> 00:06:06,200
for the probability of the data given the hypothesis?

128
00:06:06,200 --> 00:06:08,610
>> So, I don't know. I would

129
00:06:08,610 --> 00:06:11,490
write that differently. I mean, it's basically it's

130
00:06:11,490 --> 00:06:15,160
like the indicator function on whether or hot HI is in the version space of D.

131
00:06:15,160 --> 00:06:17,320
>> Right, that's exactly right. So in fact this is not a good

132
00:06:17,320 --> 00:06:19,580
way to have written it. Let's see if I can come up with a,

133
00:06:19,580 --> 00:06:21,320
a good notational way of doing it. Let's

134
00:06:21,320 --> 00:06:25,690
say, for every hypothesis that is in the

135
00:06:25,690 --> 00:06:29,900
version space of the hypothesis space given the

136
00:06:29,900 --> 00:06:32,100
labels that we've got. Okay? How's that count?

137
00:06:32,100 --> 00:06:32,460
>> Okay.

138
00:06:32,460 --> 00:06:34,788
>> So rather than having to come

139
00:06:34,788 --> 00:06:38,668
up with an indicator function, I'm just going to

140
00:06:38,668 --> 00:06:40,996
define vs as the subset of all

141
00:06:40,996 --> 00:06:45,478
those hypotheses that are consistent with the data.

142
00:06:45,478 --> 00:06:46,590
>> Yeah exactly

143
00:06:46,590 --> 00:06:49,896
>> Okay, and so whats the probability of those?

144
00:06:49,896 --> 00:06:52,970
>> One It's one and it's zero otherwise, so then,

145
00:06:52,970 --> 00:06:55,763
we can simplify the sum and it's simply what? ?

146
00:06:55,763 --> 00:06:56,974
>> The sum of the one, ooh! The

147
00:06:56,974 --> 00:06:58,609
one of each doesn't even depend on the hypothesis.

148
00:06:58,609 --> 00:06:59,200
>> mm-mh!

149
00:06:59,200 --> 00:07:04,000
>> I see wait I don't see oh yes I do, I do it's one over the size

150
00:07:04,000 --> 00:07:07,120
of version space. No its the size of the

151
00:07:07,120 --> 00:07:09,610
version space over the size of the hypothesis space.

152
00:07:09,610 --> 00:07:11,169
>> That's exactly right.

153
00:07:11,169 --> 00:07:15,249
Basically for every single hypothesis in the version space we're

154
00:07:15,249 --> 00:07:17,320
going to add one and how many of those are?

155
00:07:17,320 --> 00:07:19,980
Well the size of the version space number of those.

156
00:07:19,980 --> 00:07:23,400
And multiply all that by one over the size hypothesis space,

157
00:07:23,400 --> 00:07:26,600
and so the probability of the data is that term. So

158
00:07:26,600 --> 00:07:30,170
now we can just substitute all of that, into our

159
00:07:30,170 --> 00:07:33,880
handy dandy equation up there, and let's just do that.

160
00:07:33,880 --> 00:07:36,300
So the probability of the hypothesis given the data, is the

161
00:07:36,300 --> 00:07:43,500
probability of the data given the hypothesis Which we know is one for all those

162
00:07:43,500 --> 00:07:46,860
that are consistent, zero otherwise. The probability

163
00:07:46,860 --> 00:07:50,300
of the prior probability over the hypothesis is

164
00:07:50,300 --> 00:07:55,890
just one over the size of the hypothesis space, and the probability of the data

165
00:07:55,890 --> 00:08:01,920
is the size of the version space Over the size of the hypothesis base which,

166
00:08:01,920 --> 00:08:06,070
when we divide everything out, is simply this. Got it?

167
00:08:06,070 --> 00:08:06,640
>> Got it.

168
00:08:06,640 --> 00:08:11,790
>> So, what does that all say? It says that, given a bunch of data, your

169
00:08:11,790 --> 00:08:17,800
probability of a particular hypothesis being correct, or being the best one

170
00:08:17,800 --> 00:08:22,700
or the right one, is simply uniform over all of the hypotheses that are

171
00:08:22,700 --> 00:08:26,060
in the version space. That is, are consistent with the data that we see.

172
00:08:26,060 --> 00:08:26,680
>> Nice.

173
00:08:26,680 --> 00:08:27,180
>> It

174
00:08:27,180 --> 00:08:28,880
is kind of nice. And by the way, if it's

175
00:08:28,880 --> 00:08:31,340
not consistent with it, then it's zero. So, this is

176
00:08:31,340 --> 00:08:35,580
only true for hypotheses that are still in

177
00:08:35,580 --> 00:08:39,100
version space and zero otherwise. Now notice that all of

178
00:08:39,100 --> 00:08:41,090
this sort of works out only in a world

179
00:08:41,090 --> 00:08:45,090
where you really do have noise free examples, and you

180
00:08:45,090 --> 00:08:49,280
know that the concept is actually in your hypothesis space

181
00:08:49,280 --> 00:08:52,220
and, just as crucially that you have a uniform prior

182
00:08:52,220 --> 00:08:55,090
for all the hypotheses. Now this is exactly the algorithm that

183
00:08:55,090 --> 00:08:57,820
we talked about before right. We talked about before what would

184
00:08:57,820 --> 00:09:02,160
we do. To kind of decide whether a hypothesis was good

185
00:09:02,160 --> 00:09:05,370
enough in this sort of noise-free world. And the answer we came

186
00:09:05,370 --> 00:09:07,760
up with is you should just pick one of them that's

187
00:09:07,760 --> 00:09:10,480
in the version space. And what this says is there's no reason

188
00:09:10,480 --> 00:09:13,070
to pick one over the other from the version space. They're

189
00:09:13,070 --> 00:09:17,210
all equally as good or rather equally as likely to be correct.

190
00:09:17,210 --> 00:09:17,650
>> Yeah,

191
00:09:17,650 --> 00:09:18,060
that follows.

192
00:09:18,060 --> 00:09:20,550
>> Yeah. So there you go. So it turns out you

193
00:09:20,550 --> 00:09:22,990
can actually do something with this. Notice by the way that we

194
00:09:22,990 --> 00:09:26,600
did not pick a particular hypothesis space, we did not pick

195
00:09:26,600 --> 00:09:30,390
a particular form of our instance space, we did not actually say

196
00:09:30,390 --> 00:09:33,920
anything at all about exactly what the labels were other than

197
00:09:33,920 --> 00:09:37,160
that they were labels of some sort. The strongest assumption that we

198
00:09:37,160 --> 00:09:39,920
made was a uniform prior, so this is always the right thing

199
00:09:39,920 --> 00:09:42,790
to do. At least in a Bayesen sense in a world where

200
00:09:42,790 --> 00:09:46,340
you've got noise free data, you have to find that hypothesis space, and

201
00:09:46,340 --> 00:09:48,430
you have uniform priors. Just pick

202
00:09:48,430 --> 00:09:51,610
something from the consistent set of hypotheses.
