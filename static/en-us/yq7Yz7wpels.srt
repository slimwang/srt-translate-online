1
00:00:00,233 --> 00:00:04,632
(Sebastian) Okay, there's several questions in the online forum on particle filters, and they all boil down to,

2
00:00:04,633 --> 00:00:08,066
"Can you be more precise, Sebastian, give us more precise examples?"

3
00:00:08,067 --> 00:00:14,666
And it's a good criticism, in fact. I gave you some abstract description. I hope in the future we can do programming

4
00:00:14,667 --> 00:00:18,499
assignments for this class, and then people can get source code from me for particle filters.

5
00:00:18,500 --> 00:00:23,032
We're working with the infrastructure right now, but we haven't gotten around this class to do this.

6
00:00:23,033 --> 00:00:26,599
Meanwhile, I hope that the robotics class clarifies some of the questions that people actually have.

7
00:00:26,600 --> 00:00:34,566
So you're going to have lots of quizzes there. I can promise you if you do those quizzes right, then you've got it, you understand particle filters.

8
00:00:34,567 --> 00:00:40,732
(Peter) Okay. And here's one from Dan Smullen in your old home state of Pennsylvania.

9
00:00:40,733 --> 00:00:46,166
(Sebastian) Hi, Pennsylvania. (Peter) "I noticed that the course doesn't discuss expert systems or other rule-based approaches.

10
00:00:46,167 --> 00:00:50,466
"Are such techniques still considered part of A.I.?" So yes, they certainly are.

11
00:00:50,467 --> 00:00:58,432
Expert systems are still a big part of A.I., and it's a multi-million-dollar business, and lots of expert systems running to this day.

12
00:00:58,433 --> 00:01:02,066
But it seems to be less important than it was in the '80s and '90s.

13
00:01:02,067 --> 00:01:06,932
And since we haven't covered that much in the course, let me just say a little bit about it:

14
00:01:06,933 --> 00:01:13,332
An expert system is when you go out, interview an expert, you say, "How do you do your task, what you do?"

15
00:01:13,333 --> 00:01:21,032
And then try to encode that into a program. And rather than trying to encode it directly into Java or C,

16
00:01:21,033 --> 00:01:25,566
you do it into a language that's specifically designed to capture this expert knowledge.

17
00:01:25,567 --> 00:01:29,232
And sometimes that works great, but there are some difficulties.

18
00:01:29,233 --> 00:01:34,099
One of them is that sometimes the experts don't know how it is that they do it.

19
00:01:34,100 --> 00:01:40,199
So they know how to do the task, but they don't know how to explain it, they don't really understand the steps,

20
00:01:40,200 --> 00:01:44,766
all the steps that they're taking, and so they can't explain it properly.

21
00:01:44,767 --> 00:01:51,732
Another problem is that most of these languages were based on logic rather than on probability,

22
00:01:51,733 --> 00:01:59,166
so if it's a clear, "A implies B," then it works well, but if there's uncertainty involved, then the languages weren't very good

23
00:01:59,167 --> 00:02:04,399
at describing that uncertainty. And so they were limited in their application.

24
00:02:04,400 --> 00:02:08,265
(Sebastian) Yeah, I guess today, the best (unintelligible) to do expert systems is Bayes Networks,

25
00:02:08,267 --> 00:02:12,232
which really (unintelligible) uncertainties quite a bit. And they have their own problems, but they've been used

26
00:02:12,233 --> 00:02:15,432
just like expert systems in larger applications today.

27
00:02:15,433 --> 00:02:22,166
(Peter) Right. And I should also say that kind of the point of view of what computers are has changed since the '80s.

28
00:02:22,167 --> 00:02:27,732
In the '80s you were dealing with large mainframes or even workstations, which were supposed to be smaller,

29
00:02:27,733 --> 00:02:35,032
cheap computers, still cost $50,000 each. And so this was like the salary of an expert.

30
00:02:35,033 --> 00:02:38,399
And so you wanted to replace this expert to get your cost back.

31
00:02:38,400 --> 00:02:43,666
Today we think of computers as being helpers that are all around us. We have ones in our pockets,

32
00:02:43,667 --> 00:02:50,232
on our desk, we're always carrying a couple with us. And so the canonical application of A.I. is not so much,

33
00:02:50,233 --> 00:02:54,432
"Can we replace an expert?" but, "Can we help someone out in their daily life?"

34
00:02:54,433 --> 00:02:58,499
(Sebastian) You know, I would love to be replaced by a computer. Honestly. So we could just go to the beach all the time

35
00:02:58,500 --> 00:03:06,132
and have a good life. (Peter) I don't believe you, Sebastian. I think if you were replaced with your current job with a computer,

36
00:03:06,133 --> 00:03:10,132
you'd find out some other job to do. (Sebastian) Yeah, but how do the students out there know that we're real?

37
00:03:10,133 --> 00:03:16,432
We could be robots. (Peter) We've done a really good job of our artificially intelligent robots here, and we are on the beach right now.

38
00:03:16,433 --> 00:03:22,466
(Sebastian) I think sometimes I sound like a robot, speaking for myself. (robot sounds) All right, next question.

39
00:03:22,467 --> 00:03:27,032
"What are some big open questions in artificial intelligence, specifically for someone pursuing a Ph.D.?

40
00:03:27,033 --> 00:03:32,699
"What is the best way to prepare for a doctoral course? What should one study before doing this?

41
00:03:32,700 --> 00:03:37,366
"And sub-question, how can A.I. and machine learning be differentiated?" This is by a student in Heidelberg, Germany.

42
00:03:37,367 --> 00:03:45,366
Hi, Heidelberg. (Peter) So I would say don't worry about the big open questions in A.I.

43
00:03:45,367 --> 00:03:50,766
Just worry about doing something. So go out and find a task that's interesting to you. If you're interested in robots

44
00:03:50,767 --> 00:03:57,166
or computer vision or whatever, just go out and start doing something, and at some point you'll get to the point

45
00:03:57,167 --> 00:04:02,866
where you can't download a package or read a paper that solves the next thing, and then you'll know,

46
00:04:02,867 --> 00:04:08,132
there you have an open problem. Doesn't have to be a big open problem, but it's one that's yours and is relevant to you.

47
00:04:08,133 --> 00:04:14,499
(Sebastian) Yeah, so one of the things I teach my students at Stanford is that up to a Ph.D., you're in a mode

48
00:04:14,500 --> 00:04:17,632
where someone gives you a problem and you have to solve the problem.

49
00:04:17,632 --> 00:04:24,632
And when you do a Ph.D., you have to invent the problem at the same time, and that's a skill that is not native to people.

50
00:04:24,633 --> 00:04:30,399
So you work on something, and you think you're solving a problem, and you get stuck by doing something interesting, regardless,

51
00:04:30,400 --> 00:04:32,899
and what you really found is that you worked on the wrong problem.

52
00:04:32,900 --> 00:04:37,699
So while you're trying to find a solution, you have to also find the problem that you're trying to solve.

53
00:04:37,700 --> 00:04:45,766
That sounds like a red herring--you cannot find a solution and a problem simultaneously--but that's what it is to get a Ph.D.

54
00:04:45,767 --> 00:04:53,832
If you stick with one problem and it's unsolvable, you're stuck forever, and if it's trivially solvable, it's just not interesting.

55
00:04:53,833 --> 00:05:00,299
So being open to new problems as you work is one of the key skills. So one way to do this is when you do something

56
00:05:00,300 --> 00:05:05,466
and you come across an interesting problem, write it down so you don't lose it. And later on, when you solve your problem,

57
00:05:05,467 --> 00:05:10,032
when you get bored, go back to these written down problems and see if there's a more interesting one in there.

58
00:05:10,033 --> 00:05:14,799
Second thing I want to say is I think it's actually important to work on big open problems for society.

59
00:05:14,800 --> 00:05:21,166
Not a (unintelligible) problem, you don't want to solve all of poverty or all of cancer, but as a visionary thing that guides you,

60
00:05:21,167 --> 00:05:27,299
so that you can explain to yourself and your grandmother that if you solved the technical problems you're out to solve,

61
00:05:27,300 --> 00:05:32,799
you'd have a positive impact on society. I find this really important because a lot of academic institutions miss this point,

62
00:05:32,800 --> 00:05:37,332
and people get engraved in a specific problem formulation. And sometimes they're good, but sometimes they're

63
00:05:37,333 --> 00:05:40,999
really really bad, and even if you solve them, you haven't really changed a thing.

64
00:05:41,000 --> 00:05:50,266
I really care about changing big things in society, like solve poverty, solve diseases, solve transportation, solve architecture.

65
00:05:50,267 --> 00:05:55,399
There's amazing opportunities, I think, to solve big problems. Of course, take them as a guideline in your life,

66
00:05:55,400 --> 00:05:59,932
not as the immediate problem you're trying to solve tomorrow morning. But do that, because it's really important.

67
00:05:59,933 --> 00:06:04,866
(Peter) Another societal problem we're interested in is education, and so that's why we're here,

68
00:06:04,867 --> 00:06:08,332
and we really appreciate that you're there and are participating in that experiment.

69
00:06:08,333 --> 00:06:10,866
(Sebastian) And you should tell the students the story of the frame problem.

70
00:06:10,867 --> 00:06:19,899
(Peter) Oh yeah. So we were just talking before of when we were in school, and if we had asked our advisors,

71
00:06:19,900 --> 00:06:26,766
"What's the important problem to solve?" the list that was given then probably wouldn't be seen as so important now.

72
00:06:26,767 --> 00:06:32,099
So one of the big problems then was known as the (Prain) problem, and it was solved.

73
00:06:32,100 --> 00:06:36,332
Ray Rider was the most important one who did the most work on that.

74
00:06:36,333 --> 00:06:40,266
But once it was solved, we didn't really get to where we wanted to be.

75
00:06:40,267 --> 00:06:45,199
It didn't really mean that we could solve more practical things in the real world.

76
00:06:45,200 --> 00:06:52,666
We solved an artificial problem, technical problem within A.I., but it didn't lead to practical solutions in the world.

77
00:06:52,667 --> 00:06:57,199
(Sebastian) Yeah. When I grew up, it was the biggest problem. It was, "Solve the Prain problem and you're famous,

78
00:06:57,200 --> 00:07:02,432
"you solved A.I." And then it was solved, and it was trivially solved, it turns out, it was really trivial.

79
00:07:02,433 --> 00:07:08,899
The problem made no sense anymore, it was like an artifact of a specific way of using logic, the way logic shouldn't be used.

80
00:07:08,900 --> 00:07:13,132
And nothing changed in the world, right? When it came out, people didn't even care about it.

81
00:07:13,133 --> 00:07:17,032
And yet generations of students had written Ph.D. theses on how to solve it.

82
00:07:17,033 --> 00:07:21,532
Today, generations of students have been writing Ph.D. theses on whether P equals nP.

83
00:07:21,533 --> 00:07:25,766
And I venture to say, if you solve P equals nP, not much is going to change.

84
00:07:25,767 --> 00:07:31,032
We're going to solve it, and we're going to say, "Oh, that's interesting," or maybe it isn't, but I don't think much will change.

85
00:07:31,033 --> 00:07:35,066
(Peter) If P does equal nP, then it was constructive. (Sebastian) I doubt it.

86
00:07:35,067 --> 00:07:40,732
I think if there were an efficient solution to exponentially hard problems, you would've found at least a couple of them so far.

87
00:07:40,733 --> 00:07:45,066
And sometimes you find equivalency class complexity where there are theoretically the same,

88
00:07:45,067 --> 00:07:48,999
practically have such a vastly different constant, that the difference doesn't matter.

89
00:07:49,000 --> 00:07:52,799
So I don't think, I don't have any hope that this problem is interesting to work on, to be honest.

90
00:07:52,800 --> 00:07:57,332
(Peter) Yeah. So I guess the conclusion from all this is don't listen to your advisors.

91
00:07:57,333 --> 00:08:00,566
And so our advice to you is don't listen to us, right? (Sebastian) Yeah, that's true.

92
00:08:00,567 --> 00:08:04,766
(Peter) Okay, so go out and solve some problems. Make up some problems on your own and solve them.

93
00:08:04,767 --> 00:08:07,167
(Sebastian) Goodbye. (Peter) Goodbye, thanks. (Sebastian) Bye.
