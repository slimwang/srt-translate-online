1
00:00:00,200 --> 00:00:02,430
>> Alright, so let's say something that we actually know about

2
00:00:02,430 --> 00:00:06,450
this approach to action selection which is called Epsilon Greedy Exploration.

3
00:00:06,450 --> 00:00:10,100
And what we know is that if the action selection if

4
00:00:10,100 --> 00:00:14,410
GLIE, which means Greedy in the limit with infinite exploration, and

5
00:00:14,410 --> 00:00:16,810
that basically means that we are decaying our epsilon for epsilon

6
00:00:16,810 --> 00:00:19,440
Greedy. That we start off more random and over time we

7
00:00:19,440 --> 00:00:22,630
get less and less random and more and more Greedy. Then

8
00:00:22,630 --> 00:00:25,370
we have two things that are true. One is that Q hat

9
00:00:25,370 --> 00:00:27,490
goes to Q, which is, which comes from kind of a

10
00:00:27,490 --> 00:00:32,479
standard Q learning convergence result. But, we also have something cooler in

11
00:00:32,479 --> 00:00:35,920
this case which is that the policy that we're following, pi

12
00:00:35,920 --> 00:00:39,150
hat, is getting more and more like the optimal policy over time.

13
00:00:39,150 --> 00:00:39,480
>> Hm.

14
00:00:39,480 --> 00:00:43,950
>> So, not only do we learn stuff, but we use it, too. And this

15
00:00:43,950 --> 00:00:46,820
is an example of the exploration exploitation

16
00:00:46,820 --> 00:00:50,160
dilemma. And exploration exploitation is really, really important,

17
00:00:50,160 --> 00:00:54,050
not just because they're two words that surprisingly start with

18
00:00:54,050 --> 00:00:58,090
the same five letters, like unlikely letters. But also, that

19
00:00:58,090 --> 00:01:00,530
it is strike, it is talking specifically about this issue.

20
00:01:00,530 --> 00:01:04,650
Exploitation is about using what you know. And exploration is

21
00:01:04,650 --> 00:01:06,520
about getting the data that you need so that you

22
00:01:06,520 --> 00:01:09,750
can learn. And this is one particular way of doing,

23
00:01:09,750 --> 00:01:12,210
it turns out there's lots of other ways of making

24
00:01:12,210 --> 00:01:15,270
this trade-off and the reason it's a trade-off is because

25
00:01:15,270 --> 00:01:20,740
there's only one of you. There's only one agent acting in the world and

26
00:01:20,740 --> 00:01:23,230
it has these two actually somewhat conflicting

27
00:01:23,230 --> 00:01:25,510
objectives. One is to take actions that it

28
00:01:25,510 --> 00:01:27,930
doesn't know so much about, so it can learn about them and the other one

29
00:01:27,930 --> 00:01:31,370
take, takes actions that it knows are good. So that it can get high reward.

30
00:01:31,370 --> 00:01:33,480
>> Hm, that makes sense. You know, I didn't, I

31
00:01:33,480 --> 00:01:37,820
never realized that exploration and exploitation share the first five letters.

32
00:01:37,820 --> 00:01:38,420
>> Hm.

33
00:01:38,420 --> 00:01:40,380
>> I always knew that they shared the last

34
00:01:40,380 --> 00:01:41,000
five letters.

35
00:01:41,000 --> 00:01:43,040
>> Oh, that's interesting too.

36
00:01:43,040 --> 00:01:46,960
>> Huh. So if you take an r and turn it into an

37
00:01:46,960 --> 00:01:51,870
it, you might move from exploration to exploitation. I feel like an entire

38
00:01:51,870 --> 00:01:56,110
political movement could be founded on that. [LAUGH]. But I'm not sure exactly

39
00:01:56,110 --> 00:01:59,180
what it would be yet. Maybe I'll work on that, before our next lesson.

40
00:01:59,180 --> 00:02:02,050
>> So I have an algorithm. There, there's a standard, lemma.

41
00:02:02,050 --> 00:02:02,650
>> Mm hm.

42
00:02:02,650 --> 00:02:05,770
>> In reinforcement learning theory called the exploration

43
00:02:05,770 --> 00:02:08,870
exploitation dilemma. Sorry, [LAUGH], no, lemma. The other kind

44
00:02:08,870 --> 00:02:12,090
of lemma. The exploration exploitation lemma, which has to

45
00:02:12,090 --> 00:02:14,720
do with taking actions that are either exploring or

46
00:02:14,720 --> 00:02:18,080
exploiting. But I have one where you actually do teaching.

47
00:02:18,080 --> 00:02:18,411
>> Mm-hm.

48
00:02:18,411 --> 00:02:19,740
>> You can actually, each time you take

49
00:02:19,740 --> 00:02:21,330
an action it's either going to teach the

50
00:02:21,330 --> 00:02:23,540
agent something, it's going to explore or exploit. So

51
00:02:23,540 --> 00:02:27,510
I call that exploration exploitation, or explore, exploit, explain.

52
00:02:27,510 --> 00:02:31,270
>> Oh, nice. But you could have called it the exploration exploitation

53
00:02:31,270 --> 00:02:33,930
dilemma because you used dilemma. And di means

54
00:02:33,930 --> 00:02:37,220
two sometimes. And you do the two things.

55
00:02:37,220 --> 00:02:38,660
>> Well, in fact, dilemma does literally

56
00:02:38,660 --> 00:02:40,560
mean two. Its a choice between two things.

57
00:02:40,560 --> 00:02:42,120
>> Right, so its a dilemma.

58
00:02:42,120 --> 00:02:43,490
>> Nice

59
00:02:43,490 --> 00:02:44,480
>> Its a lemma about two things.

60
00:02:44,480 --> 00:02:46,832
>> Alright, so there is, it turns out there's a lot

61
00:02:46,832 --> 00:02:50,690
of other approaches to exploration and exploitation. And some of them in

62
00:02:50,690 --> 00:02:53,180
the, in the model based setting. You can do a lot

63
00:02:53,180 --> 00:02:56,310
more with it, a lot more powerful things with it because you

64
00:02:56,310 --> 00:02:59,650
actually can keep track of what you've learned effectively in

65
00:02:59,650 --> 00:03:01,810
the environment and what you haven't, so the algorithm can

66
00:03:01,810 --> 00:03:06,280
actually know what it knows and can use that information

67
00:03:06,280 --> 00:03:09,470
to explore things that it doesn't know and exploit things

68
00:03:09,470 --> 00:03:11,820
that it does know. Q learning doesn't really have that

69
00:03:11,820 --> 00:03:14,510
distinction. It's a much harder thing to do. So, so

70
00:03:14,510 --> 00:03:16,770
that's what I wanted to tell you in terms of,

71
00:03:16,770 --> 00:03:19,300
you know, thinking about exploration-exploitation, does that make sense to you?

72
00:03:19,300 --> 00:03:22,430
>> It does make sense to me. I think what,

73
00:03:22,430 --> 00:03:24,310
the main point I got out of this, or a main

74
00:03:24,310 --> 00:03:27,090
point I got out of this, other than our incredible ability

75
00:03:27,090 --> 00:03:30,680
to get caught up in letters and coincidences of spelling, is

76
00:03:30,680 --> 00:03:33,410
that the exploration-exploitation dilemma really is

77
00:03:33,410 --> 00:03:34,860
a dilemma. It's like the fundamental

78
00:03:34,860 --> 00:03:38,510
tradeoff in reinforcement learning. You have to exploit because you have

79
00:03:38,510 --> 00:03:41,230
to use what you've got, but you have to learn or

80
00:03:41,230 --> 00:03:44,800
otherwise you might not be able to exploit profitably. So you

81
00:03:44,800 --> 00:03:47,760
have to always trade off between these things, and if you don't,

82
00:03:47,760 --> 00:03:52,095
you're bound to either learn nothing or to get caught in local minima.

83
00:03:52,095 --> 00:03:55,140
>> I couldn't agree with you more. In some sense, if you

84
00:03:55,140 --> 00:03:56,750
think of reinforcement learning as being

85
00:03:56,750 --> 00:03:59,810
the question of model learning plus planning.

86
00:03:59,810 --> 00:04:03,480
There's nothing new here because model learning is well studied in the machine

87
00:04:03,480 --> 00:04:05,520
learning community and planning is well

88
00:04:05,520 --> 00:04:08,600
studied in the planning and scheduling community.

89
00:04:08,600 --> 00:04:09,100
>> Planning.

90
00:04:09,100 --> 00:04:10,890
>> And so, like, what are we adding

91
00:04:10,890 --> 00:04:12,930
to this? And what we're adding is the fact

92
00:04:12,930 --> 00:04:15,890
that these two processes interact with each other and depend

93
00:04:15,890 --> 00:04:19,454
on each other and that's exactly the exploration, exploitation dilemma

94
00:04:19,454 --> 00:04:22,330
and that's information has to go back and forth between

95
00:04:22,330 --> 00:04:25,380
these two processes that other people understand and we're the glue.

96
00:04:26,380 --> 00:04:26,850
>> Or the glee.

97
00:04:26,850 --> 00:04:30,274
>> [LAUGH] The glee glue.

98
00:04:30,274 --> 00:04:31,120
>> I like it. That's beautiful.
