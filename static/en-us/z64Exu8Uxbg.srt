1
00:00:00,070 --> 00:00:01,830
We're going to make
general Rmax algorithm and

2
00:00:01,830 --> 00:00:03,860
it's going to be really simple
because it's going to be an awful lot

3
00:00:03,860 --> 00:00:05,670
like the Rmax algorithm
we already looked at.

4
00:00:05,670 --> 00:00:07,090
>> Good.
>> So I copied over the Rmax

5
00:00:07,090 --> 00:00:08,460
algorithm that we had before.

6
00:00:08,460 --> 00:00:11,190
And it went like this,
we're going to keep track of the MDP.

7
00:00:11,190 --> 00:00:14,820
Which is to say we're going to have
an estimate of the transitions and

8
00:00:14,820 --> 00:00:17,690
rewards for all the state action pairs.

9
00:00:17,690 --> 00:00:21,280
Any unknown state action pair,
we're going to assume has the reward of

10
00:00:21,280 --> 00:00:25,480
Rmax and then a self loop, so we can
just get like aahh for a long time.

11
00:00:25,480 --> 00:00:28,500
Then we actually build that MDP,
we solve that MDP and we take

12
00:00:28,500 --> 00:00:32,390
an action from the optimal policy with
respect to that MDP that we solve.

13
00:00:32,390 --> 00:00:35,560
The only difference is going to
be this notion of unknown.

14
00:00:35,560 --> 00:00:37,710
And the idea is that we're
going to have some parameter.

15
00:00:37,710 --> 00:00:42,210
Call it C, that state action pair
is unknown if it's been visited or

16
00:00:42,210 --> 00:00:44,530
if we've tried it out
fewer than C times.

17
00:00:44,530 --> 00:00:47,940
>> Okay, is that C different for
every state action pair?

18
00:00:47,940 --> 00:00:48,670
>> That's a good question.

19
00:00:48,670 --> 00:00:51,470
So I want to first give
the parametrized algorithm.

20
00:00:51,470 --> 00:00:54,680
Then what we have to do is, if we're
going to say that this algorithm is

21
00:00:54,680 --> 00:00:59,730
efficient, we have to derive some value
for C and show that it's not too big and

22
00:00:59,730 --> 00:01:02,290
not too small but-
>> Just right.

23
00:01:02,290 --> 00:01:03,370
>> Just right, yeah.

24
00:01:03,370 --> 00:01:07,700
So the way that we're defining unknown
here is that we have some parameter C.

25
00:01:07,700 --> 00:01:10,710
And we consider a state-action
pair SA unknown

26
00:01:10,710 --> 00:01:13,430
if it was tried fewer than c times.

27
00:01:13,430 --> 00:01:17,560
After that point then, we switch over
to the maximum likelihood estimate.

28
00:01:17,560 --> 00:01:21,670
So, we've tried it c times,
and we saw we went

29
00:01:23,310 --> 00:01:27,690
from state s with action
a to some state, S 53,

30
00:01:27,690 --> 00:01:30,360
so if we did that some number of times.

31
00:01:30,360 --> 00:01:33,300
If we only did that once, then we're
going to estimate a probability of 1

32
00:01:33,300 --> 00:01:35,910
over c for getting that transition.

33
00:01:35,910 --> 00:01:38,100
So this gives us a new estimate
of the transitions and

34
00:01:38,100 --> 00:01:41,070
rewards based on the data
that we've actually gathered.

35
00:01:41,070 --> 00:01:41,660
Okay.

36
00:01:41,660 --> 00:01:44,920
So I just want to point out that
this is the Rmax algorithm we

37
00:01:44,920 --> 00:01:49,050
used in the deterministic case, and
this very much is related to the hufting

38
00:01:49,050 --> 00:01:53,890
bound estimate we used in the stochastic
non-sequential case in the bandit case.

39
00:01:53,890 --> 00:01:55,550
Which is grafting
the two things together.
