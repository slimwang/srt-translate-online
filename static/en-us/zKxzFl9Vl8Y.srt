1
00:00:00,310 --> 00:00:02,460
So, let's unpack a little bit about what you said Michael. You,

2
00:00:02,460 --> 00:00:05,990
you're exactly right in everything that you said and we can sort of

3
00:00:05,990 --> 00:00:09,720
write down these things I think more generally be simply noting a few

4
00:00:09,720 --> 00:00:12,530
of the pluses and the minuses, the pros and the cons. So in

5
00:00:12,530 --> 00:00:16,230
the filtering case one of the things that you, you didn't say explicity

6
00:00:16,230 --> 00:00:18,340
but I think was built into what you said. Is a question of

7
00:00:18,340 --> 00:00:23,170
speed, so filtering is faster than wrapping because you don't have to worry

8
00:00:23,170 --> 00:00:25,310
about what the learner wants, you don't have to worry about paying the

9
00:00:25,310 --> 00:00:28,160
cost of what the learner is going to do. You can basically just

10
00:00:28,160 --> 00:00:31,690
apply any fast algorithm you might imagine that takes a look at the

11
00:00:31,690 --> 00:00:36,030
features and does some sort of filtering. It is of course, as we

12
00:00:36,030 --> 00:00:39,320
pointed out before an exponential problem but you can do some kind of

13
00:00:39,320 --> 00:00:42,390
an approximation here and you can make a. Basically arbitrarily as fast as

14
00:00:42,390 --> 00:00:44,300
you want it to be. Which I think is what you were thinking

15
00:00:44,300 --> 00:00:47,540
about originally when we started the quiz. So you do get speed and

16
00:00:47,540 --> 00:00:50,560
the price that you're paying for that speed it that you tend to look

17
00:00:50,560 --> 00:00:54,130
at the features in isolation. So what do I mean by that.

18
00:00:54,130 --> 00:00:56,240
Well you may look at a feature and say this feature is

19
00:00:56,240 --> 00:01:00,510
unimportant. For whatever reason. But maybe that feature is important if you

20
00:01:00,510 --> 00:01:03,990
combined it with another feature. Say maybe we're in a kind of of parity

21
00:01:03,990 --> 00:01:07,620
problem again. And so some feature might look unimportant but actually, in

22
00:01:07,620 --> 00:01:10,040
fact, it is important. And the only way you're going to get the kind

23
00:01:10,040 --> 00:01:12,350
of speed up that you need is typically by looking at features

24
00:01:12,350 --> 00:01:15,570
in isolation. Either adding them or removing them and of course, the reason

25
00:01:15,570 --> 00:01:17,420
it gets this speed, is that it ignores the

26
00:01:17,420 --> 00:01:20,720
learning problem itself. At least the learner itself, if not

27
00:01:20,720 --> 00:01:23,680
the learning problem. Now, in contrast wrapping actually takes into

28
00:01:23,680 --> 00:01:27,450
account, model bias, it takes into account whatever the learning

29
00:01:27,450 --> 00:01:30,670
bias of the learning alogarithm is, in order to determine

30
00:01:30,670 --> 00:01:33,480
what the best sub features are. Which means it's actually

31
00:01:33,480 --> 00:01:36,470
worried about the problem of learning, itself. Which, I don't

32
00:01:36,470 --> 00:01:40,260
know how to spell, apparently. [LAUGH] But in the process,

33
00:01:40,260 --> 00:01:44,760
it's much much slower. Because every time it tries to look

34
00:01:44,760 --> 00:01:47,760
at a particular subset, a candidate subset, it has to run

35
00:01:47,760 --> 00:01:51,460
the learning algorithm. Imagine this learning algorithm is something like neural

36
00:01:51,460 --> 00:01:53,830
network learning. This could take thousands

37
00:01:53,830 --> 00:01:55,320
and thousands of iterations, you might

38
00:01:55,320 --> 00:01:58,020
have to do cross validation, there's all kinds of things you

39
00:01:58,020 --> 00:02:00,170
might have to do before you even get to look at

40
00:02:00,170 --> 00:02:02,950
the next thing and and get a score. And so wrapping

41
00:02:02,950 --> 00:02:05,410
makes a lot more sense because it takes into account the learning

42
00:02:05,410 --> 00:02:09,990
problem and im more importantly the learning bias or

43
00:02:09,990 --> 00:02:13,390
the inductive bias of the learning algorithm. But, it's going to

44
00:02:13,390 --> 00:02:14,960
take a very, very long time to do it

45
00:02:14,960 --> 00:02:17,950
while filtering tends to go pretty well. You buy that?

46
00:02:17,950 --> 00:02:18,990
>> Yeah. That makes sense.

47
00:02:18,990 --> 00:02:20,460
>> Okay. So I'm going to ask you a question

48
00:02:20,460 --> 00:02:24,440
before we, sort of move on. Can you think

49
00:02:24,440 --> 00:02:27,270
of any algorithms that we've looked at in the,

50
00:02:27,270 --> 00:02:30,930
the last mini course, on supervised learning? It basically

51
00:02:30,930 --> 00:02:32,940
was already doing filtering.

52
00:02:32,940 --> 00:02:35,660
>> So, it's not quite the same,

53
00:02:35,660 --> 00:02:37,130
because that's supervised, and now we're talking

54
00:02:37,130 --> 00:02:39,070
about unsupervised. But we did look at

55
00:02:39,070 --> 00:02:41,910
some algorithms that explicitly decided which features to

56
00:02:41,910 --> 00:02:43,590
include, and which ones not. I'm thinking

57
00:02:43,590 --> 00:02:46,770
of decision trees, maybe. And, I think boosting

58
00:02:46,770 --> 00:02:49,630
kind of has a little bit of this depending on what the weak learner set is.

59
00:02:49,630 --> 00:02:53,500
>> Right, that's true. I mean, we, the problem with, boosting is

60
00:02:53,500 --> 00:02:55,980
that it's sort of not. It's not picking the features as much as

61
00:02:55,980 --> 00:02:59,820
it's picking examples. But certainly with decision trees, you're exactly right.

62
00:02:59,820 --> 00:03:03,240
Decision trees are in it's own way a kind of filtering algorithm.

63
00:03:03,240 --> 00:03:05,950
So, this might not be obvious, and maybe I could have been

64
00:03:05,950 --> 00:03:09,770
clearer about it, you get these features and I think you actually

65
00:03:09,770 --> 00:03:12,460
asked me this question. But you get these features, but you do

66
00:03:12,460 --> 00:03:15,230
know what the labels are if there's a supervised learner built it.

67
00:03:15,230 --> 00:03:17,320
If you're going to have a learner at the end of the day,

68
00:03:17,320 --> 00:03:21,290
you, it's not considered cheating in the filtering case. To look inside

69
00:03:21,290 --> 00:03:24,020
the learner, but its not consider cheating to understand

70
00:03:24,020 --> 00:03:26,450
what the labels are so you can take advantage

71
00:03:26,450 --> 00:03:28,840
of the actual labels of your examples to do

72
00:03:28,840 --> 00:03:32,870
your filtering. So in fact if you take the main

73
00:03:32,870 --> 00:03:36,200
part of the decision trees which is the information

74
00:03:36,200 --> 00:03:39,260
gain thats actually a way of doing filtering on the

75
00:03:39,260 --> 00:03:42,630
features. So the criterion is information gain. Find me

76
00:03:42,630 --> 00:03:46,680
the features that provide you with the most information given

77
00:03:46,680 --> 00:03:49,120
the class label and that gets exactly the subset.

78
00:03:49,120 --> 00:03:51,980
>> Cool. Now I don't, I don't quite see how to iterate that

79
00:03:51,980 --> 00:03:54,540
though. So I can see how you can get the first feature that way,

80
00:03:54,540 --> 00:03:56,730
then are we talking after that we're

81
00:03:56,730 --> 00:03:59,250
going to select features that. Add information

82
00:03:59,250 --> 00:04:00,910
above and beyond what we've already gotten

83
00:04:00,910 --> 00:04:02,650
out of the other features we've selected?

84
00:04:02,650 --> 00:04:04,680
>> Sure. So in fact, imagine that

85
00:04:04,680 --> 00:04:08,170
my search algorithm here is actual decision trees.

86
00:04:08,170 --> 00:04:11,730
So I take a bunch of features. I take a bunch of labeled data. I run

87
00:04:11,730 --> 00:04:15,130
my decisino tree algorithim. It picks a subset of the features. Or maybe

88
00:04:15,130 --> 00:04:17,170
I do pruning or cross. Whatever.

89
00:04:17,170 --> 00:04:18,920
Assisted arbitrarty search algorithm. I just drew

90
00:04:18,920 --> 00:04:22,079
a box here. And then what I do is I take all the

91
00:04:22,079 --> 00:04:25,400
features that were used by the decision And I pass that to my learner.

92
00:04:25,400 --> 00:04:25,940
>> Hm.

93
00:04:25,940 --> 00:04:28,420
>> Now the learner is a decision tree learner. I presume

94
00:04:28,420 --> 00:04:30,990
we get back the same decision tree. But maybe that's a

95
00:04:30,990 --> 00:04:33,100
way of picking a subset of features that are useful for,

96
00:04:33,100 --> 00:04:37,540
for example a new network or perceptron or KNN. In which case,

97
00:04:37,540 --> 00:04:39,820
I don't quite see how to do the fed, the filtering.

98
00:04:39,820 --> 00:04:43,573
>> Well, you run a decision tree algorithm here. [CROSSTALK]

99
00:04:43,573 --> 00:04:44,180
>> Oh.

100
00:04:44,180 --> 00:04:45,370
>> That gives you the set of

101
00:04:45,370 --> 00:04:47,660
features that the decision tree thinks is important.

102
00:04:47,660 --> 00:04:48,590
>> Oh.

103
00:04:48,590 --> 00:04:51,920
>> And you return that set of features. Basically, the entire, not the

104
00:04:51,920 --> 00:04:54,100
tree, but you flatten the tree and then just turn that into a

105
00:04:54,100 --> 00:04:58,250
set of features. So imagine, for example, that we actually put inside this

106
00:04:58,250 --> 00:05:02,670
box a decision-tree learner. That uses the set of data as input with all

107
00:05:02,670 --> 00:05:04,520
of its features and all of the labels

108
00:05:04,520 --> 00:05:07,090
that it knows about. It builds a decision tree

109
00:05:07,090 --> 00:05:09,530
which gives you typically a subset, well in fact

110
00:05:09,530 --> 00:05:11,510
by definition, gives you a subset of all the

111
00:05:11,510 --> 00:05:13,760
features. And then the features that come out of

112
00:05:13,760 --> 00:05:15,770
the decision tree are passed on to another learned

113
00:05:15,770 --> 00:05:18,140
like for a example a perceptron or a nueral

114
00:05:18,140 --> 00:05:20,319
network or even something like K and N. [UNKNOWN]

115
00:05:20,319 --> 00:05:22,270
>> It always comes back to K and N.

116
00:05:22,270 --> 00:05:23,601
>> Everything comes back to K and N. [LAUGH]

117
00:05:23,601 --> 00:05:25,890
>> So let me make sure i understand we

118
00:05:25,890 --> 00:05:27,700
run a decisoin tree learner but we dont want to

119
00:05:27,700 --> 00:05:30,880
use what it actually determined? Instead, we say let's

120
00:05:30,880 --> 00:05:32,660
just look at all the things that got split

121
00:05:32,660 --> 00:05:34,850
on. And the union of all those now become

122
00:05:34,850 --> 00:05:36,800
features to hand off to some other learning algorithm.

123
00:05:36,800 --> 00:05:37,510
>> Right.

124
00:05:37,510 --> 00:05:38,840
>> But, why would we want to do that

125
00:05:38,840 --> 00:05:40,330
if we already trained up the decision tree?

126
00:05:40,330 --> 00:05:43,060
>> Because maybe we're worried that the decision tree

127
00:05:43,060 --> 00:05:44,630
doesn't do as good a job as you would

128
00:05:44,630 --> 00:05:49,660
like. On noisy data for example. Or you're just

129
00:05:49,660 --> 00:05:52,460
going to run the decision tree to some depth. Or,

130
00:05:52,460 --> 00:05:53,820
even, what you want to do is, you're going to

131
00:05:53,820 --> 00:05:56,490
run the decision tree until it actually overfits. And

132
00:05:56,490 --> 00:05:58,040
now you know all the features that you could

133
00:05:58,040 --> 00:05:59,880
use, and you pass that on to some other

134
00:05:59,880 --> 00:06:02,410
learner, which has a different bias. So basically, you

135
00:06:02,410 --> 00:06:04,270
use the inductive bias of the decision tree to

136
00:06:04,270 --> 00:06:06,830
choose features, but then you use the inductive bias

137
00:06:06,830 --> 00:06:09,130
of your other learner in order to do learning. Neat.

138
00:06:09,130 --> 00:06:10,270
>> Right, and if you think about the

139
00:06:10,270 --> 00:06:13,040
big difference between for example KNN and decision trees,

140
00:06:13,040 --> 00:06:14,840
we know that KNN suffers from the curse of

141
00:06:14,840 --> 00:06:17,820
dimensionality because it doesn't know which features are important.

142
00:06:17,820 --> 00:06:20,000
So this, and but the decisions trees are very good at figuring out

143
00:06:20,000 --> 00:06:23,260
which features are important, at least for predicting the label. And so you

144
00:06:23,260 --> 00:06:26,230
can ask you're decision tree to give some hint to KNN which has

145
00:06:26,230 --> 00:06:27,770
other nice things going for it, which

146
00:06:27,770 --> 00:06:29,120
features it should actually pay attention to.

147
00:06:29,120 --> 00:06:29,880
>> Cool.

148
00:06:29,880 --> 00:06:30,890
>> Alright so there you go.
