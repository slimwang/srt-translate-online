1
00:00:00,142 --> 00:00:01,490
>> All right, so that gets us to the end

2
00:00:01,490 --> 00:00:04,939
of talking about VC dimension. So Charles, what did we learn?

3
00:00:04,939 --> 00:00:06,610
>> Well, we learned about VC dimension.

4
00:00:06,610 --> 00:00:07,877
>> [LAUGH]

5
00:00:07,877 --> 00:00:10,310
>> I think that was probably the key thing.

6
00:00:10,310 --> 00:00:14,220
>> Indeed, which, which was capturing this notion of shattering.

7
00:00:14,220 --> 00:00:18,300
>> Right. We learned that VC dimension, the

8
00:00:18,300 --> 00:00:21,890
relationship between VC dimensions, VC dimension and parameters.

9
00:00:21,890 --> 00:00:25,180
>> Yeah. Very good, and in fact so like we'd even be able to have a

10
00:00:25,180 --> 00:00:27,160
guess to say, well what if you have a

11
00:00:27,160 --> 00:00:29,960
neural network and you're thinking of adding additional nodes to

12
00:00:29,960 --> 00:00:32,240
the hidden layer. What do we suppose that would

13
00:00:32,240 --> 00:00:34,320
do to the VC dimension of what you can represent?

14
00:00:34,320 --> 00:00:37,140
>> So, we didn't really talk about it, but it does occur to me when

15
00:00:37,140 --> 00:00:38,460
you put it that way that it's pretty

16
00:00:38,460 --> 00:00:41,370
subtle, right? Because it's the, it's not just

17
00:00:41,370 --> 00:00:43,030
that it's related to the number of parameters.

18
00:00:43,030 --> 00:00:45,720
It's related to the true number of parameters.

19
00:00:45,720 --> 00:00:47,640
Because you can always come up with inefficient

20
00:00:47,640 --> 00:00:50,700
ways to represent your parameters. So, for example,

21
00:00:50,700 --> 00:00:52,830
if you have a real number I could represent that as

22
00:00:52,830 --> 00:00:56,320
two parameters. Everything to the left of the decimal point and

23
00:00:56,320 --> 00:00:58,600
everything to the right of the decimal point, but that doesn't

24
00:00:58,600 --> 00:01:01,550
make it really two separate parameters. It's still just one parameter.

25
00:01:01,550 --> 00:01:03,380
>> All right, that's fair.

26
00:01:03,380 --> 00:01:09,650
>> We also saw how VC relates to the size of the hypothesis space for

27
00:01:09,650 --> 00:01:15,950
finite hypothesis spaces. And I guess we learned how sample complexity relates

28
00:01:15,950 --> 00:01:17,480
to VC dimension, and in fact, all

29
00:01:17,480 --> 00:01:20,000
these things are themselves related. And we learned

30
00:01:20,000 --> 00:01:23,260
a little bit, or some tricks, or at least went through some examples of how

31
00:01:23,260 --> 00:01:25,430
to actually compute the VC dimension. In

32
00:01:25,430 --> 00:01:26,850
particular, we learned that you need to give

33
00:01:26,850 --> 00:01:30,790
an example to find the lower bound, and you need to prove an upper bound,

34
00:01:30,790 --> 00:01:35,600
>> Good. And one other thing that I'd want to add to that, is that VC

35
00:01:35,600 --> 00:01:41,480
dimension captures this notion of PAC Learnability. Cool.

36
00:01:41,480 --> 00:01:43,180
I think that's enough for one session.

37
00:01:43,180 --> 00:01:45,080
>> All right. See you next time, Michael. Bye.

38
00:01:45,080 --> 00:01:46,110
>> All right. See you next time.
