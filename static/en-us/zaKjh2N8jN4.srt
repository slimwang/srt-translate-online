1
00:00:00,000 --> 00:00:03,000
Let me explain k-means by an example.

2
00:00:03,000 --> 00:00:07,000
Suppose we're given the following data points in a 2-dimensional space.

3
00:00:07,000 --> 00:00:12,000
K-means estimates for a fixed number of k. Here k = 2.

4
00:00:12,000 --> 00:00:17,000
The best centers of clusters representing those data points.

5
00:00:17,000 --> 00:00:20,000
Those are found interatively by the following algorithm.

6
00:00:20,000 --> 00:00:25,000
Step 1: Guess cluster centers at random, as shown over here with the 2 stars.

7
00:00:25,000 --> 00:00:30,000
Step 2: Assign to each cluster center, even though they are randomly chosen,

8
00:00:30,000 --> 00:00:33,000
the most likely corresponding data points.

9
00:00:33,000 --> 00:00:36,000
This is done by minimizing Euclidian distance.

10
00:00:36,000 --> 00:00:41,000
In particular, each cluster center represents half of the space.

11
00:00:41,000 --> 00:00:45,000
And the line that separates the space between the left and right cluster center

12
00:00:45,000 --> 00:00:48,000
is the equidistant line, often called a Voronoi graph.

13
00:00:48,000 --> 00:00:53,000
All the data points on the left correspond to the red cluster,

14
00:00:53,000 --> 00:00:55,000
and the ones on the right to the green cluster.

15
00:00:55,000 --> 00:01:00,000
Step 3: Given now we have a correspondence between the data points and cluster centers,

16
00:01:00,000 --> 00:01:06,000
find the optimal cluster center that corresponds to the points associated with the cluster center.

17
00:01:06,000 --> 00:01:09,000
Our red cluster center has only 2 data points attached.

18
00:01:09,000 --> 00:01:13,000
So the optimal cluster center would be the halfway point in the middle.

19
00:01:13,000 --> 00:01:16,000
Our right cluster center has more than 2 points attached;

20
00:01:16,000 --> 00:01:21,000
yet it isn't placed optimally, as you can see as they move with the animation back and forth.

21
00:01:21,000 --> 00:01:25,000
By minimizing the joint quadratic distance to all of those points,

22
00:01:25,000 --> 00:01:29,000
the new cluster center has attained the center of those data points.

23
00:01:29,000 --> 00:01:35,000
Now the final step is iterate. Go back and reassign cluster centers.

24
00:01:35,000 --> 00:01:39,000
Now the Voronoi diagram has shifted, and the points are associated differently,

25
00:01:39,000 --> 00:01:45,000
and then reevaluate what the optimal cluster center looks like given the associated points.

26
00:01:45,000 --> 00:01:47,000
And in both cases we see significant motion.

27
00:01:47,000 --> 00:01:49,000
Repeat. Now this is the clustering.

28
00:01:49,000 --> 99:59:59,999
The point association doesn't change, and as a result, we just converged.
