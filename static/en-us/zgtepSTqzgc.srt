1
00:00:00,530 --> 00:00:03,210
So here's our update rules what they end up being.

2
00:00:03,210 --> 00:00:06,220
The gradient descent rule we just derived says what we

3
00:00:06,220 --> 00:00:09,300
want to do is move the weights in the negative

4
00:00:09,300 --> 00:00:12,000
direction of the gradient. So if we negate that expression

5
00:00:12,000 --> 00:00:14,410
that we had before and take a little step in

6
00:00:14,410 --> 00:00:18,210
that direction we get exactly this expression. Multiply the. The

7
00:00:18,210 --> 00:00:22,610
input on that weight times the target minus the activation.

8
00:00:22,610 --> 00:00:25,810
Whereas in the perceptron case what we were doing is taking

9
00:00:25,810 --> 00:00:28,220
that same activation, thresholding it. Like,

10
00:00:28,220 --> 00:00:30,050
determining whether it's positive or negative.

11
00:00:30,050 --> 00:00:32,400
Putting in a zero or a one. And putting that in here, that's

12
00:00:32,400 --> 00:00:35,390
what y hat is. So really it's the same thing except in one

13
00:00:35,390 --> 00:00:37,670
case we have done the thresholding and in the other case we have

14
00:00:37,670 --> 00:00:40,240
not done the thresholding. But we end up with two different algorithms

15
00:00:40,240 --> 00:00:42,410
with two different behaviors. The perceptron

16
00:00:42,410 --> 00:00:45,290
has this nice guarantee. A finite convergence,

17
00:00:45,290 --> 00:00:47,810
which is a really good thing, but that's only in the case where

18
00:00:47,810 --> 00:00:49,640
we have linear separability. Whereas the

19
00:00:49,640 --> 00:00:51,880
gradient descent rule is good because, calculus.

20
00:00:51,880 --> 00:00:54,551
>> [LAUGH].

21
00:00:54,551 --> 00:00:56,780
>> I guess that's not really an answer is it. It's, the

22
00:00:56,780 --> 00:01:04,220
gradient descent rule is good because it's more robust to data sets

23
00:01:04,220 --> 00:01:08,120
that are not linearly separable, but it's only going to converge in the limit.

24
00:01:10,020 --> 00:01:14,240
To a local optimum. Alright is that, is that the story there Charles?

25
00:01:14,240 --> 00:01:15,080
>> As far as I'm concerned.
