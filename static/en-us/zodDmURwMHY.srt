1
00:00:00,280 --> 00:00:01,330
>> Alright Michael, what's the answer?

2
00:00:01,330 --> 00:00:03,661
>> Alright so of those others, well C

3
00:00:03,661 --> 00:00:06,055
is pretty good, because it does separate the

4
00:00:06,055 --> 00:00:09,910
pluses from the minuses. We, we even liked it so much we used it in round two.

5
00:00:09,910 --> 00:00:10,271
>> Mh-hm.

6
00:00:10,271 --> 00:00:14,500
>> But it doesn't as good to me as A, because A actually does a good

7
00:00:14,500 --> 00:00:16,630
job of separating the very, the more heavily

8
00:00:16,630 --> 00:00:19,780
weighted points. So I would, I would say A.

9
00:00:19,780 --> 00:00:26,000
>> So in fact that is what our little learning system shows.

10
00:00:26,000 --> 00:00:29,120
It shows A. Now, through the trick of animation, I

11
00:00:29,120 --> 00:00:31,740
leave you with A. And that is exactly the right

12
00:00:31,740 --> 00:00:35,350
answer. By the way, Michael, if you look at these

13
00:00:35,350 --> 00:00:39,490
three hypothesis and their weights, you end up with something kind

14
00:00:39,490 --> 00:00:43,690
of interesting. So if you look at this third hypothesis

15
00:00:43,690 --> 00:00:45,720
that's chosen here, turns out they have a very low

16
00:00:45,720 --> 00:00:47,650
error, you'll notice that the errors are going down over

17
00:00:47,650 --> 00:00:51,200
time, by the way, of 0.14. And it has a much

18
00:00:51,200 --> 00:00:55,620
higher alpha of 0.92. Now if you look at these weights and you add them up, you

19
00:00:55,620 --> 00:00:59,100
end up with a cute little combination. So,

20
00:00:59,100 --> 00:01:02,030
let me draw that for you. Okay Michael, so

21
00:01:02,030 --> 00:01:05,700
I cleaned up a little bit so that you could see it. If you take each of

22
00:01:05,700 --> 00:01:08,430
the three hypothesis that we produced, and you weight

23
00:01:08,430 --> 00:01:11,760
them accordingly, you end up with the bottom figure.

24
00:01:11,760 --> 00:01:12,650
>> No way.

25
00:01:12,650 --> 00:01:13,820
>> Absolutely.

26
00:01:13,820 --> 00:01:16,320
>> That's. Kind of awesome. So what you're saying

27
00:01:16,320 --> 00:01:19,850
is that, even though we were only using half planes,

28
00:01:19,850 --> 00:01:24,040
or, or axis-aligned semi planes, for all the weak learners, that

29
00:01:24,040 --> 00:01:25,930
at the end of the day it actually kind of bent

30
00:01:25,930 --> 00:01:29,460
the line around and captured the positive and negative examples perfectly.

31
00:01:29,460 --> 00:01:31,920
>> Right. Does that remind you of

32
00:01:31,920 --> 00:01:33,280
anything else we've talked about in the past?

33
00:01:33,280 --> 00:01:37,600
>> Sh. Everything. Nothing. No, I dunno, I mean so with,

34
00:01:37,600 --> 00:01:40,020
with decision trees you can make the shapes like that, and

35
00:01:40,020 --> 00:01:40,360
>> That's true.

36
00:01:40,360 --> 00:01:41,590
>> And the fact that we're doing a weighted

37
00:01:41,590 --> 00:01:43,640
combination of things reminds me of the neural net.

38
00:01:43,640 --> 00:01:46,060
>> Yeah. And it should remind you of one other thing.

39
00:01:46,060 --> 00:01:47,670
>> I'm imagining that you want me to say

40
00:01:47,670 --> 00:01:50,280
nearest neighbors, but I can't quite make the connection.

41
00:01:50,280 --> 00:01:52,750
>> Well, you recall in our discussion with nearest neighbors,

42
00:01:52,750 --> 00:01:56,640
when we did weighted nearest neighbor. In particular we did weighted

43
00:01:56,640 --> 00:02:00,112
linear regression, we were able to take a simple hypothesis,

44
00:02:00,112 --> 00:02:03,580
add it together in order to get a more complicated hypothesis.

45
00:02:03,580 --> 00:02:05,450
>> That's true, because it's local.

46
00:02:05,450 --> 00:02:07,140
>> Right, exactly because it's local, and

47
00:02:07,140 --> 00:02:11,590
this is a general feature of Ensemble methods that if you try to look

48
00:02:11,590 --> 00:02:14,510
at just some particular hypothesis class. Let's

49
00:02:14,510 --> 00:02:17,370
just call it H, because you're doing weighted

50
00:02:17,370 --> 00:02:21,690
averages over hypotheses drawn from that hypothesis

51
00:02:21,690 --> 00:02:26,723
class. This hypothesis class is almost all

52
00:02:26,723 --> 00:02:28,740
low, is at least as complicated as

53
00:02:28,740 --> 00:02:32,520
this hypothesis class and often is more complicated.

54
00:02:32,520 --> 00:02:34,980
So you're able to be more expressive, even though you're

55
00:02:34,980 --> 00:02:38,330
using simple hypotheses, because you're combining them in some way.

56
00:02:38,330 --> 00:02:40,800
>> I'm not surprised that you can combine simple things to get

57
00:02:40,800 --> 00:02:43,690
complicated things. But I am surprised that you can combine them just

58
00:02:43,690 --> 00:02:47,736
with sums. And get complicated things because sums often act very, you

59
00:02:47,736 --> 00:02:50,600
know, sort of, friendly. Right it's

60
00:02:50,600 --> 00:02:53,490
a linear combination not a nonlinear combination.

61
00:02:53,490 --> 00:02:54,920
>> Actually, Michael part of the reason you get something

62
00:02:54,920 --> 00:02:57,640
nonlinear here is because you're passing it through a non-linearity

63
00:02:57,640 --> 00:02:58,070
at the end.

64
00:02:58,070 --> 00:02:59,010
>> The sine.

65
00:02:59,010 --> 00:03:02,070
>> Yea, that's a good thing, we should, we should ponder that.
